[
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "My ARC A380 recently arrived",
    "selftext": "",
    "comments": [
      "Please let us know what difference does it make if you max out the power limit.",
      "Cool ! How much did you pay for it? (If you had to pay for it)",
      "It was $139.99 USD.",
      "i agree people saying their is a significant increase in performance when increase power limit",
      "nice! please do some AV1 Encoding! Hopefully Nvidia and AMD will follow suit",
      "I hope those higher power limit gaming benchmarks were legit. I was expecting from the A380 something to beat the GTX 1060, not struggle against a RX 6400.",
      "GL bro, please let me know how it runs RE2 Remake, Albion Online and LOL in case you own them. I wanna get the a7\\*\\* since Nvidia its giga overpriced here in Mexico.",
      "newegg sells them, so I assume through them",
      "Correct, I purchased it on the launch day in Newegg at around 2a.m. CST.  That was the right choice as it went out of stock a couple hours later.",
      "The performance is quite good for its price, if I had a chance to buy it in Turkey right now, I would like to buy one at this price without thinking. I have a GT 1030 right now.",
      "I'm curious to see long term improvements, since a driver update granted a 100% increase in ray tracing performance (even if it's not exactly amazing to begin with) \n\n$140 is a decent price, but I'm really hoping they can get performance to *above* mid-high end Pascal cards, seeing as they're 5 years old at this point. And if XeSS will be as beneficial as DLSS is for RTX cards",
      "I'm interested in this too. With its price it could be great for AV1 encoding, running more displays and just to play around with.",
      "Youtube supports it for Video uploads they're even upgrading all of the server hardware for AV1 Encoding same with Netflix.\n\nNow for Twitch they been working on AV1 support for some time and i think by the end of the year you will see more platforms supporting AV1.",
      "It might be the case of genuine interest and those buying it to encourage Intel to keep at it.",
      "Newegg on Launch",
      "They are currently the only GPUs to support AV1 encoding, making them well worth it for that reason alone. They have double the video output than any other budget card. And they are good for certain productivity applications and linux.",
      "How did you purchase one?",
      "I would at least get a 2 fan model if I wanted to tinker for performance.",
      "This one is their first Dedicated GPU if I'm not mistaken. They have higher end models coming out sometime this year allegedly.",
      "Have fun🤞"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "a380"
    ],
    "title": "Intel Arc 2024 Revisit & Benchmarks (A750, A770, A580, A380 Updated GPU Tests)",
    "selftext": "",
    "comments": [
      "Arc is starting to look seriously impressive. If the A750 beats the 6600 XT in 90% of games and the latter is still $240, the former is starting to look like the best value card for *everyone* in this price range, not just enthusiasts.",
      "This is really impressive, I thought the a770 was closer to the 3060/6600. Gj intel",
      "you are not missing anything with starfield. To say that this game is mediocre will be praising it. Focus on good games not 5/10 games.",
      "Why do you have such allegiance to a company that doesn't care about you? \n\nAMD, Intel, Nvidia just buy whatever seems better for you",
      "Based on AMDs slow progress to increase performance over the years.",
      "Can we not with the fanyboyism? More competition is good for everyone.",
      "A good video, and a fair comparison.  \n\nIt’s a bummer Stairfield still appears to be having issues on ARC.  \n\nHonestly probably better if Battlemage isn’t launched until another 3-6 months of driver development time has passed overall.",
      "Really pulling for Battlemage. We need a third party option.",
      "Looks like ARC is racing to the top fast. BattleImage will leapfrog AMD and get very close to Nvidia.",
      "What do promises have to do with anything? You can see current performance then realize they are making faster chips that will have more cores as well.",
      "The problem with Amd they also still have massive driver issue after years. If Intel keep progressing their driver, it's very possible for them beat Amd in gpu market especially with Battlemage.",
      "I agree - but it is one game that a lot of well known sites like to benchmark, and it certainly left an impression on many that Intel wasn't ready at launch.   Games hyped that much should be given prioritization for drivers.. \n\nI think we need to see Intel stay ahead on more AAA launches in the future. \n\nI'm optimistic..",
      "Amd drivers are pretty mature and their driver issues (Intel's too) are very overblown. Their drivers today are about as stable as Nvidia's and are still more reliable than Intel's. I really hope that battlemage will moreso give Nvidia a reality check at the high-end and upper mid-range.",
      "But but but, BIG NAVI",
      "Nvidia getting out of the GPU space would be more likely than that",
      "Remember \"RIP Volta\"? What happened after that? AMD shooting themself in the foot with their meme marketing LOL",
      "I will happily if you can point to anywhere I did that.",
      "It's the most current version of the Creation Engine, which TES6 will be based on. So it's worthwhile to keep it there for that.",
      "Driver is only matured if didn't cause serious issue, like Nvidia driver they are worth to say matured, but for Amd even many people on Amd sub reporting they still see massive problem on radeon like game crash, bluescreen, or problem on gpu acceleration, now with antilag+ which is the worst so far. The problem with Amd they always promise but rarely deliver, unlike Nvidia who are fast in fixing issues.",
      "I think the driver issues are still common enough that the rx 6600 would be the go-to entry level GPU."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "ACER to launch Arc B580 NITRO in white and Arc A380 low profile GPUs",
    "selftext": "",
    "comments": [
      "Still releasing *Alchemist*?? That's an architecture that really doesn't need more money thrown at it.",
      "Intel probably still has some TSMC N6 allocation left over",
      "Those A380 Low profile cards make awesome AV1 encode PCs and HTPCs. So by all means, if the prices reflect the age of the architecture and GPUs are priced right, there is still a market for them.",
      "Im very interested in this specific a380 to add in my unraid since it is low profile.\n\nI just hope they don't have the same fan issues the current a310 and a380 available today.  Also the pricing of a380 now is $150 in microcenter.  Acer should price it aleast $120 or $99."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "[VideoCardz] - Intel ARC A380 desktop GPU is outperformed vy Radeon RX 6400 in first independent gaming tests",
    "selftext": "",
    "comments": [
      "I think it mostly due to the drivers, we'll see how long it takes them to get to a stable driver... I guess more time.",
      "I think you mean *encoding*, the RX 6400 also does decode (excluding AV1 decoding).\n\n[Also, the drivers Xe are broken for encoding too currently](https://media.discordapp.net/attachments/682674504878522386/984701519603564544/unknown.png) lmfao\n\nEdited for sake of clarity",
      "Frankly speaking, I don't think there's a clear cut answer to that question. We don't really know how much performance is left in the tank in the first place.",
      "ELI5, but considering we're all hoping the drivers will stabilize and optimize performance to a degree where we can claim this will be competitive, what are the performance uplifts that we can expect to see once the drivers mature? 20%? 30%? 50%?",
      "I don't like how Intel is putting same price on their product as the competition eventhough it's slower, less stable and Intel having really bad reputation for their graphics. \n\nNo one except OEM and a few people who don't even know what they're looking at will buy it for this price.\n\nThis is sellable only around 100$ as dedicated gpu. Otherwise zero reason to buy over competition if it isn't even cheaper.",
      "Will intel finewine be better than amd finewine?",
      "Intel couldn't roll out a stable set of drivers to save their life. This is starting to look like a spectacular flop from where I'm standing.",
      "yes it was using \"only\" 65w while being significantly outperformed by one using 50w",
      "Dg1 is just an igpu in a separate package. So cannot even call it a dgpu. So essentially this is intels first dGPU.",
      "> Intel couldn't roll out a stable set of drivers to save their life.\n\nThe same used to be true for Radeon",
      "It's probably not really about stability. Driver optimizations are made for gaming in general and even on per game basis and that's a lot of work that needs to be done over time. Nvidia and AMD have years of work done already. And game engines also make optimizations for different graphics architectures which obviously hasn't happened for intel yet since there have not been intel gaming GPUs in the market. \n\nConsidering the card can do a lot more in the 3dmark tests than the competition when everyone has application optimized drivers I wouldn't be too worried about the actual performance of the hardware. But it is probably a good idea to wait a year or two before expecting a solid performance in all games.",
      "Source: just trust me bro.",
      "People have used \"the drivers aren't ready\" excuse for Intel for the past 15 years of gpu issues. They started hiring to improve drivers (for like the 8th time) in 2017 when they decided to push into dgpu. Why anyone thinks the drivers will turn everything around at some point still I don't know.\n\nThe big problem isn't that it's behind, it's that it's almost 50% more transistors and and what, a 53W card vs a 92W card, and it's slower. If it was 20% smaller and used less power and performed 20% less, that's great. This thing should be drastically faster than a 6400.",
      "Well this first generation is ruined, it should have come out with good drivers at the beginning of the year to be really competitive, but they could not likely for the drivers.\n\nHopefully with battlemage it will be another pair of shoes! We need a new competitive gpu manufacturer to stop the Nvidia and AMD duopoly.",
      "Intel's? Their iGPU drivers are still very stable for everything *besides* gaming. But that's also kind of an issue when building dGPUs aimed at the gaming market, you ideally want rhem to be able to... game.",
      "If that were the case the 6400 would be way worse off.",
      "It's hilarious that all the \"It's Raja's fault!\" people think Raja does everything; he does the hardware, he does the driver, he mops the office, he takes out the garbage, anything that goes wrong is Raja's fault!",
      "Wow then the A380 really sucks at gaming if it can't even beat a GPU that's not for gaming.",
      "Where does Intel say the 6400 is 10x worse?",
      "Now you're outright lying. Intel claimed it was about about 25% better performance/yuan, but it's MSRP is also about 20% lower. In terms of actual performance that gives you a lead of about 5%."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "I chose Intel Arc a380 over rx 580 and GTX 1660 super. Is it worth it??",
    "selftext": "I'm a beginner gamer. I chose Intel arc 380 over rx 580 and GTX 1660 super, for my ryzen 5 5600g processor and 24gb ddr4 memory. I did so for the Ray tracing and av1 encoder in the new Intel card, but I heard Intel GPUs are not so good with all the games, but support is upgrading day by day. Did I make a right choice or should have gone for one of the old cards without active support and new features? And is this graphic card too slow for my processor according to bottleneck??",
    "comments": [
      "The Arc cards are fine budget cards with a modern feature set. I'd choose one over either the alternatives you mentioned, but I'm otherwise and think Intel needs to succeed and would give them my money. *We* deserve that they succeed.\n\nIf you have an intel CPU with iGPU you should be able to have it share compute workloads via DeepLink, which is a very nice feature - I hope AMD is paying attention.",
      "probably the 1660 super would have been better overall vs the others HOWEVER given you went with Intel Arc, which is fine, I think the 5-series card would have been better than the 3.\n\nBut you made a choice, its fine, and you can hold onto that till battlemage comes out and perhaps an affordable upgrade that is NICE will be yours.  \n\nIn battlemage we hope!",
      "a580 is not available in our country. Moreover with 750 in existence, 580 is a mere alternative at same price. I guess since I already chose, I'll hold on to it till battlemage releases",
      "Driver support isn't on par with Nvidia and AMD, but is slowly improving.",
      "Esports is what I play mainly though. A580 is not available in our country and rx 6600 and arc a750 are way too costly here. Converting to usd, I got 380 for less than 100$ and rx6600 and a750 are 300$",
      ">techpowerup\n\nThat review was more than 1 year ago.\n\nThe drivers have improved substantially.\n\nI can play Cyberpunk 2077 with Ray Tracing on my A380 with XeSS set to Quality.",
      "Also XeSS upscaling is very good on games that support it.",
      "It's better than AMD's FSR.\n\nEven the guys at /r/AMD admit that.",
      "Yes. I'm hoping it should be enough for esports titles gaming and streaming.",
      "$/frame and your use case, you did good.\n\nJust know it'll never do raytracing to a good level and is struggling already on AAA stuff",
      "Yeah. a380 max power is 75w and it doesn't even have a dedicated power pin, so I guess it's ok with the power",
      "And make sure you have ReBar/SAM enabled.",
      "Can't wait for battlemage and celestial",
      "RX6600's are 150 used. The A580 isn't much more than that.\n\nA380 is only worth it as a dedicated AV1 rig, but not for gaming.\n\nIt'll do esports stuff fine though.",
      "RX 580 is 35% more powerful and 1660 super is like 68% more powerful than A380, according to techpowerup, so not the smartest choice imo.",
      "Well so far the letters B C and D were rumored, B for Battlemage, C for Celestial, and D for Druid\n\nSpec wise I have 0 clue, only thing I've heard is that Battlemage is gonna be in competition with the level of a 4080",
      "The techpowerup chart data is based on launch drivers. We all know how much performance has increased since then",
      "Intel GPU is a new variety and they are providing constant update which increases performance and their GPU will get update for a log period of time while GTX 1650 will stop getting GPU update after 2yrs",
      "> If you have an intel CPU with iGPU you should be able to have it share compute workloads via DeepLink, which is a very nice feature - I hope AMD is paying attention\n\nYou mean the thing AMD did 9 years ago with APUs and their lower end GPUs? Yeah, I think they know.",
      "are there leaks about the new intel cards already?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "Views on Intel Arc A50, does anyone have one?",
    "selftext": "Hi again r/graphicscard\n\nAnother question, this one a little more niche.\n\nI have a 3U rack mount server which I use for NAS/Media and offload media encoding, I essentially feed it my physical media and let the iGPU on my CPU turn it into media I can play on all my devices at leisure.\n\nIt's a Intel i7 11700T, UHD 750.\n(35W efficiency CPU, runs very cool and still packs a decent punch)\n\nI want to stick to Intel as the encoding results (at least quality on playback to my eyes) have been far superior on that iGPU compared to NV and AMD.\n\nBeing 3U, it rules out nearly all full height cards as I can't fit anything in that goes more than a few mm over the PCI bracket height and none PCIe bus powered cards have power connectors at the top.\n\nThis is where the A50 looks good, low profile, bus powered and appears to be an overclocked professional Arc A380 with a beefy blower cooler that'll throw the hot air out of the case.\n\nQuestion, does anyone have one? Do you encode media with it? Does it floor iGPU results?\n\nI know it's expensive for what it is, but it'll be on 24/7 and suit my needs if performance is worth it.",
    "comments": []
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "How do I stop my Intel arc a380 from stuttering?",
    "selftext": "",
    "comments": [
      "Yes A520 motherboards do support ReBAR according to MSI. [https://www.msi.com/blog/improve-performance-with-resizable-bar-now-available-for-msi-geforce-rtx-30-series-graphics-cards](https://www.msi.com/blog/improve-performance-with-resizable-bar-now-available-for-msi-geforce-rtx-30-series-graphics-cards)\n\nIntel GPUs absolutely need ReBAR to be turned on in order to perform as intended.",
      "Do you have both ReBAR and Adaptive Sync turned on?",
      "Does your motherboard support ReBAR?",
      "If you bought arc expecting a problem free experience out of the box, you made a mistake.\n\nIntel is new to making dedicated GPU's, and their platform is still being built up. Their drivers are not caught up with nvidia's and amd's and it'll take quite a bit of time before they do. Games aren't optimized for arc since the userbase is comparatively tiny. Then there's the question of whether intel will continue spending money on the arc division while the company is in rough waters financially and is trying to cut back expenses. Optane was cut a while back too, there's a reasonable chance Arc will be cut too or at least will be put on the back burner. \n\nAdd to that that the a380 is simply a weak card and you basically got a card that shouldn't really be used as a daily driver for gaming. I'm all for a 3rd player in the market, but Arc just doesn't compete at this point in time.\n\nMaking GPU's is hard and requires time to compete with the established brands, so no critique of the state of the arc lineup. But marketing managed to trick people into thinking it's more than it really is. \n\nMaybe there's a fix, and I hope for you there is, but otherwise you might be better off switching to nvidia or amd for a daily driver.",
      "I have adaptive sync on but not ReBar on.",
      "Yes it should. From what I know you will get more performance on the A380  by increasing the power limit rather than the GPU clock speed [https://www.youtube.com/watch?v=mBepsi1JDrA&t=296s](https://www.youtube.com/watch?v=mBepsi1JDrA&t=296s)",
      "But you're wrong to say that.  The poster did not ask about your opinion about Intel products.  He was looking for help.\n\nEssentially telling him he should buy something else or that he wasted his money is not help",
      "Would oc it also be good? since I get a maximum of 45 power draw not even close to the max 75 watts.",
      "I couldn't really do anything since in my country it's the only gpu near it's msrp price.",
      "I have a msi b520 pro vh? So I think it does.",
      "Oh sorry it's a520m pro.",
      "Yeah like others said, you need resizable bar to get the optimal experience with intel cards.",
      "I don't think it's returnable now since in my country retailers are real picky.",
      "Without re-bar on it won't help much",
      "An Rx 6600 is 100 dollars more than how much I bought the arc a380 so I don't think that's a great idea. But thanks for the suggestion.",
      "Is it B Two-Fifty or A Five-Twenty because b520 isn't a real thing.",
      "Ok cool. So like Pat said, you can flash the bios from the msi website for your board if need be and enable rebar.",
      "Did you manually download the latest drivers and enable ReBAR on your mobo? I had noticed when checking drivers that it didn’t automatically bring me up-to-date. As for ReBAR, if you have at least a Ryzen 3000 or 10th gen Intel processor your board is likely to have the option (may need a BIOS update depending on the version). I got an A380 just the other day and have been able to play most games at medium-high settings at 1080p. Hope you get your card working and enjoy! Definitely going to be a card that will need some time for drivers to catch up though and that’ll help overall performance\n\nEdit: saw in another comment that your board should support ReBAR - I would make that your main goal as that should give you a decent boost. Been seeing people say up to 30%ish",
      "Turn rebar on. Install latest drivers. (Be sure to run DDU old drivers if your system used to use either AMD or Nvidia previously). Beyond that you're kinda just out of luck. Intel is a poor choice if you're not very tech savvy and actually want to play games without issues. \n\nI like my A770. While the most recent drivers have helped a bit, it still has issues on lots of games. Especially older games. My nvidia and AMD cards game MUCH better and get much more consistant framerates. \n\nI'm sorry to say, but outside of waiting for quite a while for drivers keep rolling out, (assuming rebar is already turned on) there likely isn't a solution for your stuttering issues. But in the event rebar isnt turned on on your bios you absolutely need to turn it on. Stuttering is much much worse without rebar turned on.",
      "I turned it on before I turned it on it was averaging about 200fps when I turned it on it was about 280 fps on valorant"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "Intel Arc A380 Gaming Graphics Card Review & Benchmarks (Hardware Unboxed)",
    "selftext": "",
    "comments": [
      "AMD Unboxed back at it once again.",
      "Look at comment: WTF\n\nLook at username: what in the fu",
      "Lol so salty. :P",
      "Don't you dare ratio me!",
      "This particular model is hopeless but the other two waiting in the pipeline don't inspire much confidence either. The A750 was running Death Stranding at 1440p with the Default preset at 80-90fps, which is also what you get from the RTX 2060. Except the RTX 2060 is three years old, doesn't require resizable BAR, and won't fall apart if you show it a DX10 or DX9 title.",
      "Solid review. \n\nOther outlets have mentioned that the new Intel GPUs have trouble with DX11 and older games, and only work relatively reasonably with DX12. \n\nAs you own the GPU, have you considered trying something like [DXVK](https://github.com/doitsujin/dxvk) (it says it's for Linux, but also works on Windows) or maybe even d912pxy for older games, to see whether replacing a few DLLs in the game's folder is enough for the GPU to start performing better? As far as I'm aware, nobody has tried this yet.",
      ">officially \n\nI dont think its officially supported... but it works just fine. As expected - it is just a platform feature.",
      "To be fair, Intel has spent the better part of the last decade or so developing very rudimentary drivers for their iGPUs (since their only real purpose has been simply a display output for systems without a discrete GPU, save for some encode/decode scenarios) \n\nI'm not trying to make excuses for them, or cut them any slack--if they wanna compete in the big leagues, they need to work on compatibility and performance across the board. Rather than rely on just having cheaper pricing overall vs AMD and Nvidia. Hopefully XeSS will gain widespread adoption, and they'll continue to work on how the cards handle DX9 & DX11. They really shouldn't have even tried to claim that any of the Arc series were \"comparable to a RTX 3070\"; they still have a ways to go before they can even beat out high end Pascal / midrange Turing GPUs. \n\nFinal note, I found it kinda funny that their ray-tracing was more or less disabled initially and all of a sudden gained a 100% improvement from just updating the drivers. So far it's been a rough start, but time will tell.",
      "Yeah, this just sucks. At least the $129 price point, 6GB of VRAM, encoding features and decent efficiency show that Intel’s heart was in the right place here. This effectively kills the RX 6400 for anyone who doesn’t absolutely need a low-profile card, but that doesn’t make it any less horrible. This thing needs to match or beat a 1650 Super before it’s worth buying.",
      "It is disappointing no doubt on that, but we actually didn't expect Intel to come in guns blazing to beat both AMD and Nvidia for their first time to bring consumer based Graphics Card to the market.\n\nI really really want to see Intel become extremely competitive against Nvidia and AMD.\n\nJust imagine if they committed to this in the long run and was able to bring out a product that competes with the \"RTX 5090\" or \"RX 8900 XT\".",
      "Did you notice performance difference using Intel CPU? Just Curious.",
      "Rebar has been a part of the PCIE specification since 2008. If a BIOS exposes the option to turn it on, then it should work with cards that also support it. Just like plugging in a Dell branded USB device into a Lenovo computer. Nothing special here.",
      "Lmao. It's not.\nI would recommend a real technical channel. But 99 of people on here are gamer bros. Not worth time or effort.",
      "I don't know for sure if it works \"just fine\". It works way better than it being \"off\". \n\nGamers Nexus had the A380 faster than a 6500xt in F1 2021, while here it's like 20% slower than I expected. And they used an Intel system I believe.\n\nI'd be curious to see if other games are effected. We need like a Ryzen 5600 vs 12400f comparison using this GPU, or some up coming A700 series.",
      "Jeez. It's really gotta be priced right (e.g., no more than $120 MSRP?) to feel like a worthy buy for gamers (if and when it actually *arrives*).\n\nIt doesn't seem catastrophic (e.g., $130 A380 + reBar is about the $150 RX6400 on PCIe 4.0).  So I agree Intel should keep iterating instead of throwing in the towel (the GPU market *is* pretty large), but, yes: I'd rather *not* pray for driver updates when my return period is 30 days.\n\nHopefully, it won't turn into an Optane: great idea, good potential, painful price, few niches, and weak software support.",
      "Thing is - this is just a PCIE feature. If Intel or AMD willed it, even older FX systems, Ryzen 1000, Intel Haswell etc. could do it just fine.",
      "Probably because before Comet Lake, the only boards that have a BIOS that exposes the function were workstation and server class motherboards, including HEDT.",
      "I would argue GN's testing has lots of issues too but you will probably disagree if you are so deep into his fan club.\n\nAnd before you respond - I am not in HU's fanclub. The man makes so many mistakes and says such stupid stuff and is so unprofessional over social media - I dont like him, the person.",
      "Boy, thumbnail says it all on this one.",
      "I think there is a term for people who love brands so much they will make absurd financial decisions and buy whatever product they put out when other products are dramatically better in every metric.\n\nA card with horrific drivers, lacking performance all over the place, higher power usage and (from what I've seen and certainly from all Intel graphics products for the past 15 years) horrific IQ issues even in the games that perform okay."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "Arc A380 Reportedly Gets Extra 150MHz Clock Boost With Latest Intel GPU Driver Update",
    "selftext": "",
    "comments": [
      "Intel literally already has better ray tracing than AMD and they use pretty much all open source for ray tracing, upscaling, and frame generation, unlike the other 2. I can’t wait for battlemage gpus",
      "im glad they are doing these kind of updates. i personally wouldnt go intel gpu until 2-3 years down but i hope they are sucessful. we need another competitor. i wonder how the yare going to do ray tracing and add features that rival nvidia. right now nvidia has too many features to turn down.",
      "It's an excellent name, much better than weird acronyms like RDNA.",
      "It's probably the best GPU name of all time.",
      "It's one of the dumbest names of all time.",
      "It's not that deep. Professionals don't care what the development codename is, and it might come as a surprise, but people making computers are massive nerds.",
      "It is actually Radeon DNA. CDNA stands for Compute DNA.",
      "Confidently wrong.  \n  \nC is for Compute, and it obviously derives from RDNA. Everybody knows that, it's not confusing or ambiguous.",
      "I'll have to give this a quick check! I have an A380 machine I haven't fired up in a while so I can do a before and after.",
      "Strangest thing honestly. Don’t get why Intel’s board partners have some fetish for putting extra mandatory power extensions for a GPU that barely touches 75 watts.",
      "Can't find the LP edition anywhere",
      "For a second, I thought this said Airbus A380 and I was awfully confused :p",
      "Very interested to see this",
      "Asrock low profile a380 seems to be finally available in Europe. I have no idea about North American  market.",
      "I get it, room for OC potential. But it's not like the LPs aren't being made...",
      "Yea. I can see them in Chinese and EU market. And I can find the listing in their catalog, but it's immaterial in the US market. And I'm not willing to pay the extortion to import from EU.",
      "It became available in EU like a week ago. So I guess there is a chance they have a global launch plan.",
      "Good to know! I guess that explains the exorbitant price point.  Thanks for the heads up.",
      "RDNA is a superb acronym. Radeon + DNA.",
      "It doesn't sound professional,kinda nerdy, if the sole purpose is to sell it to gamers I guess it is right."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "GUNNIR launches Arc A380 Photon in white, over 2 years after A380 introduction",
    "selftext": "",
    "comments": [
      "We need A480. Come on Intel.",
      "💗💗😱😱",
      "Waiting for this💗"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "Intel Arc A380, A750 and A770 8GB GPUs price slashed, A380 now listed for $120 - VideoCardz.com",
    "selftext": "",
    "comments": [
      "Man, I realy wish to have your prices in Europe",
      "The A380 still isn’t even remotely cheap enough, considering that it loses to a GTX 1650. That class of product should really be, like, $70 by now.",
      "Never buy a promise. The A380 *might* be better than that by now, but there’s no 2023 review data for it, so we really don’t know. At any rate, it has a long-ass way to go before it hits GTX 1650 Super performance, which is really the minimum you should get for this price in 2023.",
      "Wish there was a arc a380 low profile",
      "It’s nonsensical to blame increasing tech prices on inflation when tech markets have found ways to exponentially improve price/performance regardless of inflation all throughout their history. The A380 is at best *identical* value to a GTX 1650 Super from four and a half years ago. That’s abysmal.",
      "Cheapest gtx 1650 on newegg is $170 and amazon for $160. A380 at $120 is not bad at all, especially with more RAM, AI cores, and a way better encoder with AV1 support.",
      "So far this is true, but nobody knows what tommorow will bring.",
      "Uk prices have been comparable with the cheapest European markets (ie Germany & Netherlands) for years  and they still are. Nowt to do with Brexit. The relative strength of the dollar due to their interest rate policies are why prices are higher in europe, including the UK.",
      "meanwhile in brexit britain A770 16gb acer bifrost is gbp 427 = 484 euro = 518 usd .....  or could just get an asus strix oc RX6750 instead.",
      "Brilliant!",
      "will only buy it under $50",
      "Let’s just remember that the GTX 1650 Super launched for $160 *four years ago*, and outperformed the A380 by ~35%. That’s the only benchmark worth comparing new budget cards to, because everything else that’s available in that segment right now is trash.\n\nI’m in full agreement that the A380 might well be the relatively best option in the hellhole that is the post-2020 sub-$200 market, but when compared to the market we should have, it’s a detestable waste of sand. Sub-$200 price/performance hasn’t improved in *four and a half years*.",
      "And if they dont? It's also perfectly possible performance could go down with more stable drivers.",
      "yes indeed. even worse when you compare it to the rx 480 which launced for around €200 7 years ago, and still should be around 25% faster on average.  \nthe rx 400 and rx 500 series gpu's also where insanely good at raw performance/compute tasks compared to other gpus. since while in gaming the difference is only around 25% back then a single rx 480 could easily beat and sometimes even double the performance of a gtx 1080 in cad software and similar compute heavy things that wheren't optimized for a speciffic set of hardware and instead relied on raw performance.  \n\n\nso actually the last 7 years there hasn't really been much advancement in gpu's in some cases the ai or raytracing can be usefull however. but ofcource we have to see how well it works on low end cards, since if it works bad then the raw performance of the old 480 might still manage to beat it in such things.",
      "I mean, Alchemist seems similarly compute-heavy, but point taken.",
      "perhaps it is indeed, I didn't yet see as many benchmarks from it outside of gaming and don't own one right now.  \nif that fully is the case, there might actually be a lot of improvement in price per performance next gen, or alchemist gpu's mught be capable of much more performance(probably won't really see that for most people, but some might experience it).  \nit makes sense since typically raytracing cores can be used quite much like cuda on nvidia, so they are typically capable of quite some raw performance.  \n\n\nso sad about performance per price not going up, but intell arc indeed seems like quite much a good trend in the gpu market, since they push the prices less insanely high. perhaps next gen or such might be a lot cheaper per performance since after all this was the first gen, and so it likely had by far more reasearch and producion cost into it, due to much more reasearc being needed for a completely new line of products, also early on optimizations for reducing cost are also limited. so I by far am more angry at amd and nvidia now.\n\nopenly I hope that Intell actually uses this, since amd and nvidia increased prices so insanely much that even their first gen was a quite good or the best competor on the market despite the pricing being as high as  7 year old gpu which performed the same. this could reduce the amount of money loss on the first gen or possibly even generate a lot of proffit and name loyality so also driver support, which might make a next generation much better in price for performance. I hope.",
      "Yeah, I think Arc has a really high ceiling in terms of raw compute performance, but I doubt it’ll ever get particularly close to that ceiling in games. It feels like a Polaris-type architecture, with more features.",
      "Your arguments make no sense, even not taking features into account. And zero goalposts were moved except by yourself.",
      "You manifestly could get something equal to or better than an A380 in… hm, probably early 2020 for $100. Also, GTX 1650 Super, for $160, 35% faster than the A380, available everywhere. We still haven’t beat that, you know, and cheap cards are supposed to be *better* value than expensive ones.",
      "And if the drivers improve?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "Should I switch from AMD rx 6400 to Asrock Intel Arc A380?",
    "selftext": "The specs of the a380 look way better (6gb compared to 4gb) but im worried about compatibility with games. I just want to know if the a380 is better for gaming / streaming. Thank you!",
    "comments": [
      "I'd recommend choosing the A380 over the RX 6400 especially if you can either recuperate some of the cost of the 6400 by selling it or if you have another use case for it. The A380 is a much better GPU overall due to it being faster and supporting features like AV1 encode and decode and the Arc drivers are pretty solid at this point.",
      "https://www.techpowerup.com/gpu-specs/arc-a380.c3913\n\nLooks like the 2 gpus are pretty much equal and I doubt the extra 2GB memory will help much with such low end gpus. Don't waste your money.\n\nIf your Dell can fit any normal psu, get a new psu and a better gpu. But it probably can't and that's why prebuilts suck.",
      "Go for RX7600",
      "i am mainly wanting it for the encode which would be very useful. just to be clear, there arent really any issues gaming with it?",
      "i can't connect to power supply, i need a low profile gpu",
      "On what system? If it doesn't have pcie 4.0 or resizable bar support I wouldn't even bother with Intel",
      "honestly feels like a side grade.",
      "Both are so close that it would be hard to justify the change. I mean sure, you get an extra 2GB of VRAM but rasterization performance (in most titles) is very similar. If anything, you will probably need a different PSU to get anything better. If you are using a pre-built of some kind that has proprietary connectors, then there's not much you can do outside of using SATA to PCIe adapters to run a single 6 or 8 pin into something faster like a 1650 Super (or even better, RX 6600). It's just not something I'd really advise, since the reliability of such a configuration is very questionable...at best.",
      "Thank you",
      "if you want to encode the definitely replace the 6400. 6400 is missing hardware encode\n\nfor gaming, neither card is high performance",
      "The system you would be putting the GPU into. The motherboard, the CPU, etc",
      "oh, just a dell inspiron 3880 i hooked up with 16gb ram and rx 6400",
      "Yes get the Intel instead.",
      "no,  get a 750 minimum",
      "That would be a decent improvement for sure.",
      "got it",
      "cant connect external power",
      "What wattage is your PSU? Do you have any molex adapters?\nYou could get an A750 with two molex to 6+2pin adapters, then use predator bifrost utility to limit GPU wattage to around 150W if you have at least a 400W unit. Also depends on your CPU and rest of your setup",
      "PCIe 3.0 vs 4.0 is basically no difference. ReBAR is more important. There's many people who got Arc cards to work without issues in Z370/390 systems",
      "200 or 260"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "Intel Arc A380 desktop graphics benchmark leaks, almost as fast as RTX 3050 Ti Laptop GPU",
    "selftext": "",
    "comments": [
      "I expected more from Airbus, but it's still decent, indeed.\n\nThe \"3\" is from the \"i3\"-like naming scheme, which makes sense. :D",
      "OK A380 appears to be the ‘full’ small die version of Arc - 128 EUs.   It looks like an even lower end A350 is coming with 96 EUs, and the top end model should have 512 EUs.\n\nThis appears to be a good result for the small die.  I was worried that A380 was the brand for the top end model…",
      "[Leaked image of the inside of A380](https://www.airbus.com/sites/g/files/jlcbta136/files/styles/airbus_608x608/public/2021-08/A380_welcome.jpg?itok=iLKytcqN)",
      "3050Ti performance with more than 4GB of VRAM would actually be quite good.",
      "Intel 380 + 2,4ghz = 5 Tflops\n\nRTX 3050ti = 5tflops\n\nIntel 512EU + 2.4 ghz = 19 tflops\n\nRTX 3070 = 20 Tflops",
      "300$? Wish it was $150 lmao",
      "~~If 128EU is almost as fast 2560 Cuda Cores (RTX 3050ti) then 512EUs should be in the somewhere between the RTX 3080 (8960 Cure Cores) and 3080ti (10240 Cuda Cores), of course that obviously doesn't mean it'll translate to gaming performance, but at least it's some hope of a high-end competing GPU for those that are interested in the high-end.~~\n\nForgot about the 3050ti being a laptop only GPU (much lower clock speed).",
      "Honestly, with the current state of the market it will be a success as long as it outputs video through HDMI and/or DP.",
      "what's the expected launch time for top model guys?",
      "Yeah I see that now.  The ‘8’ was throwing me for a loop at first.  “80” series nvidia marketing in my brain.",
      "100% this. Intel doesn't need to set the world on fire with this release.   \n\n\nHaving it available, affordable and perform good enough to run 1080p, it will sell like hotcakes.",
      "March (Rumored)",
      "More than 4GB would mean it would be a very viable mining card, driving up the prices.",
      "Yeah same. I was thinking how bad is A370 and A360 going to be. Makes sense now!",
      "Performance is somewhere around 1660 TI desktop, then. Hopefully Intel does an MRSP under $300 and has enough supply to blow up the market, but I'm not counting on it.",
      "Hasn't it been compared to the 3050ti laptop card? Which would be around half the speed of normal desktop 3050ti",
      "Goddammit",
      "Seems kinda small for a modern GPU?",
      "scalpers are probably going to get them anyways, but I hope Intel does mass production on their GPU's as I'm very curious about their open source drivers, and I would like to get my hands on one but with the current market I doubt I will.",
      "Isn't Intel going to launch their first models in march?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "a380"
    ],
    "title": "Wait for new Intel ARC or Get Arc 380 just for AV1",
    "selftext": "Got a 3080 for gaming, I have my streaming PC with GTX1070. I have eyes on this AV1 tested the quality using CPU x264 streaming looks amazing. Now I want to get myself a cheap A380 intel arc to replace my GTX 1070. just for Av1. is this a good decision? or should I wait for upcoming CHEAP AV1 intel second gen GPU?\n\nPlease suggest any other ideas as well",
    "comments": [
      "The average performance for the Intel ARC A380 is well below the GTX 1070 8 GB, RTX 2060 6 GB, or RX 6600 8 GB card. Video cards from AMD or Nvidia that originally sold at the 200 dollar and up level will outperform the Intel GPU. I don't think you know or have a benchmark rating system that tells you if the quality of AVI or any processing work by Intel, AMD, or Nvidia that can assist you in getting the best value, but the point is AVI processing is only one aspect of GPU hardware that may not fulfill the work for a specific task if the video output compression/ decompression has compatibility issues. And the only advantage if it exists within the A380 GPU is incremental speed and not so much quality of the visual presentation.\n\nYou want a streaming GPU that is better than a GTX 1070? Any video card that performs around the RTX 3070 would be a marked bigger performance boost than anything that performs at the RTX 2060 or lower.\n\nAs long as you realize you are experimenting with the A380, then your disappointment if things are less than what you want will not be as great. The Intel ARC A380 is a bleeding edge technology product. That means you are more likely to lose money and not benefit in any way, because something will go wrong.",
      "What other options do i have? \nTo enable AV1 in my stream? Cant buy 40 serious or any other expensive one. \nI felt like this thing intel arc 380 is cheap.",
      "How about 2 gpus? \n1070 and A380?\nMy streaming PC is ATX and how about 3080 + A380m\n\n\nMy main goal is to get AV1 in a cheapest possible way.",
      "You should be going to help forums that specialize with streaming video for live feeds in combination with gaming or other business uses. Most people use two hardware setups where one is exclusively for streaming and the other for gaming or performing assignments the user wants to do. Some people purchase and use hardware that is exclusively for streaming that does not require you buy a discrete video card.\n\nWhat is strange is that you want to save money, but at the same time you want to have two video cards with more than one hardware system setup. That's an oxymoron situation, because most people go with just one system and or they go with two setups but they spend whatever they need and they are not sensitive about the cost.\n\nI don't advise this direction of use or thinking, because there's a good chance you are wasting time and money and you will not get the best results. I don't think you use AV1 for resolution improvement, but rather you want to increase the bit rate quality that will require you have a better CPU both on the server side and client side to see improvements.\n\nWhen should you use the AV1 encoding for the A380? when you are streaming and you have much less allowance in broadband connection. At higher internet connection speeds, you will not see a difference.\n\nIf you plan on encoding at lower bit rate speeds, then AV1 with the A380 GPU is the ideal situation. At higher speeds and with a good internet connection, you will not get any significant improvements.\n\nThe Intel A380 is currently on sale for around 100 dollars, but Woot, Newegg, Amazon, and BestBuy occasional sell them for under 90 dollars with the lowest I've seen them at around 60 bucks about a month ago. Please note - the customer review ratings for the product are not good, so, there's a high risk of not being happy.",
      "Makes sense.... I stream from my 1070 but with avery high bitrate so that viewers can get good quality as i stream high motion content which causing my streams to buffer almost for everyone i can spare some money 90 ish if i sell my 1070 for some and add like 50$ more i can get this 380, apart from encoding to YouTube here is no use for it. Thats the actual plan. I can return it right?",
      "I do not stream enough nor do I do enough encoding to give you a useful answer for anyone that does it regularly. The return policy of BestBuy is good and Micro Center also is rated well, but all other retail sources especially those coming from outside your region of the world will not be reliable or consistent."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "a380"
    ],
    "title": "What happened to the Arc A310?",
    "selftext": "All I've heard is that Gigabyte and Matrox partnered with Intel to produce these, but so far I haven't seen a single available unit neither here in Europe nor the US.\n\nWhen are the A310 cards supposed to release?  \nWhat about pricing? Apparently there are some offers from Russian(?) sites which target a price similar to the A380, which would make it DOA.",
    "comments": [
      "Arc a580 is also non-existent at this point. Don't think Intel has explained either.",
      "while we're on the topic of missing arc cards, i'd like to point out that the Arc Pro A40 [allegedly launched in august 2022](https://www.techpowerup.com/gpu-specs/arc-pro-a40.c3925) but [vanished beyond its single review](https://aecmag.com/workstations/exclusive-review-intel-arc-pro-a40-a50-gpus-graphics-cad-bim/)",
      "Dust in the wind. The Matrox cards apparently start at $500, so they're a nonstarter for non-professional use (custom digital signage software, long term support, etc).\n\nhttps://www.guru3d.com/news-story/matrox-and-intel-arc-introduce-luma-series-high-resolution-1-slot-graphics-cards-for-multi-display-environments.html",
      "I really hoped Intel would be the one to disrupt the 1650 as somehow *STILL* the best you can do in low profile bus powered cards, excluding maybe a few Pro priced niche cards. But alas, even they don't seem interested in making low end GPUs right now.",
      "Didn't see the price when they were originally announced.\n\nThat's crazy.",
      "It's not crazy for industrial applications. But it's shame for consumers that the cards probably wont be available for reasonable prices. There has been small but clear demand for low profile passive card.",
      "It doesn't even need to be low profile for me, I just want to have a sub $100 office and transcoding card.",
      "The A380 can often be found at about $120, Newegg had one up right now for sale. I have had one for a while now running reducing my large Linux distros.",
      "But do they offer what matrox offers? Remember that for businesses the hardware cost of these devices is usually trivial. Everything else costs more.",
      "They didn't have enough juice to be competitive.",
      "can double it\ncosts about 130$",
      "Can confirm that they are sold in russia",
      "I thought I read it was cancelled.  Maybe they never officially announced that.  I keep hearing about Battlemage which is slated for next year, but there is supposed to be alchemist+ cards between now and then.",
      "A listing for the Arc Pro A40 is currently on EBay at the moment…for $899",
      "A380 is supposed to be a bus powered card, and in fact does use under 75 watts, but all we’re getting right now are “OC” cards that have unnecessary power pins.",
      "oh nice, thats probably a good sign",
      "Update: a second listing has popped up on EBay, and the original listing was cut to…$425\nAccording to the EBay metrics, one has been sold, 2 are left.",
      "AMD is much cheaper, and their cards are still not cheap.",
      "The problem is AsRock STILL won't actually sell their low profile A380, so there's basically nothing you can do.\n\nIt's not a matter of cost. *It might as well not even exist*.\n\n...and yeah, I landed on this page via web search, trying to find any information, whatsoever, on when AsRock plans to actually launch their low profile Arc A380.",
      "thanks for the update"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "Adlink announces Intel Arc A380 GPU in MXM form factor - VideoCardz.com",
    "selftext": "",
    "comments": [
      "So, will I actual be able to buy one for an old laptop or will they be yet another oem only option?",
      "Well, the article mentions MXM so I thought you meant HHHL for laptops, but after reading it again I realise you just wanted more compact Arc GPUs, my bad.",
      "Why not HHHL though?",
      "Mxm gives longevity to custom gaming laptops. Soldered cpu/gpu for thin profile laptops are only good for disposable machines.",
      "Umm.\n\nWhere did you get any of that?\n\nHalf Height Half Length is for PCI-E add in cards like GPUs, NICs, HBAs, etc.",
      "What you are proposing exists, its called egpu and its not ideal. Mxm is the best way to handle this.",
      "absolutely OEM",
      "Boo",
      "Less demand for HHHL in the industrial applications that Adlink typically targets. They'll also bring SKUs to their COM Express and PCIe/104 lines before the things consumers are familiar with.",
      "Well, shit.",
      "There's laptops with oculink? I thought that was pretty much exclusive to servers/some hedt",
      "Nope, there's not. But there easily could be if anyone cared.\n\nLike I said you can diy it with m.2 oculink adapters, obviously limited to 4 lanes but the potential is there.",
      "Isn't HHHL for storage? And even then m.2 has pretty much replaced it. Plus there's not really any other standards for GPUs in a laptop. Although, even then a majority are now embedded instead. Edit - didn't read properly-ignore",
      "It's not ideal because no one is trying seriously for reasons, be it too small of a market or whathaveya.  \nYou can diy it better than thunderbolt implementations with something like oculink. The tech already exists.",
      "Ahem, if you read my entire post I'm proposing we integrate GPUs into power bricks with some sort of standard like Thunderbolt 5 giving you a viable upgrade path. You could simply upgrade your graphics card/power brick combo instead of needing a whole new laptop when it comes time to get a new GPU. GPU upgrading tends to be slightly more frequently needed than processor upgrades, so this works out nicely to get you 1-2 extra cycles out of the same laptop. \n\nThe last time CPUs weren't soldered on laptops regularly was a decade ago, that idea is long since dead. Yes, there are Clevo and Alienware 10th/11th gen laptops which used desktop processors, but the problem with those is that they only lasted two processor generations. That'll never come back. \n\nMXM has a small potential of coming back, but when thunderbolt 4 can deliver power and graphics I really think the solution is to just build graphics cards into power bricks with the standards for thunderbolt 5 being increased.",
      "Hot take time: Laptops shouldn't even have a discrete GPU, let alone a bulky MXM style gpu. \n\nRealistically, you can't really use a discrete GPU on battery for very long and you pretty much have to plug in your machine if you're gonna do a workload actually utilizing GPU performance.\n\nIf I were Intel, what I'd be working on is integrating dGPUs into a power brick as the basis for thunderbolt 5. New standards have shrunk the size of power bricks that can supply 200+ watts, I'd leverage that technology to build a graphics card within a power brick that would be roughly the size of power bricks of a few years ago.\n\nJust have a big power brick with a GPU built in that also acts as a charger using a single cable. With how efficient laptop processors are, you could realistically have an Ultrabook sized device and just hook up your graphics card/power brick/charger to it for graphics performance. When you don't need/want the graphics performance, you could just carry a regular power cord on a business trip. It could be what External GPUs aspired to be, but actually done right this time. Intel could limit it strictly to their thunderbolt connectors to give them a market edge over AMD. It would be like that Asus 3080 mobile portable GPU, but done right this time by being seamlessly integrated and supported across multiple devices/manufacturers utilizing Intel processors and only needing one cable."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "Intel's First Arc Custom Graphics Card Pictured, Meet The GUNNIR Arc A380 Photon 6 GB With Full ACM-G11 GPU, Starting at $150 US",
    "selftext": "",
    "comments": [
      "According to TechSpot/HardwareUnboxed the 6500xt is 32% faster than the 6400. This thing works out to be 21% faster than a 6400 if you take the averages of their likely very cherrypicked benchmarks. But to be honest, with 3-6 more months of driver improvements Intel likely has a lot more to squeeze out of their cards than AMD, so it might be true. And I'm hoping it'll match the 6500xt eventually.\n\nIntel might have even better FineWine ^(TM) than AMD, because right now Intel GPU drivers are stomped grapes.",
      "Is low end with decent memory capacity finally back?",
      "i don't think you realise how expensive memory is.",
      "Faster VRAM than mobile. Note that it's 157 mm^2 vs RX 6400 with 107 mm^2 (cut down?), with 2GB more VRAM, sold for less.\n\nWhether these are a good value has less to do with drivers and silicon design than people think.\n\nAlthough if my math is anywhere near reality, and I'm not certain it is, the high end cards will be insane value for compute and relatively bad at gaming.\n\nFor comparison to prehistoric cards like mine: it seems to be slightly worse than an ~~RX 580 8GB~~ RX 570, although at half the power usage.\n\nedit: fixed bad math, I misread the relative performance comparisons.\n\nedit2: the Gunnir card is 92W and 2450Mhz vs Intel's benchmarks of a reference design at 75W and base 2000 Mhz, so actual cards may be significantly faster than their numbers and would put it back at RX580 level.",
      "I love that last sentence",
      "On top of performance you also get encode and decode, which the 6400 and 6500 don't support. And native AV1 encoding which by itself will be worth it to some people. \n\nAnd 6gb of vram while AMD's offerings are only 4gb.\n\nIf the drivers keep improving this is definitely better than AMD and Nvidia's low end offerings.",
      "3080 only has 10GB and it's fine for 4k/1440p currently, 6GB is perfectly fine for any card with less power than a RTX 3050 IMO.",
      "Coming from RX 480, I'll take cards that draw less power but perform close enough. RX 5500 XT used to fit the case but it's no longer available and RX 6500 XT is a disgrace to RX lineup. I skipped RDNA1 due to numerous end user reports on random downclocking which eerily sounds similar to how my 2500U likes to randomly downclock",
      "Also 8 lanes of PCIe on ARC — so it’ll scale better on PCIe 3.0 systems than 6400/6500XT",
      "Seems to be priced decently, at least in China. I guess there's hope for A310 to be ~$79-ish?",
      "no reviews?\n\nwhen will be lifted NDA?",
      "It's also important to note, I think, that the first 3rd Party design features an 8-pin power connecter and a reported TGP of 100W. That's up from the Intel reference design of 75W. GUNNIR is claiming the boost clock is increased from 2000MHz to 2450MHz. That *should* give some very noticeable performance gains.\n\nTo put that in perspective, the RX 6400 has a TGP of 53W, and the RX 6500 XT has a TGP of 107W.",
      "The performance is barely above the 6400 on those numbers, Intel is using performance per dollar and a price of 1030 for the a380 and 1199 for the 6400.",
      "Nope.",
      "Dunno why you're getting downvoted. I guess the people of reddit don't or can't actually read.",
      "When the competition is only peddling out 4GB at this price range, 6GB is a lot better.",
      "I’m going all Intel when they get this shit figured out properly 🤣",
      "Are we getting a 2 slot low profile version for this? That's what I'm excited for.",
      "My 2700U did the same thing. Would go from 3.6GHz to 600 MHz at the most random times. Made gaming feel awful. It wasn’t thermals either.",
      "Intel is saying that. \n\n> Claim: The Intel® Arc™ A380 GPU, with a recommended customer price of 1,030 yuan, delivers up to 25% better performance per yuan than available competitive offerings as measured by performance on a selection of popular games.\n\nhttps://www.intel.com/content/www/us/en/newsroom/article/intel-arc-a380-graphics-available-china.html"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "GTX 1650 4GB  vs  Intel Arc A380 Low Profile 6 GB🤔",
    "selftext": "What do you think … which GPU is better and would last longer? I saw size of 2 fans on Intel Arc A380 and it is kind a disappointing how small they are. Soo I don’t know about cooling performance in long term in Intel GPU 🥵\n\n🟢 MSI NVIDIA GeForce GTX 1650 D6 VENTUS XS OCV3\n- 4 GB RAM \n- GDDR6 \n- video memory frequency = 12 000 MHz\n- memory speed = 1410 MHz and boost to 1 620 MH\n- 128Bit\n- Overclock version GUP\n\n\n🔵 ASROCK Intel Arc A380 Low Profile\n- 6 GB RAM\n- GDDR6\n- video memory frequency = 15 500 MHz\n- memory speed = 2 000 MHz\n- 96Bit\n\n\nI will be using GPU mainly for watching 4k movies in my 165 Hz monitor, multimedia task and some gaming (like Fortnight, CS Go, WoW in low settings) but only occasionally I’m not a gamer. \n\nThis is going to be PC mainly for work (CPU 14700K + 32 GB RAM).",
    "comments": [
      "Are these both new? If you just want the best $100 new card then yeah get the A380. But used you can get so much better, especially considering that you don't need one that has no PCIE power connectors.",
      "Yes I will be definitely buying only new GPU. I am thinking about 99 € to 160€. That would be ideal.",
      "I mean it’s not gonna be anything particularly powerful. i’d look at benchmarks to see how the A380 does especially in fortnite.",
      "In benchmark both can do games like Fortnight in low settings so I guess it will be fine"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "Intel Arc A380 - first look at Intel discrete graphics card",
    "selftext": "",
    "comments": [
      "Crysis.",
      "**Anything you want tested on A380?**",
      "But can it?",
      "74 FPS at 3440x1440 full screen in the FPS benchmark (around 15  FPS when in smoke grenade, 80-100 when no smoke, weird...\n\nOn 1080p the drop is like 20 FPS but the benchmark result is 149,50 FPS.",
      "I did a short test in WoW and it worked in 2 out of 3 spots tested. Still too hard to tell, but A380 is entry-level, and more meaningful results would be from the A700 series cards.",
      "good luck intel",
      "Any game selection and AV1 encoding.",
      "Minecraft.",
      "Isn't Arc advertised to do raytracing IIRC? What's that like?",
      "Destiny 2",
      "As in the benchmarks in the article - 5900X, 4x8GB 3800CL19 RAM. ReBAR on.",
      "Forza Horizon 5",
      "I did tested some DXVK but on Windows for now. The thing is it's quite PC and game specific. Like I did test on AMD and Nvidia dGPU in FF14 and it didn't help while for some people it does help. SC2 on Intel has a big uplift while on AMD/Nvidia does not (and usually those shaders stutter the game a lot). \n\nOn Linux it's a bit different story as prior to Vulkan there was only OpenGL which isn't the hottest API for games ;) Will have to check how to benchmark them properly on Linux.",
      "thank you very much! yes the killer bit of the benchmark is when it’s passing through the smoke. This looks promising nonetheless :)\n\nEdit: May I ask for the rest of the pc spec?",
      "Yeah, you will need to use 6.0 kernel which is mainline, not stable release, also bleeding edge mesa drivers\n\nAlso\n\n>i915.force_probe= module \n\nOption to make GPU work. So yeah, it may be too much work and understandable then, though there is not much reviews so it will take own popularity too!",
      "Simple shade on trees seen very small performance penalty. A city saw a \\~44% one but that's what RTX 2070 lost any time DXR was enabled in the game. Will have to check other spots as it may be that Intel design will avoid slow-downs when there isn''t much or any ray tracing to do.",
      "This might not work as support hasn't been officially added yet, but I would love to see NeatBench results, which is the benchmark for the Neat Video denoise program used for video editing that heavily leverages GPUs:\n\nhttps://www.neatvideo.com/download/neatbench\n\nIn particular the only number that matters in this case is the one labeled \"GPU only\", however if you want to post the whole output to pastebin or something I can add it to [the NeatVideo speed database](http://fifonik.com/nv/).",
      "CS-GO please",
      "Can you please check out Waydroid support on Linux?  \n\nhttps://wiki.archlinux.org/title/Waydroid\n\nTechnically, A380 should work great and have hardware rendering out of the box. It's one rare case in which Intel has great advantage compared to both AMD and Nvidia, and it wasn't tested in Phoronix review either. Thanks!",
      "Rainbow Six Siege at 3440x1440 at very high preset. Perhaps PUBG at 1080p would be cool too!"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "Intel Arc A380 6GB Review: Gunnir Photon including benchmarks, detailed analysis and extensive teardown | igor'sLAB",
    "selftext": "",
    "comments": [
      "Nah, this sub allows criticism of Intel, the only hardware sub where you can't criticise is the Nvidia one",
      "igorslab is about to get banned from this sub",
      "Not too surprising. It was naive to think that a new architecture paired with brand new, hurriedly developed drivers was going to be competitive on day one with hardware and software Nvidia and AMD have been polishing for literally decades. Even if everything went perfect for Intel, at best optimization was going to be spotty and limited to the newest and most popular games.\n\nDrivers will gradually improve, which should help the poor 1% lows, but this is definitely the kind of product where you want to look at where it is 6 months or a year after launch and not jump in on day 1.",
      "> Conclusion:  [A one star review](https://www.igorslab.de/en/intel-arc-a380-6gb-in-test-gunnir-photon-total-benchmarks-detail-analysis-and-extensive-teardown/19/)\n\nNot going to lie, I'm starting to rethink a day one purchase.\n\nI've been waiting to purchase a new graphics card for seven months now. \n\nThe ARC series launch has already been delayed twice because apparently Intel's graphics team is having issues producing stable drivers for this architecture.\n\nI'll hold out a little longer, but I'm not going to pay for the privilege of being a beta tester while Intel spends the next year trying to produce stable drivers.  It really makes me wonder how this could happen to a company like Intel who isn't new to R&D, chip fabrication, and software.  I would have expected that some pre-fab R&D would have had proof of concept code for firmware and drivers working and relatively stable on engineering samples before committing to a launch.",
      "Making graphics cards from scratch is hard...",
      "Over a decade of not caring one bit about their iGPU gaming drivers has caught up with them. I remember the arguments here between the people who insisted Intel drivers were \"rock solid\" (and thus a good sign for Xe) versus the handful who were telling those people the Intel gaming drivers were terrible. *I told you so*",
      "Not sure why downvoted - it takes hundreds of million of dollars and hundreds of engineers to enter this market at this level now.",
      "The truth hurts sometimes",
      "Doesn't matter if performance is up to par as long as it's priced well. The A380 is supposed to be $130 to $140. For the performance and features it provides, that's a great deal imo. Not sure how the A770 stands, but it seems to be at least somewhat competitive with the 3060ti. I'd expect the price to be lower than the 3060ti MSRP, maybe around $350.",
      "Well, I mean if it’s priced to match its performance then it may not be a bad deal, still a bit disappointing though.",
      "not just a new architecture but a forward looking one as well. if you look around the web with the performance hit with not having resize bar support on a intel ARC gpu is massive.\n\nmeanwhile the performance bump from AMD and Nvidia is minimal.\n\nas things stands right now i think with time this gens GPUs will age rather well as driver support gets better and better. but the future if intel keeps going on the GPU can be something that properly smashes Nvidia and AMD with time due to a newer base architecture.",
      "it was a cheap build with a lot of errors made by the manufacturer effectively neutering the GPU. It's mentioned in the conclusion. \n\nThe reviewer clearly stated they did not ask Intel for a card but bought a crappy card somewhere in Asia for this test.",
      "No bad products, only bad prices.",
      "Yeah, they misspelled it. On the card is another typo",
      "Igor's lab notes problems with the partner's build.  [Here is that discussion](https://www.igorslab.de/en/intel-arc-a380-6gb-in-test-gunnir-photon-total-benchmarks-detail-analysis-and-extensive-teardown/3/)\n\nComments relating to build quality and choices of cost cutting on GUNNIR's A380 however do not negate the larger problems relating to driver stability.  Igor's made several comments about the card experiencing microstutering and  [effectively being unusable without Resizable BAR](https://www.igorslab.de/en/intel-arc-a380-6gb-in-test-gunnir-photon-total-benchmarks-detail-analysis-and-extensive-teardown/4/) (rBAR) support.  There were also comments about instability and reboots.\n\nThese comments were similar to those in the Gamers Nexus review of GUNNIR A380 and reflect issues with Intel's drivers or the architecture.\n\nNow maybe Intel manages to resolve these issues by the time they release A750 and A770.  \n\nHowever, looking at this as someone that understands marketing and project management, companies tend to carefully manage perceptions before a product launch to promote the best possible image to maximize revenue potential.   Delaying a product by seven months isn't a good sign.  Letting a bad AIB product be the first public perception of a product launch is also not a good sign.  This tells me that things are probably worse as this is what Intel is permitting to be seen publicly.\n\nIntel engineer Tom Petersen has been sent out to smooth public perceptions.  He is an energetic speaker and definitely was a good choice.  The problem for Intel and Tom is you can only hype or talk favorably so much before reviewers and consumers actually have a product in hand.  At that time if serious flaws exist they will become known.\n\nWe are seeing the tip of the iceberg.",
      "Is 'unknow' a word/saying?\n\nUnknown is how that word should be spelled, but I've seen 'unknow' many times on chicken scratch and just assumed it was a misspelling",
      "Honestly, new GPUs as a whole aren’t worth it imo anymore. Getting used is where the deals are because individual sellers have more freedom than AIBs to drop prices.",
      "> Not going to lie, I'm starting to rethink a day one purchase.\n\nHonestly, I did a LOT of day 1 installs and purchases when I was younger. I was always so enthused to be a part of something new and exciting. Windows Xp 64 bit, Vista, SLI, Ryzen gen 1... and now I think, what on Earth is the point? It's so much better to wait a year or so before bothering with any substantially new tech. You get better prices, better drivers and support, and you aren't tearing your hair out over stupid shit.",
      "First gen always sucks",
      "Intel probably told the chinese to \"come up with the packaging\" and forgot to send them a template with the wording. What a shitshow."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "Sparkle Intel Arc A380 Elf desktop graphics card review - What can you expect from Intel's 129-Euro budget GPU?",
    "selftext": "",
    "comments": [
      "This subreddit is in manual approval mode, which means that **all submissions are automatically removed and must first be approved before they are visible**. Your post will only be approved if it concerns news or reviews related to Intel Corporation and its products or is a high quality discussion thread. Posts regarding purchase advice, cooling problems, technical support, etc... will not be approved. **If you are looking for purchasing advice please visit /r/buildapc. If you are looking for technical support please visit /r/techsupport or see the pinned /r/Intel megathread where Intel representatives and other users can assist you.**\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/intel) if you have any questions or concerns.*"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "ASRock Launches Intel Arc A380 Challenger ITX OC Custom Graphics Card",
    "selftext": "",
    "comments": [
      "> ASRock Launches Intel Arc A380 Challenger ITX OC Custom Graphics Card **In China Only**\n\nFixed title to be more relevant for this sub",
      "These China only releases are weirding me out, man.",
      "In China???\n\nEDIT:  yes. It's also a pretty crazy price at 1299 Yuan. Even without tax, that's more than the RX 6400 price point.",
      "A dedicated hardware AV1 encoder is very, very interesting. Arc's AV1 is faster than NVENC, can do 10 bit encoding, and can do better quality at a significantly lower bitrate... Could realistically be a game changer in the long run for content creators, especially in the world we live in of data caps and bandwidth limitations if you go over them.\n\nRight now many big name streamers use expensive secondary PCs for stream encoding through a capture card. An Arc GPU could itself completely replace that entire secondary PC, halve your bitrate, and give you BETTER quality of stream. That's extremely impressive for ~$130.",
      "It's worth it to a lot of people for the encoder support, or as a basic gpu for certain productivity work. But for gaming it's not worth it, unless you have 100% faith that Intel can gain like 20% performance from drives, and game optimisations.",
      "I wonder where you were when the 6400 launched with no encoder and X4, with reskinned 1650 performance for 160 dollars.",
      "Most hardcore miners ran their GPUs at PCIe x1 (using a x16 riser from MOLEX + x1 PCIe wired through something as simple as an USB cable) without much performance hit.\n\nAlso the x4 and x1 PCIe slots are usually not routed to the CPU directly, but are managed by the chipset, so you wouldn't lose your sweet x16 with that approach *(but other limitations apply depending on how a specific  motherboard is wired, eg. an M.2 slot might  not work anymore)*. Though if you have two x16 slots, you should get at least x8/x8 if your motherboard supports bifurcation properly. And as we know, PCIe 4.0 x8 is enough for any GPU, at least for now.",
      "What were to happen if you were to slot something like this into a x4 pcie slot on a motherboard, with a dedicated GPU in the top slot. Would the main x16 with a GPU in it slot slow down like like x8? \n\nNow I know you technically can't fit this GPU into a x4 slot, but I once helped a totally broke friend to modify a used cheap server system to game on. Cut out part of the plastic (on the mobo slot) obstructing an x16 gaming card from fitting into a pcie x4 slot, and the GPU worked, although maybe 5% slower than it should have.  Point is that I wonder if something like the A310 could just be cut down to x4 pcie so you can just add a 2nd GPU like this.",
      "Where do you even live for it to be new at 49 dollars.",
      "It's the only one on the market, full stop. Using it as a secondary card for encoding purposes only has nothing but benefits as far as I can see.",
      "It's not worth more than $99, this is like old i740 equivalent gpu from 98.",
      "OC will help a lot.",
      "That price included the Chinese sales tax and retailer markup, its isnt Asrocks MSRP.",
      "Do NOT buy. Is full of bugs. Software and even at hardware level.",
      "6400 is worth $49.",
      "Problem is that Arc drivers are so broken that it's not worth messing with them just as an encoder."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "a380"
    ],
    "title": "Intel confirms upcoming Arc desktop SKUs: A770, A750, A580, A380 and A310",
    "selftext": "",
    "comments": [
      "Did they also leak in which decade they'll be released? /s",
      "No way. It's 3070 level hardware with unpolished drivers. Expect 3060Ti real-world performance.",
      "Intel should’ve been very clear about the release timeline. I think they still can be transparent about it.",
      "Do we have any idea how they will compare to existing gpus yet?",
      "IIRC the high end is supposed to be up around 3080 tier",
      "They already went transparent with it. \n\nThey told that early summer desktop GPUs will be launched as OEM only in china. \n\nThen in late summer launch OEM only worldwide. \n\nThen \"later\" launch as standalone GPUs.\n(So one could expect October/November for this)\n\nOverall it seems like they really really want to have most of the driver issues sorted out before releasing standalone GPUs.",
      "This century. Stay tuned.",
      "Good luck guys.",
      "There are low end GPU out there, it is just the value is so bad that people ignore it. With possible bad driver due to first launch, it will be irrelevant",
      "Existing in may 2022 or April 2024 when they are actually available? 🤣",
      "You expect the first iteration of GPU to meet the halo products of established GPU manufacturers such as AMD/NVIDIA?",
      "the lack of vram makes me sad, though i'm not sure what i should've expected from the bus widths. wish 8-12gb was standard because 4gb is proving not to be enough and 6gb is next",
      "And I honestly agree with their way of going right now. Why do people care so much that it only gets released in China. Makes no difference if they release it worldwide now, or later.",
      "Those were likely early laptop performance leaks at reduced clocks.",
      "Yes. It has enough transistors at like 22 billion and already has RT performance better than NVidia or AMD. It's tier 4 vs Tier 3, and Tier 2 for AMD.\n\nIf you include the games where it crashes due to drivers to drag down averages it'll be like 3060ti levels.",
      "It's because Raja Koduri promised stuff. \n\nHe promised the GPUs will be ready early, in Q1 2022. \nTurned out to be a lie.\n\nThen he promised GPUs in the hand of gamers for cheap - again a lie because they prioritize OEM.\n\n\nPeople say Intel GPU will be irrelevant if they launch after Nvidia launches RTX 4000 and AMD Rx 7000. \nI don't think so because all the Nvidia and AMD products from new gen are gonna be 600$+(real price, not msrp).\nIntel's best GPU is supposed to be under 500$.",
      ">He promised the GPUs will be ready early, in Q1 2022\n\nHe actually said that? When?",
      "I don't know for sure if it was him but there were messages from Intel that arc GPU lineup will roll out in Q1 2022."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "A better GPU for my coming i3 12100? - Intel ARC A380 or RTX 3050?",
    "selftext": "I will be graduating soon and my uncle is going to give me an i3 12100 as a gift for my graduation but I'm just thinking if I were going to use it.... it will be for playing games which is not a high graphically demanding games (such as valorant or csgo) and light video editing. I also want to use it for creating illustrations and key animation for my animation portfolio. So what might be the good GPU to pair it with my upcoming quad core beast? The Intel arc a380 or the RTX 3050?\n\n\n\nAdditional note: rtx 3050 and arc a380 are the only gpu covered by my budget",
    "comments": [
      "The rx 6600 should be about the same price as the 3050 and is faster, so if it's available in your region I'd go for that. Otherwise, the 3050 is definitely faster than the A380 and comes with fewer driver issues. Intel needs a bit more experience in discrete graphics before they'll really be ready to compete.",
      "Is the RX 6600/6700 not within budget? Usually a 6600 can be found for about the same price as a 3050.",
      "The Arc A380 is roughly equivalent to an RX 6400, so the RTX 3050 is better in pretty much everything. Is the RX 6600 available in your region? In most places it’s cheaper than the 3050 while also offering better gaming performance.\n\nIf you want an Arc GPU I’d recommend waiting for the Arc 5 or Arc 7 series.",
      "The 3050 is going to be way better, but it’s such a shit GPU. In literally every market I can think of the RX 6600 is 25% faster for the same street price, just get that instead.",
      "The 3050 is like double the price. It's at like AMD RX 6600 pricing but 20% weaker.",
      "The RTX 2060 is legit cheaper than 3050 and much better",
      "Personally I just had my Arc A380 get bricked by ***installing the official drivers,*** so I'd avoid it for now",
      "Go for an rx 6600",
      "No, no, no, no. The 6500 XT should never be bought by anyone, ever. It’s that bad.",
      "> intel said that the Arc 5 or Arc 7 is a limited edition GPU\n\nThat doesn't really mean much. The 'Limited Edition' is basically Intel's branding equivalent of Nvidia's 'Founder's Edition'.",
      "All Intel arc are basically products in \"beta\". \n\nA lot of things are unstable - crashing PC and games, many games run very badly, only select ones run \"okay\"\n\nIf you're not interested in it as \"new stuff to test\" then stay away from it at all costs",
      "Welp I'm expecting it to be a really messed up product from intel due to their driver issues but I'm a little interested on it. I'm also planning to test in on video editing and rendering for blender.",
      "RTX 2060. Same price as 3050 and much faster.",
      "RTX 3050 all day long.",
      "3060 or 6600 my friend.  3050 and 6500 are terribly forcibly bottlenecked.",
      "Where did you find 3050 so cheap it competes with a380?",
      "Thanks for the answer but if i remember correctly intel said that the Arc 5 or Arc 7 is a limited edition GPU so I might go for the RTX 3050 but if i find any RX 6600 in our region I might go for that.",
      "Whoa😲 thanks... others also told me to stay away from arc",
      "Especially when the 6600 is very close in price.",
      "Honestly the A380 seems pretty cool. 3050s are still overpriced as of now. I don't know your exact budget but if I were in this price range I'd go either Intel with a A380 or AMD with a RX 6500 XT.\n\nBut if you find a 3050 near the price of a A380 then def go for the 3050"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "'Overclocked' Intel Arc A380 Shows Impressive Gains",
    "selftext": "",
    "comments": [
      "Neat. If this scales to the higher tier cards as well they might be more competitive than I expected.",
      "Gamers Nexus and other reviewers also need to test ARC cards + Intel drivers against linux performance of games using Wine, Lutris, and Proton.\n\nSome of these projects translate native DirectX calls to Vulkan using DXVK default or implement specialized builds. It would be interesting to see how this ranks against intel's native DX processing.\n\nDon't know if Intel graphics engineers or reviewers lurk here but results could shed light on driver issues and lead to code fixes by Intel Engineers comparing open source code against Intel Drivers.  It could also lead to Intel committing improvements upstream to these open source projects.\n\nMaybe the mods can forward this suggestion to Intel's Graphics team.",
      "Very large share of people writing the open source code work for intel and are paid by intel to write the open source code. The drivers intel GPUs use on Linux would definitely be written by intel regardless of if the source is open.\n\nBy code lines written intel is by far the biggest contributor to Linux kernel updates in general. Intel is also a full member and one of the founders of khronos group that develops Vulkan API.",
      "So… basically Intel just self-owned by throttling their own GPU. That makes this intriguing: who knows how powerful this card would be if Intel just let it rip at 75W?",
      "I will definitely do some overclock testing when I get my A770.",
      "Please include thermal temp readings.\n\nPRO Hi-Tech's review implies their A380 OC had minimal increase on GPU temperatures\n\nhttps://www.youtube.com/watch?v=vrwfHuEb_Gw\n\nMaybe it's just me, but the image processing quality side-by-side looks sharper and more defined on Nvidia.",
      "The recent A380 review by Gamers Nexus implied Intel had poor performance with DX9, DX10, DX11 titles compared to DX12 and Vulkan.  \n\nhttps://www.youtube.com/watch?v=La-dcK4h4ZU&t=15m25s\n\nA comparison is shown where GTAV running DX11 on the A380 had nearly half the performance of a AMD 6400 card. This implies the intel driver is not mature and has issues on Microsoft Windows.\n\nI don't think you followed the intent of my comment.  I understand how the FOSS community works.\n\nMy comments were specifically tailored to Wine, Lutris, and Proton and comparing performance of games while using those DX compatibility layer programs on Linux compared against running the same games on Microsoft Windows under native Intel Drivers. \n\nDirectX is not native to linux. \n\nIf wine, lutris, proton, or DXVK on Linux outperform Intel's drivers native DirectX support on Microsoft Windows, then studying those projects might provide means of optimizing or improving intel's drivers or provide Intel a temporary workaround by translating DirectX 9, 10, 11 to Vulkan.\n\nWine's stack has included DX9 support for a very long time under WineD3D.  It was DX10/DX11 that didn't come until very recently under D3D11.  Typically both have lagged behind native DirectX in performance.\n\nWine is not an emulator, it translates DirectX calls to OpenGL\n\nhttps://www.winehq.org\n\nDXVK is a more recent project which translates DirectX 9, 10, 11 calls to Vulkan.\n\nhttps://github.com/doitsujin/dxvk\n\nProton is a patched version of wine maintained by Valve Software which includes DXVK support.\n\nhttps://github.com/ValveSoftware/Proton",
      "Shhh shhh, not yet. Wait.",
      "Some of this was highlighted in this Gamers Nexus review of the A380\n\nhttps://www.youtube.com/watch?v=La-dcK4h4ZU&t=7m30\n\nIt was then was discussed by Intel Engineer Tom Petersen in this video below.  Petersen didn't fully address if this was going to be fixed when Rebar was off.\n\nhttps://www.youtube.com/watch?v=8ENCV4xUjj0",
      "I watched that.\n\nThe comment made by Tom Petersen was primarily Linux driver support.  \n\nHis comment doesn't really apply here.  DirectX is only native to Windows systems.\n\nDirectX is not native to linux.  That's why programs like Wine, Lutris, Proton, and DXVK exist to translate DirectX calls to OpenGL (Wine) or Vulkan (DXVK).\n\nIf Intel is having performance issues with native DirectX 9, 10, 11 support and if for some reason linux translation programs provide better performance, then Intel should look at those projects.",
      "Budget GPUs that have a low stock wattage are the best overclockers usually already anyway.",
      "Intel on Linux should be fine. [LTT asked specifically about this on the WAN show last week.](https://youtu.be/fDblFRwSZNc?t=1502)",
      "Peterson acknowledged reBAR \"ON\" also having frame time spikes? The reBAR off results are nothing new, but Igor is going to publish something in the next 24 (or so I hear) that goes even further. \n\nI'll be eager to see what he discovers. I really hope it's nothing that can't be fixed. I'd probably trade my 6600xt for a A750 if I had the chance just for the fun of it, and because I have faith there is a massive amount they can still squeeze out of it. But if there truly is something wrong with the silicon, or the driver overhead is as bad, or even worse on the CPU is than Nvidia, I'll probably be forced to stay away as my 4 year old i5 can't take it.",
      "Lol a770 gonna be priced at like 400$, meanwhile miners will be selling rtx 3070s for <400$ at the time of global arc a770 release (if it actually ever happens and Intel doesn't cancel it)",
      "Apparently Igor's Lab is going to drop their review tomorrow (July 20), and they've found really bad frame pacing, and CPU overhead issues on the A380. Even with reBAR on. Might explain why the a750 and A770 don't scale to the performance level they should. No idea if it can be fixed with software alone. Also don't know if that's just DX11 titles he's talking about or DX12 as well."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "a380"
    ],
    "title": "Current state of VR support on Intel Arc (A750, likely also applies for the A770 and A380)",
    "selftext": "I hadn't posted about this here yet, but I am a collector of VR Hardware, and when the A750 and A770 first launched in Germany, I was very eager to try out VR performance on as many headsets as I could.\n\nI personally bought the A750, mostly as I intend on mostly using the card for AV1 encoding in my Server, but might as well try VR on it as well I thought. Here's my findings so far:\n\nThe Intel Arc drivers currently **don't seem to support Direct Display Mode devices at all**. This is a requirement for VR headsets that use DisplayPort or HDMI to receive the image they're supposed to display. That means that I wasn't able to test any of these headsets, and I've tried a lot of them: Oculus/Meta Rift CV1, Valve Index, HP Reverb G2, Vive Pro 2 and the Varjo Aero (tho in the case of the Varjo, there was also a software lock-out, as they don't want their customers running into issues with unsupported GPUs, which also includes ones from AMD).\n\nOfficial Link support with the Quest (both with a cable and wireless) also wasn't working. Quest Link refused to find it's connection (the PC dialog never found the headset's USB connection and I never got the Quest Link prompt in the headset itself). and Air Link straight up threw me an error that the Video Encoder was unsupported. I have no idea what kind of magic other people have pulled off to get Air Link or Quest Link working on Intel Arc, I sure wasn't able to.\n\nI was however able to run VR on my A750 through 2 other means. For one Virtual Desktop basically runs on anything that has a DirectX 11 capable video output and a CPU, so that worked with no issues at all on Arc too, and I was also able to use my Vive Pro 1 using the Vive Wireless adapter.\n\nWith Virtual Desktop performance was quite stellar and I didn't really experience any stutters or similar. With the Vive Wireless Adapter however, I immediately started noticing hitching and stuttering every so often. The more CPU heavy of a game I tried, the worse the stuttering got, hinting that large parts of the stuttering may have to do with GPU drivers reliance the CPU.\n\nFor reference, I'm running a 5900X with 32GB of DDR4 3200MHz CL16 memory.\n\nIf anyone is interested in the performance numbers using the OpenVR Benchmark Tool, I'd be more then happy to provide them later as well. I've already run the benchmark, but the results I have not saved on my main data drive...",
    "comments": [
      "Yeah, this is true. Linus from Linus Tech Tips also faced this issue. Intel has responded saying they are in the alpha stage. They should be able to release beta drivers for VR support shortly.",
      ">The Intel Arc drivers currently don't seem to support Direct Display Mode devices at all.\n\nI am curious if the Direct Mode does work on Linux, since [the implementation is shared with AMD.](https://monado.freedesktop.org/direct-mode.html#intelamd)",
      "No unfortunately not, and I was not aware of it and bought a NUC with arc card to play specifically VR....",
      "I’m running the same specs as you plus an A380 (cause why not see what it can do with VR). Oculus/Meta Rift S didn’t throw me any errors, just said display port was disconnected even tho it was connected.\n\nOn the Intel discord, I’ve brought this up and have been reassured multiple times that VR isn’t software blocked nor hardware blocked. So I’ve been assuming that Oculus/Meta VR and Steam VR hasn’t whitelisted them yet for the direct connection headset. I’m thinking they’re waiting for the rest of the 40 series and 7000 series to release before doing a bulk VR whitelisting of said cards. *People with 40 series have been reporting the same problem so I’ve heard*\n\nFrom what I understand, the wireless alternatives ignore the gpu hardware check and goes by what the gpus can run as you said",
      "I know you probably won't respond but, since then have you tried again? And if you did has oculus link still worked?",
      "I'm curious, at a collector of vr hw, you likely have some great opinions.  I'm looking to buy an older lower cost but still serviceable vr computer to drive an oculus 2.  Only need to do something like Alyx at \"ok\" levels.  It's hard to discern where that sweet spot of age vs functionality vs price is.  If you've thoughts on those lines, love to hear them!",
      "still waiting :(",
      "As soon as a Mesa driver version with Arc support ships with PopOS, I will try that out. I'm just not familiar enough with Linux yes to feel comfortable modifying the system itself through the terminal ![gif](emote|free_emotes_pack|sweat)",
      "40 Series does work with all the headsets. Nvidia changed something with the Framebuffer handling in Ada Lovelace that causes heavy stutters in VR at higher resolutions if not accounted for.\n\nThis whole topic was already discussed in great detail on the official Varjo discord, and Varjo has already released a patch to fix 5hose stutters on the 40-Series GPUs.\n\nIntel Arc isn't blacklisted or anything. Direct display mode devices shouldn't show up to the Windows desktop, but they do on Arc, and usually at the wrong resolution too. The Rift CV1 showed up at the right resolution, but I'm guessing, since it wasn't found by the Oculus software, that there's some flag missing, like HDMIs 3D side-by-side display stuff. The Index shows up as a 640x480 display, and looking into the SteamVR web console it actually says that no display with the right resolution was found, but it does list all displays connected to the Arc GPU. And in the case of the Reverb G2, the display didn't show up on the desktop (it's a built-in Windows driver, go figure), and WMR did actually launch as if everything was working right, but the displays in the headset stayed black. My guess is that yet again, the driver wasn't able to initiate the vorrect display mode (resolution, refresh rate, direct display, 2 DP lanes per screen, etc.) on the G2...\n\nThe tl;Dr is, it doesn't seem to have anything to do with the software, at least for the display connections, it seems to be a driver related issue with the hardware not showing up correctly...",
      "I haven't, tho my intention was to at some point do a 30 day Arc trial, using nothing but my Arc card for 30 days and when I do I can try Quest/Air Link again :D",
      "Honestly, that I can't really say where the sweet spot for price to performance is for VR capable PC Hardware. What I can say is that you'd want a GPU with at the very minimum 6GB of VRAM (which at least all of the Intel A7 cards do fulfill), at minimum 16GB of System Memory and preferably 6 CPU cores with HT or more. I wouldn't to older then an Intel 8th Gen CPU (or the generation after if you're going Team Red) and in terms of GPU not older then Nvidia 10-Series or RX6000.\nIf you have a Quest 2 then that leaves Intel Arc also as a GPU option open for you, as long as you're using Virtual Desktop (as like I've said before Headsets that require a Display connection to the GPU currently don't work on Arc and the Oculus software refuses to work completely on Arc)",
      "They have not fixed it yet!!!?",
      "You could try something more up-to-date like Fedora or endavourOS.\n\nI'd be very interested to see whether this works.",
      "Okay, so there has been some change as the driver updates have been releasing. My experience was with either the launch driver of the update after launch driver which just said *headset is disconnected.* I’ll try launching VR on my Rift S and see if I match what you’ve been experiencing",
      "It still says that, but at least with the CV1 a 2160x1200 display was showing up on the desktop.",
      "Damn. Sorry man"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "Intel Arc A380 (auto-translate captions for English)",
    "selftext": "",
    "comments": [
      "Usually new driver and firmware updates fixes a lot of performance and power consumption problems. It requires a lot of game tuning to perform well. This requires support from game developers and obviously takes time.",
      "Lol. That’s where your wrong.\n\nThe GPU is clearly being held back by its drivers, just look at any previous Intel GPU. Skylake, Iris Xe, etc.\n\nIt’s just like previous AMD GPUs. The RX580 could barely compete with the 1060. Now it’s faster and almost competing with a 1070.",
      "The hardware isn't actually bad, in compute it beats both competitors. \n\nThe reason why gaming performance sucks are the gaming drivers.\n\nIntel is making gaming drivers for like 2 years by now while competition is tuning their drivers 20years already, nothing easy to catch up with at all.",
      "I'm sure the dev team at Intel has been working extensively to optimize gaming performance for the iGPUs whose main focus has been gaming /s",
      "DoA unless it's priced around a used 1070. $150",
      "actually yes , intel has been devoting significant resources to optimize their igpu for many games, even advertized it as a plus, that so many games are playable on their igpu. They have optimized a lot . Sure, not for AAA games , but many esports titles have been really very well optimized (like cs go) .",
      "> Now it’s faster and almost competing with a 1070.\n\nNo, it's not even close to the 1070.\n\nGTX 1070 vs RX 580 https://youtu.be/mvs1iKfcWoU?t=533\n\nGTX 1060 vs RX 580 https://youtu.be/2fLLVWug-c4?t=752\n\nAt 1080p high the RX 580 8GB is ~6%faster than the GTX 1060 6GB, meanwhile the GTX 1070 is ~27.5% faster than the RX 580 8GB.\n\nThey went from the GTX 1060 being 6% faster 5 years ago to the RX 580 being 6% faster 1 year ago, but it's still no where close to a GTX 1070.",
      "So, somehow this 16 CU 2450 MHz card released in 2022 loses to a GTX 1650. It had better be $80.",
      "r/averageyoutuber",
      "120*",
      "The MSRP is 1030 Yuan which is currently ~$153.",
      "89.99",
      "Hmm... If so, then Intel Arc may almost be perfect for me, as I need OpenCL (but no SR-IoV is a shame).\n\nI believe that Intel drivers, similarly to AMD drivers, also are better on Linux than on Windows, which is also to my favor.",
      "I would buy 2 of them lol",
      ">It just means the card can do more with the allocated power budget\n\n.....thats the point",
      "Trash. That's what it is. They better step up the game real soon as 1650 is still a trash card.",
      "This result + Raja at Intel makes me think they simply placed a wrong bet at a wrong time. Vega was a *great* chip, but for compute (i.e. mining) not gaming - it seems Intel wanted to play like AMD/NVidia, print money by making compute-first products to sell to datacenters and miners but get gamers to subsidize its R&D on early iterations by soaking up sub-par silicon. The latest crypto crash *really* came at a bad time for Intel - with AMD and NVidia last gen outperforming Intel effortlessly at lower power consumption *and* flooding the used market i don't see Intel selling any of these at any price...",
      "What a mess.      \nBeaten soundly by GTX 1650 and RX 6400 .        \nDoes manage to beat the 1050ti ( by small margin) .        \nUses much more power than both 1050ti and 1650 .       \n\nbad. just bad.\n\nFor the power consumption, keep in mind that 1650 is built on TSMC 12nm (tweaked 16nm) . \nIntel ARC is using TSMC 6nm.           \n\nARC is being beaten by an nvidia chip built on 12nm and manages to consume more power while doing so. wow .\nAMD Vega vibes all over again.",
      "..intel is making gaming drivers for much longer than 2 years now. \nthey have been releasing gaming drivers for their igpu for a long, long time",
      "i dont dispute that drivers can improve. Power consumption though did not change.release RX580 with todays 580 have same power consumption,despite the perf increase due to drivers.  If the card gains peformance due to driver optimizations, this does not change the card TDP/ power envelope.It just means the card can do more with the allocated power budget... anyway....didnt think that that needed explaining."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "a380"
    ],
    "title": "Intel Arc Alchemist desktop series including A770, A750, A580 and A380 SKUs reportedly delayed till late Q2/early Q3",
    "selftext": "",
    "comments": [
      "Honestly, it doesn't matter.\n\nThey missed their window by not launching during the GPU apocalypse uncontested, so now they have to go against Nvidia and AMD offerings with mature drivers in a health(ier) market.\n\nMight as well take the L at this point and just figure out your drivers.",
      "What they've officially stated is launch in 'summer', compared to mobile 5/7 'early summer'. Anything past 'early summer' puts it in calendar Q3.",
      "They're going to have to seriously discount those cards to sell next to Nvidia and AMD. Can't see many people taking a chance on the first gen of cards especially in the enthusiast market when the top Arc card is only at 3070 level of performance. \n\nAnd then by August and September all of the talk will be on RTX 40 and RDNA3 which will blow these cards out of the water.",
      "This will get steam rolled by Lovelace and rdna3. Should’ve released this in June 2021",
      "Not if it's dirt cheap like polaris. \nBoth lovelace and rdna3 will be starting at minimum 400$ for the lowest model.",
      "Their gonna take an L. The market is getting better and by the time it comes out, amd and nvidia will have their gpus at a few % above msrp. Therefore making these card useless. They won't be faster, they might be a bit less expensive, and their launching gpus close to next gen.",
      "To be honest, even if they launched in June 2021, they'd have probably all been gobbled up by miners.",
      "most of them are probably going straight to OEM.\n\nOEMs will use it as a way to tell AMD/Nvidia to fuck off with their horrid \"incentives\" like priority allocation. Most people buying computers don't know what the hell goes in them so the main problem is drivers. if the drivers continue to be miserable then intel will get nowhere even giving these things away for free.",
      "At least they would’ve sold (Not defending miners). Now they have zero chance of becoming dominant in any market",
      "More competition, more fun.",
      "Well considering the frame stuttering in the arc a350m for mobile that was recently tested, my money is on bad drivers and at this point Intel should just work on the drivers. They missed a critical chance to disrupt the market. Now new gpu prices have fallen down hard. Not yet at msrp but much better. The second hand market will be flooded with rdna2 and 3000series from Nvidia and not mention the rtx4000 series and rdna 3 gpus coming soon. This will be really rough  on Intel.",
      "When will Intel learn that blatantly lying to investors will get them nowhere?",
      "Efficiency wise it's better than GTX 1600, at least the already released laptop 350m which at around 40W has same performance as GTX 1650m at 50W. \nThat's not too bad. Probably on the same level as RTX 3000 as efficiency didn't improve much compared to GTX 1600 and 2000.",
      "Well, thats basically all she wrote for ARC...\n\nI know for a fact Intel won't price them relative to their performance vs other cards.\n\nI hope I am incorrect, as aggressive pricing is the only thing that will save them. That creates an even bigger problem for them though, if they start out low priced, they will remain there for many years. Thats just the way the market works.",
      "They're making them at TSMC so it didn't matter. if these were being made at Intel fabs, well they'd be worse if they were being pushed out on a broken 10nm a year ago or 14nm due to power efficiency.\n\nBeing made at TSMC means when TSMC were starved for capacity these would have been even worse.\n\nUsing TSMC 6nm indications would be that performance is closer to AMD/Nvidia parts that use half the die size. \n\nThe only way these matter in the first gen or two is if Intel does it's Atom/phone/tablet tactic and just firebombs pricing to force their way into relevancy but if the architecture doesn't catch up then eventually when they try to charge real prices they'll be pushed out.",
      "Mmmhmm.",
      "More competition more fun works if wafer supply is ample and performance is competitive. If performance sucks and they eat up supply of incredibly limited wafers then it's terrible for everyone, Intel included. They'll be eating shit selling these at below what they cost to produce and AMD/Nvidia lose out supply to make twice as many gpus from the same wafers and the end user loses out.",
      "Thats just clock gating. Easily fixable.\n\nLook at the GPU speed every stutter, you will see it drop to 1150mhz.",
      "TSMC 6nm is a retooled 7nm, so the density is not that dissimilar.",
      "How is it fixed if u dnt mind explaining?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "The morning after – reflections of the Arc A380 premiere and (no) second conclusion",
    "selftext": "",
    "comments": [
      "I don’t doubt the thoroughness or technical knowledge of Igor’s lab.  He’s great.\n\nHowever without official pricing .. it’s hard to say if this is a ‘pass or fail’ for a GPU ..",
      "You know what, yeah. I want this card to work, and I don’t think it’s *worse* than the 6500 XT and/or 6400… but it sure ain’t much better. This could have been a great little card if the drivers were better, but right now it’s a bad card, and Intel should be ashamed of themselves for loosing it on the Chinese and South Korean markets with a promo consisting of inflated Time Spy scores and little else.\n\nWe need more competition in the GPU space, but if Intel isn’t going to be honest and ethical, they deserve to be blown out of the water.",
      "So basically this can still put some pressure on AMD/NVidia if it's something like $100 MSRP, it can still serve as an entry level barely over iGPU card and give a huge middle finger to NVidia's 1050/1650 price gauging, maybe finally put a stop to those two being churned out and let the entire segment move up in performance instead of just in price... but yeah, it's a failed product at this point",
      "Agree with both of these paragraphs\n\n> What irritated me about making comments, however, is the sudden puppy-protector instinct that erupts whenever you want to protect something small and fragile. Yes, it is Intel’s first dedicated graphics card developed for the consumer market in what feels like ages, but it is neither a sympathetic underdog nor a fragile plant that should not be trampled on. Here, a multi-billion corporation with extensively purchased personnel and know-how (also in the heads) as well as many years of development time and almost infinite seeming resources has failed grandiosely. You cannot and must not call it anything else. The saying that money alone does not score goals, which is often used in soccer, also lives on in IT, and the fact that an unfinished product with even more unfinished drivers is sold to customers for expensive money can and must be criticized.\n\n....\n\n> Of course, you should feel sorry for Intel, but even more for those who have spent their money on such products because the PR machinery produces full-length propaganda at its best and you as a consumer have trusted it. Trust is also exactly the keyword that really matters here. Because I can’t use anything productively that I can’t know if it won’t destroy the work I’ve done up to that point in the next five minutes with a crashing gesture of nonchalance just like that. And there are so many technical bugs and carelessness in the drivers and GUI that you could lose faith in programming mankind. Sure, programmers don’t grow on trees, but Intel didn’t start from scratch. This should never be forgotten, and criticism should also include those who have willingly followed the call of money and have not managed anything that would really be resilient.",
      "Blistering follow up  from Igor's lab. \n\n\nReview:\n\nhttps://www.igorslab.de/en/intel-arc-a380-6gb-in-test-gunnir-photon-total-benchmarks-detail-analysis-and-extensive-teardown/\n\nPrevious discussion:\n\nhttps://old.reddit.com/r/intel/comments/w3im2q/intel_arc_a380_6gb_review_gunnir_photon_including/",
      "I feel like those cards you mentioned aren't even for gaming. And if you're not using it for gaming, isn't the A380 kind of the best at what it does for even $130-$140 compared to the 1050 and 1650?",
      "$129.99 is official pricing",
      "Pretty much my sentiment on this. Appearing on LTT and GN to butter them up along with the viewers doesn't change anything. At all. The end product in systems is all that matters. Remember when RDNA1 ate ass for 9 months with constant black screens and other issues from users, but it beat the competition when it came to price to performance? Same situation. No need to treat them with kiddie gloves. \n\nThey should be almost giving this stuff away, and if they take a loss, then they take a loss. Better luck next time.",
      "It's a fail regardless of any price at all. And you know it's terrible, when Intel sends their employees and not hardware. They're doing damage control at this point and the CPU price raises is solely to mitigate losses created by Arc, as it simply will. Not. Sell. And they know that. Intel fked up hard.",
      "It can still be a great video encoder for home servers. If the Linux drivers are good, which they probably are, because they can build upon the integrated graphics than it could be good in there. No direct x and vulcan should work pretty good on it",
      "Oh give me a break. It's a decent first try in a duopoly market. People beg Intel to enter the market then lambast them the second thier product releases. \n\nAMD and Nvidia have decades of experience. It's actually promising that Intel is doing damage control at all. They could easily just dump the whole thing like Larrabee.",
      "What a botched launch. This card is arguably only better than the GTX 1630 from Nvidia. Anyways, I truly hope these cards end up in laptops so nobody would end up with these cards as desktop cards. Given the enormous amount of capable people and funding Intel has access to, I look forward to much better drivers and tighter execution of their battlemage line up. Please be good.",
      "Yeah, but most of those people bought them 4 years ago. Are you going to buy a 1050 today to game on? Maybe used, if you live in a country that has bad hardware supplies. I've helped some people over at r/buildapc that pretty much only had like 2 year old parts available at stupid prices in their country. Maybe saying they aren't for gaming is wrong, but I'd say you don't buy those cards for gaming anymore. Today. They are like GTX 1030 territory. I consider them display output, or plex server cards.",
      "But it could be $139.99 Tom said. Given some bad reviews it might actually be on the lower end, though. Or is there an actual official price now. It seemed they didn't even know if AIBs will even release the A380 in the west.",
      ">$129.99 is official pricing  \n\nNot a single source has validated that price.  \n\nIts MSRP in China is RMB 1030 which currently equates to around US$150.",
      "Well AIB partners can charge extra money for “premium” card variants so that is kind of a mute point. $129.99 is the base MSRP for the card. \n\nYeah, it is up to AIB partners to release their version of the card in the US, it sounds like Intel will be making their reference cards available everywhere but they are only limited runs.",
      "It needs to start working and stop randomly crashing doing its job first.",
      "It's not decent, it's an alpha stage product released to public. They deserve to be shit on. Overpromised, overcompared, severely underdelivered, region locked, review controlled and issue bombed garbage."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "a380"
    ],
    "title": "Intel ARC A750 and A380 as eGPU for GPD Win Max 2",
    "selftext": "",
    "comments": []
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "My intels arc a380 fans wont keep spinning",
    "selftext": "I have the Asrock intel arc a380 and when I start the computer up the fans spin for how many seconds and then they stop. Then when I log into Windows 11 the fans don't spin at all or nothing so I don't know if this is a hardware problem? All I do is browse the web and I don't game. When I browse the web the fans don't spin. Does anyone else have this problem?",
    "comments": [
      "I don't know if there is a problem with your GPU. But most current GPUs have 0 rpm mode, and the fans won't start spinning unless the GPU get hot enough.\n\nOpen a game, or start a GPU benchmark and check if the fans start spinning when the GPU gets hotter.",
      "Most standard GPU bios I have seen from EVGA don’t even both starting the fans until 58C so it sounds quite normal what it is doing",
      "WTF? Why do you need a 13th gen intel cpu for web browsing? A potato cpu can do that. Just wow. But hey, it's your money.",
      "I know that my GPU usually stays at 53-54 degrees and I guess that isn't considered hot if the fans aren't running thanks for the heads up.",
      "Nothing to worry about, even fans on a A770 may be at 0 rpm if the temps are low. If your GPU is working hard and they're still not spinning, then you have an issue.",
      "Totally normal. Those fans won't run until that card is under enough load to get hot past a specific temperature. No worries.",
      "I was trying to keep the cost down instead of having to pay $319 or $239 for a new Intel 13th gen CPU.",
      "Thanks for the heads up my GPU temps are usually 53-54-55 nothing higher.",
      "People are fucking special man. We had a guy in r/Intel earlier adamantly freaking out that his 30c idle on his 13900k was absolutely unacceptable and OVERHEATING WHY ARE REVIEWERS AT 27C THIS IS UNFAIR.\n\nPeople are stupid, and it needs to be called as it is.",
      "If all you do is browse the web, why did you even bother buying a discrete GPU? You could have gotten a cpu with integrated gpu and call it a day. As mentioned by the other commenter, your gpu fans likely won't start spinning until there is a load on it."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "i cant fiend intel arc a380 driver",
    "selftext": " **i cant fiend intel arc a380 driver its just for Ubuntu why???**  i want for win 10",
    "comments": [
      "https://www.intel.com/content/www/us/en/products/docs/discrete-gpus/arc/software/drivers.html\n\nIntels website is a bit cringe but here are the arc drivers",
      "Are you having a stroke?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "a380"
    ],
    "title": "Portal Prelude RTX working on Intel Arc?",
    "selftext": "Hey Intel peope, I was wondering if anyone has successfully run Portal Prelude RTX on their Intel Arc (GPU, not integrated). I tried using my A380 but got an error message. Given that it's free on Steam, I was hoping more people would give it a shot. I tried posting on the Steam game forums, but didn't get any responses.",
    "comments": [
      "I just tested it myself and recieved an error too. I'm part of the community beta testing program, so I'll file a report about this directly to Intel.",
      "Great!"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "Ray tracing and API scaling on Intel Arc A380",
    "selftext": "",
    "comments": [
      "Pretty interesting. The frame drops when turning on raytracing in WoW more or less match the performance hit, at least proportionally, I see on a RTX 3060 laptop. I'm really interested to see how the higher-end Arc GPUs perform. The low-end one we have so far is pretty impressive for a first outing. I imagine there's still some performance on the table that'll come with driver updates as well.",
      "Battle Mage or likely Celestial. It's not their intent to cover every performance bracket with first gen.",
      "Which exactly are the performances brackets covered then in contemporary competition terms? This celestial you’re on about doesn’t seem like an Ampere contemporary.",
      "There are like 2-3 other companies trying to make a consumer dGPU and as of now, they don't really have anything. Intel using their iGPU base made a dGPU lineup that works. They still have a freakton of things to do with the software, drivers, and likely multiple WTF in the silicon showed up, but they already have first gen and is working.\n\nWouldn't be surprised if next get would be pushed hard in Intel Evo, and with partners that usually do Intel premium designs. Dell XPS, Precission,L LG Gram, Lenovos and HPs - without MX450+ but with B300/500 then Asus TUF with B700 and without RTX 4050/4050 Ti or 4060 to some extent. If you make CPU+GPU intel based you get a discount! OEMs will go for it.",
      "Still 1st Gen.",
      "If they push A700 laptop they should compete with RTX 3060 laptop. A300 laptops out there already compete with MX and alike cards.",
      "So what’s their answer to a 3070ti 100-120w commonly found in 2kg thin and light gaming laptops for around $2000?",
      "This is the second generation",
      "You think those will come out ?\n\nisn’t this an unmitigated distaste for Intel ?\n\nSadly for sure , I was hoping for more competition."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "Arc A380 not displaying with old PC",
    "selftext": "Hello, I have an old desktop that I am trying to get running again. Currently it has a i5 4570 CPU and a Gigabyte B85n motherboard. It was a small mITX build from back in college.\n\nI purchased a Arc a380 and I can't seem to get it to work. If I download the drivers before hand and attempt to install, I get \"No driver was found that coils be installed on the current device. Exit code 8\".\n\nIf I plug in the GPU and boot my PC, I get a black screen. After doing a bit of research, it seems there is an issue with the on-board graphics. I disabled though, and still unable to see anything using either GPU or CPU graphics.\n\nAm I doing something wrong? Or is there a way that I can install the drivers manually, then disable integrated graphics and use the GPU?\n\nThanks",
    "comments": [
      "You’re likely running into issues with the old motherboard - it doesn’t support Resizable Bar which seems to be the fix for a lot of people. I believe you need at least a 10th gen intel processor for that",
      "I got an Arc A770 working with an Intel 8700k on a z370 motherboard, I had to use the motherboards hdmi out and set the (newest) bios for onboard graphics, then install the GPU, then install the Arc drivers, then switch the hdmi to the Arc. Don't know if that'll work for an even older system though.\n\nMy GPU had a quick life though. 3 different systems in 2 months before aRGB broke and had to be RMA'd but worked with cpu's 8700k, 9900ks and a 13500. Worked well while I had it.",
      "I had the issue where it kept blackscreening but would still boot into windows, eventually after restarting a few times it worked.",
      "Did you find a fix for this, or did it just go away on its own? I lose display signal on my A380 regularly and haven't found a solution.",
      "My issue was specifically when first installing my GPU, since then I've had no black screen issues (I *suspect* it had to do with no driver installed then, although I could definitely be wrong about that), although I don't regularly restart my PC (I've probably started my PC at most 5 times since I bought+installed my A750) and never let it go to sleep."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "Intel's Arc A380 Is The First Graphics Card To Support DisplayPort 2.0 Standard But No Monitor Out There To Make Use of It",
    "selftext": "",
    "comments": [
      "So you can play pacman 3D in 16K?",
      "Kinda curious if laptop displays will take advantage of this to save power:\n\nAlongside raw bandwidth improvements, DisplayPort 2.0 also has a few enhancements on the feature front, one of which is Panel Replay.\n\nThis makes the display work more efficiently by limiting the power it uses and decreasing the thermal output. For example, with Panel Replay enabled, a smaller device with a high-resolution display only updates elements that change on-screen. This will prevent the display from using more power to update items that aren’t being shown at the present moment, so when you stay on one webpage, it won’t constantly refresh.",
      "People are sick of waiting",
      "They already do. It's parted of eDP specifications and called panel self-refresh. It's an old feature - my laptop from 2016 supports it. This just looks like the added the feature to the non-embedded part of the specification and rebranded it for whatever reason.\n\nTraditional panel self-refresh (PSR) would only allow a refresh to be skipped if the screen contents didn't change at all. New PSR implementations have a \"region of interest\" and can also update parts of the screen.\n\nPSR requires special LCD controllers with local framebuffer, though. So there's a complexity/cost/power usage tradeoff. You have to consider that LCD panels aren't digital devices. They're analog and work very much like CRT screens. You have to refresh each subpixel regularly, or it will fade. It takes significantly longer compared to a CRT, though, so flicker is unusual. With PSR, the LCD controller will take care of refresh independently, so that the GPU and its memory can power down for a prolonged time.",
      "r/woooosh",
      "On A380? It'll still lag on it."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "I have a new Intel Arc A380 GPU, installed on a Z390-E Gaming MB, and CPU-Z shows its Graphics Interface BUS to be running on PCIe 1.0. The MB is enabled for 3.0. What can I do to fix it?",
    "selftext": "I have a new Intel Arc A380 GPU, installed on a Z390-E Gaming MB, and CPU-Z shows its Graphics Interface BUS to be running on PCIe 1.0. The MB is enabled for 3.0. What can I do to fix it?\n\nHere's a picture:\n\n[https://ibb.co/kKCbjF6](https://ibb.co/kKCbjF6)",
    "comments": [
      "run gpu-z and press button with question mark and you will see actual pcie speed left from ? symbol.",
      "\\*sigh*\n\n*This* thread again?",
      "Likely a problem with the application itself not being updated.\n\nIf your SBIOS is telling you it's 3.0, it's 3.0.",
      "When you put the gpu under load it should say 3.0, if it doesn't you have a problem.",
      "That's normal due to ACM G10 and ACM G11 having a pcie upstream downstream interface between the gpu and the hda controller. It did the same thing when I had an Arc A380. Check with GPU Z or HWINFO64's pci tree.",
      "Playing game still remains at 3.0... Not sure what to do...!",
      "I don't know what that means...",
      "CPU-Z is buggy! It has the decimal in the wrong place. You're actually using PCIe 10 because Intel is so advanced!"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "a380"
    ],
    "title": "Intel GPU labs gets hands-on with Arc Alchemist A380 desktop GPU",
    "selftext": "",
    "comments": [
      "Intel could not plow their face into the ground harder. They are going to release these cards straight into a recession, with disappointingly-bad performance numbers, into a market flooded with old mining GPUs.",
      "I think the missing \"Retail\" in the title really changes the tone of the message.",
      "Pretty bad GPU that is about to be released at the worst time possible.",
      "Intel gets ahold of Intel property.",
      "There are no recessions for data centers"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "Intel Arc xxx - GREAT DIY compute performer ?",
    "selftext": "I was just looking at the first available ARC GPU card specs, [Asrock's Arc A380](https://geizhals.de/asrock-intel-arc-a380-challenger-itx-6gb-oc-a380-cli-6go-a2791643.html)\n\nLeaving graphic driver issues aside for a moment, from pure raw computing perspective, it looks great. 4.61 TFLOPS (FP32), 1.15 TFLOPS (FP64), 186GB/​s for less than €140.\n\nBut the kicker is that its FP32 capabilities are 1/4 of FP64. \nBoth nVidia and AMD in that gaming/commercial segment can do just 1/16 or less.\n\nAnything even coming close to FP32 ( let alone FP64) or bandwidth numbers from A380 costs far more.\n\nSo, one would think that ARC series might find its home in certain uses.\nDoes anyone have more info to share on this matter ?",
    "comments": [
      "We have yet to see, because the only thing everyone seems to care about reviewing is gaming performance on Windows, and getting one out to diss it there as soon as possible.\n\nThen they use those gaming performance on Windows stats to jump to the conclusion that the card can’t do anything else well, like compute, rendering, or even run games well on Linux, and for the later case we can assume it is mostly untrue due to its decent Vulkan performance, which is what most Linux gaming care about.\n\nThen you get people suggesting cards instead of Arc, even going as far as to suggest the 6400 or GTX 970 even when the person was considering it for Plex encoding.\n\nBut yes, it’s probably going to do well in compute. Raja did make Vega, a decent compute unit with terrible gaming performance compared to NVIDIA.",
      "Welcome, beta tester, long journey is ahead us, but fear not.",
      "Yes, it's pretty good for compute.\n\nOneAPI is also an actually useable piece of software, unlike ROCm.",
      "Every DIY project involves a journey.",
      "Question is if teraflops actually converts to actual performance. Nvidia made a video on their site a while ago talking about how latency is a lot more important. You really don't need a massive amount of TF, if you can't feed enough data to the card.\n\nAMD's RDAN3 architecture is supposed to double teraflops. Navi33 is supposed to have like 140% more teraflops of what Navi23 has, and yet will likely only be 30-60% faster. Navi31 is going to have like 220% more TF than Navi21, and yet will likely only be 70-100% faster.\n\nIn some production work, it'll kick ass, I'm sure, because of those FP64 numbers like you mentioned.\n\nThe A770 especially has a massive amount of silicon, and compute numbers compared to AMD. At like RX 6800XT levels almost in FP16 and FP32. It's just weird how it's no where close to it in gaming. The machine learning numbers are also really impressive. I think it's like close to an RTX 3080 in ML matrix stuff.",
      "Good news on the drivers side: [https://community.intel.com/t5/Blogs/Products-and-Solutions/Gaming/Engineering-Arc-8-19-2022/post/1407637](https://community.intel.com/t5/Blogs/Products-and-Solutions/Gaming/Engineering-Arc-8-19-2022/post/1407637)\n\nIntel is owning its problems. They should had delayed it , probably, even considering they were already late. \n\nBut I am sure that eventually the drivers will not be an issue so I understand looking at it from a \"pure raw computing perspective\".",
      ">But the kicker is that its FP32 capabilities are 1/4 of FP64. \n\nSorry to burst your bubble, but as [Intel Arc doesn't support FP64 natively](https://www.tomshardware.com/news/intel-arc-will-not-support-fp64-hardware), [FP64 compute for Arc is simply unavailable under Windows](https://www.pcgameshardware.de/screenshots/1280x1024/2022/10/AIDA64-GPGPU-Benchmark_Arc-A770-16GB-pcgh.png), and although [FP64 emulation is available under Linux, performance is abysmal](https://github.com/ekondis/mixbench/issues/32) (I know that the GitHub issue doesn't link to something explicitly about Arc GPUs, but it can easily be inferred that a similar thing also applies to Arc GPUs given the same lack of FP64 cores). Heck, you probably got the numbers off of techpowerup's website, and if you go [there](https://www.techpowerup.com/gpu-specs/arc-a380.c3913) right now you can see that the FP64 (double) performance section under \"Theoretical performance\" has already been completely removed to reflect what I'm saying here.  \n\n\nSo yea TLDR forget about FP64 compute on Intel Arc and get yourself a consumer AMD dGPU instead if you're looking for good FP64 performance for the cheap (Nvidia has been greatly nerfing their FP64 compute capabilities on their latest gen GPUs, i.e. dialing it all the way down to 1/64th of the FP32 performance, so I won't recommend Nvidia consumer or even pro GPUs for FP64).",
      "Vega is power hungry and sadly needs the hbm to feed it.  The hbm that it needs kills it because of the cost.  But i still love my v56.  It's undervolted and lives in my htpc.",
      "I think all this matters much more for opengl etc than GPU computing.\n\nIn computing, one can tailor timings and code to  much greater degree.\nYou gave the perfect example - while nVidia was on top for gaming, AMD has been big with crypto crowd for very same reason - they were able to put its raw power to a good use.\n\nSo, why couldn't this repeat with Intel Arc ?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "a380"
    ],
    "title": "Intel Arc availability?",
    "selftext": "Aside of the long list of issues and whatnot how is the availability looking - official and unofficial via Chinese resellers? Anyone knows where to buy like a A380 to play around with it? (or when it will become available world wide ? :) )\n\nGunnir does not sell outside China and there is only one overpriced ebay listing, no Aliexpress or Alibaba from what I've searched. Asrock has a similar card but also unknown where and when. Some new floating for MSI and Asus as well. There are also like 2 laptop listings in EU with A370M and A350M but that's a full laptop...",
    "comments": [
      "Would be strange, Intel Graphics [just uploaded a video on Arc A750 performance a hour ago.](https://www.youtube.com/watch?v=6L3JcnBP_jc) If cancellation was on the table I would think they would shutter these videos",
      "Based on rumors, arc (high-end) could be given a thumbs up as early as Aug 5th this friday",
      "All anyone has said is that Battlemage might be cancelled. And that's probably 18 months away. Tom Peterson talks as if a Alchemist refresh is coming if you listen to the PCworld interview on Twitch (YouTube channel got taken down by accident, and can't be found there).",
      "Lol. MLID and their shitty flood of RTX 3060 rumor.\n\nI will believe when I see it, especially when they’re already shipping OEM PCs with Arc in China and laptops with Arc in the US.",
      "They already said the A380 would come to the US around $120-$130 USD. And MSI already sells a prebuild system with the A380 in China.",
      "No one even knows if the A380 will ever come to the west. Intel makes it sound like it's up to AIBs, and they don't know either. Gunnir won't sell here I'd imagine, so it's only Asrock that might. I don't even know which North American supplier outside of Asrock has announced even an A700 card. some are saying ASUS, Gigabyte & MSI might have something but if you were to believe what MLID is saying, then some of them are abandoning that idea, and others are confused themselves. My guess would be the A700 series is launching only from Intel for the first month at least.",
      "There's listings on taobao, so you can probably use a broker like superbuy to import it. It's about $199 before shipping and brokerage fees though.",
      "I've heard Tom say [$129-139](https://youtu.be/AN8ZAf15DrM?t=618). So $130-140 as Steve corrected him by removing the $1. Is there an updated claim of $120-130 somewhere? Also he said \"**If** this this becomes available\", and I swear there were other videos where he makes it sound like he's even more sceptical if the A380 will ever come to the NA. Where you get the $120 to $130 from, and is there actual official confirmation it'll ever come here?",
      "Based on rumors, it might be getting cancelled. Which wouldn't be surprising considering what happened to optane."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "Intel Arc A380 Benchmarked, Full Alchemist GPU Specs Reportedly Revealed",
    "selftext": "",
    "comments": [
      ">A380\n\nSo *that's* why it's so late and the performance doesn't quite meet the expectations! /joke",
      "How is it confusing? No, literally, how?\n\nA5 is better than A3, and A7 is better than A5. It's a lot better than stacking suffixes.",
      "A 75w card with that performance for $150-200 MSRP range would be excellent if they don't gimp the PCIe width like the 6500XT.  Perfect for an upgrade to an iGPU build with low-end PSU, OptiPlex retrofit, low-profile case, etc.  Seems like the perfect 1650 successor.",
      "People with small brains dont understand ☹️",
      "The higher VRAM capacity is more important, after all most of the benefit of the wider PCIE bandwith is if the VRAM capacity is too low.",
      "That's from February 23rd, by the way.",
      "How are you running 5ghz on that cpu? Lol",
      "The issue with the 6500xt is that by limiting the number of PCIe lanes to x4, the effective bandwidth when in an gen3 slot is substantially less than that of the card itself, creating an unnecessary bottleneck.  This is likely a result of the 6500xt starting life as a mobile GPU that would never be used in anything but a gen4 environment, but the limitation greatly hinders it in a desktop environment on older system. As long as Intel doesn't make this same mistake with their low end discrete cards, a low power card like this will still fill a big gap in the market not well served by AMD and seemingly abandoned completely by Nvidia.",
      "https://youtu.be/-EDJXISD6RY\n\nI meant that Hardware Unboxed already proved that if the VRAM capacity is higher the GPU will have not to rely on system memory via the PCIE lanes as much.\n\nIf the 6500 XT was 8GB because of a 128-bit bus or 64-bit with double the VRAM density that would've been the single most beneficial improvement. VRAM is primary in the memory bottleneck the 6500 XT suffers, PCIE lanes are secondary.",
      "That definitely makes sense, but I doubt you'll see much more than 4-6gb of GDDR6 in the sub-$200 price range due to GDDR prices.  Making the bus x8 instead of x4 would be far less expensive to design-in than doubling the VRAM... with the 6500XT, it was already a design limitation of a repurposed mobile GPU not originally intended to be used in anything other than a notebook/laptop with a Gen4-enabled platform.\n\nThen again, I'm one of the few that doesn't think the 6500XT is hot garbage outside of the GPU market as a whole being extremely inflationary... reviewers just seem to cling to where a $200 card USED to sit in the lineup compared to where it does now.  A year from now, I wouldn't hesitate to put a $125-$150 clearance 6500XT in a $500 i3-12100 build for a kid's gaming PC... but the PCIe lane x4 limitation prevents it from being utilized with even more modest components that would otherwise be a good fit in terms of performance and price.",
      "That's a good point, I was only thinking about how the 8 GB would be more effective than the doubling in PCIE bus width, but the PCIE bus width increase probably improves the performance to price ratio with better scaling.\n\n> I'm one of the few that doesn't think the 6500XT is hot garbage\n\nKind agree with that, the GPU die itself is very good from a budget perspective, just mostly crippled by its memory bandwidth problem."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "can i use a intel arc a750 whit i3-10100f?",
    "selftext": "hi guys, i want to buy a new pc (i have a old one whit c2d e8400 and gma 4500 \\[very bad\\]), whatcing some videos and stores i have find an arc a750 and a i3-10100f, next i try to search some bottleneck test (nothing found). Yesterday i found a video about the arc a380 and they says that for installing the drivers yu have to use a IGPU, so i have to use a igpu for installing the drivers of a arc a750 whit a i3-10100f? \\[sorry if my english is bad\\]",
    "comments": [
      "Yea you can use it perfectly no need to worry",
      "Yes. You don't need another GPU just for drivers, microsoft has a default driver for video cards. It makes your dGPU run (but poorly) so you can get a copy of GPU driver and install it."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "Intel Arc A380 MXM Card Surfaces with 50-75 W Power Limits",
    "selftext": "",
    "comments": [
      "Why haven't we gotten USFF GPUs.\n\nThat would be the best market for Arc GPUs.",
      "It's mxm which is/was used in laptops to allow upgradeable gpus. Kinda surprised it's still a thing since Nvidia stopped supporting it.",
      "Is that a ramslot connector"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "a380"
    ],
    "title": "Bykski releases waterblocks for Intel Arc A770 and A380 GPUs - VideoCardz.com",
    "selftext": "",
    "comments": [
      "Why would anybody watercool A380?",
      "Stupid"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "can anyone with an intel arc A380 do a blender 3d benchmark",
    "selftext": "hi, I just want to ask anyone with an intel arc A380 to do a benchmark on the blender 3D benchmarking tool. this will help a lot of 3D artists decide whether they will buy the card or not. if you do a benchmark make sure to put the score you get in the comment\n\nthanks in advanced",
    "comments": [
      "https://techgage.com/article/an-alluring-alchemist-intel-arc-a380-gpu-creator-performance-review/\n\nthis could help",
      "Doesn’t it release on the 12th",
      "Are we going to get HIP on Arc?",
      "it did help , the article has blender benchmarks , thanks",
      "Nope. A380 are available since September on newegg"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "Intel Arc A380 Desktop Graphics Card Leaks Out: Up to 100 FPS at 1080p in Popular eSports Games | Hardware Times",
    "selftext": "",
    "comments": [
      "This is the basic Arc model, nowhere near the flagship. It's a 3 series. There's a 5 and a 7 series aswell.",
      "5600g doesn't do 100FPS in OW even on low all settings at stock speeds and 1080p, it's more around 80 FPS average. And without knowing the exact settings of 'medium/low' this Chinese company is citing it's hard to directly compare, but the A380 is already faster if we compare low to low.\n\nAs for LoL, it's a worthless benchmark, even the 12400 IGP can do around 180FPS, the game isnt GPU intensive enough to differentiate between GPUs.",
      "Ah, 780, my bad. Thanks!",
      "12400f + this just to get similar 5600g performance? this amd apu is 100fps on overwatch, 200 on lol, same as article claims for this dgpu.",
      "Bold of you to assume the performance of an APU that isn’t out yet.",
      "This 7600G will probably do. By the time this card is released.",
      "100 fps in eSports games isn't that impressive to be honest. Isn't the A380 supposed to be the flagship, comparable to a \\~ RTX 3070?",
      "Wow so it has performance like GTX 1650 non-super ahahaha. Basically 1650 but with bugs, glitches and some games not turning on.\n\nWhat a crap GPU..  \n\nI already see how much these Intel GPUs will get bashed by all unpaid reviewers after it releases for having outdated performance and unstable drivers.\n\nThey will get bashed much more than 6500xt."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "a380"
    ],
    "title": "Intel Arc Graphics A380: Compelling For Open-Source Enthusiasts & Developers At ~$139 Review",
    "selftext": "",
    "comments": [
      "Might be fun for open-source people, but for gamers this card is utter trash, and will be until it beats the 1650 Super.",
      "I don’t think it’ll ever get there, it’s trading blows with the base 1650 right now, and the 1650 Super is *35%* faster.",
      "If they manage to beat the 1650 super by improving the drivers, at 139$ this is a pretty good entry-level linux gaming card. Since the drivers are open source, the card should be pretty much plug and play on Linux - as opposed to NVIDIA. On Windows I think they are beating the card by 10% so it should be possible to achieve it. Even though the 1650 is a very old card, I'm happy to find any gpu in that price range.",
      "I wish the test included a comparison with an Intel Integrated GPU. How much faster is it than an Alderlake iGPU?",
      "It's really damn tempting to get one of these just for the encoder, but do we know if we're going to be getting this in the iGPUs for Raptor Lake? Going to be purchasing either the 13900k or 7950x3D and I'm currently leaning AMD because the cache is going to be great for my use case, but a good AV1 encoder might change my mind."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "Intel Arc A380 AV1 encoding",
    "selftext": "Gaming PC with\n13600k\nRtx 3060 \n32gb Ddr5 6800\n850watt PSU\n\nI am planning to add an ARC A380 to my system for AV1 Encoding. I heard about this in a Linus Tech Tips video. Does anyone know of a place where I can find a guide for how to setup/enable this? Thanks",
    "comments": [
      "make sure your mobo has enough PCI slots/lanes, plug it in and select it for the application you want to use its en-/decoding",
      "Ok thanks! I have 3 slots",
      "Wouldn't it just, work out of the box? (obs atleast)"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "How many simultenous encoding sessions can an Arc GPU handle?",
    "selftext": "I need to stream and record at the same time, so I want a card that can do at least 2 encoding sessions simultenously.\n\nIn the spec page of the Arc A380, under \"Supported Technologies\"', there are 2 \"Multi-Format Codec Engines\". Not sure what this means",
    "comments": [
      "> It has a hard time starting them after 3 1080p streams that require conversion\n\nSo it does at least two.\n\nhttps://www.reddit.com/r/PleX/comments/yb4cg3/my_experience_with_intel_arc_a380_plex/",
      "Nice! Thanks for your answer"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "Intel ARC A380 desktop to be 23% faster than ARC A370M mobile - VideoCardz.com",
    "selftext": "",
    "comments": [
      "How shocking",
      "A780 is their high end which matches 3070-3070ti\n\nA580- 3060 to 3060ti\n\nA380- 1650 to 1650 super",
      "Gap is always larger on higher end compared to mid to low end ie 3060 desktop vs laptop is smaller.",
      "Much smaller difference than between 6800 and 6700m or 3080 and 3070m.",
      "I have been told than fire is hot.",
      "Also Water is Wet",
      "Sure. And desktop 1650 (not super) vs laptop 1650 is almost the same. But this intel A380 should be their highest end.",
      "Which is somewhere between 3060 and 3070, maybe it is a decent gap."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "rx 6400 or arc a380",
    "selftext": "",
    "comments": [
      "I would go with whichever is cheaper. They're both pretty matched, with rx 6400 pulling in the lead by a fairly small amount.\n\nEdit: saw the video from the other comment, I was wrong. Rx 6400 is much better, but see if you can find a gtx 1650 as well.",
      "The benchmark testing of both the AMD RX 6400 4 GB and the Intel ARC Alchemist A380 6 GB is more than 3 months old, but I expect the current performance levels to have changed less than 20% from the initial testing, which means that the RX 6400 will still have more than a 30% performance advantage over the A380 rather than the massive over 50% better rating in their initial testing.\n\nBoth are regarded as entry level gaming cards, but the A380 has severe video driver problems, and no matter how good the hardware is, if the software can't play the programs and games you want, then you are out of luck. The much safer perspective is to go with the AMD product, because even if it is a weak offering that requires PCIe version 4.0 installation to maximize its performance, the card is a much more stable product with fewer major video driver issues.\n\nBoth products are better than the GT 1030 or GTX 1050, but neither give enough boost to replace a GTX 1050ti, GTX 1650, RX 470, RX 480, RX 570, or RX 580.\n\nYou are gambling with an A380, while the RX 6400 as well as the even slower RX 6300 cards are not worth buying if you are upgrading from just about anything else.\n\n\nHere's a review of the A380 with the RX 6400 as one of the competitor test comparisons: \n\nhttps://www.youtube.com/watch?v=e3o7tKRGcMY"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "GUNNIR Intel Arc A380 Commericial",
    "selftext": "",
    "comments": [
      "Hope a reviewer gets their hold on one and reviews it in English.",
      "Is it just me or is the video being tagged as [\"potentially sensitive content\"](https://imgur.com/a/zzEpP9M)?",
      "Not just you.",
      "I have no idea why, unless it has to do with playing video games after 9pm or prominently wearing an Apple Watch in an Intel commercial.",
      "cheesy but they do imply its usecase fairly okay; accelerated workload and lightweight gaming. wonder how long will it take for Arc to stabilise, it took iris Xe around a year"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "a380"
    ],
    "title": "Full arc dgpu media capabilities?",
    "selftext": "Hey everyone! I wanna get an arc a310/a380 for media encoding/decoding purposes. Does a sheet like the nvidia nvenc/nvdec matrix exist for arc? I wanna know what it can decode, what it can encode, how many streams of encoding it can handle. Thanks!",
    "comments": [
      "https://github.com/intel/media-driver/blob/master/docs/media_features.md#media-features-summary"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "I have a new Intel Arc A380 GPU, installed on a Z390-E Gaming MB, and CPU-Z shows its Graphics Interface BUS to be running on PCIe 1.0. The MB is enabled for 3.0. What can I do to fix it?",
    "selftext": "I have a new Intel Arc A380 GPU, installed on a Z390-E Gaming MB, and CPU-Z shows its Graphics Interface BUS to be running on PCIe 1.0. The MB is enabled for 3.0. What can I do to fix it?\n\nHere's a picture:\n\n[https://ibb.co/kKCbjF6](https://ibb.co/kKCbjF6)",
    "comments": [
      "Weirdly enough they do that. When it's not doing anything for some reason it falls back to PCIE 1.0. Something is wrong though since it says it's got a max link width of PCIE x1. Is the card plugged into the primary slot just below the CPU?",
      "Yes it is.\n\nIt also remains at 1.0 during any gameplay... :(",
      "Have you tried benchmarks to see if it performs like other a380s?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "a380"
    ],
    "title": "Hardware Unboxed's Intel Arc Alchemist A380 Review Summary",
    "selftext": "",
    "comments": [
      "Why would you not just link the source video instead of some site ripping off their results?  \n\nHardware Unboxed video: https://youtu.be/e3o7tKRGcMY",
      "The video is clearly divided into labeled sections you can very easily click through to whichever part interests you.",
      "Then don't visit the site, then. You have a choice; no one has taken that away from you.",
      "if someone doesn't want to watch an 18-minute long video, they can see the results in much less time by simply looking at the screen-grabs in that post."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "Anyone have experience using ARC A380 for Plex Transcoding",
    "selftext": "How stable is this card for Plex transcoding, or should I go with Nvidia in this case.",
    "comments": [
      "I don’t know if they are supported yet. I have an A380, but not a Plex system currently to test it, but I don’t think it has officially announced as being supported yet."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "Update On My 3.1GHz Intel Arc A380 - SkatterBencher",
    "selftext": "",
    "comments": [
      "It's amazing how quickly GPU clock rates have risen."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "ARC A380",
    "selftext": "Anyone been able to get the A380 here in the UK. I’m still waiting for Ebuyer to get stock. Thanks",
    "comments": []
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "I was surprised by the Intel ARC A380",
    "selftext": "I can't wait for the next release of drivers to see how it improves things on a very capable and affordable card.",
    "comments": []
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "12:51 / 29:37 Intel Arc A380 Gaming GPU Review & Benchmarks vs. AMD RX 6400, GTX 1650, & More",
    "selftext": "[https://youtube.com/watch?v=La-dcK4h4ZU&feature=share](https://youtube.com/watch?v=La-dcK4h4ZU&feature=share)",
    "comments": []
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "a380"
    ],
    "title": "Deciphering the new Intel Arc \"A380\" Naming Scheme",
    "selftext": "It's been reported that the entry level Intel Arc GPU will be called the A380. And this is what I think is the justification for the name.\n\n**A**: Alchemist (kinda obvious)\n\n**3**: Perhaps keeping in line with how we have the i3, i5, i7, and i9. The i3 and A3 are the low end products. \n\n**80**: This is tougher but it might have to do with how GPU naming conventions has xx60, xx70, xx80, xx90 has the more **gaming-focused** vibe to them. The 1060, 2060, and 3060 cards are considered to be the ideal starting point for gaming performance, while everything beneath it like the Nvidia GT 910, 640, etc. is usually considered low performance for laptop meant for basic productivity (i.e. Adobe) and light gaming. The 1050 and 3050 cards are in this 'just decent enough for gaming' area but not recommended in the long term (i'm starting to see games where the bare minimum requirements is a 1060, which sucks for me since I have a 1050 Ti...)\n\nSo with that in mind... the A380 will be the **low-end,** **Alchemist line** but **high-performance-gaming-focused.** At least for that A3 family. \n\nSo, with the other cards... I could see something like an A580. A5 = i5, 80 for that continued gaming-focused branding. The last two numbers could be swapped around because 60-90 is associated with high end gaming, so maybe like an A990 could be the pinnacle Alchemist GPU instead of A980.\n\nAnd yeah, when Battlemage and the others come out, it'll be B380, C780, D980 or so.\n\nAny other speculations?",
    "comments": [
      "Yup this makes total sense to me.\n\n I just only wanted to say these arent the coolest sounding product names in the world imo it sounds like mb chipsets lol.",
      "I felt like the 80 part is just marketing so that it's felt close to top tier card like 2080, 3080.\nIn the I think we will see only a380, a580, a780 maybe.",
      "They honestly should've done word names. Alchemist Alpha/Delta/Gamma/Omega, Alchemist Bronze/Silver/Gold/Gold Philosopher's Stone Edition.. whatever."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "PC Inside Xbox One Case",
    "selftext": "Hello! My name is Kyle and I recently built a secondary streaming/server/portable gaming PC inside an original Xbox One case!!\n\nI have a YT video explaining the build here: \nhttps://youtu.be/4y6Y-KIb09o\n\nSpecs: \ni3 10100\nIntel Arc A380 (I love AV1)\n16GB DDR4 3000\n2tb M.2\nFlex Atx 600 watt (might explode one day)\nNoctua 60mm intake\n2x notcua 40mm intake\n\nTemps under load\nCpu: 94\nGpu: 85\n\nIt was a very constrained build and the back mount for the GPU isn't perfect, but I'm so unbelievably happy with how it turned out!\n\nI will try and reply to as many questions as I can! Thanks!!!",
    "comments": [
      "Super cool! I have a dead Xbox one that would be awesome to do this with!",
      "Pretty neat. In the future, you could get a low profile GPU which would make space for better cooling to keep those temps down. They're spicy hot under load.",
      "I wanted a low profile arc a380, in the video I mention that they don't have them in the US.... for now :)",
      "About 700$ for absolutely everything all said and done. And it's mainly for streaming so it's very overkill in the spec department, I could've even done a pentium and would've been fine, the gpu does all the work",
      "Update 6/7/2023\n\nQuestions answered\n\nThermals: aren't great, BUT I am planning on flipping the cpu fan as exhaust, and hopefully adding a exhaust fan next to the end of the PSU to correlate with the airflow path of the cpu fan being exhaust, I did already flip the intake noctuas to exhaust and it made the GPU worse, but the CPU a little better\n\nPrice: 700$ all said and done, I could've done this for under 500$ but I had parts from my last streaming PC I wanted to keep (gpu, ram, ssd)\n\nSize: xbox one is 7.2 liters \n\nPsu: I want a HDplex PSU, I'm waiting on them to release a higher wattage variant, but it hasn't come out.. yet.. but has been announced! (Will cut down size significantly) also might have to sleeve my own cables since this psu is proprietary \n\nThe back: I know the GPU doesn't look great from the back, the power cable barely presses it up so it looks canted, I plan on adding washers on the standoffs the riser cable is mounted to, which will even it out and is a super simple fix\n\nWhat I cut: I used a Dremel and cut the back of the xbox and the old hardware standoffs in the xbox.. and that's literally it, the power cable cutout in the back I used side cutters which wasn't great because it looks sloppy, and I borrowed the Dremel which I will clean up with in the future\n\nFuture Plans: improve thermals. Upgrade CPU to a 11400 just so I can play more games and do a little more video production,\nUpgrade GPU to rtx 4050/4060 low profile / blower style (I just want AV1 encoding, and being able to have a to go gaming box would be sick)",
      "Pretty dope!!",
      "Rad",
      "That’s pretty freaking cool",
      "How much was this all together?",
      "Bow does it feal to be in the top one per cent of the world.",
      "Cool (even if it's pretty hot)",
      "amazing",
      "I've always wanted to do this with an old console that doesnt work anymore.",
      "Very cool project, been thinking of doing something similar with my old one/the original modded xbox ones I got laying around. I'd be turning them into a NAS though,",
      "Nice! I built a similar rig in 2017 with a 970 and 3570k. It’s honestly the perfect form factor.\n\nYou’ll likely see much improved thermals with a blower style card.",
      "Well done, really good job bro",
      "Use it to emulate Xbox game",
      "Absolutely epic! I am Xbox owner as well, so this clicked with me right away!",
      "Thanks! I could've chose a proper sff case, but having stupid financial decisions comes out to some interesting ideas",
      "For sure share the link to those drive bays if you can :3c"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "a380"
    ],
    "title": "Intel A380 USA Release August 22, 2022?!",
    "selftext": "So Newegg has the Asrock challenger version of the Intel A380 listed for $139.99 on back order with an ETA of 08/22/2022.  It's shipped and sold by Newegg, it's not a 3rd party seller on the site, so this seems pretty legit.  Last time I looked at this listing, it was just listed as \"out of stock\".  So it sounds like it's actually getting a USA release next week, and yes it's actually letting me put it in my cart.    \n\n\nListing is here:  [https://www.newegg.com/asrock-arc-a380-a380-cli-6g/p/N82E16814930076?Description=intel%20a380&cm\\_re=intel\\_a380-\\_-14-930-076-\\_-Product&quicklink=true](https://www.newegg.com/asrock-arc-a380-a380-cli-6g/p/N82E16814930076?Description=intel%20a380&cm_re=intel_a380-_-14-930-076-_-Product&quicklink=true)  \n\n\nIt's actually here.  At $139 I might pick one up for it's AV1 encoding features, I'm optimistic that this can be used as a second GPU to use for AV1 encoding while using a more powerful one for everything else.  If it actually comes out next week, I'm sure we'll find out a lot more soon.  \n\nHaving it released to a larger audience should be useful for getting more feedback on drivers and hopefully speed up the process of those being optimized.  This is kind of exciting, now I want to see the more powerful A750 and A770.",
    "comments": [
      "100% agree. Looks to me like what theyre doing is releasing the card that people will buy and wont really expect any crazy good game preformance out of it and using that to get user feedback on drivers, probably so that when they release flagship there are less driver problems and have a comparatively smoother launch. \n\n&#x200B;\n\nEveryone needs to remember that intel getting into the gpu space is a good thing! more options and more competition driving prices down, and who knows maybe one day you can buy and intel gpu made in america.",
      "what's the alternative? at this point people know what they are getting themselves into buying one. If you don't want to pay to be a beta tester that's totally fine but at least they are persuing this route so that people who are ok with it can use them, help them iron out the bugs and so you, the customer that is waiting, might have to wait less time for a better product.\n\n&#x200B;\n\nIntel is getting an insane amount of shit for these issues theyre having and people need to understand that this will likely be 1st gen bugs (probably rolling over to later generations but not as bad) and that creating a product like this from the ground up is insanely hard.\n\n&#x200B;\n\nWhile i respect your outlook on the situation, you kinda gotta look at the bigger picture. All this negativity generated around these cards isnt exactly going to encourage intel.",
      "like i said, you could but that pushes back your launch date",
      "So, in other words they want people to pay to be beta testers for their drivers.",
      "There is no 8 GB version.",
      "This fits into systems with a PCIe slot, so anyone that has wanted to use AV1 in some capacity now reasonably can. \n\nI  don't think Plex supports AV1 yet, but Jellyfin does, and it would help a lot of people format shift their stuff into a more space and bandwidth-saving container while things are being better prepared on the client side.\n\nAV1 aside, some people might also want to make use of Quick Sync on a desktop that doesn't support it.",
      "100%",
      "That certainly is a lot of projection. The card will have uses other than just GPU and isn't even aimed at any segment Nvidia nor AMD are worried about, related like its encoding, RT Cores and base 96bit interface makes it viable for a lot of older system integration for low cost. While  no one thinks this is where they wanted to be this isn't the worst place just their starting point, which again is a good thing overall for consumers which keeps getting lost  in translation.\n\nCheers",
      "I got a tickle in my Pp when you said gpu made in America",
      "I'm right there with you being unsure of any actual use case, but some people might want to do... something with it.\n\nNot sure what that is but it'll be there soon enough.",
      "Why are people simping for AV1 encode/decode? Literally nothing uses it yet. YouTube, Twitch, Etc. Are all still H264 and will be for the next few years.",
      "Pretty expensive for a last gen card that sucks up a bunch of power and has tons of bugs. But to be fair, that's their entire lineup. They need 3060 (MSRP $329) performance for $189 - but apparently thats their A770 model (with bugs and more power). Maybe we can sell the cards in 20 years to retro gaming enthusiasts, kinda like voodoo cards.",
      "There are no cards on the market that can saturate x8 bandwidth on PCI-e gen 4 or newer.",
      "Lmao",
      "already been done for years, no one is ever forced either.",
      "It can take the load off of other components when encoding streams for example",
      "OBS can use AV1 for recording, that's a big deal for creators.  Encoding matters for recording far more than it does for streaming. With Youtube allowing bitrates up to 51K, the current encoders are sufficient for streaming 4k 60FPS.  So it's not a feature that's really needed for streaming, AV1 however is very useful for recording stuff at higher quality while taking up less storage space.  This can speed up rending times, upload times, etc.  For creators, time savings = money savings.",
      "You can record with it though.  AV1 can deliver a higher quality at a lower bitrate.  This can result in significant savings in storage, reduce rendering times etc.  If you're a creator, saving storage space and time is worth spending extra money for.",
      "It's a bit of a 'chicken and egg' situation. Do sites work to support AV1 before there are any AV1 encoders or do HW manufacturers support AV1 encode even if there aren't any sites that support it (for streaming at least). Intel has effectively broken that 'chicken and egg' situation and now it's on sites to support it.\n\nTwitch doesn't support it currently, but they have [talked about that they are working on it](https://www.nvidia.com/en-us/geforce/news/rtx-30-series-av1-decoding/), and there is even a [test VOD](https://www.twitch.tv/videos/637388605) on Twitch that uses AV1 and demonstrates 1440p 120Hz streaming. It's only a matter of time before Twitch supports it.",
      "So to sum things up.\n\nThere is a chipshortage. \n\nA chip nobody needs (yet) will be shipped.\n\n/s"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "celeron g6900 bottleneck",
    "selftext": "hi guys, just a question, a celeron g6900 will bottleneck a arc a380 at 1080p?",
    "comments": [
      "I did something similar years ago, but I'd really stretch for a 12100f or 13100f. You'll get tons more runway out of them.",
      "It's usually a bad idea to try to game on a 2 core/2 thread CPU these days, unless you specifically only play old/undemanding games. Many games won't launch or won't play properly on 2 core/2 thread CPUs. GTAV is a popular example.",
      "I’d go AM4 at such a tight budget. Used parts are incredibly cheap on that platform and hold up relatively well even the oldest chips",
      "I have both the G6900 and Pentium in machines, if you haven’t already bought the G6900, the Pentium does much better with hyper threading. But in the end a 12100 would be a far better choice with four real cores. The G6900 is ok for basic tasks or a NAS, not much more.",
      "In your case, if you don't mind AMD, I think it makes more sense to go with AM4 since you could go with an 1st/2nd gen Ryzen (You're likely going to be GPU bottlenecked even with these old CPUs) then upgrade to something like a 5600(X) when you have the money.\n\nIf you do go with a g6900, it's going to be a pretty awful experience since it's going to be a stuttering mess, at least until you replace it; The \"bare minimum\" 7-8+ years ago was considered a 2C4T CPU, the G6900 is even worse than that.",
      "You shouldn't pair ARC with a weak CPU, ARC's drivers have higher overhead.",
      "G6900 is not a good idea. 2C2T in 2022, as amazing Golden Cove is, will not do it.\n\nGet at minimum a 12100F, or if you can’t spare it, then maybe one of those 4500 or 5500 that have been going on sale for pretty cheap.",
      "Resizeable Bar (AKA \"SAM\") is a pcie capability, as long as it was added to the motherboard bios, I see no reason it wouldn't work.",
      "Why do you specifically want a380, for AV1 encoding? If you need AV1 encoding, you can get the cheapest ARC GPU and get an Nvidia or AMD GPU for gaming. Otherwise, 12100+a380 is a bad choice IMO. The GPU is forcing you to buy a slower CPU just for its iGPU.",
      "In that case, a380 is the better choice for productivity. But 12100 isn't, it's only 4 cores.\n\n> but in a video i saw that the arc a380 has some problem that if you don't have an igpu you can't install the drivers\n\nI don't know anything about this, couldn't find any info. Maybe it's not a common issue. People have been using arc GPUs on Ryzen systems without issues.",
      "Yes, also why would you buy a G6900 on socket 1700?",
      "I mean I guess OSRS or EQ1 or something not run ok on that chip...maybe.",
      "the is that: i have to buy the parts in physical store and in italy the g7400 cost like 115€ and i3-12100 like 150€ so i want to buy the celeron (only 80€) and take it for 2 months and next i wont to buy a i5.",
      "Would those even work with an A380 that well though since they don’t support Resizable BAR? OP should at least go with 3000+ in that case.",
      "to have the new platform and to be able to update in the future, if it bottlenecks I will take an rx 6400 but it is much more limited, so I was thinking of the a380, so I use this processor for a short time then I want to change to an i5.",
      "Unless the machine is desperately needed... why not just wait until you can afford what you really want?",
      "> i want to buy the celeron (only 80€) \n\nThat's not an insignificant amount of money. If you don't have a big budget, AMD 5600 is the best choice, unless you have intel specific requirements. It is a much better CPU and it costs well under $150 in most regions. You can combine it with a b450 board for around 200€ and nothing can beat that value (physical stores might have mark-ups, so I'm not sure about the price).",
      "yes I know that AMD costs less but they told me that it causes problems, I didn't understand which ones but to avoid the risk...",
      "No problems with ~~560~~Ryzen 5600+b450. It's solid. Only common issue was usb disconnects with \"some b550 motherboards\" but that's mostly solved.",
      "ok i changed the configuration and i managed to put a 12100F on it, but in a video i saw that the arc a380 has some problem that if you don't have an igpu you can't install the drivers, i just can't find a 12100 that costs little, it's true this problem?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "a380"
    ],
    "title": "A380 QSV encodes worked fine, then suddenly \"Failure to initialise thread 'Quick Sync Video encoder (Intel Media SDK)\" error",
    "selftext": "This is somewhat of a cross-post from a similar inquiry on the Handbrake Community support forum, but as the issue seems to be Intel-related, I figure I'll post here as well since I \\*really\\* can't figure out what's going on.\n\nI'll try to be brief, but this is still likely to run a bit long...\n\nI got and installed an Asrock Challenger A380 GPU a couple of days ago and it's worked GREAT with Handbrake QSV encoding (h264, h265, AV1) - MUCH faster than an iGPU.\n\nAfter finishing an h265 QSV encode, I attempted to do another one literally 15 minutes after that last one - and that failed outright. I tried multiple times (after reboots, just to \"clear the decks\"), with the same results. The Handbrake logs point to this error:\n\n*qsv\\_hevc\\_make\\_header: MFXVideoCORE\\_SyncOperation failed (-17)*\n\n*encqsvInit: qsv\\_hevc\\_make\\_header failed*\n\n*Failure to initialise thread 'Quick Sync Video encoder (Intel Media SDK)'*\n\nI upgraded Handbrake to the latest version and tried again, with identical results. I then did  MULTIPLE graphic driver uninstalls / reinstalls (both using DDU and the regular way) - covering the gamut  from SAFE MODE and 'regular' uninstall / reinstalls; all with  corresponding Registry cleanups, to no avail. This is using the latest Intel Driver Support Assistant-vetted driver 'igfx\\_win\\_101.1743'\n\nSo... did the QSV capability on the GPU just \"crap out\" (is that even possible)? It seems to point to that since I also tried QSV encodes in h264 and AV1 and those essentially returned similar 'failed to initialise thread Quicksync Video encoder' errors. I should note that I've also successfully completed h264 and AV1 QSV encodes yesterday on this same hardware.\n\n Any insight would be MOST helpful!",
    "comments": [
      "That feature also uses QSV, it might provide more insight if the issue is with Handbrake or with the GPU itself...",
      "While I can't offer a definitive solution, try the latest beta graphics driver (.3278) rather than the one recommended by the support assistant. The one it suggested is several versions old, despite being fairly recent. They've been pushing out rapid driver releases due to the, uh... surplus of software testing opportunities which Arc has presented.\n\n[https://www.intel.com/content/www/us/en/download/729157/intel-arc-graphics-windows-dch-driver-beta.html](https://www.intel.com/content/www/us/en/download/729157/intel-arc-graphics-windows-dch-driver-beta.html)",
      "Have you tried using the latest handbrake nightly? From previous reviews, Intel's latest updates and fixes are in there.",
      "To follow up on my prior response to your post, I went into Handbrake settings, specifically the \"video\" settings, and UNCHECKED \"Enable Low Power QuickSync Hardware (where supported)\" and it's now successfully encoding in h.265 again.  Will try AV1 after that using QuickSync.\n\nJust FYI, I have Handbrake as well as Handbrake \"Nightly\" both installed.  I opened both programs and changed the settings just in case, and clicked \"exit\" rather than the \"x\" box to close the window, just to make sure that the settings update for Handbrake/Nightly.",
      "A lot can happen in 15 minutes! For example, there was a time (and presumably, still is) where Windows Update was reverting Intel Xe/iGPU drivers without user interaction, as it treated any more recent betas as an old version. There have been similar issues with AMD GPUs as well.\n\nOne other thing to check would be for some kind of software change/bug in \"waking\" the encoder core. In Handbrake, I *believe* there's a low power state toggle under Preferences / Video for QuickSync. If low power encode is enabled, disable it, relaunch and try it one more time to see if it properly gains consciousness.\n\nI remember dealing with something similar a few years back but I don't remember what finally sorted it out. Driver or bus/encoder power state toggle seemed vaguely familiar, though. Hopefully you find a fix!",
      "If it helps, this is a ground-up new-build PC specifically for QSV AV1 encoding. It does NOTHING else, literally just has whatever Win10 comes with + Handbrake.\n\nFoundation specs:\n\ni5-12400 (iGPU disabled in BIOS) | Asrock B660 ITX/AC mobo, latest BIOS installed, REBAR enabled, XMP enabled | 2x8GB T-Force Vulcan Z DDR4/3000 | Crucial P2 NVMe SSD |Asrock Challenger ITX A380 GPU",
      "UPDATE: I managed to snag an A750 and have now installed it in the same system which previously hosted the A380.\n\nDid a fresh Win11 H2 install with latest ARC drivers and using latest nightly Handbrake build.\n\nSo far so good...",
      "Did you tried to record desktop/game using Intel® Graphics Command Center Capture?",
      "I unfortunately have no ideas for you, but am just wondering if you could link your post in the handbrake forum; I'm actually considering doing the same thing as you and am curious what the handbrake folks say about it.",
      "After some (semi)exhaustive testing, I'm 99% sure that \"something\" in the GPU did indeed go wonky for whatever reason. I arrived at this by doing a System Restore to the Day One state of the PC (nothing installed, no drivers, etc). Installed the vetted Intel graphic drivers, installed Handbrake+dotnet, rebooted, then tried QSV encode again - and it fails with the same error. So it's not some rogue software silently installed/updated by windows - recall that the GPU worked fine up until it didn't yesterday.\n\nDid Day One System Restore again, shutdown, unplugged the GPU (so only the iGPU would be active), booted and installed the Intel graphic drivers + Handbrake+dotnet again, rebooted, and tried QSV encode again - and that worked!\n\nThen another Day One System Restore, shutdown, re-plugged the GPU (this also involved re-seating the GPU, in case that was an issue somehow), booted to BIOS and made sure iGPU was disabled, did all the driver and Handbrake installs, tried QSV encode again - and that fails with the same exact error.\n\nUnless I'm missing something, that's a fairly definitive indication (to me) that the GPU has gone wonky for some unknown reason in those 15 minutes when QSV worked, and then didn't.\n\nI've filed an RMA for it, but considering every retailer carrying these cards is back-ordered, who knows when I'll get a replacement.\n\nTLDR: this doesn't seem to be an SDK or software or Handbrake issue, but definitely something with the GPU.",
      "Do you see any error messages in the event viewer in the system category?",
      "I have a similar setup (Asrock B660 + A380) and had the same thing happen; worked one minute \"dead\" the next. When I checked in Device Manager it showed that it lost the drivers for both dGPU and iGPU, and reinstalling them didn't fix anything. I'm curious if you find out anything. The latest thing I tried (didn't work) is installing the intel media SDK, but I couldn't get a hold of the 2022q2 version, just the release before that, even though it is listed on their github. I'm on the latest Win11 btw.",
      "First time posting on Reddit.  I've had a similiar if not identical issue.  Nothing changed on my system, at least not anything that should have changed Quicksync, and now I'm getting the same error message.  I'll copy and paste it below:",
      "Here's the pertinent portion of the log:",
      "JtFYI, I have Handbrake as well as Handbrake \"Nightly\" both installed.  I  \n opened both programs and changed the settings just in case, and clicked  \n \"exit\" rather than the \"x\" box to close the window, just to make sure   \nthat the settings update for Handbrake/Nightly.",
      "Yup, tried that too, no go.\n\nI'll also paste what I posted in response to u/somethingknew123 \\-\n\nAgain, we're talking a span of 15 minutes :)",
      "Yup, did that too, no difference.\n\nAnd not to be ungrateful for the (or any) comment, but I'd still be bothered (if that had somehow fixed it) by the fact that it was chugging along fine with \\_no\\_ changes between when it last worked and when it suddenly didn't.\n\nAgain, we're talking a span of 15 minutes :)",
      "no because that has nothing to do with what I use the GPU for",
      "It's not the forum in Reddit, but at Handbrake themselves. I'm not sure if Reddit will retain the URL but here it is:\n\nhttps://forum.handbrake.fr/viewtopic.php?t=42494",
      "I'm not so sure it's your particular GPU.  Mine was also working fine, encoding great using AV1 QuickSync, h.265 or what have you...basically matching the speed of my 3090 for h.265, but now I'm getting the following error messages:\n\nencqsvInit: MFXVideoENCODE\\_Init failed (-15)\n\nFailure to initialise thread 'Quick Sync Video encoder  \n(Intel Media SDK)'\n\n\\# Job Failed to Initialise. Check log and input settings (3)\n\nIf I understand your post correct, when you physically took the Arc out of the PCIe slot, QSV worked fine (albeit just from the CPU), but when you plugged it back in, it stopped working?  And this is even after a complete restore?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "Resizable BAR Z170 with A380 on Linux (PVE)",
    "selftext": "I installed an Arc A380 6GB in my Gigabyte Z170X Gaming 3 Motherboard with a i7-6700. I was reading there are UEFI mods([REBarUEFI](https://github.com/xCuri0/ReBarUEFI)) to enable resizable BAR in older chipsets, while some don't require it if using linux AND the mobo has the '4G decoding' option. I'm wondering if anyone else has tested this with linux or knows how to check if it's working. My motherboard UEFI has a '4G decoding' option that I enabled, but not sure if that is sufficient. The card works, and I'm using it for transcoding with jellyfin on proxmox. Just not sure if I'm getting the full performance with ReBar enabled. This is the result of my `lspci -v` :\n\n>03:00.0 VGA compatible controller: Intel Corporation DG2 \\[Arc A380\\] (rev 05) (prog-if 00 \\[VGA controller\\])  \n>  \n>Subsystem: ASRock Incorporation DG2 \\[Arc A380\\]  \n>  \n>Flags: bus master, fast devsel, latency 0, IRQ 149  \n>  \n>Memory at ee000000 (64-bit, non-prefetchable) \\[size=16M\\]  \n>  \n>Memory at d0000000 (64-bit, prefetchable) \\[size=256M\\]  \n>  \n>Expansion ROM at ef000000 \\[disabled\\] \\[size=2M\\]  \n>  \n>Capabilities: \\[40\\] Vendor Specific Information: Len=0c <?>  \n>  \n>Capabilities: \\[70\\] Express Endpoint, MSI 00  \n>  \n>Capabilities: \\[ac\\] MSI: Enable+ Count=1/1 Maskable+ 64bit+  \n>  \n>Capabilities: \\[d0\\] Power Management version 3  \n>  \n>Capabilities: \\[100\\] Alternative Routing-ID Interpretation (ARI)  \n>  \n>Capabilities: \\[420\\] Physical Resizable BAR  \n>  \n>Capabilities: \\[400\\] Latency Tolerance Reporting  \n>  \n>Kernel driver in use: i915  \n>  \n>Kernel modules: i915\n\nIt looks like it sees the card is capable, but the 'Memory at' showing size=256M seems to indicate it can't see the full amount",
    "comments": [
      "Did you get to figure this out?\n\n&#x200B;\n\nI'm pretty much having the same config as you, same CPU, MOBO and GPU. but instead of Jelly I'm on Plex.   \n\n\nI havent managed to get the system using the ARC 380  for transcoding :(",
      "What did you have to do to get your PVE host to detect your A380? I'm having a hard time getting mine to be recognized. Currently on PVE 8.1.4.",
      "Well, I know people have more issue with Plex than Jelly with newer hardware. I'm able to use the card to transcode just fine. I'm actually on Proxmox and passing it through to a privileged LXC container running debian 12. My only issue is having the OS/Mobo utilize resizable bar with the GPU to get the full performance. Now, I don't have much to compare it to see if I'm actually taking a performance hit and how much, but based on the attached lspci output, it looks like it is not utilizing the ful resizable bar. I found more info, but it looks like I'll have to do a bios mod to get it working with full resizable bar. Right now I get about 95fps for a 4K HEVC 10bit > 1080P transcode, which is more than enough for my usecase, so I will most likely just keep it as-is and see if I get a boost once I upgrade my server to a newer generation platform that fully supports ReBAR.",
      "I don't recall having any issues with the host detecting it. What are you using to view it?\n\n    lspci -nnv\n    \n    VGA compatible controller [0300]: Intel Corporation DG2 [Arc A380] [8086:56a5] (rev 05) (prog-if 00 [VGA controller])\n\nDo you see this under your lspci?",
      "Oh, did you install any drivers?\n\nI followed the jellyfin guide here: [https://jellyfin.org/docs/general/administration/hardware-acceleration/intel/#lxc-on-proxmox](https://jellyfin.org/docs/general/administration/hardware-acceleration/intel/#lxc-on-proxmox)\n\nThe first step is to install the drivers on the host.\n\n    sudo apt update && sudo apt install -y intel-gpu-tools\n\nThen try:\n\n    sudo intel_gpu_top",
      "In my case, when I'm running the following command intel\\_gpu\\_top it doesn't detect the Arc 380.  Even being in Debian 11 with a back ported Kernel 6.5.9\n\n&#x200B;\n\nroot@bignas:\\~# intel\\_gpu\\_top  \nFailed to detect engines! (No such file or directory)  \n(Kernel 4.16 or newer is required for i915 PMU support.)\n\nroot@bignas:\\~# uname -r  \n6.5.9-zabbly+",
      "You're on baremetal Debian 11 or is it a VM or LXC? I'm running proxmox with kernel 6.2.16-12-pve, which is what the LXC uses as well.",
      ">baremetal Debian 11\n\nYeah baremetal Debian 11. Checking if maybe it's the kernel installed the one causing the problems",
      "OK, I enabled something on the BIOS and now I get some output   \nroot@bignas:\\~# vainfo  \nerror: XDG\\_RUNTIME\\_DIR not set in the environment.  \nerror: can't connect to X server!  \nlibva info: VA-API version 1.17.0  \nlibva info: Trying to open /usr/lib/x86\\_64-linux-gnu/dri/iHD\\_drv\\_video.so  \nlibva info: Found init function \\_\\_vaDriverInit\\_1\\_10  \nlibva info: va\\_openDriver() returns 0  \nvainfo: VA-API version: 1.17 (libva 2.10.0)  \nvainfo: Driver version: Intel iHD driver for Intel(R) Gen Graphics - 21.1.1 ()  \nvainfo: Supported profile and entrypoints  \nVAProfileNone                   : VAEntrypointVideoProc  \nVAProfileNone                   : VAEntrypointStats  \nVAProfileMPEG2Simple            : VAEntrypointVLD  \nVAProfileMPEG2Simple            : VAEntrypointEncSlice  \nVAProfileMPEG2Main              : VAEntrypointVLD  \nVAProfileMPEG2Main              : VAEntrypointEncSlice  \nVAProfileH264Main               : VAEntrypointVLD  \nVAProfileH264Main               : VAEntrypointEncSlice  \nVAProfileH264Main               : VAEntrypointFEI  \nVAProfileH264Main               : VAEntrypointEncSliceLP  \nVAProfileH264High               : VAEntrypointVLD  \nVAProfileH264High               : VAEntrypointEncSlice  \nVAProfileH264High               : VAEntrypointFEI  \nVAProfileH264High               : VAEntrypointEncSliceLP  \nVAProfileVC1Simple              : VAEntrypointVLD  \nVAProfileVC1Main                : VAEntrypointVLD  \nVAProfileVC1Advanced            : VAEntrypointVLD  \nVAProfileJPEGBaseline           : VAEntrypointVLD  \nVAProfileJPEGBaseline           : VAEntrypointEncPicture  \nVAProfileH264ConstrainedBaseline: VAEntrypointVLD  \nVAProfileH264ConstrainedBaseline: VAEntrypointEncSlice  \nVAProfileH264ConstrainedBaseline: VAEntrypointFEI  \nVAProfileH264ConstrainedBaseline: VAEntrypointEncSliceLP  \nVAProfileVP8Version0\\_3          : VAEntrypointVLD  \nVAProfileHEVCMain               : VAEntrypointVLD  \nVAProfileHEVCMain               : VAEntrypointEncSlice  \nVAProfileHEVCMain               : VAEntrypointFEI  \n\n\nroot@bignas:\\~# ls -l /dev/dri  \ntotal 0  \ndrwxr-xr-x 2 root root        120 Nov  3 16:03 by-path  \ncrw-rw---- 1 root video  226,   0 Nov  3 16:03 card0  \ncrw-rw---- 1 root video  226,   1 Nov  3 16:03 card1  \ncrw-rw---- 1 root render 226, 128 Nov  3 16:03 renderD128  \ncrw-rw---- 1 root render 226, 129 Nov  3 16:03 renderD129"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "a380"
    ],
    "title": "Anyone here with an A380 that can test DPC latency with Latencymon?",
    "selftext": "I primarily use my computer for music but do other things with it as well and would like to add a lightweight gpu. Before I pick up a 6600 (AMD is know for low DPC) I would be interested in knowing what DPC numbers are like with ARC.",
    "comments": [
      "Running it now. I'm not very familiar with this benchmark, do you have any specifics for what you want to see? Or, how should I present the results?",
      "I know it’s not specific to AMD. Different gpu drivers can produce different dpc figures (which can interrupt audio tasks). Nvidia drivers are known to produce higher figures in comparison to AMD, and I’m curious as to where ARC stands with this.\n\nhttps://gearspace.com/board/music-computers/1212416-dpc-latency-better-amd-graphic-cards-3-card-comparison.html\n\nhttps://vi-control.net/community/threads/i-swapped-my-nvidia-card-for-an-amd-radeon-and-now-my-daw-is-running-much-better.71482/",
      "Is this the page you're looking for? [Pass1](https://imgur.com/t4j34vA) [Pass2](https://imgur.com/7cwDhP4)",
      "Deferred Procedure Call? If so this doesn't have anything to do with AMD specifically. DPC is a MS operating mechanism allowing for high priority tasks to override low priority tasks as needed. For most folks working in audio and computers DPC can be an issue but its rarely if ever solved by a GPU in of itself. What exactly is the intent and desired purpose/outcome one is looking for here. Without understanding that how are we to recommend or not?",
      "Thank you so much! I know this is a lot to ask but if you have the time to do this - close all apps, let latencymon run for a couple min and then just screen capture the results (two passes preferably). I don’t blame you at all if this is an annoying request.",
      "Just my two cents here, I think he was speaking about DPC with regards to a AMD-GPU, since it's known that AMD has fewer issues with latency as opposed to NVidia, which have been known to be at times horrendous due to unknown changes in their GPU-drivers. It may not solved by a GPU, but for sure a GPU can be the issue for it.",
      "This is exactly it",
      "This is perfect! Thank you very much again",
      "True, Windows 10 itself introduced a nice shipload of especially audio-issues on that.",
      "Speaking of AMD, my monitor going to and out of sleep causes audio stutter.",
      "DPC isn't a AMD or Nvidia thing specifically. It has nothing to do with either in of itself. It certainly can affect various components though. This is why more information is needed as posted otherwise we spin our wheels and no one gets helped."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "A380 World of Tanks Benchmarks",
    "selftext": "Hello,\n\nI'm building my first PC in the coming month or two and I need some advice. I only play two or three games, one of which is World of Tanks. I can't seem to find any benchmarks for it using the Arc A380, however, which is a card I'm very interested in due to its price and its looks/form factor.\n\nI play at 1080p and I'd love to hold a stable 60-80 fps at medium settings. Is this possible with the A380? Or do you know where I can find this information?\n\nAssuming all the other specs of the computer can handle it, and I've ensured that they can.\n\nThank you for your help!",
    "comments": [
      "I don't think you'll be able to achieve 60 fps with that unless you slightly lower the graphics.\n\nThere's an Encore benchmark on YouTube and its usually at 57-59fps.",
      "That test was on ultra, plus the drivers came a long way since then."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "Any A380 users here?",
    "selftext": "I'm undecided on whether to get a Arc A380 or GTX 1660 Super. \n\nThe arc would be brand new, the 1660S would be second-hand.\n\nThe plan is 1080p 75hz gaming. Bigger on JRPGs... And Dota 2. Probably want an Android emulator to run Arknights. Potentially wanna dip my toes into streaming a little, some video editing and I feel the Arc's AV1 encoder could do me good.\n\nBut I know about the Arc's unoptimized drivers. I do plan to delve into some older titles via emulators, so I'm not sure if the Arc would be able to run them decently enough. \n\nI've had a 1660S in the past, it's a great budget card and I know it'll do well gaming-wise. What would your thoughts be on this?",
    "comments": [
      "Get a 1660 super. Intels arc drivers are still buggy and have inconsistent performance. The a380 is not a good card. Nvidia's drivers are way better and also the 1660 super doesn't need resizable bar",
      "1660s is a lot more powerful, but is missing RT and upscaling support, if that matters to you. That said, my A380 did surprisingly well for me when I was messing around with it in a few games. It was even perfectly playable in SOTTR with the medium preset, + medium RT shadows, + XeSS, at 1440p. \n\nBut it's still an entry level card vs. an older low midrange card. The 1660s should wipe the floor with it overall.",
      "a 1660 super walks all over an a380, not even a competition",
      "Overall gaming usecase, I'd say 1660S. \n\nFor Android emulator, if you're in a region where [Google Play Games for PC](https://play.google.com/googleplaygames) beta is available, Arknights should be able to run well regardless of both GPU. Don't use WSA though, that one currently has terrible GPU performance where I can run Arknights at 300-600MHz on Google Play Games with RX 480 while WSA stutters too much even at 1300MHz",
      "1660S, it's just the better card.",
      "1660 Super hands down",
      "generally u want to go nvidia if u are looking to emulate anything. a380 is too new imo"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "a380"
    ],
    "title": "Do I have to have rebar support on server",
    "selftext": "Hey all! Simple question \nDo I have to have rebar support on my media server (running jellyfin) if I want to use an arc card for encode/decode?\nI’m running a 4th gen i5 in it currently and was thinking about grabbing an a380 for av1 video. Will my encode/decode performance get crippled? Will it work at all?",
    "comments": [
      "If you are using it solely for encoding or decoding, in my experience rebar does not need to be enabled.",
      "Will it work at all? Check out this page and follow instructions to see if that option is available in your BIOS.\n\nhttps://www.intel.com/content/www/us/en/support/articles/000090831/graphics.html"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "a380"
    ],
    "title": "Intel A310/A380 as 2nd GPU for encoding ?",
    "selftext": "Hello everyone, not sure if I'm really gonna get a definitive answer here since my personal research weren't very successful, but I'm asking anyway : do you think an Intel A310 is a good choice for the sole purpose of encoding video, more specifically AV1 ?\n\nI work in the video industry and my machine is already pretty good for editing and encoding what I need. But when I sometimes want to record gameplay sequences on my PC with a good video quality, my poor 3060Ti is often overwhelmed by both rendering the game and encoding in real-time, which leads to choppy footage. Operating a second GPU to encode could free me from this hassle, and not have to worry about limiting framerate/balance settings, and always have a nice clean output everytime I need it.\n\nBesides, I'm also thinking that encoding all my video projects in AV1 instead of the good'ol H.264 would both take less space and offer a higher quality output. Since I have no hardware encoder for AV1, I was thinking it could be worth the investment to go buy a low-end arc GPU for this purpose, preferably the a310. And more generally, make it my go to encoding card for everything I need to render, as it might be faster than my current hardware.\n\nBut this is where I'm doubting if this is a good idea or not. For two reasons :\n1. I've not found any benchmarks focusing on the encoding capabilities of the a310, in term of speed and visual quality. I don't know if the encoder inside of it is as good as the a770 or even a380.\n2. The Arc GPUs are not even available yet in my country (apart the a770 in only one store, at 450€, ouch...), and taking into account the dollar/euro conversion and taxes, it might be a pricy upgrade, or at least pricier than I would want.\n\nNow with that said, what do y'all think ?",
    "comments": [
      "Been curious about this + having the Intel run/process video/streams/youtube on my second/third monitors.  \n\nI haven't dug too deep yet considering availability, but I'll need to see how Nvidia & Intel drivers/hardware play together side by side.  \n\n\\+Ideally the primary card can full idle when this 2nd/3rd monitor workload is active. Should lower total system power? Only time will tell.",
      "Thanks for your answer!\nI would have thought that AV1 was better quality than H.265 for video production, given that it is theoretically superior in term of codec efficiency. However, maybe the encoder in the Arc GPUs is not yet optimized, like what we had with the first gen NVENC that was not that good.\n\nWell it certainly decrease the value of such a purchase for me. But still, AV1 have the advantage of not being a proprietary codec, so in the future I expect it to be way more universal than H.265.\n\nFor the rest, I sadly doesn't have an iGPU, because I run a Ryzen 5900X. So, I could potentially do software encoding, but in my experience, it is substantially less efficient than encoding with my GPU. It is a solution in some games where I'm really GPU limited and have a lot of CPU headroom though. Besides, I've had troubles with OBS studio, often dropping video framerate when the CPU was solicited more heavily in game. I could again dumb down quality settings to have a very stable recording, but this is precisely the type of compromises that had me wanting a A310 dedicated to this type of tasks.\n\nIn the end, I understand that it is probably a bit unnecessary as for now. So, I'll wait, maybe for the next gen? For the moment, I'll just continue to work with what I have, and bear with the frustration ahah",
      "I've never considered this option but theoretically it could be lower than using a big Nvidia GPU for decoding video. I don't think it would be more power efficient than just using one GPU for everything tho, I suppose the idle power consumption of the primary GPU would compensate the gain easily.\n\nIn practice however I have no idea how that would work. I don't know if Windows (no idea if it is possible on Linux) handle very well to have two monitors connected to different GPUs on the same system. I suppose you would deal with some compatibility issues over time depending on the utilisation you make of your two conjoined GPUs, regardless of the screens. This is also one thing I'm afraid of, considering I would be running two very different cards on one PC, even if it is for a specific usage."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "a750"
    ],
    "title": "Intel Arc A770/A750 mark two-year launch anniversary",
    "selftext": "",
    "comments": [
      "I hope they will come up with the next gen GPUs soon",
      "You don't have an Arc GPU if you say this, the improvement has been amazing, the stability and compatibility has been improved so much since launch.",
      ">And they did nothing about the software, Linus in one of his videos told that Intel promised a big revamp, nothing! Liars!\n\nBack to basics. Drivers are software.",
      "Yea they totally care about those 3 people who use linux when fixing drivers of a gaming product (majority of gamers are on windows and the linux statistics are inflated by steam decks)",
      "Can somebody explain the possible reasons they cant launch Battlemage yet?\n\nhardware was ready months ago. And the software is already kinda running on Lunar Lake?",
      "\"hardware was ready months ago. \"\n\nIs there any evidence that this is true?",
      "welp....Ive seen LOTS of people talk nice about the improvements of the drivers.",
      "The architecture is, but that's not the same thing as the whole BMG themselves.",
      "If Arc could one day meet the performance of the 70 or 80 series Nvidia GPUs, I'd consider going a full Intel build on my next gaming PC.\n\nThey've got a long way to go.",
      "And A became B",
      "It will be out soon. We are using it internally",
      "well thats a fair Point.\n\nbut isnt it already running in Lunar Lake?",
      "Why? Statistically at least 15x more people use windows. They'll obviously put all their resources into making functional drivers on windows before making functional drivers on linux. That's like saying app developers should still consider blackberry or windows phones.",
      "My only gripe about my a750 is the lack of VR support.",
      "Arrow Lake using that sweet 2 year old alchemist based igpu.",
      "Update your knowledge",
      "Soon(tm) is somewhat imprecise, but ok...",
      "It will be end of this year",
      "Like you just use Arc control everytime like it is a game, yeah, sure it is annoying that the driver updater not works. But that's pretty much all. I just install the drivers, open Arc Control, change one setting and leave the rest alone.",
      "I do hope Intel delivers Battlemage. Even as Nvidia fans i admit Nvidia have been selling such an overpriced GPU. I don't even doubt if RTX 5000 series will be even more overpriced."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Arc A750 available October 12 for $289",
    "selftext": "",
    "comments": [
      "Arc 750 & 770 come with MW2, and three other games if you join intel access",
      "Copypasting same comment to here as you did on r/hardware it seems. Why would they compare to something with rebate price. That doesn't make much sense.",
      "Some things to note on this comparison. One they're comparing only to the RTX 3060 and not AMD's competition. Two they are taking the **average** price of a 3060 on Newegg and comparing it to the **MSRP** of the A750.\r  \n\r  \nA Zotac 3060 is $380 on Amazon with Prime right now. Likely other even cheaper options too.\r  \n\r  \nI am curious what the numbers would look like if they used the $300 after rebate 6650XT on Newegg as the competition.\r  \n\r  \nAs usual though, wait for independent benchmarks and not Intel's misleading ones which compare price/perf instead of performance.",
      "I guess is the economist on me, but I could not help but smile at their time series chart of prices: 150 dollars for in 2012 they show for a 650Ti, is really equivalent to 200 dollars to todays prices given the latest inflation!",
      "depends per region, AMD and Nvidia still has that \"scalper\" price in my country.\n\n6600XT is at $400\n\nIf this releases at lower price then its better",
      "Makes sense to me to not compare to AMD. The Arc GPUs are more modern like they claim. AI engines and xess, actual ray tracing not done in shaders like amd, and a thread sorting unit. And add to that the way better media encoders and decoders.\n\nAmd has gotten better lately in GPUs, but they're still not as caught up to nvidia as intel is feature-wise, and that's crazy to say.",
      "> 770 basically compares to the 3060 Ti or the 6600XT.\n\nIntel still compares it to the 3060. It's not quite a 3060ti competitor",
      "Is this only if you get an Alder Lake CPU with the GPU? Reading some articles that pointed to this doc [https://softwareoffer.intel.com/Campaign/Terms/617C2EAE-CE89-4199-9805-470E829228A8](https://softwareoffer.intel.com/Campaign/Terms/617C2EAE-CE89-4199-9805-470E829228A8) and it seems like you need a qualifying CPU too unless you're talking about a different offer?\n\nAlso strange that you can't get a Raptor Lake CPU with an ARC GPU in that offer, but I guess it makes sense since it's dated back to August.",
      "Actual ray tracing is amongst the absolute silliest replies I've seen trying to talk up one product against another. It's literally the same code, written and calculated the same. The reality is those 'shaders' are simply capable of doing far more complex and wide ranging calculations and to a greater degree of accuracy while the 'real ray tracing' is just simplified smaller cores that are good for ray tracing and much worse for other things.\n\nYou're implying AMD can't do real ray tracing, real ray tracing was being done back in the 90s on a cpu, just really, really, insanely slowly. Performance is the only metric that matters and we've seen benchmarks and how the Arc gpus really stack up, using a huge die and huge transistor count, power, on a better node all to match dramatically more efficient lower power gpus from Nvidia and AMD.\n\nIn terms of being caught up, this is even more laughable and AMD has pretty much since DX9 been at the fore front of all new technologies, when they are sensible to do so. Ray tracing on a card that performs at this lower midrange level is pretty much worthless and dedicated hardware for it only wastes performance when you don't have RT enabled. That's a bad thing in this range, not a good thing.\n\nAMD don't have dedicated RT cores, because they are still very very wasteful.\n\nAlso did you see the massive range of RT benchmarks Intel pushed to show how much better it was than AMD today, none, because showing 15fps ray traced instead of 60fps without isn't much of a win if your competition's card in the segment gets 12fps ray traced but 65fps without.",
      "I do look for affordable ones like that are on sale or instant rebate (which i never see anymore) but I hate dealing with rebates, always have had terrible luck with rebates so i even pay a little more just to ignore them",
      "Any pics of AIB's?",
      "In the US on NewEgg right now, the 6650XT is on sale for $319, the 6600XT is on sale for $329, and the 6600 is on sale for $239.\n\nWith Arc, you gain:\n\n1. Better encoder \n\n2. Better RT performance\n\n3. Access to the XMX version of XeSS\n\nBut you also lose out on AMD's generally much better driver stability in games specifically. Arc still has issues in this respect, and you can actually see it on some the performance charts they provided.\n\nI think an argument can be made for both GPUs, frankly. At the low and mid ends of the market, stability in games matters. Somebody buying into this section of the market can't just buy another GPU if a game they want to play doesn't work. \n\nBut on the flip side, Arc hold much better potential for the future with all of the extra features it supports, and future driver improvements could also bring signficant performance improvements too. Perhaps in the long run that Arc GPU could compare against a 6700XT/3070. But you're taking a risk by betting on that happening.",
      "A770 is the top end Alchemist SKU, there will be no A780 or A790",
      ">Amd has gotten better lately in GPUs, but they're still not as caught up to nvidia as intel is feature-wise, and that's crazy to say.\n\nLol..you guys..\n\nIts quite crazy how you framed things. They are not closer to Nvidia than AMD. The problem isnt having the features. Its the performance. Period. AMD and Nvidia are launching in a month or two.\n\nIts one thing to praise and encourage intel for what theyve already accomplished and the potential for greater competition in the low end space, but lets be real.\n\nThe targeting Nvidia instead of AMD is because Nvidia is the market leader. They are not going after AMDs meager marketshare. They are going after Nvidias slice of the pie. Intels GPU tech is not superior to AMDs 2 year old tech if we are being honest. At the same time, entering the discrete GPU space and only being 1.5 gens behind isnt all that bad. Its like AMD was with Vega. Its actually a great *starting* point. I mean nobody was expecting them to compete this gen right? They can catch up in 2 gens potentially - coming from almost nothing.",
      "Man, I really hope Intel does well with this, im kinda excited to get one.",
      "Only from Asrock and Gunnir so far\n\nhttps://wccftech.com/intel-arc-a770-arc-a750-custom-graphics-cards-from-asrock-gunnir-displayed/\n\nMSI should have some coming too.",
      "It seems it is you who can't seem to discern between technology and PR/Media speak.",
      "[Offer.](https://softwareoffer.intel.com/Campaign/LearnMore/7c4740d1-7b93-4626-9fbe-d58a7a0a38ef)\n\n[It's a different offer you just need one](https://softwareoffer.intel.com/Campaign/Terms/7c4740d1-7b93-4626-9fbe-d58a7a0a38ef)",
      "Can you blame them though they are new to the dgpu market",
      "Go to eligibility in my second link you just need the gpu"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "a750"
    ],
    "title": "Intel Arc 2024 Revisit & Benchmarks (A750, A770, A580, A380 Updated GPU Tests)",
    "selftext": "",
    "comments": [
      "Arc is starting to look seriously impressive. If the A750 beats the 6600 XT in 90% of games and the latter is still $240, the former is starting to look like the best value card for *everyone* in this price range, not just enthusiasts.",
      "This is really impressive, I thought the a770 was closer to the 3060/6600. Gj intel",
      "you are not missing anything with starfield. To say that this game is mediocre will be praising it. Focus on good games not 5/10 games.",
      "Why do you have such allegiance to a company that doesn't care about you? \n\nAMD, Intel, Nvidia just buy whatever seems better for you",
      "Based on AMDs slow progress to increase performance over the years.",
      "A good video, and a fair comparison.  \n\nIt’s a bummer Stairfield still appears to be having issues on ARC.  \n\nHonestly probably better if Battlemage isn’t launched until another 3-6 months of driver development time has passed overall.",
      "Can we not with the fanyboyism? More competition is good for everyone.",
      "Really pulling for Battlemage. We need a third party option.",
      "Looks like ARC is racing to the top fast. BattleImage will leapfrog AMD and get very close to Nvidia.",
      "What do promises have to do with anything? You can see current performance then realize they are making faster chips that will have more cores as well.",
      "Amd drivers are pretty mature and their driver issues (Intel's too) are very overblown. Their drivers today are about as stable as Nvidia's and are still more reliable than Intel's. I really hope that battlemage will moreso give Nvidia a reality check at the high-end and upper mid-range.",
      "But but but, BIG NAVI",
      "Remember \"RIP Volta\"? What happened after that? AMD shooting themself in the foot with their meme marketing LOL",
      "Nvidia getting out of the GPU space would be more likely than that",
      "The problem with Amd they also still have massive driver issue after years. If Intel keep progressing their driver, it's very possible for them beat Amd in gpu market especially with Battlemage.",
      "I will happily if you can point to anywhere I did that.",
      "I don't understand why Starfield is even used as a benchmark though. It's not even in the top 100 most played games on Steam right now. If they're  using that game to benchmark GPUs, they should use Cities Skylines 2 to benchmark CPUs.",
      "I agree - but it is one game that a lot of well known sites like to benchmark, and it certainly left an impression on many that Intel wasn't ready at launch.   Games hyped that much should be given prioritization for drivers.. \n\nI think we need to see Intel stay ahead on more AAA launches in the future. \n\nI'm optimistic..",
      "I think the driver issues are still common enough that the rx 6600 would be the go-to entry level GPU.",
      "Based on what? Do you have battlemage leaks of performance targets?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Nvidia's RTX 4060 Ti and AMD's RX 7600 highlight one thing: Intel's $200 Arc A750 GPU is the best budget GPU by far",
    "selftext": "",
    "comments": [
      "I ordered one yesterday after seeing the price drop.  It’s not as fast as the newer amd and nvidia stuff but at that price its a great deal.  I’ve been waiting to see if the drivers would get better and it appears intel has gotten a lot of things worked out.  \n\nI also hope intel succeeds here because we need more competition.",
      "That is what I gathered from the reviews I watched. “Here are the new offerings from Nvidia and AMD. And here trailing by 10% or less in performance is the A750 which is going for $200 and $70 less respectively. “",
      "no",
      "Only thing that keeps ARC from being the the choice is the limit to CPUs that supports resizable BAR.",
      "which is basically everything in the last 3 generations from both AMD and Intel.  \n\nif just looking for 1080p and fps per $ then the rx6600/6650xt is where its at.  \n\nI have an Arc card though and its nice to play with and see what you can get out of it.  Good at 1440p or even 4k in some games and can ray trace better than the 3060.",
      "It would be a downgrade",
      "With rebar, the GPU can request bigger data chunks to the CPU, instead of making thousands of small default requests. This way the CPU can load data in vram faster",
      "I mean, was that at launch? They were totally unacceptable then but they're very functional now. I've been daily driving one for months with no issues.",
      "2080 performs better then the RX 7600",
      "For $200 it's finally a strong recommendation, imo. I say that as a guy who bought the a770 le at launch to experiment with and support competition with GPUs. I'd never recommend that GPU, it's not powerful enough to run games at settings that need 16gb VRAM but the a750 at $200 is the best GPU deal we have seen since before the pandemic.",
      "I literally stated that intel will improve their drivers a lot more compared to AMD/NVIDIA and most likely A750 will be a lot closer to 7600/4060 than it's now (while AMD/Nvidia drivers are matured and the gains won't be as big). You are writing like I am stating the opposed, I am confused.",
      "What does resizable bar do?",
      "4060ti is 50% more expensive than 7600, they are in different leagues.\n\nAlso if you are esport player playing in 1080p then maybe 4060ti is ok, otherwise just don't.\n\n7600 is sidegrade or downgride from 2080",
      "For long-term, yes, however many people this just isn't an option because of drivers.\n\nMine idles @ ~40w (ASRock has some sort of guide to \"fix\" this issue, but all that did was cause games to randomly crash, so I had to undo that), I was having some sort of flicking issue when hovering over a youtube video but I don't seem to see it now.\n\nI also have performance issues in Halo MCC Reach (I seem to be roughly averaging around 10-30 or so fps on high but it feels like a slide show, on lowest everything it's around 25-40, but can spike into the hundreds, but it's still a pretty game gameplay experience, the GPU only runs a 600MHz @ ~35% usage; Ironically, Halo Infinite I'm pretty much always had around 110-130 fps on high despite it being far more demanding, but it's a DX12 game).\n\nI was also having pretty bad stuttering in another game, but was able to fix that issue with changing it from DX9 to DXVK, although many games that's not an issue because of anti-cheat.\n\nFor most people, it'll just make more sense to go with an RX 6600, *usually* price is pretty similar, although usually favoring the RX 6600 as being the cheaper card, although right now I'm seeing 3 different 6600's for $200 and there's even one for $180, the A750 has been around $230 for a while with a drop to $200 just a few days ago, but that price went back up, however in the long run I'd definitely expect the A750 to keep gaining performance as its been doing, I also don't mind being on *questionable* drivers since it runs the two games I mainly play fine, I also like having AV1 encoding, it was a large part of the reason I bought my A750.",
      "but compared to AMD/Nvidia you have basically a given big performance jumps via drivers updates. So your GPU will get closer to 7600/4060 in a year vs how it stands now.",
      "Apart from fixing fixing broken CS:GO performance and Forspoken, gains in most games have been pretty modest.\n\n[CS:GO with last month's driver](https://youtu.be/xFLDxMwPcrw?t=342)\n\n[Same performance as driver from 3 months ago, comparison with 3060 and 6600XT](https://youtu.be/00T15aL1pkA?t=371)\n\nForspoken last month has improved over launch and 3 months ago\nhttps://youtu.be/xFLDxMwPcrw?t=366\nhttps://youtu.be/00T15aL1pkA?t=402\n\n\n\n[Cyberpunk 2077 last month results](https://youtu.be/xFLDxMwPcrw?t=200) are the same as [3 months ago](https://youtu.be/00T15aL1pkA?t=162)\n\nSimilar situation as Cyberpunk for Shadow of the Tomb Raider\nhttps://youtu.be/xFLDxMwPcrw?t=294\n\nhttps://youtu.be/00T15aL1pkA?t=297\n\n[Horizon Zero Dawn had modest improvement over launch.](https://youtu.be/xFLDxMwPcrw?t=241) Sits about the same performance as [3060/6600XT](https://youtu.be/00T15aL1pkA?t=215)\n\n\nThing is, people shouldn't expect the same pace of improvements to continue indefinitely. Rate of improvement tends to slow down after the low hanging fruit has been snapped up. Just saying.",
      "Fairly complicated, it allows pcie hardware to renegotiate how much of the device internal memory (vram in this case) is mapped to system address space at once, basically it allows making large concurrent transfers over pcie instead of queuing small ones. \n\nThis doesn’t have direct performance effect in most cases but intel cards and drivers have been designed assuming it works, they really suck without it.",
      "If I have RTX 2080 do it worth upgrade to RX7600?",
      "Not at launch, 3 months ago, it was driving me crazy, arc control doesn't save the settings, old games.. such as mad max, crashed, a lot of PC shut downs, out of the blue, messed up HDR, so many problems that i gave up and bought a 4070 bundled with Diablo 4, i don't regret it.",
      "I had a arc a770 for 4 days, that's how much i lasted before starting to loose my sanity, great looking product, ok ish performance, but when it comes about the drivers and the problems it comes with them, no! They have a duty to fix the driver side, until then, no, no. It was sent back without a second thought."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "a750"
    ],
    "title": "Intel® Arc™ A750 Graphics Benchmarked in Nearly 50 DX12 & Vulkan Games",
    "selftext": "",
    "comments": [
      "These are seriously going to have to be dirt cheap. 3-5% margin in their best suit is no victory in itself. Let's not forget they cherry picked like 6 games to showcase, commanding a 13% lead last time. They've compared it so much to the 3060, but here's the elephant in the room: **Intel is not the first to have to sell their products with a better price to performance compared to Nvidia.** AMD exists. Intel will have to 'out AMD' AMD. \n\nSo right now, the market has basically decided that a 3060 is worth $369, while a 6600XT is $300. These perform very similarly at 1440p. Now where does that leave the A750? It's gotta be under $300. Well under. $200-250 wouldn't be too little with the state of their drivers. They've taken the crown of the third rate player so that's what they must do to survive.",
      "Underclocking their DDR5 from [5200 to 4800C38](https://game.intel.com/wp-content/uploads/2022/08/config-chart-a750-3060-01.png) on the test bed is an interesting choice.\n\nNumbers appear solid, though. One set of outliers caught my eye - Call of Duty: Warzone shows up at 53% faster than the RTX 3060 at 1440p High, but Call of Duty: Vanguard flips it, losing to team green by 30% under the same settings.\n\nGlad they released some useful numbers, though, for a change.",
      "They sorta skipped over 1% lows though.  Let’s wait a little more…",
      "Never belief marketing, wait for reviews.\n\nNo info about fram pacing or 1% lows, which in many reviews of a380 were often bad, despite nice average fps.\n\nAlso in a 1-3 months new gen from competitors should be on the market, maybe not whole series, but high and mid should be.",
      "I want one. Or the A770.",
      "I wouldn't plan on anything yet, considering the numerous driver issues with the A380 and the fact that Arc underperforms in many unpopular titles (especially without reBAR).\n\nAlso, the rumors that Intel will axe its GPU department and the entire Arc project don't give me a lot of confidence in this first gen of GPUs...",
      "Looks pretty good honestly. It's basically an RTX 3060 with an AV1 encoder in the 50 games showed. I'd actually buy the A750 over a 3060 if they had the same price.",
      "The low end amd and Nvidia cards for the next release are way out though. Still not horrible timing",
      "There's also the RX 6700 (non XT) 10GB, which goes for [$369](https://www.newegg.com/sapphire-radeon-rx-6700-11321-02-20g/p/N82E16814202424?Description=RX%206700&cm_re=RX_6700-_-14-202-424-_-Product&quicklink=true) on [Newegg](https://www.newegg.com/sapphire-radeon-rx-6700-11321-02-20g/p/N82E16814202424?Description=RX%206700&cm_re=RX_6700-_-14-202-424-_-Product&quicklink=true) Kinda hard to compete with that in this price bracket.",
      "if its 200-250 yes, if no, why bother with the risk?",
      "If they ever get their drivers sorted these would make pretty good cards. I gotta give props to Intel on a solid first attempt at a modern graphics card.",
      "It's also releasing later this quarter and going to run into the Nvidia RTX 4000 series launch\n\nI think your right about price\n\nIt's going to have to be a bargain to get people to take a risk on a new GPU line",
      "Sure, for DX12 titles.  Not sure I would take it over a 3060 if price the same as many titles are still DX11.",
      "Yep.  Planning on the A770 for a new build.  Unless the 4000 series kicks their butts at similar prices.",
      "> They sorta skipped over 1% lows though\n\nThat's nothing. They skipped DX11.",
      "Yeah old titles may not run as well.  But looking forward to the new titles things look bright already.\n\nIntel will improve its dx11 drivers.  And I don't think they will axe ARC considering how much it's talked up, promoted and forward planned.  If they do I'll buy a 5000 series next time...",
      "The price drops are because of the crypto crash, not because they want Intel to fail. The reviews of the cards and driver suite aren't to flattering to Intel either, this is pretty much a self inflicted wound. These cards were supposed to be in our hands months ago, yet they still aren't. Now bout AMD and Nvidia are bringing out their new cards shortly, who is going to bother with Intel cards aside from tinkerers and curious people. I'd buy a Arc card just for the heck of it, but if I wanted a sure-shot gaming system, I'd go with AMD or Nvidia.",
      "Doesn't matter how hyped up ARC is. Intel needs to deliver fully-working products to its customers and it seems they are not capable of doing so.\n\nIf there is an architecture/hardware flaw within the GPUs, the dGPU sector may be axed. Billions have already been invested and lost for Intel, but how can the company fully commit to the product if it's broken from the start?\n\nOf course, nothing is certain as of right now, but it's important to be ready for this plausible outcome.",
      "You're literally talking about a product they said would ship well over a year ago, after a decade of INtle missing targets on nodes, laptop, desktop and server chips constantly.",
      "i want to preface my comment by saying that it has nothing to do with consumer choice, just my observation on a technical basis.            \nIf this releases at a good price, it will be a great product.       \nNow :   \n3060  :  Samsung 8nm   276mm size,  12 billion transistors.                  \nA750 : TSMC N6  6nm  406mm size , 21.7 billion transistors                 \n6600XT: TSMC N7 7nm, 237 nm size,  11 billion transistors.                            \n\nif i grant that intel will get their drivers working for DX11,DX9 on the level of optimization they have now for DX12 & vulkan, then i would accept that A750 would be slightly faster/better than the 3060 and a bit slower than the 6600XT.                          \nBut, from an architecture perspective, they are spending almost double the transistor budget to achieve a similar result.  3060 is also a tiny die by comparison, even on a much much less dense node, so much cheaper to manufacture for nvidia.             \nSeems that alchemist is an ok first attempt, but needs a lot of work till the future architectures can claim to be on par with amd/nvidia on a perf per transistor basis.              \nRight now, AMD & Nvidia have what looks like a very large advantage .                          \nKeep in mind that ACM-G10 is almost the size of Navi21 and has almost as many transistors (21.7b  vs 26.8b) , but their performance delta is massive."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "a750"
    ],
    "title": "Intel Arc A770/A750 Graphics Card Review Roundup | VideoCardz.com",
    "selftext": "",
    "comments": [
      "I want to buy an a770 not because I want an a770, but because I want a more reasonably priced 50 series and 8 series.\n\n\nCheering for you arc",
      "i'm pretty impressed with the card given its their first attempt, in a few games it was only ~5% behind the 3070, which says that the hardware is at least good. amd and nvidia have had years to optimize drivers for their hardware, so i think intel is almost definitely leaving a lot of performance on the table here due to how behind they with optimization.",
      "Just a PSA, according to Linus the 16gb A770 LE is going to be available in very limited numbers (probably due to low margins). So if that's the GPU you want you need to be ready to buy on launch day.\n\ntimestamp: https://youtu.be/6T9d9LM1TwY?t=701",
      "770 destroy the 3060 in RT at 1440p, and better when xess, go intel go ! \\*in digital foundry review",
      "It’s not about the card, it’s about sending a message",
      "The drivers stack though is brand new from the begining of the year. They tried to port from their mobile drivers and that did not work. Had to start from scratch. This will get better.",
      "We need heroes like you",
      "They'll probably be able to fix it - or at least make it better - with driver updates sooner rather than later.",
      "Why hasn't anyone tested these cards with dxvk for the older titles?",
      "the performance is there, it can compete with its targeted NVidia, Amd counterparts. but without a serious discount why would customers buy intel instead of more tried , proved Nvidia,AMD?",
      "Buy cheap now, wait for driver upgrades while hitting some bumps in the road for a few months, have a card with superior price performance in the near future.\n\nPeople with more money than time can spend two or three hundred dollars more for some guaranteed gaming hours a week. Not everyone has extra dollars and for these people who are looking for maximum performance per dollar and advanced tech like RT and AI optimization, this could be a good buy if they are willing to endure some bumps in the road.",
      "This is 100% not a card to give to a family member who doesn't know how to deal with tech problems. Crash to desktop and system lockups are common across many reviews and they are showstopping problems to average users. \n\nIts fine to believe Intel will fix their drivers, but if you give a device to someone who isnt able to deal with these first (hopefully) months, they will be soured on the PC for years.\n\nThe Apples/Dells/HPs of the world, even with all their BS marketing, will always be attempting to design stable systems first.",
      "As someone on the market for a new GPU, the thing that scares the shit out of me is the rumors that Intel is considering pulling the cord out of its dGPU, Drivers improve with time if effort is put on it, but that is the catch.",
      "Is there a link yet to where it will be sold so I can bookmark it?",
      "The i740 was not a GPU, it was a graphics accelerator. The term GPU didn’t apply until hardware transform and lighting was added to the 3D chip, starting with the GeForce SDR. Larabee was not a GPU and lived on his Xeon Phi. The Arc A7 is the first gaming GPU from Intel.",
      "But dxvk translates dx calls to vulkan, and intel gpus are supposed to be alright with vulkan",
      "Every company at the top plays bad cop. Until competition humbles them.\n\nIntel has had their turn with CPUs. Next it's Nvidia's turn to be humbled. Buy AMD if you have to. Avoid buying the 4000 series for a software update.",
      "I am buying an ARC A770 Limited to replace my wife’s Radeon 6600.   She’ll have an all-Intel build (i3-12100, Intel 660p) :). \n\nThe only thing I (really) don’t like is the high idle power consumption..",
      "Uhhh, why?",
      "Voting with my wallet / to support the brand mainly.  \n\nBut actually usable ray trace performance is a bonus and I think these cards will get faster over time.  \n\nHer card is also the vanilla 6600 so A770 is faster all around."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "a750"
    ],
    "title": "Intel Arc B580 Limited Edition tested in 3DMark, outperforms RTX 4060 and Arc A770/A750",
    "selftext": "",
    "comments": [
      "This is hardly surprising since Intel Arc A770 already outperforms GeForce RTX 4060 in 3DMark Time Spy",
      "i care about game fps comparison more",
      "Now let's see gaming performance.",
      "Huge if true, could be a new budget build king.",
      "Wow 3dmark.\n\nWorthless test thru",
      "Since it might be around 250 bucks. Like there are not many good budget cards eight now. The 4060 is not that bad, just too expensive for its performance (should have been aroumd 250 bucks max)",
      "Is the embargo not over until launch day?",
      "Since when we feel excited for a new card outperforming a terrible card such as 4060? Even NV's ancient 3060ti already done so.",
      "For the price 249$, Intel is king of budget now",
      "https://redd.it/1hbrxdg",
      "Hey SilasDG, your comment has been removed because we dont want to give *that site* any additional SEO. If you must refer to it, please refer to it as *LoserBenchmark*\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/intel) if you have any questions or concerns.*"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Arc A750",
    "selftext": "",
    "comments": [
      "Recently bought the A750 at my local microcenter for msrp (shocking right? lol)\n\nI'm actually pretty happy with it. It plays all my games and all my work needs. I do see the occasional stutter and issue here and there but I'm sure this will all be worked out with future drivers. For the price. You can't go wrong for 1440p gaming.\n\nAND I got a free game! MW2 💪🏻\n\nProud early adopter 👍\n\nEdit...\n\nSo far the games I’ve played.\n\n**Modern Warfare 2**\\- Multiplayer Performance is great and smooth. Especially with having the ability to mess with XeSS. Hard to say if some of the stuttering is GPU related or new game release that still needs some tweaks. Regardless I didn’t feel like it hindered my ability to have fun. I messed with competitive settings and eye candy settings. Competitive without a doubt is smoother. But eye candy with XeSS is impressive. I’m going to try out campaign/co-op soon as it’s a slower play style and you’re more aware of the ‘*little things*’.\n\n**Battlefield 2042** \\- definitely needs to be optimized. There are artifacts. Some graphics don’t load properly. Lighting is horrible... My laser sight literally looked like a light saber and blinded the whole screen. Overall performance isn’t great. Even with competitive settings it wasn’t great. (*I know the game isn’t great in terms of game optimization, I’ve played with other graphics cards so I know what to expect at its best and worst*). I don’t recommend this game at the moment until drivers are better optimized or the game is optimized for the GPU. (*Some games perform better with AMD or Nvidia. So it potentially COULD be an issue for Intel*)\n\n**Planetside 2 -** (DX11 title. Originally was a DX9 title overhauled to DX11). For this title it actually performed better than expected, for an older title. This game is CPU heavy but does like a good GPU paired with it. Higher resolutions provided better performance and I was able to load Shadows with little impact. Before it would tank performance in big battles. I need to test this more as I’m really surprised by the results.\n\n**Halo Infinite** \\- The game plays well with medium to low settings at 70ish average. However optimization is needed. It would stutter and some lighting was a little off. Turning everything to low helps with smoothness but I’m a sucker for eye candy. Comparing it with my previous graphics cards I would consider it under performing and it’s something that could be fixed with future drivers.\n\n**Medal of Honor Allied Assault** (DX8 title) - this game is old but I love it. No impact on performance due to older API. Maxes out FPS and no artifacts. Graphics appear to load just fine.\n\n**Titanfall 2** \\- (DX11 title) probably one of the best optimized games I’ve played. No issues with A750. Maxes out frames on maxed out settings with no stuttering or artifacts (at the moment)",
      "Thank you for your service. Hopefully this paves the way for a 3rd GPU provider to compete with red and green.",
      "We now have three teams. Team Red. Team Green. Team Blue. RGB. Omg we have made full circle ⭕️ \n\n/s",
      "I would consider this card as a replacement for my son's RX480, but for whatever reason it doesn't perform anywhere near as well at 1080p as it does at 1440p, relatively of course.",
      "What do you mean?  There is no bare PCB visible, thus the card has a decorative backplate installed already.",
      "What a beautiful card",
      "how much fps you getting in mw2 in ground wars/invasion mode?",
      "Sounds like an expensive addiction :)",
      "In ground wars with settings set to auto I’m getting about 80-90 fps. \n\nAuto is a mixtures of medium and low with a few high settings. XeSS is set to quality.",
      "It really is. Quality build for the price point. \nThis is the kind of card you want to show off and not keep closed up. \n\n*Starts looking for a new case*",
      "The rgbs are included with the A750?",
      "Just managed to get an order in for a A770 less then an hour ago!\n\nCollectors item for me as i already have a 4090 lol... plus i might also get the XTX.. i might just have a crippling a GPU addiction lol",
      "That's a bummer I was thinking the same thing for my son.  Might have to go amd though",
      "Any artifacts with XeSS that you can perceive with your own naked eyes?",
      "Finally the dream of having amd cpu with intel gpu. And OP missing motherboard backplate.",
      "What would this be on par with performance wise from Nvidia?",
      "Yeah in 1080p for some reason it doesn’t perform well. Well might not be the correct word but like you said relatively speaking it doesn’t perform as expected. But the higher the resolution the better it gets. Kinda minds me of the Vega dGPU series with higher resolutions it performed better. \n\nFor your kiddo I would definitely recommend something more reliable. Tried and true. Something I wouldn’t want to mess with. \n\nIf you like to tinker I recommend this. With future driver updates I’m certain performance will improve. I’m hopeful when afterburner is able to read the bios we can fine tune and unlock what this GPU is capable of.",
      "No not on the a750 model. That is exclusive for the A770 limited edition 16gb model (correct me if I’m wrong).",
      "Oh ok",
      "I agree lol. Very expensive. \n\nBut to a point yes I do agree. I’m glad the A750 functions well enough for me to be happy. It’ll be a Collector’s item in the event Intel quits on the whole GPU thing lol"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Arc A750 vs RX 6600 GPU faceoff: Intel Alchemist takes on AMD RDNA 2 in the budget sector",
    "selftext": "",
    "comments": [
      "The intel arc sub loves to take this article out of context and just read the benchmarks section to then conclude the a750 is the winner\n\nA750 does win in these sets of benchmarks, ray tracing and 4k, not by much and with some outliers in some games. But at ray tracing and 4k, performance is suboptimal at best.\n\nAlso have to note that the arc card still has compatibility issues and stability problems, in some games it performed significantly worst than the other brands(halo infinite as an example). \n\nNotably power efficiency is terrible, consuming as much as 200W unlike the 6600's 130W+-. If that's a concern, the a750 isn't a good choice especially if it's just better in games by a few % more but taking in that much energy to do that\n\nArticle seems to be in favour of AMD unless you have AI applications to run and want to support ARC",
      "Winning by 100% at 4k\n\nBoth cards barely hit 60FPS\n \n4K\nSpider man: A750 17.2 FPS, 6600 8.6fps\n\nBoth are PowerPoint presentations",
      "Right, amd doesn't win \"hands down\", but the a750 winning by 100% in a resolution and setting that both cards struggle to play at isn't what made the 6600 lose out in this comparison",
      "AMD wins hands down. A750 is a good card, but some older games don't work. Rx6600 works for everything. The amd drivers are better too. Intel is great to fill a niche if you're not in that niche, skip it and go straight for the rx6600 for entry level 1080p.",
      "That's not so true anymore. I've been going back and testing games like Fable Anniversary, Hitman Absolution and Space Marine, like they're running really well, over 150 -200 fps 1440p. Then most modern games run great, too. The main issue is new releases where Intel doesn't get early builds of a game to make drivers from. \n\nThat's definitely a problem, but that makes arc a gpu for patient gamers. \n\nI'm using an a770 on a 7800x3d bench, fwiw.",
      "A770 can push halo infinite at 1440p Max settings @ 70-90 fps. What are you talking about instability for halo infinite?",
      "Yup, but this article is comparing products at similar price\n\nNo doubt the a750 is good for what it is, but it's more meant for tinkerers and experienced troubleshooters",
      "The a580 is comparable but cheaper than the 6600 which are probably more comparable products",
      "Counter point. Enemy territory (and et:legacy) work on my a750 in Linux but not on my 6900xt on windows 11.",
      "Just my experience when using the a770. I admit it was few months ago, the driver might've improved it. Had switched to a 6700xt since then",
      "They might be a tie in gaming (except for older titles where the arc is still a bit problematic), but Arc GPUs are much better for video editing, 3D modeling, and other productivity tasks. They're the best for the price by a long shot, even better than Nvidia cards weaker than the 4070. You can check the Blender scores and Puget benchmarks. Not to mention cheap 16GB VRAM for the A770.",
      "Rx 6600 owner here, happy with it at 1080p Flawless experience. I would have gotten the ARC had it been available at the moment I bought AMD. I want Intel to succeed and take Nvidia down from the pedestal. AMD has risen prices because only competition is Nvidia and can mirror prices, but with a 3rd quality player we should get these silicone bits at reasonable price. \n\nI want to move to 2K this fall. I am on the market, so if Intel offers a decent GPU I will back them.",
      "Why would anyone be trying to do 4k with either of these cards? Who cares which ones worse or better at 4k?",
      "I’ve got a a750 in my Linux box. Many games work on it in my steam library. \n\nYou may not get day one support for a new game but it’s working a lot better than at launch.",
      "Counter video. https://m.youtube.com/watch?v=Y09iNxx5nFE&pp=ygUaaGFyZHdhcmUgdW5ib3hlZCBJbnRlbCBhcmM%3D\n\nIf you want to bring up one game that you love to counter, fine. For most buyers out there, this would be a better representation of what's to be expected\n\nIs Intel arc bad? No, they're well on their way to be good. However, I never buy based on potential.",
      "It also ignores older games entirely.\n\nThe situation would certainly change if they threw Detroit: Become Human, Bioshock Infinite, or maybe even CS:GO - but I've heard Intel has improved CS:GO performance significantly since I last tested ARC.",
      "A770 16GB could be found for 270-280 USD. And sure, while that's only 13% cheaper than 7600 XT ($320 at the cheapest). While game performance isn't great, ARC has much better productivity performance, like 1200-ish vs nearly 2000 Blender scores, nearly 4070 level HVEC performance, much better hardware for streaming and video capture, etc.\n\n\nI've never said theyre the best cards for gaming. I usually recommend pure gamers Radeon GPUs, except when they're really cash strapped, in which case A380 and A580 are better values than radeon.\n\n\n6600's competitor would be A580. Gaming and 3D rendering performance isn't too far off from A770 actually. It's a really good card for the price, way better value than A750 even though A750 and A770 are good value already. Plus it's got most of A770's features intact, which means video capturing and streaming are just as good.",
      "I want to upgrade my GPU (GTX 1650) but I don't have a lot of money (not from the USA/UK/Canada/Europe) the a750 is 190 dollars (if I purchase something above 200 dollars in my country they are going to demand taxes, 18% of the product value to be more specific. For example, if I buy the a770 that is 280, they are going to demand 50 more dollars in taxes and instead of 280, I'm going to pay 330. Not to mention the taxes they demand when it arrives and the payment of the weight I have to pay to the courier 💀). Should I buy the a750? Or is it better to get another GPU and pay the 18% tax of the product value?",
      "Yes, but at the a770 16gb prices, it's competition isn't the rx6600 now is it? Arc is way better than Nvidia value wise, but it's a niche card that does do a whole lot for gamers. At a770 16gb prices, it's losing cleanly to the 7600xt. Intel cards simply need to take another 15% to 20% haircut to remain competitive in my book.",
      "what niche ?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "a750"
    ],
    "title": "Hitting the Shelves: Intel® Arc™ A750 and A770 GPUs Release Today!",
    "selftext": "",
    "comments": [
      "OOS @ newegg",
      "Yea this launch has been kind of a mess so far. The A770 and A750 went up earlier than they were supposed to on Newegg and the A770 16gb sold out immediately. The A750 came in stock again on Newegg but now it is also out of stock\n\n**EDIT: THE A770 CAN BE BACKORDERED: https://www.newegg.com/intel-21p01j00ba/p/N82E16814883001**",
      "Aaand .. it's gone",
      "Such a whimper of a launch. They must have made one whole box per country.",
      "True limited edition cards",
      "Complete mess. Release a budget friendly card with all this hype....but don't tell us officially where or when we can buy it until an hour AFTER they sold out of the a770?",
      "Had the A750 in cart and by time I went to check out it went out of stock. Bummer",
      "its my bad, i bought them all",
      "https://youtu.be/-DT7bX-B1Mg",
      "Unironically, I think a *lot* of these LE cards are going to end up as shelf art.",
      "Werent a ton of these things supposedly sitting in warehouses for months?",
      "maybe demand is high?\n\nSounds a little like copium",
      "If they had all this cards since February.... and they are gone... that meas this is it, there should be no more stock.  \n\n\nThis is the first GPU in history to be designed as a collectible item.",
      "So where's that discount code from the HPG scavenger hunt :/",
      "Fucking backorder won't go through. Just keeps putting me back at the secure checkout page.\n\nEdit: It's aggravating. I work on a PC. I had auto-refresh set every 30 seconds since this morning. Missed the first drop. I got up to use the bathroom ONCE and of course that's then they come back in stock lol. Ugghhhh\n\nEdit 2 hours later: Still won't let me add the backorder.\n\nEdit: 5 minutes after previous edit I finally got the backorder to go through. Had to use the phone app (I was able to spam the confirm order much faster over and over). Worked on the 5th try.\n\nETA on the backorder is 10/19....if of course they get the stock in for it.",
      "Man… what a joke of a launch.",
      "They delayed these cards for months in preparation for this launch.   How did they mess this up so bad???",
      "I can’t believe you’ve done this.",
      "Scalpers gonna scalp…the shoe market is slowing and PS5’s are in stock.",
      "Dumb thing to scalp, frankly. These are the higher end models, and there are already better nVidia and AMD cards in the used market. Prices on those and new old stock of 3000-series cards keep coming down. \n\nNo one is going to pay much above MSRP on an Intel Arc card today 😂"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Upgrading from a 1050 to Arc A750. Would it bottleneck?",
    "selftext": " I am planning to upgrade my GPU from a GTX 1050 to a Intel ARC A750.\n\nMY CPU is a Intel Core i5 7400. Would it pair well or would it bottleneck like crazy? (I am also planning to double my RAM from 8GB)",
    "comments": [
      "The problem is, ARC needs Resize BAR, and that requires 8th gen or later CPU.",
      "im glad arc is getting traction.",
      "This is 100% true. Performance drops below the earth if ReBar isn't enabled. I like what Intel is doing though. With XeSS and the way their drivers continue to mature. Watch out, AMD and Nvidia.",
      "6600 is 180, 6600xt is 230, 7600 is 250, 3060 is 280. \n\nIt's fine if you want to buy 750, but these are all great options that perform better than/equal to 750 at lower/equal price.",
      "Not recommend. Your CPU does not support rebar",
      "If you plan on upgrading the CPU soon you can get it. If you want to keep the 7400, don't get the A750.",
      "The A750 is also an 8GB card. So it doesn't have an advantage on that front either.",
      "That'd be a pretty bad pairing imo, 4C4T CPU just isn't enough for newer games while some older games play terribly on Arc, upgrading to a 6700(k)/7700(k) would definitely help a lot, although if you do that and go with an A750 you'll need to force enable ReBar since your motherboard doesn't support it (I think it wasn't until 10th gen when they added ReBar, but it's a pcie feature, so even if it isn't there, it's still supported).\n\nA good example of older games playing terribly, Halo: Reach from MCC I get around 15 FPS on high, changing to low I can get maybe around 30 FPS but it's a terrible gameplay experience, I've also put DXVK into it and it runs incredibly (Depending on where, I can get anywhere around 150 fps to 300 fps) well with the translation layer, however I can't use online features with DXVK.\n\nIf you don't mind not having AV1, a 6600/6600XT/6650XT are good GPUs for the price, although depending on the game you'll lose some performance from being on pcie 3.0 since the GPU only has 8 lanes.",
      "Oh dear, you haven't watched the [reviews for the 7600](https://youtu.be/Yhoj2kfk-x0?t=1028), have you? All the GPUs released this generation are horrible, except for the 4090.\n\nSteve recommends the 6700xt here, it's faster and has 4 GB VRAM extra. Depends on your local pricing though. You can even buy it used (if you think it is worth the headache), since the market is awash with used GPUs from the bitcoin mining rush.",
      "Really? There is a [6700xt for $309 on Newegg right now.](https://www.newegg.com/asrock-radeon-rx-6700-xt-rx6700xt-cld-12go/p/N82E16814930059?Item=N82E16814930059&Description=rx%206700%20xt&cm_re=rx_6700%20xt-_-14-930-059-_-Product) So just $40 more.\n\nBuying an 8GB VRAM card like the 7600 right now is rather risky, more so at 1440p.",
      "[https://pcpartpicker.com/list/KmGkk9](https://pcpartpicker.com/list/KmGkk9)\n\n&#x200B;\n\nWould this be an okay PC for Arc A750?",
      "They need to get the price of their cards down.  I can't justify the extra $100-$200 for maybe 5%-10% performance, not to mention 50%+ TDP increase.  $400-$550 for a $280 rx 7600 competitor? Not a chance.",
      "That looks like a very good list to me.\n\nI would cheap out on the case if I were building for myself, you can get one thats perfectly functional for around $40-50. However if you like the way that one looks maybe its worth the extra $50 to you.  If you do choose to cheap out on the case, you can expect to get what you pay for which means assembly is typically harder because something will be made with bad tolerances and hard to put together.\n\nYou also don't need a 750w psu for those parts, but there isn't much price difference between a decent 650w unit and the one you have picked. Its fine if you like that one and would let you upgrade to something more power hungry in the future.  You might save about $30 dropping to a good 650w unit, but you do not want a bad psu from a no name company.  Unlike the case it isn't something you should ever cheap out on.",
      "No.\n\n7th Gen doesn't support Rebar.",
      "Both the 4060(ti) and 7600 (is it the AMD equivalent?) are horrible value cards.\n\nI had no interest in these cards anyways, but the problem is that with dGPU prices like this, consoles will stay the only budget choice. Which means we will keep getting more and more shitty games that are optimized towards consoles.",
      "How is the 7600 horrible value? What would you buy instead?  I have the 7600 sitting in my cart right now waiting for my next paycheck.  It'll be paired nicely with my 10900K",
      "the fuck are you talking about. the arc 750, at its cheapest, is 250.",
      "That are a whopping 10% faster than previous gen from over 2 years ago, lol. Meanwhile, games have gotten a lot more demanding with some even needing more than 8GB VRAM at 1080p.\n\nIf Intel ups their driver game, there might finally be some proper perf-per-$ gains in the GPU budget segment.",
      "On pcpartpicker, cheapest I can find is $229. You can get a 6650xt same price, 6600s for $179, and 6700s for $279. All of these have better drivers, no rebar needed and less cpu overhead.",
      "They need to put bar on 200 series. It’s been proven you can do it. Takes a custom bios though. And some soldering I believe. Still running a 7700k here"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Arc A750 with Resizable BAR working on i7 6700 using ReBarUEFI BIOS mod",
    "selftext": "",
    "comments": [
      "Oooh, sweet. I keep waiting for an X99 mod I can put on my old system.",
      ">little chance of recovery from error\n\nyou can buy a ch341a bios flasher\n\n&#x200B;\n\nor just trust dual bios like i did on my b75 board",
      "Motherboard manufacturers typically make very little in the way of profit and only have one person handling their in-house BIOS, so how many generations of boards do you want this one person to keep updating? Assuming Intel and AMD release a new platform every two years and we're talking about 2014 platforms here, that's 5 or 6 generations of boards. Every board generation has 2-4 different chipsets and every chipset typically has at least three different boards per manufacturer to hit different price points. Obviously there's a lot of re-use with AMI/Intel/AMD providing the base BIOS and the board partners basically customizing to their liking, but they still need to bake in support for the specific components on their board and add memory support and the like. That's a huge undertaking for a single person IMO.",
      "Why do we have to mod stuff like this in when it obviously works and the manufacturer could just support it. I know it’s a. Money making scheme, but just….annoying.",
      "Have you done any benchmarks to verify that it has had the intended performance increase?",
      "Eh. Most of the mod procedures I’ve seen have been very finicky with little chance of recovery from error. Keep hoping a repository of updated/verified BIOSes eventually gets created!",
      "> Motherboard manufacturers typically make very little in the way of profit and only have one person handling their in-house BIOS, so how many generations of boards do you want this one person to keep updating? Assuming Intel and AMD release a new platform every two years and we're talking about 2014 platforms here, that's 5 or 6 generations of boards. \n\nWith the amount they are charging me for a motherboard these days, I expect much longer support. They can hire more people. If they won't support it, open source the damn thing so the community can do it themselves.",
      "works for any uefi system actually",
      "They just don't want to support really old stuff. Skylake is from 2016 which is now... 7 years ago, geez. \n\nI wish they did, but I can see why they don't.",
      "Some boards also have a BIOS flashback method that works. It’s a specific port that you can insert a USB flash drive into with the BIOS file named correctly and it will rewrite the flash.",
      "Oh, good catch! I didn’t think BIOS Flashback existed on boards that old, but it seems the TUF X99 does actually have it.",
      "This isn't my screenshot but it makes Arc GPUs go from unplayable stuttering to performing as should.\n\n\nOn my RX 580 it does give me a ~10% fps boost in DX12 games and improves the 1% lows alot",
      "No that won't work because the PCI initialization has to be done by UEFI. Maybe it is possible to make it do the PCI initialization but idk",
      "First, motherboards today are in no way worth the money being charged for the “tiered” features they add(see $500 dollar motherboards, cost more than the cpu it can’t run without). Second, If it is possible and modders have acheived it, then how hard could it be for a manufacturer? Btw, BIOS support tends to suck terribly after the major push of the original hardware. They only update old stuff if it’s due to security. As far as platform support and length, as long as it’s viable. If a prebuilt from an OEM like Dell and HP can be trusted to do this, then mobo manufacturers should be even better.\n\n To address the single person, this isn’t some dude that is a hobbyist. If his one job is to work on BIOS, he aught to be pretty good and efficient at it. Also, why have one dude to write BiOS, what happens if he’s sick or dies in a car crash. “Whelp, Dave’s dead. We might as well give up the board business!!!”",
      "Open source you say, but that might cost us money!!!! This is right there with right to repair.",
      "Sadly, no dual BIOS on my TUF X99 Sabertooth.\n\nWhich is actually a bit ironic considering the “military grade” marketing of the Sabertooth back then. The even thing came with complete dust shields for every port and slot. But, I need a GPU for that system and it would be nice to put an Arc in save for the Rebar issue.",
      "As far as business sense, no reason to do this. This is to be read, best solution for business. As far as it being an exercise for nerds, or power users, or maybe just the frugal, it’s just pure fun or free performance. You’re kinda asking why did AMD offer backwards support for so long on CPUs. Well, they didn’t want to, but the potential loss to Intel over breaking their own word could have cost more than losses from some people not buying new CPUs. Some people remember things like this and remain loyal. Some people, like me, don’t care about that and chase whatever is best ftm. To ignore public sentiment and pretend it doesn’t matter is just head in the sand. Even court cases depend on public perception and emotion. Guilty can be set free and innocent imprisoned for their perceived traits and behavior. CPU/motherboard sales are far less important than law. Or are they ;)",
      "Its not legacy. This is not due to some limitation of the hardware. The board cpu/chipset is capable of the tech. It simply isn’t able to be toggled on. \n\n In computing, a legacy system is an old method, technology, computer system, or application program, \"of, relating to, or being a previous or outdated computer system\", yet still in use. Often referencing a system as \"legacy\" means that it paved the way for the standards that would follow it.",
      "I can't seem to get it working on my kabylake g4560 CPU with rx 5500 XT in an MSI gaming motherboard. In bios I can enable above 4g Decoding so I thought it might work but there's probably some other limitation I'm unaware of",
      "There were uefi bios workaround for old platforms like x58. You basically used a usb drive to boot i to a uefi loader. Wondering if you could piggyback this on top of that to make it work. Probably not."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "My Arc Experience",
    "selftext": "I purchased an arc a750, upgrading from a gtx 1660. It is paired with a r5 5600 and 16gigs of ram at 3200, all of this on windows 11. No overclocking, I do have resizable BAR enabled. I play mainly popular games, call of duty, battlefield, forza, rocket league. those games run perfect for my use case (1440p at 120hz) when the drivers work.\n\nHowever, the drivers are some of the most temperamental and pissy things I have ever dealt with. I cannot go a single day without my drivers failing, needing a reboot or reinstall. I have older titles that just don't work (civ V, medal of honor airborne, old NFS games just to name a few), even a newer game like ready or not crashes frequently. I cannot use VR at all because the drivers have no support on Oculus. to say the drivers make the product hard to justify to a friend is an understatement. It has gotten to the point where if I have any issue at all, I chalk it up to Arc and give up.\n\nI understand this is a new generation of product with challenges I don't have the technical knowledge to pretend I can fix, but when people say arc was half baked coming out the door, it was an understatement. I have used both NVidia and AMD gpus and have never experienced anything this intrusive in my gaming experience ever before.  I would love to hear what others have experienced, maybe I'm the unlucky one here.\n\nps, this was typed in a fit of rage after the third crash within two hours, so forgive the lack of structure and thorough explanation.",
    "comments": [
      "I have had an a380 for 2 months and I have only played warzone, warzone 2 and mw2 and I’ve had almost zero issues. First few days of mw2 I kept getting an error status thing but it must have been fixed quick. I can’t remember the last time I had an issue of any sort.",
      "You say youve never had these kinds of issues with amd cards.... ima say you never used a Radeon VII or a 5700XT at launch lol",
      "I have done a lot of testing and fiddling with an A380, A750, and A770, and I have very rarely run into game-stopping issues. If you haven't yet, I would recommend using DDU to remove the Nvidia drivers, as sometimes having multiple different graphics drivers can cause some weird issues.",
      "Radeon 7, literally every reviewer ever said \"card borderline unusable due to drivers\" even for a few months after launch.",
      "This is why being a pioneer of the first generation tech is always a risk. Want stability? Get NVIDIA.",
      "Even AMD is in a good state relative to intel driver wise. I know people crap on them a lot too, but honestly I've had both good and bad experiences with nvidia and AMD over the years. However, I know my friends with intel IGPs have had trouble in the past with certain games, and I know that the ARC GPUs seem temperamental as fudge. \n\nI'd like to see arc mature and get to a point where they're more on par with nvidia and AMD, but right now it's like...yeah no. This just isnt working. I dont have a ton of money, i want something that works reasonably well and provides a good bang for my buck, and buying into an extremely experimental technology that either works or doesn't is just not something i wanna do.",
      "I had crashes on Radeon VII while overclocked and stock, that card was beautiful build wise, but a horror story performance wise.",
      "I actually did a fresh install of windows 11 on this PC, so it has no other drivers on it. However I might ddu the current drivers and start anew, maybe a bad file is in there somewhere. I see lots of big YouTubers with similar videos where it works fine. My issue is i use it every day, so maybe I'm more prone to finding these issues.",
      "I had issues at first, but most of all my issues were resolved with the most recent drivers, [3975](https://www.intel.com/content/www/us/en/download/729157/intel-arc-graphics-windows-dch-driver-beta.html). However, when installing the drivers I literally install the drivers ONLY, no software.\n\nEver since I've had no issues. Even with prior drivers, most of my stability issues went away with ONLY installing the driver.\n\n&#x200B;\n\nMy main games I play with no issues:\n\n* Modern Warefare 2 (DX12)\n* Planetside 2 (DX11)\n* Overwatch 2 (DX11)\n* Medal Of Honor Allied Assault (DX8)\n* Halo Infinite (DX12)\n* CS:GO (DX9)",
      "Darn I wish I was in your shoes. Glad you have had a good time with it.",
      "Windows stock drivers might have downloaded themselves. Its really annoying",
      "[Unrelated to Intel/Arc driver issues] but a 3600Mhz set of ram + running the infinity fabric @ 1800 (1:1) will help with 1%/0.1% lows. While the memory controller on Zen3 is far better than previous gens, from my experience I've noticed there's still performance on the table when running ram a bit beyond the \"rated\" speed in tandem with matching it with the IF speed",
      "I appreciate your word of advice, I am aware that the ram isn't perfect, but I am sooner to blow 300 on a card that doesn't suck my cock",
      "I will say I have seen lots of comments here and the acr specific subreddit saying they have zero issues. I may be the worst case I have seen. The performance is honestly great when it works. And the feature set with av1 is great.",
      "Fairly redundant but true",
      "Dawid Does, did this already ( he wasn't alone )....",
      "Hmm I saw you posted about battlefield blue screening on the previous card.  Is this system definitely stable? \n\nThe no VR support is good to know and disappointing :(",
      "which driver version are you using?   \nmay need to use ddu (Display Driver Uninstaller) to uninstall drivers and install the new ones.\n\nI haven't had many problems if any with my a770 16gb version",
      "If you have any driver related issues take notes of what they are which game what it's actually doing and happened and report all driver bugs and faults to Intel so they can fix it.",
      "I have A770 for a months now and no problem so far. Everything runs without problem, just 1-2 older game with a little stuttering but nothing else. (12700K)"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "ASRock Challenger Arc A750 GPU Drops to $179 at Newegg",
    "selftext": "",
    "comments": [
      "Now that's a killer price.",
      "Night and Day difference from the launch, my main issues are older games that the developers won't release patches for like Fallout 3 - requiring user tweaks for them to run.\n\nDetroit : Become Human is the only newer game that I really have problems with anymore. Intel says it's up to the developer to fix it *shrugs*",
      "A750 is just slightly slower than the a770.\n\nAnyway still wouldn't buy given the rx 6600 is also $180.",
      "Which NVIDIA card is this comparable to in terms of performance?",
      "RTX 3060",
      "Went up $10 to $190 apparently but...\n\nhttps://www.newegg.com/asrock-radeon-rx-6600-rx6600-cld-8g/p/N82E16814930066",
      "How’s the driver support? I know the Arc cards were pretty glitchy at first. Any improvement on that front?",
      "I thought the a770 was comparable to rtx 3060?",
      "Nope! But it’s better to recoup some money than none.",
      "They both are more or less comparable to the RTX 3060, the gap between the A770 and A750 is much smaller than the gap between the RTX 3060ti and RTX 3060.\n\nThe 3060 will run better most of the time in older titles, ARC will run better in most newer titles.",
      "That’s a good deal. Comes with Starfield too. Thanks!",
      "There's like no way Intel is making money off of them",
      "Damm that is a great deal!",
      "Practically giving them away 👀👀👀",
      "\\>me having just gotten a non-returnable $165 sapphire RX 6600 from ebay yesterday\n\nOh well I use Linux anyway and Arc is way worse there for now compared to Windows. \n\n*For now*\n\nFor most Windows users the a750 is now the better buy at this price though.",
      "Beat bang for the buck GPU on the market now, by a large margin",
      "Detroit : Become Human doesn't work period with Intel Graphics",
      "if you're having weird stutters on the steam version of detroit become human, you have to go into the steam folder and delete the vulkan overlay files which are automatically generated every time you boot up steam",
      "Fallout 3 also runs badly on AMD. It's one of those games you need Nvidia for and I don't think that will change. \n\nI have dxvk for it on an AMD build and it still loses out to an old 1060. I've tested a 3060 ti and 4090 with it, and it's smooth there.\n\nAlso, I always love seeing a fellow FO3 fan, have you tried out the HD Overhaul mod? It was released really recently, looks absolutely fantastic, I have it loaded after NMC just in case NMC does anything it doesn't (but it probably has everything), but it also covers the DLCs and is just flat out more detailed and amazing. Looks like a modern game again.",
      "Does anyone have an a750 and can give a real world run down as to whether or not this is worth it? Currently my wife’s machine is a 2060S and believe it or not THATs more than she needs so I’d like to use that card in a frankenvuild for a friend if the a750 is worth it. She plays a lot of like build it type games like civ, tycoon games, Minecraft with the fam on game nights etc…"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Intel Arc A750 & A770 Meta Review",
    "selftext": "- compilation of 11 launch reviews with ~2240 gaming benchmarks at all resolutions\n- only benchmarks at real games compiled, not included any 3DMark & Unigine benchmarks\n- geometric mean in all cases\n- standard rasterizer performance without ray-tracing and/or DLSS/FSR/XeSS\n- extra ray-tracing benchmarks after the standard rasterizer benchmarks (at 1080p)\n- stock performance on (usual) reference/FE boards, no overclocking\n- factory overclocked cards _(results marked in italics)_ were normalized to reference clocks/performance, but just for the overall performance average (so the listings show the original result, just the index has been normalized)\n- missing results were interpolated (for a more accurate average) based on the available & former results\n- performance average is (moderate) weighted in favor of reviews with more benchmarks\n- retailer prices and all price/performance calculations based on German retail prices of price search engine \"Geizhals\" on October 9, 2022\n- for the full results plus some more explanations check [3DCenter's launch analysis](https://www.3dcenter.org/artikel/launch-analyse-intel-arc-a750-a770)\n\n&nbsp;\n\n1080p|Tests|6600|6600XT|6650XT|3050|3060|3060Ti|A750|A770LE\n|:--|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n[ComputerBase](https://www.computerbase.de/2022-10/intel-arc-a770-a750-limited-test/)|(10)|-|-|_124%_|81%|114%|143%|100%|107%\n[Eurogamer](https://www.eurogamer.net/digitalfoundry-2022-intel-arc-7-a770-a750-review)|(8)|-|116.4%|-|-|101.6%|131.2%|100%|108.5%\n[KitGuru](https://www.kitguru.net/components/graphic-cards/dominic-moass/intel-arc-a750-limited-edition-review/)|(10)|95.1%|_110.8%_|-|-|_97.6%_|128.0%|100%|108.4%\n[Le Comptoir](https://www.comptoir-hardware.com/articles/cartes-graphiques/46698-preview-intel-arc-a770-le-16-go-a-a750-le.html)|(10)|93.8%|-|_115.5%_|-|101.8%|135.3%|100%|109.2%\n[PCGamer](https://www.pcgamer.com/intel-arc-a770-limited-edition-review-performance-benchmarks/)|(9)|99.8%|119.3%|-|78.4%|106.8%|-|100%|109.9%\n[PCGH](https://www.pcgameshardware.de/Intel-Arc-Grafikkarte-267650/Tests/A770-A750-Test-Benchmarks-Preis-Release-1404382/)|(20)|-|112.7%|118.0%|72.9%|100.3%|-|100%|107.1%\n[PC Watch](https://pc.watch.impress.co.jp/docs/column/hothot/1445247.html)|(10)|-|-|-|-|_104.2%_|-|100%|110.9%\n[PCWorld](https://www.pcworld.com/article/1341464/intel-arc-a770-a750-graphics-card-review.html)|(11)|98.7%|-|-|-|99.3%|-|100%|106.0%\n[TechPowerUp](https://www.techpowerup.com/review/intel-arc-a750/)|(25)|100%|116%|-|76%|104%|132%|100%|106%\n[TechSpot](https://www.techspot.com/review/2542-intel-arc-a770-a750/)|(10)|99.7%|112.1%|119.1%|75.3%|104.7%|130.6%|100%|105.8%\n[Tom's Hardware](https://www.tomshardware.com/reviews/intel-arc-a750-limited-edition-review)|(8)|95.4%|111.5%|113.7%|72.6%|98.8%|128.4%|100%|111.9%\n**average 1080p performance**||**98.4%**|**113.8%**|**118.4%**|**74.6%**|**102.5%**|**131.6%**|**100%**|**107.9%**\n\n&nbsp;\n\n1440p|Tests|6600|6600XT|6650XT|3050|3060|3060Ti|A750|A770LE\n|:--|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\nComputerBase|(10)|-|-|_112%_|74%|107%|137%|100%|109%\nEurogamer|(8)|-|104.6%|-|-|95.8%|126.0%|100%|108.7%\nKitGuru|(10)|86.6%|_102.4%_|-|-|_93.6%_|124.5%|100%|110.9%\nLe Comptoir|(10)|85.0%|-|_104.2%_|-|97.1%|130.6%|100%|110.1%\nPCGamer|(9)|92.3%|111.5%|-|74.8%|103.7%|-|100%|112.6%\nPCGH|(20)|-|104.2%|109.6%|69.5%|97.0%|-|100%|108.8%\nPC Watch|(10)|-|-|-|-|_101.7%_|-|100%|114.4%\nPCWorld|(11)|86.9%|-|-|-|94.2%|-|100%|108.2%\nTechPowerUp|(25)|87%|103%|-|69%|96%|125%|100%|107%\nTechSpot|(10)|86.6%|98.3%|105.2%|68.7%|94.4%|123.8%|100%|106.9%\nTom's Hardware|(8)|85.7%|102.0%|104.1%|69.1%|95.4%|126.7%|100%|112.7%\n**average 1440p Performance**||**88.4%**|**103.3%**|**107.8%**|**69.4%**|**97.0%**|**127.2%**|**100%**|**109.4%**\n\n&nbsp;\n\n2160p|Tests|6600|6600XT|6650XT|3050|3060|3060Ti|A750|A770LE\n|:--|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\nEurogamer|(8)|-|93.4%|-|-|92.9%|124.3%|100%|110.2%\nKitGuru|(10)|75.8%|_89.0%_|-|-|_96.8%_|132.0%|100%|120.5%\nPCGamer|(9)|80.9%|99.0%|-|68.9%|97.2%|-|100%|112.6%\nPCGH|(20)|-|96.5%|102.2%|69.4%|99.8%|-|100%|117.6%\nPC Watch|(11)|-|-|-|-|_104.5%_|-|100%|123.6%\nTechPowerUp|(25)|74%|88%|-|64%|92%|122%|100%|109%\n**average 2160p Performance**||**78.5%**|**93.3%**|**~98%**|**67.0%**|**96.4%**|**127.3%**|**100%**|**114.6%**\n\n&nbsp;\n\nRT@1080p|Tests|6600|6600XT|6650XT|3050|3060|3060Ti|A750|A770LE\n|:--|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\nComputerBase|(4)|-|-|_84%_|74%|115%|148%|100%|111%\nLe Comptoir|(10)|60.1%|-|_73.7%_|-|101.4%|138.9%|100%|107.3%\nPCGH|(10)|-|80.2%|83.8%|73.7%|103.5%|-|100%|119.4%\nTechPowerUp|(8)|67.1%|78.5%|-|67.2%|93.2%|120.7%|100%|107.6%\nTom's Hardware|(5)|62.1%|73.9%|76.1%|65.2%|93.0%|125.0%|100%|114.3%\n**average RT Performance**||**66.5%**|**76.7%**|**80.5%**|**70.3%**|**100.1%**|**131.8%**|**100%**|**112.3%**\n\n&nbsp;\n\n&nbsp;|6600|6600XT|6650XT|3050|3060|3060Ti|A750|A770LE\n|:--|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\nGen & Mem|RDNA2 8GB|RDNA2 8GB|RDNA2 8GB|Ampere 8GB|Ampere 12GB|Ampere 8GB|Alchemist 8GB|Alchemist 16GB\n1080p Perf|98.4%|113.8%|118.4%|74.6%|102.5%|131.6%|100%|107.9%\n1440p Perf|88.4%|103.3%|107.8%|69.4%|97.0%|127.2%|100%|109.4%\n2160p Perf|78.5%|93.3%|~98%|67.0%|96.4%|127.3%|100%|114.6%\nRT@1080p Perf|66.5%|76.7%|80.5%|70.3%|100.1%|131.8%|100%|112.3%\nU.S. MSRP|$329|$379|$399|$249|$329|$399|$289|$349\nGER Retail|290€|380€|380€|300€|380€|470€|~350€|~420€\nPrice/Perf 1080p|119%|105%|109%|87%|94%|98%|100%|90%\nPrice/Perf 1440p|107%|95%|99%|81%|89%|95%|100%|91%\nPrice/Perf 2160p|95%|86%|90%|78%|89%|95%|100%|95%\nPrice/Perf RayTracing|80%|71%|74%|82%|92%|98%|100%|94%\nofficial TDP|132W|160W|180W|130W|170W|200W|225W|225W\nIdle Draw|4W|5W|~5W|9W|13W|10W|40W|46W\nGaming Draw|131W|159W|177W|129W|172W|202W|208W|223W\nEfficiency 1440p|140%|135%|127%|112%|117%|131%|100%|102%\n\n&nbsp;\n\nSource: [3DCenter.org](https://www.3dcenter.org/artikel/launch-analyse-intel-arc-a750-a770)",
    "comments": [
      "Tbh Props to Intel for making a card better than a 3060 as their first gpu",
      "Last gamers Nexus benchmarks the A770 was right behind the 3070 in some cases. So once Intel fixes the driver issues I  really want to see how it shines",
      "Better seems like too much of a blanket statement, especially with the long list of caveats for the A770.",
      "These cards were manufactured Q1 this year (based on the GN teardown video). Drivers have been worked on since then (if not earlier). And the reason these were delayed as much as they were was because of the drivers. I'm gonna go out on a limb here and say that \"a few months of driver work\" have a high chance of amounting to nothing. It could go either way. I am hopeful, but don't count on it.",
      "Getting it into the hands of users is the key to making \"game-ready\" driver updates.\n\nSome things will not fix the issues with pre-DX12 games/engines. However, I will at least give Intel props on this - they've been clear they're looking forward with this platform. \n\nDoes that hurt adoption rates in the short term? Yes. But Intel has been pretty clear that these cards aren't for everyone, but that the development of the platform and the drivers is a forward-looking project.",
      "So all in all, A770 is just beating RTX 3060 and the RTX 3060 Ti smacks them both around.\n\nSounds about right to me. Hopefully they're able to get drivers better, but I don't have any hope for non-DX12 games.",
      "Even if the drivers get sorted, I don't think that solves the old titles issues? I know they are emulating directx9, so I imagine that will take more than driver optimizations to sort out. Next intel cards might be out before that is fixed.",
      "With a few months of driver work it'll FineWine(tm). In some games it's almost 3070 levels and in 1 or 2 compute tests it was hitting 3080 levels.\n\n16GB variant could be an ML monster for the price.",
      "Holy shit, the idle draw. I've missed that part up until now. That's a no thank you from me. That and a bit too much power draw in general.",
      "Yeah [just look at what 2 years did for their DX11 driver!](https://media.discordapp.net/attachments/682674504878522386/999402021474009198/unknown.png?width=1595&height=897)\n\nOh wait.",
      "Valve's own Proton compatibility layer operates in similar fashion to whatever Intel is using, and Proton is sometimes even capable of out-performing native support. I'm 100% confident Intel can make improvements, it just takes time.",
      "same, I noticed it recently too. I thought it's just inefficient at gaming and for casual use it would be ok. Can't buy with this idle / multimonitor draw. If you're European then when long term running costs are considered, it's straight up 6700xt/3070 price point competitor.",
      "True. Maybe 50% fps disadvantage for Arc A700 on older games. But still way over 100 fps.",
      "I don't necessarily think it's locking out older gamers. A decent-spec modern PC with a higher-end Arc card should hit 150+ FPS in CS:GO. Keeping in mind most people also don't run a monitor with a refresh rate higher than 144Hz, I don't think this will make the card completely out of reach for budget/mid-range gamers.",
      "Well now they're under market pressure with a real release out and \"many eyes\" reporting bugs.\n\nTake a look at how stable and fast they are under the open source linux drivers, for example. That's the driver where some tests were hitting 3080 levels.",
      "As much as I want Intel to succeed in the GPU market, and as much as the feature suite is extremely compelling and fully competitive with Nvidia, there’s really no reason to buy either of these cards when the RX 6600, RX 6600 XT and RX 6650 XT are all such amazing $200-300 options.",
      "Right, and I think it was Steve (GN) that said alot of the older games where it falls behind, it's still plenty fps avaliable.",
      "from the same page no, 6750xt jumps from 7w to 39w (6700xt 33w). it's recent chip so recent drivers too. yet still below arc. video playback is 20w for 6750xt, while arc is at 50w...\n\nhttps://www.techpowerup.com/review/intel-arc-a770/38.html https://www.techpowerup.com/review/asus-radeon-rx-6750-xt-strix-oc/35.html",
      "Right, I am i Europe. So I'd much rather run an undervolted 6700xt for efficiency.",
      "Both Ryzen 1000+ and 8th gen Intel support Rebar. Alder Lake represents a fair leap over ol Sky Lake, and each Ryzen generation improved quite significantly."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Im very concerned about buying an arc a750.",
    "selftext": "I'll just ask a couple of questions to get some of my confusion away from everything.\n\nThe fact is, i've got a killer deal on a prebuilt with an a750, a rebar and sam compatible mobo and an i3-12100f.\n\nI'm just scared because these arc cards are apparantly still in their infancy and not fully developed.\n\nHere are my questions;\n\n\\-I will probably play alot of newer AAA titles (which from what i've seen they all work fine) the thing is, i also want to play alot of older games as well (like perhaps far cry games, crisis games, just cause games, bioshock games and more). The reason i'm confused about this is because apparantly games which use older versions of directx run poorly on these cards. So shortly, will games with older dx versions run as well? (especially with all the driver updates the cards have been getting)\n\n&#x200B;\n\n\\-My second question is about some games i personally play. I want to make sure that, TF2,CSGO and potentially LoL all run well. I'm pretty sure they do, just that i couldn't find any benchmarks on tf2 or league.\n\n&#x200B;\n\n\\-Lastly, as newer drivers were released the cards had performance increases and better compatibility. Are these still occurring? If so at what kind of rate?\n\n&#x200B;\n\nThanks for anyone who took the time to answer my questions. To be honest, if i can't get this card i'll probably settle with an rx 580 and at worse a 1650. Hoping i get a good turnout, once again.. Thanks",
    "comments": [
      "Then simply don't buy it.",
      "The last driver release for the ARC video cards made HUGE improvements, even for older titles.  It actually was enough to open the door up for Intel to be a legit contender in the video card market, even right now with the current lineup.  It also brings hope to the future intel cards being more competitive.",
      "I am having good results with my A770 16gb. I've mostly been playing newer games so idk about older DX versions.",
      "source engine games like csgo and tf2 run fine. LoL runs fine too\nif you couldn't find LoL running with an arc card, it's because you didn't look very hard.\nthis is 3rd result when searching \"intel arc lol\" on youtube\nhttps://www.youtube.com/watch?v=KM9xTcq1MTU\n\ncsgo\nhttps://youtu.be/DMscPJ_ojSU?si=ngbt450205EBMdID&t=563\n\n\ndont settle for a 1650 or rx580, the arc a750 is much more powerful than either of those two cards. if you're going to settle for a similar tier card, it'd be a 3060 or 6600xt",
      "To be fair, no drivers we/are ready for Starfield.  It was yet another premature failed game launch.  They're rushing these games out before they're ready to appease the shareholders of the company.  People need to stop paying these companies money until they have a game ready and deserve your money.",
      "Intel didn't get a review copy to make drivers with, so it's not entirely their fault there.",
      "I used an A770 for (iirc) ~6 months; in that time the drivers went from being literally unusable like 25% of the time, to being just a tad buggy in some games\n\nCS:GO ran great for me, but pretty much everything runs that great; I never tested tf2 or LoL\n\n\nI did swap away from intel GPU(s) because of driver issues in certain older games (bioshock being one of them); otherwise Arc GPUs are pretty good for the price",
      "Even in the worst case scenarios, ARC a750 will still be much better than a RX 580. Go for it.",
      "\"Does anyone have information on how these games play on this card?\"\n\n\"If you're worried about it, don't buy it.\"\n\n10 upvotes. Okay, reddit. Okay.",
      "I would be more worried about the 12100F than the GPU. if you're playing new titles the ark gpus aren't too bad at all and are completely different than what they were when they released, don't let a bunch of people who have never used touched or even seen one in person dissuade you if the price is right. I personally have a 4080 but I have high hopes for battle mage and can't wait to see what Intel can do now that they have a little experience",
      "Well Arc has XeSS which amounts to the same thing - except like DLSS it needs to be baked into the game (unlike FSR) and adoption has been slow .\n\nI'd have a little more faith. Intel has MASSIVE resources to throw at this particular problem, and if it wasn't for the fact that I want to have a usable 1440p experience, I'd have bought an A750 by now. The incredible media capabilities it has are the icing on top.\n\nThey're playing catchup for sure, but this is no friggin Noble Dragon Hung Low silicon effort. It's Intel bruv!",
      "as an addition i'll be upgrading from an i5-7400 with no graphics card (integrated)",
      "I've been gaming on an Intel Arc A380 @ 1080p / 60fps Med-High settings since release and everything has been fine, driver keeps improving.",
      "With current triple A titles, I wouldn't reccomend anything less than an i5 and an A770. I bought 3 A770s for testing and at the time shortly afyer release, they were crap. But as I understand it now, they are pretty dang good with all of the driver updates. But I wouldn't go below that i5 and a 16G  A770 range for longevity and playability reasons. You might get away with it at low to medium settings for a kittke while, but not for long.",
      "In general, arc gpus are still not recommended for users who don't want to encounter problems. If you are one of those users who want to fiddle with things and early adopters, it's definitely for you. I personally would stay away from it until it's good to use in general. For now, I'll stick with Nvidia as I use the gpu for work.",
      "Pretty sure that Intel arc has been high tiering themselves for giving older games incredible FPS boosts and overall quality in games like AC unity for example, I think Intel are dedicated and is trying to get these cards to a standard against money scammers r us Nvidia and simplistic AMD",
      "I’m using an a750 in Ubuntu. It even works well in Linux. I can play games that don’t work on my windows pc with 6900xt like enemy territory.",
      "Then don't buy arc, arc is cool but it's basically a beta, I'd u don't have much cash, buy amd card",
      "I don't know what specific deal that is, but the RX7600 has gone on sale for around Arc A750 prices. In Europe Its consistently around Arc A750 pricing to begin with. When these cards are similarly priced, the RX7600 outperforms it overall. Not only handily in DX12, but also by a larger margin in DX11.",
      "I play several DX11 and DX9 titles and they all work, but sometimes a little tweaking is necessary. \n\nI have owned an A770 since December last year, and while I have experienced some issues on older games, they are (almost) always fixed with DXVK. Basically you just drag the same set of files to each older game's directory."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "a750"
    ],
    "title": "ASRock launches Arc A770/A750 Challenger SE graphics cards",
    "selftext": "",
    "comments": [
      "article says they will be available in japan on the 12th. Other places \"mid july\" so yeah, wouldn't expect them to be available anywhere right now.",
      "doesn't look like it. The source for it posted it on the 5th, and asrock will be shipping the cards starting this month.",
      "yeah, these are new it seems. It makes sense to think it was old though since a770 and a750 are old hat at this point. The article starts out with \"with no battlemage in sight...\". It doesn't sound good to me to have the next generation nowhere to be found and for partners to be launching new versions of the existing stuff this late in the stage.",
      "Can’t be found anywhere, except all the articles talking about the performance gains. They probably bought left over chips for a discount and released some budget friendly cards.",
      "Oh, i probably confused it with some other ASRock arc cards",
      "Isn't this old news?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Intel Arc A750 Graphics Card Demoed In Control at 1440p High, Faster Than RTX 3060 & RX 6600 XT",
    "selftext": "",
    "comments": [
      ">The specs for the Intel Arc A750 Limited Edition graphics card include a cut-down ACM-G10 GPU with 448 EUs, 3584 ALUs, and 12 GB of GDDR6 memory running across a 192-bit bus at 16 Gbps, and a TGP around 200W.\n\nWell that's just plain wrong. It's 8GB across a 256 bit bus. Also, I think they STILL have not given us the official EU and Xe core count. 448 is what MLID is saying, and it could be right based on the fact he was right about it being 8GB, which Intel has already confirmed themselves. That also means the A770 at max could be 14% faster, but realistically will probably only be 10% ahead of this.",
      "No. They'll show that off later. No idea when. Some say these are supposed to launch in 2 days on the 5th, so I don't get how.",
      "It’s doable with the 3060 with dlss.",
      "And with Ray tracing on?",
      "I don't understand the issue with the naming.  \n\n\nA is for the generation (Arc), 7 is the model line and the last two are version of that model line.  \n\n\nNext Gen will be B###, then C### etc.",
      "Why is it that when I shared the direct youTube video it got downvoted to 0, and when Wccftech just shares an article with false information it gets upvoted. I'm confused.",
      "When XeSS launches in the coming month it should be doable. The RT hardware in Intel's GPUs could be close to Nvidia. Technically it's more advanced on some levels, but that does mean it's as good since I think they have dedicated less die space to it than Nvidia. Way ahead of AMD most likely, though.",
      "If it can barely do 1440p60, raytracing probably isn't a great idea.",
      "Nah, I disagree. First, the letter for a generation. Then the usual 3/5/7/9 level. Then the position within said level. Sounds almost better than what nvidia and AMD have to me. A rare thing with intel - naming that actually makes sense.",
      "Assuming not 100% bandwidth limited, A770 could clock higher too due to better binning making the max more like 20-25% higher.",
      "Oh, I don't know where to start. If you don't notice how bad it is... then I don't even know how to begin to explain it honestly. 1035G4, L13G4, 10510u, 1065G7, L16G7, 10610u - all in one gen beside each other. Everything mixed up together, no coherent meaningful naming whatsoever. Tiger lake H (35) and H (45) processors with vastly different performance under the same letter. It's a minefield which you can orient in only if you are an enthusiast or if you look up every 'i<somethingmeaningless> model' and compare directly.\n\nIn comparison with that ARCs naming is a breath of fresh air, straightforward, no Ti / Super / RTX 20 GTX 16 coexisting / XT bullshit.\n\nEDIT: Also. \"K\" means unlocked, right? You don't need overclocking you think so you get a 12600. Intel says fuck you - less cache and 4 fewer cores to you!",
      "Depends on the volume of a770.  It's possible 750 is the big sku for board partners and 770 is Intel only.  \n\nI'd expect at least an extra 5% clock speed though on top of 14% more units, similar to 6900xt vs 6800xt.",
      "Linus literally just touched one settings twice. Ona a borked early software. Is this really how we measure oc potential now?",
      "Going from A380 tests, it sits neatly in the middle of AMD and Nvidias efforts in raytracing, so a great first effort if it scales up.",
      "I actually kind of doubt that RT performance is held back much by drivers. I think it's probably straight forward. Same with machine learning. A 770 should be at RTX 3080 levels of or even 3090 levels. \n\nI actually would not be shocked if just the rasterization part if broken, or unoptimized on A770, and underperforming by like 30% of what they were expecting, and RT performance is acting like intended. In a way the card might be like an RTX 3060ti in rasterization, with the RT and tensor corers of an RTX 3080 bolted on.",
      "Yeah, The A in A380 etc isn't for ARC it's for Alchemist. Next gens model will be the ARC B380, where B will be 'Battlemage'",
      "Decent hardware that will be handicapped by awful drivers. Hopefully Intel can get that sorted out faster than AMD with the 5000 series cards.",
      "Is Intel Alchemist discrete GPU available in China yet? When does it come to North America?",
      "And were they allowed DLSS and FSR?",
      ">A rare thing with intel - naming that actually makes sense.\n\nCan you explain where this comes from? Intel naming scheme for consumer products is dead simple, and has been like that since forever. Mobility chip naming could be called wonky I guess, but it still gives you enough information about the product."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Acer Arc A750 8GB GPU is now available for just $169 - VideoCardz.com",
    "selftext": "",
    "comments": [
      "Problem is the A750 is competing where around the 6600/6650XT in performance, considering how cheap those GPUs have gotten, the A750 has to be cheaper because of the drivers (To this day, I'm still having driver issues, specifically the driver crashing randomly when using the hardware encoder, maybe about 5 months ago I never had issues with drivers crashing, but one of the games I played back then was unplayable).\n\nYou can get a 6600 for $185 and 6650XT for $210.",
      "Does Intel have so much overstock that they need to sell them at a loss? There is no way they are making any money at this price.",
      "Makes me wonder what the battle age price point is going to be.",
      "Not this low if it's a good product. They aren't going to sell the B series at a loss if they can avoid it.",
      "Most likely. People dont want to \"beta\" test them so they can have good, stable drivers in a couple of years. Was and still is the same with AMD gpus.",
      "Lately I've been pushing near RTX 3070 levels of performance in some games, which is darn nice for my A770 16 GB.",
      "They still have an uphill battle of market penetration.\n\nAlchemist isn't selling well and has an *abysmal* reputation over driver issues.\n\nBattlemage should be undercut vs the competition, finally geared with decent drivers, and spend its cycle gaining trust.\n\n*then*, and *only then* can celestial be priced more in line with the competition, per tier.\n\nMaybe even wait till druid to hike prices.",
      "Yeah I’m getting between 3060 and 3060ti levels (though usually closer to 3060) of performance with my A750 over the last couple months",
      "That's is genuinely a good price for an 1080p baller build",
      "Oh, they'll still undercut AMD and Nvidia. They won't have a choice. But they won't be selling Battlemage at a loss if it's good. Maybe only 10% to 20% margins instead of 50% to 100% margins like AMD and Nvidia.",
      "So is this a significant upgrade coming from a 2060?",
      "HUGE upgrade. It was a much bigger upgrade from my 2060 than I expected. Especially at 1440p",
      "Hmm. Should I sell my 2060 and get the ARC 750 then? Or would you recommend 770. Not sure how much more it is",
      "The A770 isn’t THAT much better than the A750, just a huge improvement in VRAM. For best value and bang for buck the A750 is the better value. There are a few posts in r/intelarc that lay it all out there better than I could. But definitely recommend. I would snag an A750 first and get it installed and updated and live with it for a few weeks before you sell your 2060 just in case you decide it’s not for you. But I’ve loved mine",
      "Still too much."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Intel Arc A750 Limited Edition Graphics Card Performance Showcase",
    "selftext": "",
    "comments": [
      "I know the state of Intel GPU’s is on super rocky ground and the drivers are broken etc. \n\nBut man do I REALLY want them to succeed.  A third competitor in the GPU space would be HUGE even if they spend the first few gens sticking to low/mid tier brackets.",
      "i am more worried about the alleged talks of cancelling the project. i know it's from MLID and all, but still, he does appear to have *some* source at intel at least.",
      "The 3060 Ti seems to be 10-20% faster in nearly the exact same scenes that they showcased. Quite a few dips below 60, and that's just the beginning. It gets much worse in clustered combat. So it checks out to probably be a little bit faster than a 3060 here.  \n\nThey really have to put up or shut up. No more teases, dangling the same 5 cherrypicked games while deliberately hiding the rest. No more Mr. corporate nice guy coming onto Techtubers show to butter people up. Get the card into real people's hands so they can do some actual benchmarking.",
      "Arc will succeed for 2 simple reasons, it will be in damn near every mobile system & pre-built PC.",
      "I just want intel to disrupt the market, it does not matter if they can or cannot beat team green and team red, if they have better price offerings, i am going with them",
      "He said they'll beat Nvidia by 20% in fps per dollar in these select titles, so this better be $299. 10% faster than a 3060 and 10% cheaper.\n\nEven at that price you're taking a leap of faith going Intel. This thing could be 3060ti or even 3070 performance in a years time in new dx12 titles, or it could be 3060 levels forever.",
      "Yeah looks like they're desperately trying to prop up this semi-broken turd before it hits the retail. The card might not be as bad as it looks, but goddamn do the drivers suck major knob at the moment. \n\nIf I was Intel, I'd make sure the e-sport titles work flawless on those cards first then move onto other stuff.",
      "He probably just latched onto the faintest rumor and peddled it as if it was the largest possibility instead of just a few executives grumbling about mismanagement. Probably was angry that Intel outright discredited him, which he made a rant about in his video peddling this rumor.\n\n&#x200B;\n\nAt most, the gaming side would be cancelled, but compute and iGPU would still be developed. Even then, this is unlikely, as all the components required to make a dGPU would already be there if you have to cover those two categories anyways.\n\n&#x200B;\n\nIt's wildly absurd to think that Intel would cancel such a product, as a lot of R&D has already gone into making it, and talent needed to be hired. And before you point out Optane, that has been ongoing for 5 years before it got canned and was struggling for years.",
      "Comparing their GPU prices to Nvidia is ridiculous, since Nvidia is hugely overpriced at the moment. AMD's ~~rx 6600~~ rx 6600 XT beats the rtx 3060 handily in everything except ray tracing. So even if this Intel GPU is beating Nvidia by 20%, it will be at best comparable to AMD's offering ... which is really bad for Intel. People who want the best possible GPU with ray tracing will go Nvidia. People who are conscious about their budget will go AMD.",
      "Using Jarrod’sTech’s RX 6600 vs RTX 3060 comparison, this card does appear to be meeting Intel’s performance claims at least in this one game, but the drivers are still absolutely horrible.",
      "It was hilarious when he noped out and hurriedly scrolled back up as soon as they got to the ray tracing settings at the bottom of the menu (which were all disabled).",
      "It doesn’t beat the 3060 in anything, according to basically every comparison of the two by a major TechTuber. The 6600 is still easily the better buy, what with the cheapest one being just $260 right now, but it isn’t a faster card than the 3060.",
      "I'm a regular consumer of MLID, but without anyone jumping down my throat, I'm just gonna have to wait and see. He's had some good stuff on Intel since at least Alderlake, but the thing is that he seems really upset that Intel people were insinuating he was a liar for saying there'd be a A780, or 512 eu Arc, while others in January were saying that their intel sources hadn't heard of sych a product. He may be open to being biased to the ultra negative side of sources he has but yeah. I really hope Arc pulls through.",
      "You know i dealt with another version of you a few months back. [Guess how his take aged....](https://www.reddit.com/r/hardware/comments/skd37h/comment/hvlkkr3/?utm_source=share&utm_medium=web2x&context=3) relevant links in there too",
      "You're thinking of Krzanich , previous previous CEO the fu*k up who basically collected premiums on previous engineering work and did fuck much else ... Keller only joined Intel in 2018, and left in 2020, and he's a tech guy, he worked on Zen in AMD, basically he kinda did run Intel into the ground sort of /s",
      "I'm subscribed to this channel on YouTube and I can even see it on their channel page but for whatever reason this video does not appear in my subscription page.",
      "their mistake was teaming with raja, who cant fit in the same room with his own ego. He beefed with the interim/acting CEO and was so toxic that Jim Keller, who was even doing some oversight on the GPU, bailed from the toxicity.",
      "No, once they cancel Battlemage it's all over. No celestial. That's what the rumors are.",
      "What the heck are they gonna price these at? Its new tech on a rocky driver base. It has to be cheaper than a 3060 I.E. sub $300 to succeed.",
      "I'm still pulling for you, Arc. Get them drivers where they need to be."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "a750"
    ],
    "title": "Intel Arc A380, A750 and A770 8GB GPUs price slashed, A380 now listed for $120 - VideoCardz.com",
    "selftext": "",
    "comments": [
      "Man, I realy wish to have your prices in Europe",
      "Never buy a promise. The A380 *might* be better than that by now, but there’s no 2023 review data for it, so we really don’t know. At any rate, it has a long-ass way to go before it hits GTX 1650 Super performance, which is really the minimum you should get for this price in 2023.",
      "Wish there was a arc a380 low profile",
      "The A380 still isn’t even remotely cheap enough, considering that it loses to a GTX 1650. That class of product should really be, like, $70 by now.",
      "Cheapest gtx 1650 on newegg is $170 and amazon for $160. A380 at $120 is not bad at all, especially with more RAM, AI cores, and a way better encoder with AV1 support.",
      "It’s nonsensical to blame increasing tech prices on inflation when tech markets have found ways to exponentially improve price/performance regardless of inflation all throughout their history. The A380 is at best *identical* value to a GTX 1650 Super from four and a half years ago. That’s abysmal.",
      "So far this is true, but nobody knows what tommorow will bring.",
      "Uk prices have been comparable with the cheapest European markets (ie Germany & Netherlands) for years  and they still are. Nowt to do with Brexit. The relative strength of the dollar due to their interest rate policies are why prices are higher in europe, including the UK.",
      "meanwhile in brexit britain A770 16gb acer bifrost is gbp 427 = 484 euro = 518 usd .....  or could just get an asus strix oc RX6750 instead.",
      "Brilliant!",
      "will only buy it under $50",
      "Let’s just remember that the GTX 1650 Super launched for $160 *four years ago*, and outperformed the A380 by ~35%. That’s the only benchmark worth comparing new budget cards to, because everything else that’s available in that segment right now is trash.\n\nI’m in full agreement that the A380 might well be the relatively best option in the hellhole that is the post-2020 sub-$200 market, but when compared to the market we should have, it’s a detestable waste of sand. Sub-$200 price/performance hasn’t improved in *four and a half years*.",
      "And if they dont? It's also perfectly possible performance could go down with more stable drivers.",
      "yes indeed. even worse when you compare it to the rx 480 which launced for around €200 7 years ago, and still should be around 25% faster on average.  \nthe rx 400 and rx 500 series gpu's also where insanely good at raw performance/compute tasks compared to other gpus. since while in gaming the difference is only around 25% back then a single rx 480 could easily beat and sometimes even double the performance of a gtx 1080 in cad software and similar compute heavy things that wheren't optimized for a speciffic set of hardware and instead relied on raw performance.  \n\n\nso actually the last 7 years there hasn't really been much advancement in gpu's in some cases the ai or raytracing can be usefull however. but ofcource we have to see how well it works on low end cards, since if it works bad then the raw performance of the old 480 might still manage to beat it in such things.",
      "I mean, Alchemist seems similarly compute-heavy, but point taken.",
      "perhaps it is indeed, I didn't yet see as many benchmarks from it outside of gaming and don't own one right now.  \nif that fully is the case, there might actually be a lot of improvement in price per performance next gen, or alchemist gpu's mught be capable of much more performance(probably won't really see that for most people, but some might experience it).  \nit makes sense since typically raytracing cores can be used quite much like cuda on nvidia, so they are typically capable of quite some raw performance.  \n\n\nso sad about performance per price not going up, but intell arc indeed seems like quite much a good trend in the gpu market, since they push the prices less insanely high. perhaps next gen or such might be a lot cheaper per performance since after all this was the first gen, and so it likely had by far more reasearch and producion cost into it, due to much more reasearc being needed for a completely new line of products, also early on optimizations for reducing cost are also limited. so I by far am more angry at amd and nvidia now.\n\nopenly I hope that Intell actually uses this, since amd and nvidia increased prices so insanely much that even their first gen was a quite good or the best competor on the market despite the pricing being as high as  7 year old gpu which performed the same. this could reduce the amount of money loss on the first gen or possibly even generate a lot of proffit and name loyality so also driver support, which might make a next generation much better in price for performance. I hope.",
      "Yeah, I think Arc has a really high ceiling in terms of raw compute performance, but I doubt it’ll ever get particularly close to that ceiling in games. It feels like a Polaris-type architecture, with more features.",
      "Your arguments make no sense, even not taking features into account. And zero goalposts were moved except by yourself.",
      "You manifestly could get something equal to or better than an A380 in… hm, probably early 2020 for $100. Also, GTX 1650 Super, for $160, 35% faster than the A380, available everywhere. We still haven’t beat that, you know, and cheap cards are supposed to be *better* value than expensive ones.",
      "And if the drivers improve?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "a750"
    ],
    "title": "Intel Arc Graphics A750 + A770 Linux Gaming Performance Review",
    "selftext": "",
    "comments": [
      "They seem solid. We just don’t get insane frame rates on ancient games.",
      "Pulling for Intel, currently have an all AMD build but I plan on upgrading once I get a 4K monitor that meets all my personal requirements.. hopefully by then (a few years) Intel will have a high end card out and their drivers are fully sorted out, I’d love an all Intel build.",
      "I'm really rooting for Intel, and it feels good to see some competition, as the 3060 is mainly crushed and being sold for way more. However, AMD has the high ground on ~$300 with the 6600 and 6600 xt's, that's some crazy value for the money that can hardly be beaten.",
      "That performance in dirt rally is weirdly high",
      "I don't think anyone buying a budget card should care about raytracing.",
      "Wednesday the 12th from the looks of it",
      "Buying parts with the promise of future updates is almost always a bad idea \n\nHowever, in the case of ARC where the biggest issue seems to be drivers I would imagine these cards will age well. But we’ll see. \n\nMaybe they’ll copy AMD’s old FineWine technology lol",
      "Unless you actually like Raytracing. That's one place the ARC series seems to justify its cost vs AMD",
      "launch day is october 12th",
      "tbh,i think the cards could go even higher than 6600xt.",
      "Raytracing is not a gimmick - it's not an Nvidia creation, it's a lighting technique that's been around for some time (e.g. CGI film creation) but only relatively recently been possible to render in real time. \n\nWhether it's good value or not is entirely different, and certainly less relevant to lower priced GPUs, but it is something that is here to stay and will be improved upon by all 3 companies over time.",
      "I’d like to know how dx9 titles run as on windows it uses an translation layer IIRC so how would something like proton compare?",
      "I was watching the LTT Livestream, and it did give me hope, but it did also highlight issues with frame consistency.",
      "Then it wouldn't be constructive criticism. The article seems to answer some questions I had, therefore it was valuable to me. I have issues with Phoronix's website though:\n\n1. site stopped loading for me on page 4. Honestly, it could be Internet or my phone. But I suspect the website.\n2. I in the past I tried to give Phoronix some money, but their website makes it a hassle to do so. You have to do a bunch of steps, that I just aint going to do. I like things like Patreon sites. I recall I set up a forums account but ran into issues.  And loss interest in figuring it out.\n\n![gif](giphy|sDcfxFDozb3bO)",
      "Also the AV1 codex encoding support seems to be a selling point for a handful of people who want to stream.  I believe the RTX 3060 decodes but not encodes. The RTX 40xx also encodes. I imagine AMD's next card will also.\n\nOf course, i didn't read the whole article to see if Phoronix tested this with Mesa (page wouldn't load). It would suck if you buy if for that and it doesn't work.",
      "Considering AMDs OpenGL compatibility has been a mess through the entire time I have owned my rx580 I don’t have particularly high hopes for older APIs",
      "Raytracing is a gimmick from Nvidia to sell things for higher price. With the current market, it doesn’t make sense for buyers of mid range cards to care about raytracing.",
      ">Unless you actually like Raytracing\n\nRay tracing is only relevant at the high-end unless you enjoy playing games at 20fps. \n\nMore realistic, AV1 support is a nice thing to have.",
      "on linus tech tips they ran some games on a770 recently and TF2 and Half Life 2 were same performance as RTX 3060.",
      ">With the current market, it doesn’t make sense for buyers of mid range cards to care about raytracing.\n\nEspecially on AMD'S cards, where it might as well be broken."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "New hardware day! i5-12490F and Arc A750.",
    "selftext": "",
    "comments": [
      "as a 12400f owner I want to know what the hell is 12490f?",
      "now I am even more mad that China is getting exclusive CPUs",
      "https://www.tomshardware.com/news/intel-core-i5-12490f-review-chinas-exclusive-black-edition-gaming-chip",
      "Chinese market. Clocked like the 12500.",
      "Wtf is a 12490F?",
      "https://www.tomshardware.com/news/intel-core-i5-12490f-review-chinas-exclusive-black-edition-gaming-chip",
      "And with a black box, which is all what really matters! :)",
      "Off meta build",
      "Interesting",
      "If ever curious, look at South Korean eBay stores too and you sometimes find pretty interesting SKUs or larger inventory of harder to find western SKUs just chilling out.\n\nThat's how I nabbed a 3300X for MSRP when they were either OOS or inflated in NA.",
      "Congrats, enjoy!.",
      "Are these Made from an Intel factory in China?",
      "I ended up getting one of these. Have it on the Port Royal Hall of Fame right now 😁.\nHow you liking it?",
      "I reject the meta completely and follow my heart, no matter how weird it is!",
      "Has the cache of 12600k though. Should be basically a 13400 sans e cores.",
      "Going with a vertical ultrawide display setup? ;P",
      "Do we know if the i5-13400 will have e cores?",
      "Tie fighter with two vertical ultrawides and a central 4:3 CRT actually (no)",
      "Every leak points to a 6+4 so it looks that way."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Intel Arc A770 16GB and Arc A750 8GB available now on Newegg",
    "selftext": "Looks like the listings are now active, and the GPUs can be added to your cart and checked out. Have fun, everyone!\n\nA770 16GB is $349.99, with the A750 8GB at $289.99.\n\nEdit: It's now showing out of stock for the A770, but I'll leave this post in case they bounce back in and out throughout the day.",
    "comments": [
      "Grabbed one!",
      "Already sold out for me.  I hate this so much.",
      "I think 750 would be the card to get if you wanna support intel. However, if you are on a tight budget and want the most from your pc, 6700xt is 360 now, and 6650 is 270.",
      "https://www.anandtech.com/show/17263/intel-arc-update-alchemist-laptops-q1-desktops-q2-4mil-gpus-total-for-2022\n\n\"Intel is expecting to ship over 4 million units/GPUs for 2022\"\n\nThat was published in feb, the same month TSMC production ramped for these. Meaning it's probably an accurate accounting sourced from Intel.",
      "They made millions of units.\n\nTBD if they just sold through them all or if it's a trickle.",
      "([Availability showing on my end](https://imgur.com/a/db6j5mH))\n\nLink to Newegg landing page, if you didn't grab it from the other posts:\n\n[https://www.newegg.com/promotions/intel/22-1809/index.html](https://www.newegg.com/promotions/intel/22-1809/index.html)\n\n[Link to A770 16GB](https://www.newegg.com/intel-21p01j00ba/p/N82E16814883001?Item=N82E16814883001&Tpk=14-883-001)\n\n[Link to A750 8GB](https://www.newegg.com/intel-arc-a750-21p02j00ba/p/N82E16814883002?Item=N82E16814883002&Tpk=14-883-002)",
      "No Canadian Newegg joy for me :(",
      "I'm seeing the same thing. It's possible that they're going to go in and out of stock, as I have a hard time believing it would have completely sold through this fast. Unless the stock consisted of whatever Ryan Shrout could fit in his car's trunk to drop off at Newegg HQ this morning. There are also the ASRock A770 and A750 cards which have not yet appeared in listings, although the LE cards look much cleaner.\n\nETA: ASRock cards are listed, but not live yet.\n\nA770 **8**GB: $329.99\n\n[https://www.newegg.com/asrock-arc-a770-a770-pgd-8go/p/N82E16814930077](https://www.newegg.com/asrock-arc-a770-a770-pgd-8go/p/N82E16814930077)\n\nA750 8GB: $289.99\n\n[https://www.newegg.com/asrock-arc-a750-a750-cld-8go/p/N82E16814930078](https://www.newegg.com/asrock-arc-a750-a750-cld-8go/p/N82E16814930078)",
      "It makes no sense to me either. I mean, there can't be this much demand, especially when Nvidia/AMD cards are all in stock. Something doesn't seem right, unless they launched with 10 cards in inventory.",
      "I can see the A750 and that hasn’t gone out of stock at all.  I was able to add A770 to my cart but went out of stock again.",
      "Had one in my shopping cart but it was gone before I pulled my credit card out of my wallet.",
      "I was able to place a back order too but the 19th has come and passed with no change to my order’s status. Any luck for you?",
      "To be fair that was including both the laptop arc cards and their add in cards for prebuilts(like NUCs). I really don't think there is really as much volume left over for DIY as we would hope.",
      "How is this out of stock??? I've been checking since 8:15 est and did not see the a770 listed as available even one time....\n\n Intel didn't even officially announce where they were selling these units unitl 9am est this morning.....\n\nWhat a joke of a launch",
      "I managed to place an order for an A770 when it was listed as a back order. Newegg took my order.  My order says \"release date: 10/19/2022.\" Maybe that's when Newegg thinks they'll get the next restock.",
      "Are there any retailers up here who have this up yet?",
      "I agree with you. I woke up this morning a little late (Intel never said what time it was launching) I think it was around 11AM and there was nothing. No way these cards are that popular. Intel probably made 100 total for the whole US.",
      "My suspicion is either they have yield issues, which would explain why there seemed to be many more A750s available, or they’re purposely slowing supply so they have more time to improve the drivers.\n\nThey won’t be slammed with as many bad reviews and tech support problems if they release the supply slowly.",
      "Yeah, there's some BS going on. How is it sold out, can't be that popular when there are tons of AMD/Nvidia cards in stock.",
      "Canada Computers has it listed, but OOS for now: https://www.canadacomputers.com/index.php?cPath=43\\_557\\_5769"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "bad fps on arc a750 any help?",
    "selftext": "im getting like 15 to 40 fps on rust and i dont know why im on the latest update and it just feels like nothing a changed for this game since October.....",
    "comments": [
      "Is this on the Ryzen 2600. It doesn't support resize Bar which Intel Arc needs to function properly. With that you should be getting ~90 fps at 1440p on high graphics settings.",
      "Welp that definitely isn’t on the box when I bought it so I wouldn’t think it’s a unfinished product that the are selling",
      "Yes it on a b350 motherboard I have the option to have it enabled so I and it says it enabled in the arc control settings",
      "I think Zen 2 CPUs are too weak for current driver overhead for the ARC GPUs. I read of another user on a Ryzen 4K series APU having performance issues on Cyberpunk 2077.\n\nTheir driver has bottlenecks even in modern APIs such as DX12 and likely even Vulkan. The Golden Cove cores in 12th and 13th gen are stronger than Zen 3 nevermind Zen 2, so low end 12th gen/13th gen/Ryzen 5K/Ryzen 7K is good.",
      "Thanks man appreciate the help I ended up getting an r5 5600x and some ddr4 3600 from 3200 and I went for a choppy 40 to 60 to 100+",
      "Intel graphics card are product with unfinished drivers, by buying it you agreed to be a beta tester for Intel. \n\nSo report this issue to Intel through their website.",
      "So update took out the intel gpu and gave up on it im going to sell it or trade it and just get a 3070",
      "My test bench is currently out of action until my new CPU cooler arrives, otherwise I would test it out, but Rust is a DX11 game I believe, so it's possible that it has performance issues on ARC. \n\nYou can report issues [here](https://github.com/IGCIT/Intel-GPU-Community-Issue-Tracker-IGCIT/issues).",
      "I installed a r5 3600xt still getting max 50 fps",
      "Yeah like I can average 60+ on my rx 470 so I was just confused on why the performance is below 40 fps most of the time. Now that I know that it was an unfinished product I wish I didn’t buy it now and I could have used the 300 dollars on and actually good gpu… I guess going off of specs isn’t something I should do with Intel",
      "It looks like some motherboards are able to enable it, but it doesn't actually function properly on 2000 series.",
      "What's your ram lookin like",
      "idk why the downvote on this comment but its the reality, even polaris were cursed for its dx11 perf, they still tank alot on vulkan, dx12, and recently, opengl.\n\nfun fact you could use resizable bar on polaris gpu with 8gb of vram.",
      "are you still using arc a750? how is it performing now?",
      "Thanks man I would have gone on forever trying to get it to work right I have no problem with the cpu power it runs every great but I guess I’m being forced into upgrading",
      "24gb at 3200",
      "It’s gotten better with the gpu update no getting massive drops into 15 to 30 range I stay between 50 and 70 after some more test",
      "I love it has awesome ray tracing I get any where from 80fps to 120fps depending on the game’s settings and capabilities, but I’ve been amazing can’t wait for the next line up. Definitely sticking to intel if they keep the prices good",
      "Just picked up a r5 3600xt so hopefully",
      "I’m new I thought ram only play a part in like loading speeds not fps too lol i have to much into the build as is so looks like I’m giving up on the arc"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Rtx 3060 or Intel arc a750/rx6600",
    "selftext": "Rtx 3060(12gb) costs me $325 and\nIntel Arc a750(8gb) and RX 6600(8gb) costs me $240. \nI have i31200f cpu. Which gpu should I go for? ",
    "comments": [
      "Following this because in on the same circumstances, but just rx 6600 VS arc750.\nThe 6600 is it usd 200 brand new rn on ebay, and the a750 can be found for usd 180 brand new as well, but it looks quite more robust than the rx for the price/performace, the drivers thing seems like a concern though.",
      "rtx 3060 12 GB, because it is the only card of the 3, that has enough vram.\n\nalso the intel card would be out, even if it had enough vram, because of the driver issues."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Intel floods European market with Arc A750 GPUs, French retailer has 1233 in stock",
    "selftext": "",
    "comments": [
      "> WoW dragonflight\n\nThe game engine supports DirectX 12 and Arc GPUs run DirectX 12 games flawlessly, performance should be good.",
      "I frequently see the 750's but why not 770?",
      "Anyone have data on WoW dragonflight performance of one of these? Might be a worthy upgrade from a 1070",
      "Are you located in the EU?\n\nTheres a marginal difference between the a750 and the a770 in gaming, I want to say 10%? But I need to go back into the benchmarks by GN\n\nThey probably did a market analysis and thought the a750 would sell better in those markets\n\nThe “limited edition” is just in the name of the product and is by no means limited in terms of production capacity",
      "Anything poorly optimized is going to run horribly. I can put square tires on a car and it will run like crap regardless of how tuned that car might be and how flawlessly it will run with round. \n\nThat doesn't take away from the fact that it will run DX12 games well.",
      "That's obviously not an Arc issue, it's a specific-game optimization issue https://www.youtube.com/watch?v=KbOyz9W3aUU",
      "Probably due to yields almost all produced dies qualify for the A750 bin but markedly less are 100% perfect as needed for A770.",
      "Sorry, let me revise that. I guess you could say Dead Space DX12 runs ‘errorfully’ on Arc.",
      "Probably because the 770 is supposed to be the \"Limited Edition\", I guess so",
      "Hopefully we see some offers with this much offer",
      "My guess is nobody buys 750s.",
      "It's quite obviously an Arc issue seeing that other manufacturers cards work perfectly fine.",
      "And it doesn't run that DX12 game well, so the DX12 API is not a guarantee that Arc will run well. If AMD and Nvidia ran Dead Space at an unplayable level everyone can be suffering together in misery, but here Arc is the odd man out. \n\nThat's a good analogy. Putting an Arc gpu in your pc is like putting square tires on your car.",
      "> flawlessly\n\n[Hmmm..](https://www-computerbase-de.translate.goog/2023-01/dead-space-remake-benchmark-test/3/?_x_tr_sl=auto&_x_tr_tl=en&_x_tr_hl=en#abschnitt_das_testsystem_und_die_benchmarkszene)"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "How stable is the Intel Arc A750 for normal PC (non-gaming) use?",
    "selftext": "Anyone using the Intel Arc A750 with Lightroom Classic?\n\nThis is the last piece to my new i7 13700, Z790 Gigabtye Elite AX & 32 gb DDR5 photo editing build.\n\nI need a GPU for a photo editing build using Lightroom/photoshop. It  will do NO gaming and NO video editing. Photography editing is 2D  computations.\n\nSo I read a lot of negative reviews/remarks about the A750 with respect to gaming performance as well asstability. Is that irrelevant for my use and is this  a better value GPU than the RX 6600 for MY usage? Should I care if it  can't run certain games that I never heard of before? or would it impact  my use of Lightroom/photoshop? I also wonder if there is even more  improvement upcoming as they continue to refine the drivers? I doubt the  RX 6600 is going to see improvements. Both of these GPUs are on sale  for roughly the same price. Need to pull the trigger tomorrow and get it  built!",
    "comments": [
      "The Arc should work fine I haven't heard any complaints about it aside from gaming so you should be good to go",
      "It’s definitely better than the RX 6600 for photo editing - hell, it’s even consistently better in gaming now.",
      "Thanks...glad to see there were no issues while doing non-gaming tasks",
      "From my experience the two main areas you would see a benefit from the GPU in photo tasks are in preview rendering on higher res monitors, and some passive tasks like filters. Do you feel like there’s any improvement / disadvantage in regards to those?",
      "Early days.....I purchased the a750 and put it in the new rig I had built 7 days ago - absolutely NO issues so far....I have processed photos in LR- superfast - and watched videos on youtube. So I have not stressed it and with my usage I never will. It has been solid and a great value purchase....test will come when I update the next driver updates.",
      "At the start it was a bit unstable had random blue screen and didn’t wake up from sleep and so on. They seemed to have ironed this stuff out I have not had a blue screen for a long time. It is very stable now. The games I run have not crashed on me specially mw2 and god of war, halo infinite, apex legends.",
      "It's still a little too soon to depend on it professionally in my eyes. The 6600 will certainly be more reliable.",
      "If you end up getting it I would love to hear an update on how it works out for you! Using a lot of Photoshop/LR as well.",
      "Last time I checked? About as stable as a one legged tight rope walker. But it's been three or four months since I last checked. Bit from what I understand, things have gotten much better since then. \n\n And about three years since I lost my leg, but I can still walk a tight rope! Just not real well.",
      "I'd be more concerned about that 32 GB of RAM than a discrete GPU. GPU will barely matter, but RAM on the other hand...",
      "That's good news....trying to design a new build after 11 years with my current rig was quite an ordeal. It seems every review of components is from the POV of a gamer or overclocker since they probably are 95% of the people that build their own PCs.....most photo enthusiasts don't have the stomach to learn about pc components and shell out big money for some nice MACs.",
      "I've seen very few reviews of the A750 (maybe 4) with a lightroom/photoshop benchmark but ALL indicate it is great for 2D photo rendering - i was just scared by the ton of driver related warnings and hoped they wouldn't be an issue with my usage. THANKS!",
      "Early days.....I purchased the a750 and put it in the new rig I had built 7 days ago - absolutely NO issues so far....I have processed photos in LR- superfast - and watched videos on youtube. So I have not stressed it and with my usage I never will. It has been solid and a great value purchase....test will come when I update the next driver updates.",
      "I'd hope so considering it costs as much as a 6600xt",
      "So you bought it before the recent driver update? I don't game. I do photo editing (for a hobby, not income) and I don't want to lose a day's work because of instability.",
      "Will keep you posted if I have the nerve to buy it!  ;)",
      "Do you have any real experience with the A750?",
      "The GPU WILL matter if it is unstable, thus the query.\n\n32 gb of RAM should suffice for photo editing (NO video).",
      "There are a few uses of it in Lightroom - exporting uses it a bit. But you are right, it is of benefit more for high -res (especially multi monitor set ups) - which I don't have. I am thinking that this is a GREAT low budget GPU for those who don't game or render video.",
      "I tried looking into things to figure out why Lightroom didn't let me check the box for my 3080 ti for\"GPU acceleration\" in Lightroom. I can't remember exactly what I found, but it had to do something with the CPU is mostly going to do the heavy lifting for edits anyway, so the GPU doesn't really seem to make much difference in the end. \n\nI wish both classic and CC versions were more smooth in windows at least. I use a11800h laptop and a12900k i9 and on both computers it will still stutter and not be smooth."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "intel arc a750 triple setup",
    "selftext": "someone who knows how to adjust the monitor setup in the software for the Intel Arc A750",
    "comments": [
      "This subreddit is in manual approval mode, which means that **all submissions are automatically removed and must first be approved before they are visible**. Your post will only be approved if it concerns news or reviews related to Intel Corporation and its products or is a high quality discussion thread. Posts regarding purchase advice, cooling problems, technical support, etc... will not be approved. **If you are looking for purchasing advice please visit /r/buildapc. If you are looking for technical support please visit /r/techsupport or see the pinned /r/Intel megathread where Intel representatives and other users can assist you.**\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/intel) if you have any questions or concerns.*",
      "Just go to display settings in windows"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Arc A750 with Ryzen 7 2700x ?",
    "selftext": "I  have an Arc A750 GPU on order, and despite the  requirements/recommendations with minimum compatible processors not  showing this gen of CPU's, everything I read seems to point to the need  for ReBAR being the reason. I have an ASUS TUF Gaming X570-Plus (Wi-Fi)  motherboard which has been flashed with the latest bios and has let me  enable ReBAR in the bios even with my 2700x (and I have set all the  proper settings to allow this)\n\nI  have used GPU-Z to confirm this as being active. Should there be any  reason given this is the case that the A750 still wont work with my CPU?  The only thing I might guess would be if the currently shaky Intel  drivers simply won't work with it for some reason beyond the need for  ReBAR.\n\nFrom reading up on SAM and  ReBAR it seems some companies boards were locking people out of turning  on this feature if they detected certain CPU's but others had no issue. I  obviously feel I am in the latter camp. Short of intel drivers not  playing nice with it for whatever reason, should the A750 have no issues  in terms of functioning properly so long as ReBAR is enabled on my system?",
    "comments": [
      "So did you ever manage to benchmark this? I've a 2700x and an MSI B450 Tomahawk that has ReBAR support with the latest BIOS. But the question is would an Arc card run like crap with it. I really don't feel like upgrading both the CPU and the GPU, especially considering I barely have any time to game these days. The 2700x is still excellent for work and productivity on a budget.",
      "I have my 2700x working with ReBAR currently despite thoughts that it may be too old for some boards. I am thinking in my case it will run due to the fact that it is enabled and seems to be functioning. I checked this with gpu-z to make sure. Though I will certainly agree a newer processor wouldn't hurt, particularly to get pcie 4.0 vs 3.0 https://imgur.com/a/cDQr1ej",
      "There have been issues with Arc Control showing ReBAR as being disabled even though GPU-Z, AIDA64, Windows Device Manager and other software list it as enabled. It seems to just be a cosmetic problem that needs to be fixed.\n\nhttps://www.reddit.com/r/IntelArc/comments/zsepcv/rebar_issue_gpuz_vs_intel_arc_software/",
      "Let me guess, broke/fell out of the pcie slot?",
      "2700X is a decent productivity CPU, and was, in its time, a \"decent\" gaming CPU. But zen+ is a bottleneck these days.\n\nEven zen 2 is a bottleneck these days. Lots of benchmarks from techtube back this up. digital foundry, HUB, gamers nexus, etc. Even at 4K, if you game at such a res.\n\nZen 3 or intel 10th(ideally, 12th) gen are the minimum starting points you want to be at. and luckily 5600non-x and 12400f are cheap as dirt lately.\n\nSince you have the X570, just drop a 5600 non-x or 5600X in it. It'll run circles around the 2700X.",
      "OP, listen. You do NOT have Rebar support. You need at least a 400 series motherboard AND a 5000 series CPU. You need both to support it in order to utilize it. Just because you enabled it in the BIOS doesn't mean you have it.\n\nThe Arc runs like crap without Rebar. Therefore, unless you plan on upgrading the CPU (which you should do anyway, because a 2700x is, what, 5 years old and missing relevant technology), then DO NOT buy the Arc. Intel CEO even said that if you dont have Rebar, don't buy it. \n\nGo with a 3060ti instead.",
      "Is it for gaming or office ?",
      "I have not yet. I still do have my 2700x, but on some level now that it has been safely removed and is stored in the 5600x's plastic case and box I am tempted to just get it sold and avoid having to deal with removing the annoying cpu fan and reapplying thermal paste on both processors for a test. It would be pretty useful for the community though. I did manage to read some other stuff about older processors frame timing? being janky. Like the FPS is smooth other than various moments where it hangs and then zaps back to smooth. So consistent but with jumps and hangs?  \n\n\nI am sort of glad I upgraded my processor though, because in all honestly there are still quite a few issues and bugs to work out of these cards, and probably will be for a while. It is nice to know that any of the issues I experience are worth investigating/reporting for a fix.   \n\n\nI feel like if I was still running the 2700x, I would never be sure if an issue I am having is due to the unsupported processor, settings in my software, or legitimately a problem with the card. I did manage to figure out how to get the idle power draw down on my motherboard, despite it missing the settings to do so. So that was quite the useful trick that I hope ends up helping someone down the line at least.  \n\n\nI will be sure to update this post if I get bored and decide it would be worth running some tests for science sake. It was kind of annoying to have people try to dismiss running it with a 2700x after all. And I only just barely happened to be able to swing a new processor on top of everything else.",
      "Problem is I have no money for a new CPU atm or I certainly would. I just moved across the country and my motherboard and rx580 gpu both took a shit in shipping. New parts were not in the budget by any means, but I had to drop $220 on the new board already \\*cries\\*.   \n\n\nI was watching some benchmarks comparing the 2700x and 5600x and it seems once you went up to 1440p/4K that the differences closed in FPS as the GPU took over the heavy lifting. This doesn't of course address frame stability or whatever unfortunately which I am sure would also be much better on a new cpu.\n\nThere certainly was a noticeable gap at 1080p with the card they were using for the comparisons, no doubt about that. I won't need to go above 60fps on the TV I am using (fresh is 60hz) so I figure so long as it works at 30 to 60fps on anything I toss at it I would be happy for now. Getting a better CPU would be next on my list when I can though I totally agree, hell... if even to get pcie 4.0 vs the 3.0 I am stuck with atm.",
      "Why would windows and gpu-z show it, and why would my motherboard (an x570) allow it to be enabled if my processor doesn't support it. I have found upon further research that other people in other posts show improvements on frame rates with processors that supposedly are not supported. They showed screen shots.   \n\n\nIt seems like the processors were only software locked. I am open to the idea that even with it enabled (and everything in my system settings and gpu properties showing its on) that the processor will still handle the job too slowly to function properly, but by all reports it seems to be on according to windows and bios. They just added the 3000 series of processors after a driver update so I would think its entirely possible that its functioning.",
      "It is kind of my all in one machine. I do use it for office stuff when I am actively running my (sorta) business. Mostly sales on social media, printing, shipping, payments, etc. But it lives in my bedroom and I use it for all my entertainment as well like movies, music, gaming, graphic design (which is business as well as pleasure).   \n\n\nI would also use it for any work from home jobs that might come my way as well. The only other computer I have is a shitty half working dell laptop from 13 to 15 years ago, so I tend to want this PC to work and work well for all my needs for a few years at a time. I used my last setup for about 8 and a half years before I built this one 3 years ago. I love getting mileage from decent budget builds :) I am super impressed at how good the rx980 GPU I had was and how long it has lasted me. I was only just starting to hit acceptable to me limitations but thought I had at least a few more years to squeeze out of it.",
      "That's not right. Having it enabled doesn't mean anything if the software won't take advantage of it (for many reasons). For example, you can enable ReBAR and get all the indicators that it's enabled on AMD RX 5000 GPUs yet gain no performance since the driver doesn't support it.\n\nRefer to the [official Intel statement](https://www.intel.com/content/www/us/en/support/articles/000091128/graphics.html), not your intuition or random people on Reddit. If it worked, they would probably list it there.\n\nIf you want to verify it yourself, use your current rig to run a game that has measurable (>10% fps) gains from SAM - eg. Forza Horizon 4/5.",
      "While you may have it enabled in your BIOS, that doesn't mean you have it. Your processor doesn't support it. Therefore, you don't have it.",
      "2700X are bidding above 100 on ebay. A 5600 is $130.\n\nYou got 30 bucks laying around?\n\nThe difference is huge at 4K and 1440p. Ignore avg fps charts and look at frame time graphs, stutter, 1% lows.\n\nI game at 4k60 and saw a huge, HUGE improvement in smoothness going from a 3800X to a 5600X. Even in cases where fps is locked at 60, frame pacing issues were fixed.\n\n>  I have no money\n\nIf true you shouldn't even be here. Sell what you have, focus on survival, and buy back in when you have spare cash to pull trigger on hardware buys.",
      "Thanks for the lookout! I could see that throwing me for a major loop. I will have to run some benchmarks and see how well its running compared to others with slightly newer processors.",
      "Yeah I have a 6500xt atm I could probably do a test on something like forza like you mention. That and/or maybe some kind of benchmark utility. \n\nI can also try tossing in a Ryzen 5 5600x, that I have very temporary access to, and see if there is some night and day change at 1440p/4k with the arc card vs my 2700x. If ReBAR isn't actually functioning/providing any benefit with my 2700x CPU it should hopefully be clear by the vast difference in FPS since there seems to be little to no difference in fps improvements at these higher resolutions based on the cpu.",
      "People are reporting it as working for them in getting higher frame rates even with processors such as mine (I have found upon further research). It seems like it is indeed functioning. While I can't be certain the older processors are actually efficient in doing it, or it technically works but with little to no improvements due to some form of slow emulation is another story.   \n\n\nI have a 5600x on very temporary loan that I may try after running some benchmarks with the 2700x and see if they don't seem to meet what others are experiencing. At least with 2k to 4k resolutions where the processor power matters less and its all about the gpu. Or as another user suggested, to turn it on and off with my current card to see if it changes anything for the better as is.",
      "Hehe worse on some level! It looks totally fine... But trying out a new power supply, processor, ram, and even a new video card on the board wasn't working. Could get no POST screen at all. And when I put a new video card in the old board, it acted like it wasn't even plugged in (beep error codes). Tried everything 5 times. I finally bought and tried a new board, and it shot to life, but my original video card was throwing scrambling purple and green lines down the screen like an old dirty nintendo (even in POST/bios screens). New temp card worked flawlessly.\n\nI tried cleaning its contacts properly in case it was somehow dirty and such, but no dice. The reason I say \"almost worst\" is because of how frustrating it was to trouble shoot everything else but the main board, the waiting and wondering aspect. If it had just broke/fell at least I could have taken care of everything much faster. I now know to take my shit apart and wrap it up individually if I am going to ship something like that.",
      "My PC was damaged in shipping from a cross country move. My main board and the GPU both. So here I am... I spent $200 something already on the board, and don't want to waste more than half the cost of a decent gpu that can carry me into the future with a crappy card. My computer is an essential item to have running at least semi decently (even if gaming technically isn't). I just wasn't expecting to have to replace a MB and a GPU randomly but they needed it or else, no computer...\n\nI am not disagreeing with you that stutters and performance will improve with a better cpu, just that I don't really have more money or time to sell mine and not have a functioning machine while I wait for the other. But if I can sell my 2700x on ebay for $100 that will make holding onto a 5600x a lot easier. Though on amazon I see them for more like $165 after tax. ![gif](emote|free_emotes_pack|grin)",
      "The ebay idea for the 2700x is a great idea though, It could easily pay for more than half of a new cpu... making obtaining one much more possible."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "a750"
    ],
    "title": "Intel Arc Graphics A580 / A750 / A770 Linux Performance For Early 2024",
    "selftext": "",
    "comments": [
      "This subreddit is in manual approval mode, which means that **all submissions are automatically removed and must first be approved before they are visible**. Your post will only be approved if it concerns news or reviews related to Intel Corporation and its products or is a high quality discussion thread. Posts regarding purchase advice, cooling problems, technical support, etc... will not be approved. **If you are looking for purchasing advice please visit /r/buildapc. If you are looking for technical support please visit /r/techsupport or see the pinned /r/Intel megathread where Intel representatives and other users can assist you.**\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/intel) if you have any questions or concerns.*"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "12Gb RTX3060 Vs 8Gb Intel Arc A750. Need only for video editing .    ",
    "selftext": "Which is good for video editing \nCpu. I5-14500 \n",
    "comments": [
      "We don't talk about the Intel Arc."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "a750"
    ],
    "title": "Intel confirms upcoming Arc desktop SKUs: A770, A750, A580, A380 and A310",
    "selftext": "",
    "comments": [
      "Did they also leak in which decade they'll be released? /s",
      "No way. It's 3070 level hardware with unpolished drivers. Expect 3060Ti real-world performance.",
      "Do we have any idea how they will compare to existing gpus yet?",
      "Intel should’ve been very clear about the release timeline. I think they still can be transparent about it.",
      "IIRC the high end is supposed to be up around 3080 tier",
      "They already went transparent with it. \n\nThey told that early summer desktop GPUs will be launched as OEM only in china. \n\nThen in late summer launch OEM only worldwide. \n\nThen \"later\" launch as standalone GPUs.\n(So one could expect October/November for this)\n\nOverall it seems like they really really want to have most of the driver issues sorted out before releasing standalone GPUs.",
      "Good luck guys.",
      "This century. Stay tuned.",
      "There are low end GPU out there, it is just the value is so bad that people ignore it. With possible bad driver due to first launch, it will be irrelevant",
      "Existing in may 2022 or April 2024 when they are actually available? 🤣",
      "You expect the first iteration of GPU to meet the halo products of established GPU manufacturers such as AMD/NVIDIA?",
      "It's because Raja Koduri promised stuff. \n\nHe promised the GPUs will be ready early, in Q1 2022. \nTurned out to be a lie.\n\nThen he promised GPUs in the hand of gamers for cheap - again a lie because they prioritize OEM.\n\n\nPeople say Intel GPU will be irrelevant if they launch after Nvidia launches RTX 4000 and AMD Rx 7000. \nI don't think so because all the Nvidia and AMD products from new gen are gonna be 600$+(real price, not msrp).\nIntel's best GPU is supposed to be under 500$.",
      "the lack of vram makes me sad, though i'm not sure what i should've expected from the bus widths. wish 8-12gb was standard because 4gb is proving not to be enough and 6gb is next",
      "And I honestly agree with their way of going right now. Why do people care so much that it only gets released in China. Makes no difference if they release it worldwide now, or later.",
      "Those were likely early laptop performance leaks at reduced clocks.",
      "Yes. It has enough transistors at like 22 billion and already has RT performance better than NVidia or AMD. It's tier 4 vs Tier 3, and Tier 2 for AMD.\n\nIf you include the games where it crashes due to drivers to drag down averages it'll be like 3060ti levels.",
      ">He promised the GPUs will be ready early, in Q1 2022\n\nHe actually said that? When?",
      "I don't know for sure if it was him but there were messages from Intel that arc GPU lineup will roll out in Q1 2022."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "[GamersNexus] Intel Arc A750 GPU Hands-On, Driver Challenges, & Overclocking",
    "selftext": "",
    "comments": [
      "This was a really good overview, and the Intel guy seemed pretty open.  I just wish they had revealed some real hints at performance and release dates. GamersNexus did a good job (as usual) with what they have..",
      "Absolutely loved the entire video, no BS and people that were excited to talk and work with each other.",
      "Performance is an easy guess.\n\nthe 8 Xe core GPU's confirmed performance is ~1050Ti.\n\n32 Xe cores would be 4x that in compute, but it also gets a wider memory bus, so picks up a few more % fps.\n\nOn napkin math, that puts it somewhere around a 3060/3070, which is in line with leaks from last year.",
      "Pretty insane to think about all the hard work behind the current performance of arc GPUs, even if we think its lackluster compared to Nvidia/AMD.",
      "It's their first effort at a \"real\" GPU. Nobody expected them to flounce on Nvidia and AMD out of the gate given their history in the iGPU space.\n\nBy their C or D gen they should have the knowledge under their hats to be equals though.\n\nAfter they get parity, it'll just depend on who has the better process technology and architecture.",
      "> increase competition in the marketplace.\n\nPour one out for the boys (Matrox, 3dfx, S3)",
      "I'd be conservative and say more likely closer to a 3060 in performance than a 3070 but who knows tbh.\n\nIt was never realistic for Intel to compete with Nvidia's high or even upper mid range cards with their first generation. Important thing is that they got something out of the gate and probably learned a lot in the process to be more competitive going forward.\n\nI probably won't buy an Intel GPU for a long time (until they reliably compete with Nvidia's high end at least) but I do want them to do well to further increase competition in the marketplace. A three way competition between Nvidia, AMD, and Intel will be great for consumers.",
      "If it honestly matches a 3060 or 3070 that is very impressive as a first attempt. It's like people are expecting them to match the best of the competition straight out of the gate. If the price is right based on performance, these can sell well",
      "This. \n\nTBH you’ll probably see 3060 levels from it most of the time with optimized “game ready drivers” pulling fine wine 3070 perf out of it.",
      "Intel does at least have years worth of iGPU driver fixes in place for older titles. It may not be 100% optimized for Xe but the major papercuts are gone, and it's not like those old engines need to maximize their usage of a beefier GPU than anything that existed during their launches.\n\nMy Xe iGPU (which shares drivers with dGPU) runs FNV great, for example. I can even load ancient obscure dx8 stuff like Live For Speed and it runs perfectly.",
      "> It's their first effort at a \"real\" GPU.\n\nThat was Larrabee like 10 years ago.",
      "One thing we haven't seen yet is RT performance, that might be up being Intel's drawcard.",
      "Yep. The right price, good drivers (coming along slowly, but surely), and they've got most of the market served.\n\nHigher end cards are nice, but in terms of market saturation they don't get very far because they're priced out of most consumers hands.\n\nIntel is 100% gunning for mass market mindshare saturation before ever targeting the high end. If they went high end out of the gate they'd flop, especially with A-gen driver quality.\n\nThey need millions of people going \"hell yeah I can play fortnight (insert any wildly popular eSports or zoomer game) at 400 fps on intel!\" not a few thousand neckbeards going \"Hell yeah I can do Cyberpunk at RT 60!\"",
      "Think Intel is gonna skip the western markets for the alchemist generation and try a proper release for battlemage?\n\nHigh end Lovelace and RDNA3 is around the corner, current Ampere and RDNA2 availability is high, with prices on a downward trend. It will be difficult to carve a niche unless the AIB and Intel are willing to lose money to differentiate themselves significantly.",
      "It's really hard to go from having no high power GPU at all to having one. All the other graphics chip manufacturers gave up fighting Nvidia and AMD decades ago.\n\nJust glad to see competition isn't just two companies anymore. Miss the glory days when you had 3dfx, Nvidia, ATi (now AMD), Matrix, S3, etc. all producing graphics cards. After Nvidia started the GPU era they really whittled down the field because most companies just couldn't put the resources in to build a competitive GPU",
      "Don't make any moves right now. That RX 6600 is still good and Arc, IMO, should only weigh in your purchasing if it's cheap and exceeds what you have. And to know that, it has to launch and get reviewed and have pricing announced, land on shelves, and actually test market pricing. I don't think there's any outcome where the Arc GPU will sway a current RX 6600 owner to switch, as it won't be *that* much faster.\n\nNothing wrong with grabbing a 4060 later, but the pricing on that may end up shocking us.\n\nRegardless of the shakeout, having 3-way competition is exciting."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "New Arc A750!",
    "selftext": "",
    "comments": [
      "I really love the looks of the Arc GPUs.\n\nCables could use some time tiding up.",
      "Yeah, they're clean.  The gamer stuff certainly has an audience - and that's fine - but I doubt its even 50% of the GPU market.  Really wish manufacturers would tone it down more for at least 1 or 2 models.",
      "The Arcs are beautifully made GPUs. Great looking rig as well. I think in less than 5 years, Intel GPUs will rival Nvidia/AMD in compatibility and performance, possibly even top them.",
      "Enjoy your new toy!",
      "Yeah, so far I haven’t run into any major compatibility issues, but there was a blue screen with some Graphics error code while playing RDR2 at 4K. Besides that, I have literally no complaints",
      "Sweet pick up",
      "I recently switched my mounting positions from horizontal to vertical. I think it is a lot nicer looking at the whole card itself, especially with the RGB. I only did this because, with my new motherboard, I could not mount my GPU  horizontally for some damn reason. Luckily I had a spare riser cable that I wasn't using, and now the whole setup is super nice.",
      "Really wish AIBs would follow suit and stop with the cringy \"gaming\" designs. I like all the reference cards from green/red/blue but i fucking hate all the bling and slogans of the AIB custom models.",
      "Yeah I would totally do that, except this card doesn’t glow :( it only have a white “Intel Arc” logo on it, and nothing else. Not complaining, it just is what it is",
      "I agree, I have a reference 6750xt and love the look and these Arc GPU’s look even better.. but AIB will stop designing them like that when consumers stop buying them."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Arc a770 or Arc a750?",
    "selftext": "I'm looking at building my first pc and I want to go with an intel arc graphics card. I don't need crazy performance, but I want something that can run the latest games and will last for a decent amount of time. \n\nWith that being said, I'm debating between the Arc a750 and Arc a770. I can buy the a750 for $220 on Amazon and the a770 for $460 on Amazon (also have a bid on ebay open box with a maximum bid of $350). What is your recommandation?\n\nThank you!",
    "comments": [
      "The A750, it’s not even a question. You’re getting 92% of the performance for half the price.",
      "Just get a750, there is no point of 770 . a750 is 90% of a770",
      "The problem with that is the RX6700XT exists for the price of the A770. The A750 is easier to justify the cost of.",
      "Fair enough, my A750 is unbeatable in price/performance.",
      "A770 should be around 350$ and I'd go for it, I already have an A750 and I feel like I'm gonna need the extra VRAM in a not so distant future.",
      "If you want longevity and insist on an Arc card then id suggest the 770 over the 750",
      "I have the 750 and like it a lot but the only game I play is a modded GTA V with video quality maxed out. I still get 40fps",
      "I went for the 750 with my Ryzen 7 5700X as it had the two items I was looking for. 1) An 8-core processor, and 2) good 1080p graphics.\n\nWhat is now being raised as an interesting question is how much 8MB VRam is useful in today's GPU - but for low spec gaming - I can't see how that will affect me for the next 2 years",
      "Out of the two, I'd reccomend splurging on the A770. But either way, be prepared for possible issues with different apps, including games. I was an early adaptor if the A770, bought two different models, and had so much trouble I sent them both back. But from what I understand, Intel has come a long ways since then and they're looking like a pretty good option with the understanding that you may run into some snags along the way. Good luck and have fun Mate!",
      "A750 is way better since it's almost all the performance for a much cheaper price. But 16GB of VRAM on the A770 I think is mandatory for a new card in 2023 unless you're planning on playing in 720p.",
      "When you say low spec gaming, do you mean 1080p?",
      "Yes, when compared to 4/5/8K @ 150fps average.\n\nOne thing that I didn't include in the op was my need to be as efficient as possible with the power.\n\nHaving an 8-core at 65W met my requirements for the CPU, however, researching power draw on graphics I found that the higher resolutions require more energy. \n\nI then had to make a choice based on voltages against price - and price won out with the 750."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Will it be downgrade if I sell Arc A750 and buy Quadro P4000?",
    "selftext": "I currently have Intel Core i7-6700 system with Arc A750 using custom Resizable Mod on motherboard. The only problem is that my Arc A750 makes a huge framedrops during the gameplay like PUBG or Call of duty which I've noticed on my previous RX580.\n\nRecently, I found a very good condition Quadro P4000 for $120 on local used market. Will it be the downgrade if I replace my A750 with P4000?",
    "comments": [
      "According to benchmark testing about a year ago and recent video YouTube reviews of the performance of Intel ARC video cards 3 months ago, the Intel ARC A580 8 GB and Quadro P4000 8 GB had a performance boost about 20% above that of the AMD Polaris RX 580 8GB. They had FPS in the 50 to 70 range while the ARC 750 8 GB had 70 to 90 and the ARC A 770 16 GB at 90 to 110.\n\nBased on these reported numbers, the P4000 is likely to give you less or equal gaming experience to the A 750 at best. All of the reviewed video cards have performance targets in the 200 to 300 dollar range where they are in the same gaming category as a GTX 980 to GTX 1660. All of them have difficulty outperforming an RTX 3050 8 GB or an RTX 2060 6 GB card. An RX 6600 8 GB card outperforms all of these cards except for the ARC 770 16 GB GPU.\n\nIf you want to make sure you purchase a better video card than an Intel ARC A750 8 GB, then look at GPU hardware that is better than a GTX 1660 Super. The ones you should be looking at are only the 8 GB memory capacity or higher video cards such as the GTX 1070 8 GB, RX 6600 8 GB, RTX 2060 Super 8 GB, RX 6600XT 8 GB, RX 6700 10 GB, etc. Please note that most of these are currently selling above 200 dollars in the used market and they rarely drop below 150 at this time. It will be another year before the products regularly sell below 200 dollars once the next generation of video cards releases at the end of 2024.\n\nThe GTX 1070 8 GB is probably your best bet. It does not have DLSS nor ray tracing but the card is selling used for between 80 and 150 dollars on eBay. Its processing power is similar to an RTX 2060 which is above an RTX 3050.",
      "A750 is 2-3x faster than Quadro P4000, so yes it's a huge downgrade."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Intel Arc A750/A770 will be available at 6 Micro Center locations",
    "selftext": "List:\n\n-\tDallas, TX\n-\tHouston, TX\n-\tChicago, IL\n-\tWestbury, NY\n-\tDenver, CO\n-\tOverland Park, KS\n\nSource: https://game.intel.com/story/intel-arc-graphics-release/\n\nEDIT: Looks like the A750 only, I don't see any A770s even listed at these locations.\n\nEach location seemingly only received seven A750s and no A770s. So the entire Micro Center chain received 42 Arc 7 GPUs lol",
    "comments": [
      "What went so drastically wrong that it seems only a few hundred GPUs were made available? Intel certainly is no stranger to product launches, I wonder if there was pressure just to launch something at this point.",
      "Yep that seems to be the case. The fact that they only had stock at a handful of Micro Center locations is a pretty big red flag. Meanwhile my single Micro Center location had over 120 4090s this morning.",
      "All these cards were made back in Q1, only 4 million arc cards were made overall and that covers the entire stack from a380 to a770 (not sure if that is just discrete cards or whether that includes laptop gpus as well).\n\nIt's pretty much a case of get these cards from somewhere now because they ain't coming back (great for gpu collectors though)",
      "About par for Intel with how the Arc GPU is going. So many people want them to succeed after being fleeced by Nvidia and AMD over the last couple years, but that just can't deliver on the demand that is there waiting... I almost went to my local Micro Center in Fairfax, VA but glad I didn't waste my time",
      "Wow, that's a lot of 4090s...who has the money to blow on a 4090?! I was hopeful with the 770 to finally get a GPU with good specs at a reasonable price. Gonna have to hope my 1070 holds out a little longer.",
      "No 770 at overland park.. i hope there are some",
      "Nice, did you receive a software bundle email from Micro Center?",
      "In the UK, only seen listings so far for just one major online retailer and the big pc one (overclockers) , haven't heard anything from them. There is zero volume but hopefully it's the sort of thing where it'll be available on off for a few weeks on Newegg for you guys",
      "It seems I did. I looked back at the receipt and I got COD MWII for Free.",
      "I hope they have more than 100000 gpu in stock.",
      "I stopped by the NJ Microcenter in Paterson. They said that they just did not receive them yet and to check back tomorrow. I also called their support line and it seems like they don't really have any answers.\n\n\nEdit: Spoke with a store manager and they confirmed that Intel barely sent any stock. They suggested to keep an eye out on the site and to check back in on delivery days (wed,fri). They also said they did not receive any promotional material, what’s going on Intel!?!?",
      "What a 🚀 launch Intel!",
      "I actually just got one. They just received some at the Dallas Store. The funny thing is that no one here seems to want one.",
      ">certainly is no stranger to product launches, I wonder if there was pressure just to launch something at this point.\n\ni heard they didn't want to sell directly to people because there is no margin. The plan is to try to bundle with systems, and cpus to make money.\n\n\\>> The source is moreslawisdead and some one line articles. Couple. Nobody is on the record but is the likely explanation. \n\n&#x200B;\n\nMy take:\n\nSo i'm cool with companies making money and such, but they annoying thing is this isn't' how ti was marketed, and i'm kind of annoyed about it tbh. wasting our time with a marketing gimmick if true. \n\nMy guess: it's all about money though they have cards. they are trying to look like a release, part marketing stunt, and part use it as a loss leader as part of the stunt to sell more computers etc. Right because once you bundle the idea is it can help sell some chips.  I can see the point.   \n\n\nSo weird to have a release of product like this that you don't intel to really sell, so you don't believe it in but your customers do. Never seen anything like this i can think of.",
      "That's a lot of foot warmers.",
      "that's what i feared, i would happily buy one if I could get a 770",
      "Just to put this all into perspective, Nvidia + AMD shipped together something around ~50 million desktop units for 2021. This is likely 4 mil all in, including mobile.",
      "So, if they produced these back in Q1 is production done or what? Surely, we'd see more form AIBS etc later on. I have seen rumors about how the entire gpu division is done for. I am having a hard time reading the market and seeing if they are in this for the long haul or not. I ask bc what's the point of fixing this if they aren't?",
      "No, just walked in and got the card no bundles.",
      "When gamers nexus did his teardown, confirmed that that die was produced in Q1 and the rumours have been of a warehouse full of alchemist dies waiting for the software to catch up.\n\nAs far as I'm aware in terms of alchemist, that initial Q1 run was it, especially considering 6nm is in much hotter demand than it was back at the start of the year.\n\nThe gpu division isn't done for but I think it's going to be a very very long time before we see the arc division on parity with someone like amd where obviously they have the cpu side of the business, while also being able to make half a dozen different gpu dies for their products stack. Current rumours are Intel focusing on data centre and laptop (which makes a lot of sense) and then just keeping some sort of presence in the discrete market, so maybe just one gpu die for the battlemage launch to keep costs low. This is a awful market to be launching a new gpu into when supply is far outpacing demand.\n\nSo long story short, Intel is in the long haul for the graphics business but no one including Intel has a clue about them being involved in the discrete gpu space long term"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Intel Arc A750 Freezing",
    "selftext": "Recently I just got the intel Arc A750 and it works fantastic, but one thing I noticed is there is some momentary freezing with the latest drivers I have installed. I was wondering if this freezing is caused by the older drivers still being installed or if the new ones are having some issues?",
    "comments": [
      "Try rebooting to safe mode, then display  driver uninstaller, then install the driver again. \n\nThis is how you make sure that nothing was left from previous install",
      "I have an odd issue where the screen frequently freezes.  I have to turn off the monitor and turn it back on to \"unfreeze\" the screen.  It will freeze again in 5 or so mins.  I used DDU and then reinstalled the latest driver.  Still unresolved.  Driver is from 10MAY2023.  Cant enable BAR as my computer is PCI 3.0.",
      "I enabled ReBAR and that worked! But I did recently install the new driver. Are you suggesting that I uninstall any of the old drivers on the arc gpu?",
      "I see. The problem is rather frustrating but are you absolutely certain you cannot enable Resizable Bar? I think 3.0 boards do support it. What generation cpu are you running?",
      "It's not a bad idea to use a ddu, especially if you had a different gpu before without a fresh windows install. That said rebar was probably just the issue.",
      "If it's already fixed then you don't need to do anything",
      "It is not available on the X399 motherboard.  Yeah, still rockin' the Threadripper 2950X.",
      "Appreciate the help and suggestion! Something else I noticed is that in some games such as ROBLOX the screen is flickering? Any suggestions on how to fix that?",
      "What brand motherboard do you have?",
      "https://rog.asus.com/uk/motherboards/rog-strix/rog-strix-x399-e-gaming-model/",
      "Checked out the motherboard, it does seem that ReBar support is compatible.",
      "I double checked in the BIOS…\n\nIt's not there on the STRIX X399.  Seems other companies did include it.  Typical.  😑\n\nThanks for your help though.  ☺️",
      "Do you have the most recent bios? I only au that because I also have an older mobo by then (z390-e) and they did release the rebar support. I do suggest you upgrade to the latest bios. Without Rebar your gpu is rather slow and hard to use. Rebar is the foundation for ARC performance. I do know of a program that gets rebar on older motherboards..may help",
      "I also do say this because if the motherboard is compatible with windows 11, it may be likely that you’re behind on a BIOS and it has ReBar."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Need Help: Persistent Driver Crash Issue with Sparkle Titan OC Edition Intel Arc A750 GPU",
    "selftext": "https://preview.redd.it/i20sfwnchkzb1.jpg?width=3000&format=pjpg&auto=webp&s=2d648370c4493f796746e84a9503e1b7b4f089b9\n\n \n\nBody:\n\nHey fellow Redditors,\n\nI recently built a system with the following specs:\n\n* Sparkle Titan OC Edition Intel Arc A750 GPU\n* ASUS H610M-E D4 Motherboard\n* Intel Core i5 12th Gen\n* 16GB RAM\n\nHowever, I've been encountering a frustrating issue – persistent driver crashes in every game. I've tried various troubleshooting steps, including proper driver reinstalls, updates, and even a fresh Windows installation. Unfortunately, the problem persists.\n\nI've also reached out to the help center, but so far, we haven't been able to identify the root cause or find a solution. The crashes happen regardless of temperature and frame rates.\n\nHas anyone else faced a similar issue with the Sparkle Titan OC Edition or Intel Arc A750 GPUs? If so, were you able to resolve it, and how? Any insights or suggestions would be greatly appreciated.\n\nThanks in advance for your help!",
    "comments": [
      "Submit your ticket to intel on their ticketing website. They are pretty good with escalating issues. Good luck.",
      "I've been noticing more crashes as well, although it's usually OBS crashing (Replay buffer using AV1 encoding+2 fairly lightweight games running), although was playing Halo Infinite yesterday and it crashed on me twice before it stopped crashing.\n\nLast month or so has been pretty bad for crashes for me, however I can't say for certain for me if it's related to Intel Arc drivers or OBS since besides Halo Infinite crashing yesterday, the crashing has only occurred when OBS is running (OBS would crash, which would follow with other stuff crashing).\n\nYou could try an older driver, I personally haven't tried older drivers since for me it isn't so bad that it's every game, sometimes a few times a day, sometimes can go multiple days without a single crash.\n\nPersonally, I suspect the crashes are from a botches bios updated that happened a few months back, before then I don't remember any crashes (Besides bad performance in some games and some weird visual bugs, it was fairly stable for me for many months since I bought my GPU up until then), but around then I remember seeing a post about them pushing a bios update through the driver update and someone mentioned it on the [arc community forum](https://community.intel.com/t5/Intel-ARC-Graphics/bd-p/arc-graphics); Although if it is the bios update that is the issue, then going back to older drivers probably won't help.\n\nAlso, nearly forgot to mention, my A750 is the ASRock Challenger D, so it's not specific to Sparkle A750's.",
      "Yes I'm going to be that guy. You've bought a brand new GPU and it's giving you this much trouble... Return it.\n\nBuy something from AMD or NVIDIA and only consider Intel when they've got their drivers in order. This is not acceptable from a multi-billion dollar company.",
      "can you suggest me a stable driver?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "What CPU will be considered balanced well with a Arc A750?",
    "selftext": "I made a post earlier on r/intelarc asking for Arc GPU advice with the i5-12400 because that's what I settled on before.\nI don't think I need a high core count because I will be doing little or no multitasking. So am I better off with a 12th gen or 13th gen i3? And since the 13100 and 12400 support both DDR4 3200MHz and DDR5 4800MHz, do I go with DDR4 to keep prices low or DDR5 for future proofing?",
    "comments": [
      "There is no price difference between 12600kf and 12400 around here in NA. Maybe look for that? I am sure every market is different though so can't say but its a way better get.\n\nI3 12100 has terrible frame times even though avg looks good typically. Its fine but when it's $100 for it vs like $170 for 12600kf I would pay the extra.\n\nIf you are going turbo cheap  on all parts then I get it. You can get a $80 h610 board something like that for 12100. Still 12600kf works ok on these cheap boards too as long as you are not running cone bench for hours. You dont need a z board.",
      "12100-13100 are the same 4 core 8 thread cpu (little bit higher clock on the 13th gen one), true raptor lake is from 13600k up, even the 13500 still has alder lake cores. 12400f would be a fine choice for your build it has 6c 12t.",
      "Both 12th Gen and 13th Gen will do the job for an Intel Arc A750, might as well get whichever is cheaper. Might not hurt to wait for Black Friday sales when retailers will be trying to empty their stock of 12th and 13th Gen chips to make room for 14th Gen, if you can wait about a month.",
      "Here the 12400 is $167, the 12100 is $125 and the 13100 is $143 not a large difference and they all can run on a B660 or B610 motherboard.\n\nI don't want to get those suspiciously cheap boards as they might die on me a few weeks out of warranty.",
      "What do you mean by terrible frame rates?\nI have played 80% of the newest AAA games and they run at 100+fps with a 12100F on high-ultra settings. The only game where i am noticing some choking in the most intensive parts of the map is Cyberpunk.\n\nGot it for €100 here in Europe and its the best €100 i have ever spent. Wipes the floor completely with my old 4690K.\n\nMost of the games nowadays get boring after a short time or are complete garbage. IMO not worth to spend a lot on a PC rn.",
      "There is no black Friday in my country but price should drop after 14th gen releases"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "a750"
    ],
    "title": "Current state of VR support on Intel Arc (A750, likely also applies for the A770 and A380)",
    "selftext": "I hadn't posted about this here yet, but I am a collector of VR Hardware, and when the A750 and A770 first launched in Germany, I was very eager to try out VR performance on as many headsets as I could.\n\nI personally bought the A750, mostly as I intend on mostly using the card for AV1 encoding in my Server, but might as well try VR on it as well I thought. Here's my findings so far:\n\nThe Intel Arc drivers currently **don't seem to support Direct Display Mode devices at all**. This is a requirement for VR headsets that use DisplayPort or HDMI to receive the image they're supposed to display. That means that I wasn't able to test any of these headsets, and I've tried a lot of them: Oculus/Meta Rift CV1, Valve Index, HP Reverb G2, Vive Pro 2 and the Varjo Aero (tho in the case of the Varjo, there was also a software lock-out, as they don't want their customers running into issues with unsupported GPUs, which also includes ones from AMD).\n\nOfficial Link support with the Quest (both with a cable and wireless) also wasn't working. Quest Link refused to find it's connection (the PC dialog never found the headset's USB connection and I never got the Quest Link prompt in the headset itself). and Air Link straight up threw me an error that the Video Encoder was unsupported. I have no idea what kind of magic other people have pulled off to get Air Link or Quest Link working on Intel Arc, I sure wasn't able to.\n\nI was however able to run VR on my A750 through 2 other means. For one Virtual Desktop basically runs on anything that has a DirectX 11 capable video output and a CPU, so that worked with no issues at all on Arc too, and I was also able to use my Vive Pro 1 using the Vive Wireless adapter.\n\nWith Virtual Desktop performance was quite stellar and I didn't really experience any stutters or similar. With the Vive Wireless Adapter however, I immediately started noticing hitching and stuttering every so often. The more CPU heavy of a game I tried, the worse the stuttering got, hinting that large parts of the stuttering may have to do with GPU drivers reliance the CPU.\n\nFor reference, I'm running a 5900X with 32GB of DDR4 3200MHz CL16 memory.\n\nIf anyone is interested in the performance numbers using the OpenVR Benchmark Tool, I'd be more then happy to provide them later as well. I've already run the benchmark, but the results I have not saved on my main data drive...",
    "comments": [
      "Yeah, this is true. Linus from Linus Tech Tips also faced this issue. Intel has responded saying they are in the alpha stage. They should be able to release beta drivers for VR support shortly.",
      ">The Intel Arc drivers currently don't seem to support Direct Display Mode devices at all.\n\nI am curious if the Direct Mode does work on Linux, since [the implementation is shared with AMD.](https://monado.freedesktop.org/direct-mode.html#intelamd)",
      "No unfortunately not, and I was not aware of it and bought a NUC with arc card to play specifically VR....",
      "I’m running the same specs as you plus an A380 (cause why not see what it can do with VR). Oculus/Meta Rift S didn’t throw me any errors, just said display port was disconnected even tho it was connected.\n\nOn the Intel discord, I’ve brought this up and have been reassured multiple times that VR isn’t software blocked nor hardware blocked. So I’ve been assuming that Oculus/Meta VR and Steam VR hasn’t whitelisted them yet for the direct connection headset. I’m thinking they’re waiting for the rest of the 40 series and 7000 series to release before doing a bulk VR whitelisting of said cards. *People with 40 series have been reporting the same problem so I’ve heard*\n\nFrom what I understand, the wireless alternatives ignore the gpu hardware check and goes by what the gpus can run as you said",
      "I know you probably won't respond but, since then have you tried again? And if you did has oculus link still worked?",
      "I'm curious, at a collector of vr hw, you likely have some great opinions.  I'm looking to buy an older lower cost but still serviceable vr computer to drive an oculus 2.  Only need to do something like Alyx at \"ok\" levels.  It's hard to discern where that sweet spot of age vs functionality vs price is.  If you've thoughts on those lines, love to hear them!",
      "still waiting :(",
      "As soon as a Mesa driver version with Arc support ships with PopOS, I will try that out. I'm just not familiar enough with Linux yes to feel comfortable modifying the system itself through the terminal ![gif](emote|free_emotes_pack|sweat)",
      "40 Series does work with all the headsets. Nvidia changed something with the Framebuffer handling in Ada Lovelace that causes heavy stutters in VR at higher resolutions if not accounted for.\n\nThis whole topic was already discussed in great detail on the official Varjo discord, and Varjo has already released a patch to fix 5hose stutters on the 40-Series GPUs.\n\nIntel Arc isn't blacklisted or anything. Direct display mode devices shouldn't show up to the Windows desktop, but they do on Arc, and usually at the wrong resolution too. The Rift CV1 showed up at the right resolution, but I'm guessing, since it wasn't found by the Oculus software, that there's some flag missing, like HDMIs 3D side-by-side display stuff. The Index shows up as a 640x480 display, and looking into the SteamVR web console it actually says that no display with the right resolution was found, but it does list all displays connected to the Arc GPU. And in the case of the Reverb G2, the display didn't show up on the desktop (it's a built-in Windows driver, go figure), and WMR did actually launch as if everything was working right, but the displays in the headset stayed black. My guess is that yet again, the driver wasn't able to initiate the vorrect display mode (resolution, refresh rate, direct display, 2 DP lanes per screen, etc.) on the G2...\n\nThe tl;Dr is, it doesn't seem to have anything to do with the software, at least for the display connections, it seems to be a driver related issue with the hardware not showing up correctly...",
      "I haven't, tho my intention was to at some point do a 30 day Arc trial, using nothing but my Arc card for 30 days and when I do I can try Quest/Air Link again :D",
      "Honestly, that I can't really say where the sweet spot for price to performance is for VR capable PC Hardware. What I can say is that you'd want a GPU with at the very minimum 6GB of VRAM (which at least all of the Intel A7 cards do fulfill), at minimum 16GB of System Memory and preferably 6 CPU cores with HT or more. I wouldn't to older then an Intel 8th Gen CPU (or the generation after if you're going Team Red) and in terms of GPU not older then Nvidia 10-Series or RX6000.\nIf you have a Quest 2 then that leaves Intel Arc also as a GPU option open for you, as long as you're using Virtual Desktop (as like I've said before Headsets that require a Display connection to the GPU currently don't work on Arc and the Oculus software refuses to work completely on Arc)",
      "They have not fixed it yet!!!?",
      "You could try something more up-to-date like Fedora or endavourOS.\n\nI'd be very interested to see whether this works.",
      "Okay, so there has been some change as the driver updates have been releasing. My experience was with either the launch driver of the update after launch driver which just said *headset is disconnected.* I’ll try launching VR on my Rift S and see if I match what you’ve been experiencing",
      "It still says that, but at least with the CV1 a 2160x1200 display was showing up on the desktop.",
      "Damn. Sorry man"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Intel Arc A750: The Best Value For Sim Racing in 2024?",
    "selftext": "",
    "comments": [
      "This subreddit is in manual approval mode, which means that **all submissions are automatically removed and must first be approved before they are visible**. Your post will only be approved if it concerns news or reviews related to Intel Corporation and its products or is a high quality discussion thread. Posts regarding purchase advice, cooling problems, technical support, etc... will not be approved. **If you are looking for purchasing advice please visit /r/buildapc. If you are looking for technical support please visit /r/techsupport or see the pinned /r/Intel megathread where Intel representatives and other users can assist you.**\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/intel) if you have any questions or concerns.*"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "I wanna replace my RTX 2060 with an Arc A750, is it worth it.",
    "selftext": "Hello, i am currently using a 2060 6GB and i have an I3-12100F. I want to get an Arc gpu, since they seem like a pretty good value for money and they offer better performance than my 2060.\n\nMy concern is ReBar. I just enabled it in my BIOS, and it didnt switch out of CSM mode (I cant boot in UEFI mode for some reason). It stays in CSM mode, even though it says that ReBar is enabled. \n\nSo is it worth replacing my 4 year old 2060 for an A750?",
    "comments": [
      "It’s at least 20% faster than your 2060 - probably a little more since drivers have improved so much, but not more than 35% faster. That’s not worthwhile.",
      "I'm quite sure ReBar isn't enabled for 20xx serie, your mobo can support it but the nvidia driver and firmware just won't let you use it, or better, nvidia marketing reasons.",
      "No.",
      "A750 is faster although I normally only upgrade for 2X performance jumps which I am pretty certain this is not",
      "Yeah, i decided to not get the A750. Maybe a 3070 would be a better choice.",
      "The 3070 is alright but 8gb is quite limiting at its performance tier the 6700 xt is almost as fast as doesn't require texture settings to be dropped in some of the games where the 3070 needs them dropped\n\nIt's out of stock in my area and more expensive in some but in places like the United States it's cheaper and arguably better",
      "In my opinion no.",
      "CSM sounds like an Asrock mb? If it’s set you have windows installed on mbr rather than gpt partition. If you need to be in uefi mode for some reason and changing it stops you booting you need reinstall windows (or transition the partition table to gpt, possible but not sure if officially supported)"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Intel arc a750 freezing problem",
    "selftext": "I recently replaced my gtx1650 with an intel arc a 750 and now it keeps freezing at random moments it only refreshes when i replug the hdmi connector i already tried the ddu and i already installed the latest firmware and i enabled rebar also i already updated the drivers can sombody help me\nPc specs:\nCpu:Amd 3800x\nMobo:Gigabyte aorus b550 pro ac\nRam:16gb 3600mhz g.skill trident z\nGpu:Intel arc a750\nPsu:Be quite 750watt \nSsd:1tb samsung 980\nCase nzxt h550",
    "comments": [
      "Sounds like a sporadic driver issues I would take a video of this and send it to Intel as a driver bug report",
      "So this is not like application or OS freeze but screen is not updated until you unplug /plug HDMi connect again ? If this is so can you try Display Port or different HDMI cable ?",
      "I will try it thanks",
      "I will try it thanks for the support",
      "Thats a last resort option",
      "btw it is still crashing",
      "Return gpu, get another one.",
      "Also, see if there is a newer driver for your monitor.",
      "Wait does a monitor have drivers?",
      "Contact intel gpu support, visit forums.",
      "Yes, most devices have drivers. Windows typically automatically downloads a functional compatible one that works out of the box. Sometimes when you have shit like this going on, it is worth looking into if there is a more recent driver. I do agree with the other guy though, try display port if you haven't."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "a750"
    ],
    "title": "Intel Arc Alchemist desktop series including A770, A750, A580 and A380 SKUs reportedly delayed till late Q2/early Q3",
    "selftext": "",
    "comments": [
      "Honestly, it doesn't matter.\n\nThey missed their window by not launching during the GPU apocalypse uncontested, so now they have to go against Nvidia and AMD offerings with mature drivers in a health(ier) market.\n\nMight as well take the L at this point and just figure out your drivers.",
      "What they've officially stated is launch in 'summer', compared to mobile 5/7 'early summer'. Anything past 'early summer' puts it in calendar Q3.",
      "They're going to have to seriously discount those cards to sell next to Nvidia and AMD. Can't see many people taking a chance on the first gen of cards especially in the enthusiast market when the top Arc card is only at 3070 level of performance. \n\nAnd then by August and September all of the talk will be on RTX 40 and RDNA3 which will blow these cards out of the water.",
      "This will get steam rolled by Lovelace and rdna3. Should’ve released this in June 2021",
      "Not if it's dirt cheap like polaris. \nBoth lovelace and rdna3 will be starting at minimum 400$ for the lowest model.",
      "Their gonna take an L. The market is getting better and by the time it comes out, amd and nvidia will have their gpus at a few % above msrp. Therefore making these card useless. They won't be faster, they might be a bit less expensive, and their launching gpus close to next gen.",
      "To be honest, even if they launched in June 2021, they'd have probably all been gobbled up by miners.",
      "most of them are probably going straight to OEM.\n\nOEMs will use it as a way to tell AMD/Nvidia to fuck off with their horrid \"incentives\" like priority allocation. Most people buying computers don't know what the hell goes in them so the main problem is drivers. if the drivers continue to be miserable then intel will get nowhere even giving these things away for free.",
      "At least they would’ve sold (Not defending miners). Now they have zero chance of becoming dominant in any market",
      "More competition, more fun.",
      "Well considering the frame stuttering in the arc a350m for mobile that was recently tested, my money is on bad drivers and at this point Intel should just work on the drivers. They missed a critical chance to disrupt the market. Now new gpu prices have fallen down hard. Not yet at msrp but much better. The second hand market will be flooded with rdna2 and 3000series from Nvidia and not mention the rtx4000 series and rdna 3 gpus coming soon. This will be really rough  on Intel.",
      "When will Intel learn that blatantly lying to investors will get them nowhere?",
      "Efficiency wise it's better than GTX 1600, at least the already released laptop 350m which at around 40W has same performance as GTX 1650m at 50W. \nThat's not too bad. Probably on the same level as RTX 3000 as efficiency didn't improve much compared to GTX 1600 and 2000.",
      "Well, thats basically all she wrote for ARC...\n\nI know for a fact Intel won't price them relative to their performance vs other cards.\n\nI hope I am incorrect, as aggressive pricing is the only thing that will save them. That creates an even bigger problem for them though, if they start out low priced, they will remain there for many years. Thats just the way the market works.",
      "They're making them at TSMC so it didn't matter. if these were being made at Intel fabs, well they'd be worse if they were being pushed out on a broken 10nm a year ago or 14nm due to power efficiency.\n\nBeing made at TSMC means when TSMC were starved for capacity these would have been even worse.\n\nUsing TSMC 6nm indications would be that performance is closer to AMD/Nvidia parts that use half the die size. \n\nThe only way these matter in the first gen or two is if Intel does it's Atom/phone/tablet tactic and just firebombs pricing to force their way into relevancy but if the architecture doesn't catch up then eventually when they try to charge real prices they'll be pushed out.",
      "Mmmhmm.",
      "More competition more fun works if wafer supply is ample and performance is competitive. If performance sucks and they eat up supply of incredibly limited wafers then it's terrible for everyone, Intel included. They'll be eating shit selling these at below what they cost to produce and AMD/Nvidia lose out supply to make twice as many gpus from the same wafers and the end user loses out.",
      "Thats just clock gating. Easily fixable.\n\nLook at the GPU speed every stutter, you will see it drop to 1150mhz.",
      "TSMC 6nm is a retooled 7nm, so the density is not that dissimilar.",
      "How is it fixed if u dnt mind explaining?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Acer Predator BiFrost Arc A750 OC desktop GPU review - Intel's second-strongest gaming graphics card has only been given 8 GB VRAM",
    "selftext": "",
    "comments": [
      "I wonder what's the rationale behind that cooler. Seems easier to just have dual axial fans.",
      "The blower keeps it super cool"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "I found arc a750 for very cheap price. I will be using it for gaming and solidworks. Is it performing badly? Anyone using this graphics card for Solidworks?",
    "selftext": "",
    "comments": [
      "Maybe someone will help you, but seem to be possible to fix this with registry edit. https://community.intel.com/t5/Intel-ARC-Graphics/Solidworks-on-an-ARC-A770/m-p/1425976",
      "How much of an estimated performance increase does it provide?",
      "Here's an updated and more in-depth chart: \n\n[https://techgage.com/wp-content/uploads/2023/03/Dassault-Systemes-SolidWorks-2160p-Viewport-Performance-Subtests.jpg](https://techgage.com/wp-content/uploads/2023/03/Dassault-Systemes-SolidWorks-2160p-Viewport-Performance-Subtests.jpg)\n\nFrom here:\n\n[https://techgage.com/article/specviewperf-deep-dive-february-2023/](https://techgage.com/article/specviewperf-deep-dive-february-2023/)\n\nIntel has workstation-focused Arc Pro cards that are said to improve performance in multiple CAD-like workloads, but I've yet to be able to benchmark one.",
      "I haven't ever used Solidworks or Catia, but my experiences last year with the A770 for pro/compute workloads were dreadful, usually \\~20% behind a 6600 at twice the power, so that image tracks. Not to mention the some of the truly lateral thinking required to get things running at all.\n\nI'm quite glad that I returned it for a secondhand 6600 - you may be well-served by doing similar, if that chart is reflective of your workload.",
      "https://community.intel.com/t5/Intel-ARC-Graphics/Solidworks-on-an-ARC-A770/m-p/1481553#M3829\n\nOne person stated that with the change he made in the registry, it went up to 125 fps.",
      "A750 looks much more powerful in tests",
      "My flair?",
      "Damn, if only it was a little more than that"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Can I use a 1650 super 4gb and a arc a750 in the same pc",
    "selftext": "I’m buying the arc a750 and is wondering if I can use another gpu I have lying around",
    "comments": [
      "It's possible to use several video cards within the same computer, but the guidelines as to how and why it works or not is based on whether you can assign programs and workloads to specific hardware.\n\nThere's various conditions on why some setups work and why others don't. You probably need to go to a multi-GPU usage forum to determine what you can get away with.\n\nCan a GTX 1650 Super 4 GB card work in the same computer as the Intel A750? As long as you have both video drivers installed and you assign with non-video output processing to one of them, then you can use one for video output, while the other one is crypto mining. Please keep in mind that two or more video cards likely means you do not have enough power supply juice to provide cabling to each video card.",
      "Ok thanks"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "This thing is a beauty, Acer Predator Arc A750.",
    "selftext": "Just go from recent Tiktok shop black friday deal of $129, this thing is such a beauty.\n\n[Box art](https://preview.redd.it/5xwdmc910n3c1.jpg?width=1280&format=pjpg&auto=webp&s=a496053da633b151f342f75868f30162b27e63b2)\n\nOpen the box, it's well protected by foam.\n\nhttps://preview.redd.it/coi4ca150n3c1.jpg?width=1280&format=pjpg&auto=webp&s=0ca32fa05acf3f21f93e9d9496bce6615b9fbb58\n\nThe card is well made, I'd say it's on level of med tier card, with axial fan in center is very old fashion, reminds me of AMD or Nvidia founder edition.\n\n&#x200B;\n\nhttps://preview.redd.it/qj7o3kkt1n3c1.jpg?width=1280&format=pjpg&auto=webp&s=95313c387ffb09f3b7c8eab401f695c08bf81e71\n\n&#x200B;\n\nhttps://preview.redd.it/rbx9ccpq2n3c1.jpg?width=1280&format=pjpg&auto=webp&s=7306e7df77842971aafada595137ed22d5abb2ac\n\nThe backplate is also made of metal and thermal pad can be seen from the gaps.\n\nhttps://preview.redd.it/yhhh33e52n3c1.jpg?width=1280&format=pjpg&auto=webp&s=e81f6e209dd85c47e02d88c46d70bc5cb9d19d9c\n\nThere might be LED lights on the side, besides the logo:\n\n&#x200B;\n\nhttps://preview.redd.it/t61wg9lc2n3c1.jpg?width=1280&format=pjpg&auto=webp&s=dda1179bbcaf8b7a0755f748705ce4749cafefca\n\nA peak into the cold plate, there is a very visible \"tail\" on the side, which also means it's cooled by vapor chamber.\n\n&#x200B;\n\nhttps://preview.redd.it/pmslfe2o2n3c1.jpg?width=1280&format=pjpg&auto=webp&s=fd615226056b814f98e5026284f505e6dddea639\n\n Just hold it, I'd say it feels premium in hands, can't wait to install it later in the weekend.\n\n&#x200B;",
    "comments": [
      "Nah it was for Black Friday and ended around cyber Monday. Infinite 30% off coupons",
      "Is the deal still on?\nCan you tell me more about seller? I noticed that the newegg is selling through the tictok."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Intel Arc A750 Limited Edition 8GB + Assassin's Creed Mirage on sale for $180 on Amazon",
    "selftext": "",
    "comments": []
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "i5 9400f with Arc A750",
    "selftext": "i know that arc cards need ReBar to get full use of them and 9th gen don't have that feture in bios.\nbut recenty i was looking in my bios and my mother board (msi H310M pro) has ReBar option.\n\nso will it work on 9th gen cpu, because it's very affordable in india compare to RTX 3060 or my next choice RX 6600\n\nplease let me know if anyone has tried this with ReBar.",
    "comments": [
      "8th and 9th gen fully supports ReBar now, just update your mobo's bios to the latest and you should be able to use it.\n\nI also have it on my ASUS Prime H310M-K R2.0, it's just that my GPU doesn't support ReBar hence why I'm not using it at the moment.\n\nbut I did turn it on, it just didn't make a performance difference since my GPU doesn't support Rebar.\n\nyou should be clear to use ReBar now that 8th and 9th gen platforms are capable of using it.",
      "You have obsolete cpu for this gpu.",
      "It did when I ran an A770 on an EVGA Z370 Micro with a 9600k.",
      "> i know that arc cards need ReBar to get full use of them and 9th gen don't have that feture in bios.\n\nYou can force enable it, since it's a pcie feature it doesn't matter if your motherboard does or doesn't support it.\n\nAlthough Arc does still have driver issue, they're definitely being slowly patched out, but if you don't want to deal with some games being much slower than you'd think, you might want to skip ark (Really depends on if you're fine with some games being nearly unplayable for a while).",
      "Rebar works fine on 8/9th gen provide the board (like yours) supports it.",
      "Beta bios E7B25IMS.2B1 or newer should have “support” for rebar, but your mileage may very, many people see stability or crashing issues with it enabled.",
      "You are looking at 20% to 50% performance loss without rebar depending on the title. Also the 1% lows are also severely impacted which is far worse for fluid gameplay.",
      "so it should work right?",
      "i know that but i can turn on ReBar from bios will it work on 9th gen cpu that is what i want to know",
      "Nope."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Will the Intel arc a750 be able to outperform other cards?",
    "selftext": "So far the arc has outperformed the 3060 and 6600 in many instances, but will it be able to beat other cards? Such as the 3070 or 6700/xt",
    "comments": [
      "Not even close to those two. Might see it challenge the 6600XT in some games.",
      "The A750 already surpasses the 3060 in some titles.",
      "Why?",
      "I never said that the A750 outperforms a 3060. Also the main point if my comment wasn’t the \"some\" but the \"already\". I also used \"some\" because I wasn’t sure how many games already run better on the A750. The 3060 only has ~5-7% more average performance depending on sources. Tom's hardware for example has around 6% but it isn’t easy to tell since his chart only goes in 20 steps. So assuming the 3060 has around 70 fps on 1080p max and the A750 has 66 fps so the difference is 6%. In another comparison on YT there are 20 different games and the difference is around 5% on 1080p. So drivers should definitely close this little gap at least. For comparison: the 3070 was a bit better than the 6700 XT until late 2022 drivers update. Now there on par at least. \nAlso what’s \"Y'all Arc people…\"? Am I an \"Arc people\" just because it’s right? Are you a Nvidia people then? \nAlso your \"Oh but in a year bla bla\" doesn't make sense in this case because this is basically OP's question. \"WILL the intel arc a750 be able to outperform other cards?\" OP literally wants to know what could be in the future. \nAlso nice car and crush comparisons altough they don’t make sense in this debate.",
      "RTX 3070 or the RX 6700 XT? hell no. the GTS 450/GeForce 210? hell yes",
      "its a great card, problem is intel may be ditching arc.",
      "they fired there lead engineer",
      "I know from owning many NVidia, AMD cards and system boards that drivers are what make those parts really go. also, I think once intel gets the driver to what it should be the 750 and 770 will benefit huge as it has in the past with both NVidia when it had motherboards and still has video cards that users can increase performance using driver alterations such as nvclean install and such. even better if one can program. S I think intel has a way to go same as AMD for a very long time and still some issues. The issue I have with NVidia is their video card Biosses. Pend that much on a card one should have full control just like motherboards. Hope one day they will give us that option and all cards come with the parts on them to allow that kind of adjustment. Hang in there as I am going to get the a770 and work on the drivers myself as the cars comes with everything in hardware to compete with the 3080, just the drivers suck but better than they were.",
      "From where do you know that the Arc A770 has the hardware like a 3080? I only heard that the A770 should have the hardware of the 3070 which is still very impressive since it’s a lot cheaper and has the double amount of VRAM.",
      "I didn't say it has hardware like the 3080 as they work very differently, It has hardware that is capable of competing, until they get drivers up to par it won't use what it does have in hardware fully. AMD is still working on the same thing. Really haven't seen any major change in the drivers for NVidia for 8 years, now game settings such as RTX, DLSS and related stuff for individual games have been added. but not huge performance gains since those add ons. Intel will get those gains but will take a while. the A770 has the hardware to compete with the 3080.",
      "\"Some\" games, is not outperforming a different graphics card. My old Saab can also reach 250km/h, but it doesn't outperform a comparably priced Mercedes.\n\nDrivers will not push FPS in games much beyond what it is now. Y'all Arc people need to get that out of your head. \"Oh but in a year this card MIGHT outperform a 3060 on average\" Nobody can use that to anything. That's like saying your crush might eventually be yours, but she has to get used for an entire year beforehand.\n\nThe card performs on average, worse than a 3060. The 770 16GB performs on average, between 3060/3060 Ti. Regardless of singular games performance, the average is what is useful. Telling new buyers otherwise is just stupid and will eventually kill any enthusiasm connected with the cards.",
      "No. And it is not outperforming 3060 either."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "GUNNIR launches new Intel Arc A750 Index graphics card - VideoCardz.com",
    "selftext": "",
    "comments": [
      "That actually looks very clean. Wish more brands would follow suite instead of some of those gamer aesthetics."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Arc A750 vs RX 6650 XT",
    "selftext": "Looking to get a gpu to pair up with my R5 5600X, options are XFT SWFT 210 RX 6650 XT and Intel Arc A750 lmk which I should get",
    "comments": [
      "The a750 was faster than a 3050, 3060 8gb and the Rx 6600 but I am pretty sure the 6650 xt is faster.\n\nAlthough the a750 has better raytracing and av1 so it depends what your specifically doing",
      "I’m pretty sure the 6650 XT is still faster overall.",
      "If you plan to  play old games it is safer to go Radeon. If you dont really touch them, intel should be decent and has really impressive ray tracing.",
      "In addition the the above comments, xess is better than fsr",
      "The Arc should be a fair bit cheaper for similar performance (and a lot cheaper for much better performance when using RT/XeSS if one wanted to compare it that way)"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Will Intel Arc Gpu work on Amd Ryzen 7600",
    "selftext": "I am creating this post want to get feedback, bought an Intel Arc A750 along with Ryzen 5 7600 since the X version was not available am preparing to build a pc, and not yet ready for 4k. Also have the A770 limited edition as plan B, & very willing to try their Intel gpu.",
    "comments": [
      "Yes it'll work. What makes you think it won't?\n\nAlso the A770 is barely faster, only get it if you absolutely need the 16GB VRAM.",
      "First time building it myself, and asking those who know what they are talking about appreciate the response.",
      "As long as your CPU supports Resizeable BAR, you should be fine with Intel GPUs. Which the 7600 does. You may as well use the A770 since it has more VRAM since games even at 1080p these days seem to be gobbling up as much VRAM as you can throw at it.\n\nI actually daily drive a 3700X and A770 and the experience has been decent so far. You're actually way better prepared for future games though than me since PS5/Series X games tend to make CPUs cry no matter the resolution.",
      "VRAM is video RAM, which is how much graphics data your GPU can store. Bigger is better, because it lets you play the game at higher resolutions and texture quality. Doesn't mean that smaller is necessarily worse though. The performance will be about the same at lower resolutions when the compute power of the smaller VRAM GPU equals that of the higher VRAM GPU.\n\nArc 770 with it's 16GB of RAM is good for 1440p gaming and even 4K in some cases. Arc 750 is only good for 1080p gaming.",
      "yes, but if he wants to the silicon to go a little further, the 770 is better for the 16gb along - also, plenty of things will increasing use above 8gb (RTX for example) as time goes on.\n\nWe need to know his price difference."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Has anyone tried to use Arc with a b450 amd motherboard?",
    "selftext": "I may or may not have bought an arc a750 and am running to massive problems. I just checked the website and apparently you need a 500 series motherboard. Obviously I made a mistake but has anyone else gotten it to work? Or any suggestions.",
    "comments": [
      "Do you have one? I've had an A770 since launch. No actual issues with the hardware, just issues with games. They have slowly been ironing that out though. I would make sure your mb has the latest bios and has resizable bar (amd smart access) enabled. I have been running the A770 with an Intel 12400 and a 670 chipset mb without issue. I haven't tried it with one of my AMD systems yet, but to say it doesn't run on any CPU or mb is just plain false.",
      "I tried an Arc A750 LE on an Asus B450i Strix with a Ryzen 5 3600 back in Oct and besides the drivers being much worse than they they are now, it worked with resizable bar enabled.\n\nTBF, Arc has issues with anything, it all comes down to the drivers.",
      "ARC still has issues on any motherboard and any CPU, it's mostly due to drivers. Unfortunately, it's not a reasonable choice for active gamers.",
      "Some people have reported you might need a bios update for the motherboard, so it starts working.\n\nYou might want to send the manufacturer a question about it. Arc hasn't been tested on everything. You're participating in what is essentially a GPU beta. Product is promising, but software is still flaky.",
      "Like said, yes it has issues with games due to drivers but you can't say it has hardware compatibility issues with any motherboards and cpus.",
      "yes if it's not a gigabyte with zen1/2. you have to mod bios in that case",
      "Works for me with rebar enabled, although arc control center says rebar is not supported. A770 with asrock b450 itx. However still facing many bugs. Every time PC sleeps I lose HDMI audio until a reboot",
      "Hmm maybe it’s something else that’s wrong then. It hasn’t stayed in windows long enough for me to install any drivers. Most of the time it doesn’t even get past the loading windows screen. BIOS works perfectly fine though.",
      "It's a shame, cause if the A770 wasn't having driver issues *even worse* than the 5700 XT (at launch) it would almost be competitive. That said, Alchemist/1st gen Intel GPUs are definitely having some growing pains but might become more worth it over time. Hopefully Battlemage/gen 2 will fix a lot of the problems. Or at the very least make Arc control center less trash lol",
      "Could you clarify what “mod the bios” means? Luckily BIOS does work fine for me",
      "I am using Gigabyte b450 ds3h and am on the latest bios update.",
      "A couple of guys I know bought one each and suffer from issues in many games, even popular ones like Battlefield 1. And yes, they know how to update drivers. But dozens of games they've tried worked horribly (crashes, artifacts, etc)",
      "Who cares why? You buy a GPU, you want to be able to play games, period."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Should I buy a arc a750 graphics card for productivity purpose??",
    "selftext": "My computer specification is Processor Ryzen 5600x ; Motherboard b550mds3h ; Ram16gb 3200mah; SSD 250gb nvme. ; Hdd 1tb ; Psu 650w bronze I want to do productivity work is it ok to get Intel ARC A750 GRAPHICS CARD with this computer specification?",
    "comments": [
      "Depending on what you’re doing specifically. But I’ve seen the a770 as a popular choice due to the 16gb. But the A750 will do just fine.\n\nOf course there are other GPUs to consider when they will provide the same or better performance, but at a price.",
      "Depends on the programs you’ll be using it. At the current price point the A750 is a great buy. You’d have to buy a Rtx 3060 to have somewhat better performance but the margins aren’t that static to really justify (of course depending on your use case). There are a bunch of reviews check them out.",
      "The a750 is better than the 3060 8GB model. \n\nThe 8GB model of the Rtx 3060 has a smaller memory bus and capacity. This has an impact on its performance versus the 12GB model.",
      "Which graphics card will give me same or better performance at this price?",
      "Is rtx 3060 8gb version is better than Intel Arc A750 or not??\nWhat you suggest rtx 3060 8 gb or intle arc a750??"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Arc A750 or RX 6700XT",
    "selftext": "So as title says I am thinking about upgrading my gpu to a750 or RX 6700XT  \nI game on 1440p  \n\n\nRX 6700XT 470 Euro  \nArc A750 350 Euro  \n\n\nWhich should I get and is it even worth it going with RX 6700XT due to 120 euro difference and not to say arc A750  \n probably isn't at its max due to not the best drivers from intel \\[as we've seen with latest driver update]\n\nEdit: I plan using it primarily for gaming maybe some occasional streaming/recording.\nAlso i won't even consider nvidia as 3060 is over 1k euro let alone anything better (in croatia)",
    "comments": [
      "Can't speak for the Arc but I've been rocking the RX6700XT  for over a year now and it has been an excellent experience. \n\nMost games on high/medium settings and well over 100fps@1440p. Have not experienced any driver issues or anything.",
      "Id say 6700xt , but I’m most likely going to buy the arc as my next upgrade. I want to support new competitors, especially with such an impressive first prototype.",
      "If you can hold on and wait for the RX 7600XT, that potential video card is likely going to be priced in the 300-400 US dollar range while holding 8-12 GB of DDR6 video ram and performing somewhere between an RTX 3060ti to RTX 3080 or above an RX 6700XT and potentially up to an RX 6800XT.\nIf the RX 7600XT contains 12 GB, I think AMD will try to push performance to the RX 6800 gaming level which is what an RTX 3070 would game at.\n\nThe RX 6700XT has performance similar to an RTX 3060ti or an RTX 2080 Super, while the ARC Alchemist A750 needs further driver upgrades because the performance is usually somewhere above an RTX 3050, but it struggles to routinely perform above an RTX 2060 and definitely below the RTX 3060 12 GB. The A750 is not better than an RX 6600 8 GB card which you can routinely purchase in the 210 to 240 US dollar range in America, while the RX 6600XT is usually priced below 300 on Newegg.\n\nIf you are risk averse, then just buy any RX 6000 series card you can afford and/or wait if the RX 7600 or RX 7600XT is priced well enough to tempt you to get better performance. The RX 6700 10 GB card should also be for your consideration, since, it often has been seen selling just above 300 dollars.",
      "There's a significant performance difference between the two. The arc 750 is on par with a rx6600. If you can't afford the 6700xt just get a rx6600 or 6600xt, whichever is cheaper",
      "If you're not going with tried and true mainstream nvidia quality, you might as well go with the untested cheaper intel. I don't see any reason to ever get AMD when they price their cards the same as nvidia. You may regret not having DLSS for gaming on resolutions over 1080p on such a weak card though. But if you are not going to use any such \"gimmicks\" (RT too; but for RT you need nvidia and only nvidia) anyway, Arc looks pretty compelling for just it's raw performance. Obviously, driver support and compatibility will be the big question going with it.\n\nPersonally, if I was shopping for anything less than 4090, I'd just get a used nvidia (e.g., 3080 for 500-600). Although for the price of Arc (300-400 bracket), the Arc still looks like one of the best options, even compared to used. In the cheapest bracket, Arc really looks like the best deal, even if it is a potential gamble on what it will run as well as expected and what it may not run at all.\n\nOn the other hand, neither Nvidia, nor AMD has released the newest series direct low-end competitors for the Arc, so there may be reason to just wait for those. Although, seeing the pricing on the upper bracket, I have a feeling there may be no offers at all in Arc's low price range.",
      "alright noted thx",
      "exactly but I hope arc gets even better before I am ready to upgrade",
      "I appreciate it but sadly in croatia there is no discount on new/current gen stuff and I can't order from amazon as that is atleast another 100 euro for delivery plus you can't pay with cash and rx 7600 is selling for over then 2k euros here on most websites and on others its hardly under for e.g. a ps5 in croatia sells at 800 euro used and abt 800-1700 brand new \\[no games 1 controller\\] also an rtx 2060 still sells at 800 and rtx 3060 is usualy double the price if not more",
      "God I envy north americans for pc building. Saw an Intel i5 12400 for $130 and a ryzen 6700xt for $320 on amazon but shipping from NA to Europe was $40-70. Same prices asocal stores.",
      "yeah I understand your reasoning i have an rtx 2060 rn but I am planing on building another pc and making this one a family pc and here in croatia as I said nvidia gpu's skyrocketed in pricing",
      "They already have. Had a 70% increase in gaming experience from one singular driver update and more are to come. And, realistically, that is ONLY with old games using DX9. if you only play newer games, you quite literally have nothing to worry about",
      "Unfortunately, the way the distribution scheme for the world is structured, countries, governments, and businesses negotiate lower shipping costs and fees to North American locations in order to take advantage of a larger market that buys more products. This happens on almost everything. This means pianos that are made in Singapore costs more to ship and sell within a hundred miles of that location, but the same product sold in North America that has a lower price will have lower taxes and little to no shipping cost.\n\nMost people travel to America and buy a ton of stuff to ship back to their own country, because it's cheaper to do it this way. Some locations in North America have tax holidays where sales tax is reduced or not charge, and many products and services have frequent sales or discounts based on various excuses such as friends and family sales, flash sales, bulk discounts, early release promotions, clearance sales, discontinued sales, leftover items sales, etc.\n\nPlease note the AMD RX 7600XT 16 GB video card is going to be released near the end of January 2024 at around 330 US dollars while several Ryzen 7 and 9 CPU chips are currently dropping prices for the third generation series between 150 and 350 US dollars."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "For those having problems with ARC cards.",
    "selftext": "So I bought a Arc A750 and it has been great however when I went to plug in my secondary display it did not work no matter how hard I tried. However I tried replacing the cable with a newer HDMI cable it worked perfectly fine. The old cable still worked I just think that the newer version of HDMI did not like the older cable. Hope this helped!",
    "comments": [
      "![gif](emote|free_emotes_pack|upvote)",
      "What monitor is your secondary display? HDMI should be backwards compatible, just with the limiting factor of bandwidth depending on the older HDMI cable.",
      "Had the same issue with my 4070 and display port actually.",
      "Interesting it might just be HDMI 2.1 having some weird problems."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Intel's new desktop graphics card Arc A750 surpasses RTX 3060",
    "selftext": "",
    "comments": [
      "Is not confusion is follow their CPU naming convention. A IS the generation, 7 is tier of the GPU like i7 CPU and the other 7 or 8 is the tier inside that bracket.",
      "I made [this](https://imgur.com/a/k9w7O7C) very rough guide to best understand how the nomenclature scheme relates to AMD and Nvidia.",
      "This looks amazing,\n\nCan't wait for A770 and A780, wish they would follow nvidia/amd xx50 xx60 xx70 xx80 xx90 naming scheme. The current naming scheme is very confusing IMO",
      "Fuck, I hope. Nvidia deserves to be ruined for what they did to gamers in 2020 and 2021. I’ll honestly let the 40 series rot on shelves for as long as I can justify it.",
      "How those compare price wise?",
      "looking forward to AV1 benchmarks",
      "Intel has decades of brand history.\n\n*MOST PEOPLE* aren't fully aware (or care) that Intel iGPU is why their laptop sucks at gaming, they just know \"hurr I didn't buy a GAMINGZZZZ LAPTOPSZZZZ\"\n\nThe instant Intel drops a billion dollars on marketing \"OMG RGB GAMING LAPTOP!\", stuffs store shelves with them, etc, it'll fly off shelves.\n\nNvidia's dominance isn't because of skill or a superior product(they've often had the inferior products). It's because they *vastly outspend* AMD on *marketing*. Marketing is *the* lynchpin on which this industry turns and will be the only determinant between success or fail. And Intel can 100% outspend Nvidia on marketing.",
      "nVidia are being greedy, no doubt but they will pay for that this year. Word on the street is, AIB partners want nVidia to delay the launch of the 4xxx series because they still have piles of unsold current gen GPU's.",
      "[Here's the video that this stuff is from in case anybody wants to watch it.](https://www.youtube.com/watch?v=uctDN9uYcXI) Happy to see Ryan Shrout in the spotlight again. Loved listening to the PC Perspective stuff while he was there.\n\nI think it's pretty impressive. Would have liked to have seen ray tracing relative performance numbers, but I suspect those aren't great or they would have shown them off. Also, that [Limited Edition cooler](https://cdn.videocardz.com/1/2022/07/ARC-A750.jpg) [looks fucking awesome](https://cdn.videocardz.com/1/2022/07/INTEL-ARC-GPU.jpg).\n\nI don't doubt that Intel has technology that can compete, but they're gonna have to price it carefully. They can't expect consumers to pay Nvidia/AMD money for an unproven product with no brand history, especially with Nvidia expected to release new hardware over the next 6 months.",
      "Looking forward to it, most people don't need a card better than the 3060 anyways. Lets hope that they work without crashing.",
      "Not just marketing, but volume sales and O.E.M connections as well.\n\nIntel can Easily strong arm Nvidia out of the mx450~gtx 1650 mobile market segment.\n\nSince they're running with 6gb of gddr6 as well, they can also steal market share away from the 4gb RTX 3050(Ti) laptops.",
      "They can claim anything... It doesn't mean it going to outperform 3060...",
      "Hope it does full AV1 transcoding at 1080p @30FPS at least…",
      "Maybe they could ship DXVK with their drivers?\n\nSeems like that could be a stopgap measure for a lot of their driver issues at the moment, seeing as their Vulkan performance is better than DX11.",
      "They could, that’d be an intriguing option. Even so, Intel missed the boat: Alchemist is basically a dead generation now, which may be an intriguing beta test but certainly won’t be anything more than that.",
      "How much do you wanna bet it’s a 3050 in everything else? I don’t wanna go there, but the driver troubles are so much worse than I thought, and this isn’t like Zen, where AMD punched a lazy and complacent Intel in the mouth, because Nvidia and AMD are both at the absolute top of their game right now (in a features sense for AMD and in a cynical corporate price-gouging sense for Nvidia).",
      "These things dont even have a release date as far as we know, even if this is released I wonder how much available they will have."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Arc Control not responding",
    "selftext": "I own an Intel Arc A750. \n\nThe Arc Control center isn’t responding when I try to do anything on it. \n\nThis is not a matter of old hardware or old drivers. \n\nI am on the most recent beta driver (31.0.101.4335) I installed the beta driver due to someone from another post saying it wasn’t working and posts inside that post suggesting the beta driver fixed their issue. Likewise I am running a brand new system. Ryzen 5 7600 32gb ram and Arc a750. I’ve had no issues with the gpu itself but the control center is another story. \n\n",
    "comments": [
      "Report it to intel gpu support team.",
      "The gpu itself works fine. This control software is another story",
      "Have you tried ending it via Task Manager (May need to use the \"Details\" tab) then restarting it?",
      "Hi! I had the same issue. Could you try to disabling and enabling again you resizable bar option in your motherboard bios? It worked for me :)",
      "Can you tell me how to do that? I googled “Intel arc gpu support” with no results",
      "Yeah. That’s the only way I can close it.",
      "Which is…still a problem. If someone wants to return a GPU because the provided software doesn’t work whether it be the control software or the drivers themselves, then why shouldn’t they?\n\nNo idea why that dude is being downvoted.",
      "I ended up returning the card, too many problems to worth keeping it, Intel need to get their drivers right"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Intel Confirms Arc A770 & Arc A750 Launch For 12th October, A770 Limited Edition For $349 US, A770 8 GB For $329 US, A750 8 GB For $289 US",
    "selftext": "",
    "comments": [
      "Great, especially for GPU computing apps.\nCompared to nVidia & AMD, these things have especially great F64 performance.",
      "Bah, when’s the last time that ever stopped us. If we’re lucky we can also use it to contribute to heating the house :D",
      "I was under the impression that Intel gaming gpu's don't support F64, see toms article [https://www.tomshardware.com/news/intel-arc-will-not-support-fp64-hardware](https://www.tomshardware.com/news/intel-arc-will-not-support-fp64-hardware)",
      "I think we all know an unreliable source for leaks :)",
      "I hope they will perform well. I need a new affordable GPU",
      "Who’s gonna tell him…",
      "Also have open source drivers on Linux, which Nvidia does not.",
      "How limited will it be? 16 gigs will be perfect for CGI work",
      "Those prices are actually pretty damn good. I can't wait for the next gen chiplet design.",
      "The A770 pulls 55W more than the 3060, and 25W more than the 3060Ti."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Intel ARC A750 GPU | Here comes a New Challenger !",
    "selftext": "",
    "comments": [
      "This is old news.",
      "New for India, or that I got access to a card only now. 😬"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "a750"
    ],
    "title": "[Digital Foundry] Intel ARC A770 / A750 Graphics Review: Here Comes A New Challenger",
    "selftext": "",
    "comments": [
      "The one i was waiting for.",
      "It's actually crazy how well Doom Eternal runs that you could buy a $350 graphics card and get a locked 4K 60 Ultra experience. And it's not even a tier 1 game.",
      "Can we get clarification about the free copies of MW2 and Gotham Knights? \n\nI see on Intel's website they want a purchase of 12th gen 12600K or greater. I am willing to get the A750 on launch for media applications because I was going to buy those two games anyways and the value works for me - but not if I need to buy an intel 12th gen CPU.",
      "https://youtu.be/a-KmhAEuHXw?t=748",
      "Shrout clarified on twitter that you'll get the free copies regardless of CPU purchase \n\nhttps://twitter.com/ryanshrout/status/1576198656428244992?s=20&t=Jru4IGNddradHgzc2LBGdw",
      "1440p BEAST",
      "The moment intel can compete, the prices will become comparable, the only reason they are cheaper now is because no one would buy them if they weren't due to their performance",
      "I am very impressed by arc 770 RT performance.If their perf target for next gen is something faster than 6800XT(i have one now) and they keep good price i might swap my 6800XT for that. I am sick of AMD/NV playing DUOPOLY and price fixing last 8+years.",
      "More supply is more competition. The only issue is that all three of the companies are using TSMC right now. \n\nIt would be better (for competition and supply) if Nvidia would have stayed at Samsung and Intel would use their own fab i guess. Right now TSMC is the big winner with 3 companies competing with each other but also buying their services and products from TSMC.",
      "Lowkey don’t take digital foundry’s hardware stuff seriously anymore, look at what happened with Turing and Ampere…"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Is Arc a750 compatible with MSI b450 gaming plus max motherboard??",
    "selftext": "I have a ryzen 5 3600x with a MSI b450 gaming plus max motherboard and a 600w psu. Can I use intel arc a750 with this configuration or do I need to upgrade my psu and/or motherboard?",
    "comments": [
      "If you have rebar in bios then yes.",
      "Yeah it will work well\n\nCheck for resizeable bar support as that can have a big impact on arc graphics\nhttps://youtu.be/eYsk7U3vYRk?si=cMZjDi9P8ayP3gdK"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Gonna buy an Intel Arc a750, what should i look out for? Is it even possible to use it in my build?",
    "selftext": "My specs:\n\n\n\n\nProcessor: AMD Ryzen 5 3600\n\n\nMotherboard: ASRock B450 Pro4",
    "comments": [
      "Eh theyre kinda twitchy and you need a new CPU to get the most out of them, but I'd say given what nvidia is selling at their price range,t he A750 is worth looking into. I'd still prefer the 6650 XT I actually bought though. \n\nNvidia is just way overpriced atm and has no decent options under $300. Heck even under $400 if you're going by value. \n\nThe A750 or 6650 XT would compete directly with a 3060. Might even win a lot of the time.\n\nArc is still kinda immature so id expect a lot of \"edge cases\" where things dont quite work well, but when it works, it works. \n\nIdk, I mean, given the market, Arc is an option worth considering at the current price. I'd still prefer AMD though tbqh. THey've been making GPUs for longer, they (generally) know what they're doing, and I'd expect it to \"just work\" tbqh. \n\nYes yes, AMD has driver issues time to time. So does nvidia, but no one talks about that. They all do. Only reason im giving arc a hard time is because for a while they just flat out ran older games worse than my old 1060.",
      "You might go for the Arc a770 as the uplift is much higher and you can also enjoy double the amount of vram. (Be aware there are also arc a770 8gb vram models)\n\nBesides that definitely enable Rebar (Resizable BAR) in your bios. Because if it's disabled the gpu won't work properly or at it's full capacity.\n\nOther than that make sure to download latest intel arc drivers from their official website.",
      "Alright! Thanks a lot, u bet there are tutorials on tebar somewhere on youtube.\n\n\n\nAnd the 770 isn't really interesting to me in this build at least, as the pc is a gift to someone who mainly plays Sims 4 on 1080p xD",
      "Are the intel arc gpus any good compared to Nvidia “budget” gpus and amd budget gpus?",
      "Tutorials aren't really needed. Go to bios and search for Rebar (Resizable BAR). Or if you prefer research before it, look first online with a google search (your mobo) enable Rebar, etc.",
      "The A750 is very competitive for its price. A770 less so but still pretty decent.",
      "Thanks a lot for your kind help 🙏",
      "No problem!",
      "With a B450 you *may* need a bios update to get ReBar.",
      "Alright, i dont worry too much about that as i did these 2 or 3 times already, never had problems :)"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Intel ARC A750 and A380 as eGPU for GPD Win Max 2",
    "selftext": "",
    "comments": []
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "a750"
    ],
    "title": "Intel arc won’t POST or hangs if it does",
    "selftext": "So I literally was intel’s hype man. I recommended A750 as a great buy but with the caveat of driver issues.\n\nI did not expect this much headache! It’s in a new build and when PC didn’t POST first boot, I had a heart attack not sure what it was.\n\nAnyway luckily ryzen 7900x comes with integrated graphics, and the minute I plugged DisplayPort there everything worked, except the A750 isn’t even detected in device manager.\n\nHours of troubleshooting later and I give up. I’ve tested this card in 3 systems, B650, B550, 8th gen intel, all the same result. Black screen with signal, or showing no signal, or it’ll work for a second then nothing.\n\nI’ve flipped rebar on and off, manually set PCIE x16 slot 1 to gen4. \n\nDid not try single ram (well I did, just not thoroughly). Did not try secondary x16.\n\nWhy is this card such a POS, like how can they even release a card that can’t reliably do basic video out.",
    "comments": [
      "What are the chances I just got a bad card? I like a 3rd company in the GPU market but idk if I want I’ll just get burned buying another arc.",
      "There's chance for a GPU to arrive dead or damaged, probably like 0.01%\n\nYours seems to be dead.\n\n\nIf a GPU doesn't do display out when correctly plugged in 2 PCs it's just dead, no point holding onto it and trying whatever sorcery",
      "RMA.",
      "My A770 didn't show up in device manager or anything else until i installed the drivers for it. Although i did get a display signal out of it.",
      "You make sure the pcie power cable is seated all the way, on both sides?",
      "Had the Same...its toasted..rma...the new Card works now..."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Intel ARC A750 GPU | Here comes a New Challenger !",
    "selftext": "",
    "comments": []
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Arc A750 or RX 7600",
    "selftext": "I was wondering that should I get the A750 or RX 7600. The RX 7600 is 330$ and the A750 is 310$(other gpus are very overpriced) here. I was thinking of paring it with a R5 5600 and a B550 mobo. So whats the best option?",
    "comments": [
      "The performance of the Intel A750 is between an AMD's RX 5500XT 8 GB and an RX 6600 8 GB. The A750 games above a GTX 1650 Super, but usually below that of an RTX 2060. It's performance is better on newer games that use DX12 protocols, and it finally can play older games at adequate gaming rates due to using emulation software for titles that require DX 9 through 11 commands. The stuttering is still there but tolerable. The price in the States is currently in the 200 to 220 US dollar range.\n\nThe preferred video card is the RX 7600 8 GB that performs about the same as an RX 6700 10 GB card. The current price of the RX 7600 8 GB is about 270 US dollars while the RX 6700 10 GB is around 280 dollars. Both cards perform at the same level as an RTX 2080 and above the RTX 3060 8 GB or 12 GB versions.\n\nWhich is better? The RX 7600 is more than 20% better with less stuttering. But the initial production of the new video cards have some brands may have bad power cable connectors.\n\nIf you do not use the latest triple A games and stick to less than 1440p or 2k resolution, then either card will be okay. Basically it's your decision between budget to spend and gaming quality.",
      "Id go 7600 for the more mature software.",
      "I see the 7600 for $270 US.",
      "Yeahh amd rx 7600 go for",
      ">ands m\n\nI wont be having any problems with the bad power power connectors because I will also buy a new PSU."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "I betten buying a Rtx 3060, Rx 6600 or Arc A750. Which one is better?",
    "selftext": "For me all 3 graphics cards are the same price, but i'm not sure which one is better. I use my pc mainly for gaming, 3d renders and sometimes for vr. Which card do you guys recommend.\n\n&#x200B;\n\nAlso the Rtx 3060 is the 8g model and I know the Arc A750 cant run vr, I can wait when they add support",
    "comments": [
      "Pricing video cards for the RTX 3060, RX 6600, and the Intel ARC A750:\n\nCurrently, the RTX 3060 8 GB is as low as 250 while the RTX 3060 12 GB is in the 280 dollar range. I don't understand why someone would not be willing to spend an extra 20 or 30 dollars to get double the amount of video memory for an RTX 3060. This card is more than 10% to over 20% better than either the RX 6600 8 GB card or the Intel ARC A750.\n\nThe RX 6600 8 GB is priced in the 210 to 240 dollar range, while the A750 is around 240 give or take 20 dollars. The RX 6600 can beat the RTX 2060, but it's more than 10% weaker than the RTX 3060. \n\nThe Intel A750 does have the processing power to beat the RX 6600, but it cannot compete well, because it has video driver issues and a manufacturing design defect in the chip that slows processing down. It's rated gaming experience puts it above an RTX 3050 8 GB card and below an RTX 2060 8 GB for games that use DX12 (API). Games that use DX11 or older have performance levels that are 20% lower because many games are played using an 'emulation mode' that makes playing the game possible. Most people using the GPU will not notice the lower performance because they have no experience in comparing if they are actually getting a better value.\n\nMy recommendation is to either get the RTX 3060 12 GB card which is the best performer of the three choices, or get the RX 6600 8 GB which is the cheapest, and it's better than the Intel A750 8 GB.\n\nIf you are going to spend at least 260 to 300 dollars, just get either the RX 6600XT 8 GB, RX 6650XT 8 GB, RX 6700 10 GB, RX 6700XT 12 GB, or RX 7600 8 GB, because all of these AMD video cards are more than 10% better than the RTX 3060 12 GB card while all of them are currently selling between 250 and 300 dollars.",
      "If theyre the same price, I'd go the 3060.\n\nEDIT: NOT THE 8 GB MODEL THOUGH.\n\nOkay, now I'm inclined to say 6600. The 12 GB 3060 is better than the 6600 but vs the 8 GB version (which sucks) go 6600.",
      "Don't listen to anybody else. If the ram is the same go for geforce you will have tue oat support for games. Ie performance will be better",
      "Thanks man! This helped me a lot."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "arc a750 / a770 idle power draw",
    "selftext": "According to [techpowerup.com](https://www.techpowerup.com/review/intel-arc-a750/38.html) the idle power consumption for the new Arc a750 and a770 is at 39W and 44W. Intel gave instructions on how to fix this issue found [here](https://www.intel.sg/content/www/xa/en/support/articles/000092564/graphics.html?countrylabel=Asia%20Pacific). Has anybody tried this? did it reduce your idle power consumption?\n\nHonestly, this is the only issue I have with Intel's new graphics cards.",
    "comments": [
      "Correct. I mentioned that before I had gotten my card and realized there was native power monitoring built into the arc control panel.",
      "Same here on Intel ARC A770 LE.  \nMy MSI-B550-PRO motherboard doesn't seems to have those settings.  \nAnd the Widows setting did nothing.",
      "Just try it.",
      "Remind me to try this tonight. My A750 comes in, and I’ve got a Kill-a-Watt meter I can test with. \n\nI’m not holding my breath though, because on high refresh rate monitors the clocks usually stay up (Intel even admits it). Same thing happens on my 6800XT.",
      "My Z390 Master's BIOS (F11n) has some corresponding options:\n\n* Platform Power Management\n* PEG ASPM\n* PCH ASPM\n* DMI ASPM All of them could either be enabled or disabled and neither of them did anything. I've also got the corresponding Windows power setting set to *Maximum Power Savings*, yet idle consumption remains \\~40 W.\n\nCuriously, GPU-Z reports the Bus Interface as `PCIe x 16 4.0 @ x 16 3.0`, which is expected on my Z390 platform, but with the 2080 I had previously installed, it would switch down dynamically to `@ x16 1.1`. Maybe I should reinstall it at some point to see if the 2080 still does that, though I am not very keen to uninstall the A770 (it's the 16GB LE) again just yet.",
      "Z490M-ITX Asrock. Native ASPM = Enabled and no other BIOS setting along with the Windows settings has made my idle power consumption as low as 10W per GPU-Z. Typical is 15-20W.",
      "Sorry, I should of been more clear. I don't own one and want to see if the idle power consumption fix works before I buy one.",
      "be reminded",
      ">Kill-a-Watt meter\n\nDoesn't that test whole PC's power draw? IMO you can instead see the GPU power draw in some program like HWInfo64 instead. Mine draws \\~40W on idle.",
      "I couldn’t find the BIOS settings, but changing the PCIe power saving did nothing. Average idle before and after was within margin of error sadly."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Intel Arc A750 Flickering",
    "selftext": "So I’ve noticed when playing games like ROBLOX the Intel arc A750 is flickering? Any suggestions on how to fix that?",
    "comments": [
      "Report it to intel support.",
      "Latest drivers? Try running it in windowed mode, Roblox seems to have troubles with full screen",
      "Definitely will do, thanks for the tip!",
      "Latest drivers and windowed mode both were already in use, I wonder if it’s just a problem with ROBLOX and the optimization for the arc? Every other game seems just fine?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "a750"
    ],
    "title": "ASRock Intel Arc A770 8GB and A750 8GB GPUs in stock at Newegg. Go!",
    "selftext": "[https://www.newegg.com/p/pl?N=50001944%20100007709%208000&Manufactory=1944&SrchInDesc=arc](https://www.newegg.com/p/pl?N=50001944%20100007709%208000&Manufactory=1944&SrchInDesc=arc)",
    "comments": [
      "A770 now showing OOS. Pain.",
      "I'm really curious, if they are selling like hotcakes or if supply is low, if i remember correctly its been said these cards had already been made and sitting in warehouse while they optimize the drivers",
      "An Intel employee in their discord confirmed that AIB buyers will also be eligible for the game bundle, not sure on the process though. (whether it's automatic or not)",
      "I don't know for certain, but the T&C aren't clear on it either. They reference Intel Arc products, which in this case could be both a brand or a manufacturer. Might have to literally tweet Ryan Shrout to find out for sure.\n\n[https://softwareoffer.intel.com/Campaign/Terms/7c4740d1-7b93-4626-9fbe-d58a7a0a38ef](https://softwareoffer.intel.com/Campaign/Terms/7c4740d1-7b93-4626-9fbe-d58a7a0a38ef)\n\nETA:\n\n[https://twitter.com/fsucory/status/1580256023096721408](https://twitter.com/fsucory/status/1580256023096721408)\n\nSomeone's ahead of us. No answer yet though.",
      "I did. I mean I'm cool either way, but the added benefit is nice (plus I could cancel my MW2 preorder...)",
      "Haha, well then I gave your reply a like. ;)",
      "\\*whispers\\* That's me...lol",
      "Is the deal for the four games and software included? Not showing up on the page for either card",
      "Which discord is that? Do you have a link?",
      "It's the Intel Insiders Discord: https://discord.gg/r6Az4q6D"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Should I get a used 3060 or brand new arc a750? For 1080p vr and ray tracing",
    "selftext": "I’m planning on getting it in march so hopefully prices are cheaper but I can get a used Asus Tuf 3060 12gb for £310 or a brand new arc a750 8gb for the same price what would be the better option for 1080p vr and ray tracing? Or should I save my money and get a rx6600 for £240?",
    "comments": [
      "6650XT new for £299.99 and it outperforms the 3060. Not as good for ray tracing but a better buy IMO https://www.overclockers.co.uk/msi-radeon-rx-6650-xt-gaming-x-8gb-gddr6-pci-express-graphics-card-gx-38s-ms.html",
      "Um with all those u not really gonna be able to do raytracing but if I would to pick I would say 6600 xt",
      "Good RT only exists at the high end, it's overrated at this point IMO, but don't buy a low end card if it's that important to you.\n\nThe 3060 12gb is no better than the 6gb version, it's not powerful enough to use the extra memory.\n\nThe only reason to get the Intel is if you need av1 encode.",
      "Yeah lol \nWe don't even know the CPU... but we do know exactly what they're getting with Nvidia/AMD. Unless Intel were like 50% cheaper for similar performance it makes no sense to even consider them.",
      "3060 cause its made for gaming",
      "why would anyone buy the intel card",
      "This.\n\nAnd ray tracing at this performance point is just not good. It barely makes sense at the high end.\n\nFSR 2 / 2.1 is getting pretty close to DLSS, too.",
      "They’re all made for gaming, the ones he listed at least lol.",
      "AV1 encoding... nostalgia one day...\n\nThat's about all.",
      "lol exactly"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Should I can buy Intel arc a750 graphics card for productivity purpose??",
    "selftext": "My computer specification is Processor Ryzen 5600x ; Motherboard b550mds3h ; Ram16gb 3200mah; SSD 250gb nvme. ; Hdd 1tb ; Psu 650w bronze I want to do productivity work is it ok to get Intel ARC A750 GRAPHICS CARD with this computer specification?",
    "comments": []
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Should I buy arc a 750 graphics card for productivity purpose??",
    "selftext": "My computer specification is Processor Ryzen 5600x ; Motherboard b550mds3h ; Ram16gb 3200mah; SSD 250gb nvme. ; Hdd 1tb ; Psu 650w bronze I want to do productivity work is it ok to get Intel ARC A750 GRAPHICS CARD with this computer specification?",
    "comments": []
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Will buying an Arc graphics card be worth it with my AMD Cpu?",
    "selftext": "I'm currently running an AMD Ryzen 7 5800x on a MSI Tomahawk B550 Motherboard, and am really considering upgrading my GTX 1070 to an ARC A750. I've been watching all the reviews and they say I need to enable Smart Access Memory too take advantage of the full power of the card but I don't think I can since I have an Nvidia gpu. Would I be able to enable it with the Arc card, and my 1070 is just too old or something?",
    "comments": [
      "B550 should support SAM, so that part would be okay. Not sure if I would recommend an Arc Graphics card for gaming, if you use your GPU for something else then that would depend on your workload.",
      "I have that motherboard with a 3070 and sam enabled. They released a bios update to support it so if your bios is still original from factory that will probably need to be updated\n\n[https://www.msi.com/Motherboard/MAG-B550-TOMAHAWK/support](https://www.msi.com/Motherboard/MAG-B550-TOMAHAWK/support)\n\n&#x200B;\n\n1070 wont have it as it only started at 30 series.\n\n&#x200B;\n\nhttps://www.nvidia.com/en-au/geforce/news/geforce-rtx-30-series-resizable-bar-support/",
      "6600XT. Unless you want to do free QA for Intel.",
      "Resizable BAR is supported on ARC, RTX 3/4000 and RX 6000 series cards, and SAM is only supported on RX 6000 series cards paired with Ryzen 5xxx and 7xxx series CPUs.\n\nRebar should work on your proposed setup, but I would think carefully about your use case before buying an ARC GPU.",
      "Don’t, just buy a 6650 XT. Arc just ain’t it right now.",
      "Yeah I just can’t get SAM to enable on my 1070 so I wanted to make sure I could on the Arc. And I use it for gaming and blender. I really just want to see these intel cards succeed",
      "Thank you!!!! This answered my question perfectly.",
      "I'd say go for it but expect DX9/11 problems. I suspect the interim solution to that is to either run Linux in a partition on a Arch/Fedora based disto to take advantage of Proton and the built in DX9-12 translation to Vulkan for problem games. Assuming that doesn't scare you.\n\nOr figure out which particular DXVK DLLs you will need to make the game run on Windows and setting startup parameters needed and locate the appropriate folder in the game to place the DLLs in to override DX9-11. Certainly a bodgier solution than letting SteamOS or the Linux Steam client sort it out for you (DXVKs target is Linux and in particular the SteamDeck) but with some fiddling you could use that to get some potential performance back.",
      "Arc gpu are good cards but they will need a time and lot of work to make them use their HW potential especially for older games so if you accept to be open minded during the voyage then sure, if you are faint of the heart then choose green or red league."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Should I buy a Intel arc a750 graphics card",
    "selftext": "My computer specification is Processor Ryzen 5600x ; Motherboard b550mds3h ; Ram16gb 3200mah; SSD 250gb nvme.  ; Hdd 1tb ; Psu 650w   bronze I want to do productivity work is it ok to get Intel ARC A750 GRAPHICS CARD with this computer specification?",
    "comments": []
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "a750"
    ],
    "title": "Should I buy ARC?",
    "selftext": "Am considering buying a750 or a770 second hand for 450 AUD, new ones like 650$+ but wanted to know if its a good idea with my specs as they are a bit dated. i7 8700k, PCIE3 mobo, \n\ni heard stuff like resizable bar and dunno if that's really gonna work with my stuff or do I need to look for Nvidia. i play at 1080p60, do i really need the 16gb model for arc gpu?",
    "comments": [
      "My Asus ROG Strix Z370-I Gaming mainboard with i7-8700K got ReBar support through a BIOS update. Kudos to Asus!\n\nWorks fine with the Arc A750, apart from Intel's bugged drivers...",
      "Iirc the i7 8700K motherboards don’t have the option for REBAR in the BIOS. I think 9th gen (Z390 chipsets) was as far back as they went. Due to this I would not get an Arc card as they are designed to use REBAR and not having it obliterates the performance of the card.",
      "Some z370 boards have beta or regular bios updates that enabled resizable bar.\n\nYou need to check with your motherboard manufacturer to see if that’s available. If you cannot enable resizable bar then I wouldn’t bother with arc",
      "Ah, thanks for correcting me. I wasn’t completely sure how far back they went with it.",
      "Seems like a good price! I'd buy it if I could.",
      "A litte late on I know, but some gen8 have it. Running z370-p with 8600k and rebar",
      "FYI, ReBar is only available on 10 series and newer CPU's. Therefore, your 8 series CPU doesn't have that capability. The head of their GPU division said himself that if you dont have it, then dont buy the Arc. It's whole architecture is built on it.\n\n\nhttps://www.guru3d.com/news-story/intel-arc-to-require-at-least-core-10-or-ryzen-3000-cpu-otherwise-performance-issues,6.html#:~:text=Minimum%20system%20requirements%20call%20for,versions%3B%20this%20feature%20is%20mandatory.\n\nBuy a 3060/3060ti instead if you aren't willing to upgrade the CPU/mobo."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "a750"
    ],
    "title": "Intel, why not do a showing of Unreal Engine 5's Matrix City Sample project to sell your ARC GPUs?",
    "selftext": "Tom Peterson and Ryan Shrout have mention multiple times how they are targeting \"next gen features\", and \"new standards\" when it comes to GPUs. As well as touting how great it is in DX12 titles.\n\n\"*Among game developers,* ***48%*** *of announced next-generation console titles are powered by Unreal*,\" - Epic Games CEO Tim Sweeney said.\n\nIf this GPU is targeted towards the *future*, and the selling point is how Intel will clobber the competition in performance per dollar in DX12 titles, prove it. You're claiming you're beating an RTX 3060 in [Fortnite](https://cdn.videocardz.com/1/2022/07/ARC-A750-vs-RTX3060-1200x675.jpg) already, which is using UE5. I feel like nothing represents next gen gaming more than that engine does.",
    "comments": [
      "Does anyone in this sub understand that intel just started from zero with DGPU business and only time will tell how much success it will have, major obstacles are:\n\n1. Drivers, they will need 1-2 year to get optimized and more people working in driver development/department.\n2. Prices should be 20-30% lower than equivalent counterpart from Green/Read team, yeah they will lose money in beginning but that doesn't matter they need to be competitive.\n3. If they fall to achieve 1&2 then they are fucked.",
      "Probably makes them look bad. It's easy to market your product when it has some kind of performance win.",
      "> Now, according to ~~available information~~ **speculation**, they are running into hardware scheduler issues...",
      "What sucks is that the GPUs actually have an FP64:FP32 adder ratio that’s significantly higher than any of their competitors. On the Intel architecture it’s 4 FP32 for every 1 FP64 ALU, where as on Nvidia it’s 64 FP32 to 1 FP64, and on AMD it’s something like 16 FP32 to 1 FP64. If you know how to write your software you can achieve a MASSIVE speed up, especially for things like scientific simulations. But for gaming only FP32 is really used…",
      "Because it would probably run at like 10fps or just insta-crash on current drivers",
      "My understand that I got from someone that seems to know a lot about graphical optimization, is that a massive amount on even DX12 titles falls into developers hands. Because DX12 is so light on the driver, and close to the metal, it's mostly in the game developers hands to figure out how this architecture works and to squeeze it to get more out. \n\nThat kind of sucks for Intel because the majority of work needed for new DX12 titles isn't really in their own hands. And game devs don't have the budget or time to figure out how to use Intel's hardware to it's fullest. They have no incentive to do so because hardly anyone even uses the hardware. Intel has to pay developers to even bother. And they probably are already. I'm sure the with Epic and CD Project Red had done wasn't free.",
      "Maybe. But since they got Fortnite to run well in DEX12 mode at least, they must be working with Epic. They got a few weeks to figure it out before launch.",
      "Year it depends on how much effort the game made to work on their GPUs. DX12 puts more responsibility on the developer to work close with the hardware to fit their game specifically. It should be better than DX11, but the worst ones won't be by much. But there could be titles where a dev actually tries, and puts a few hundred man hours in, and an a750 could be pushed to 3060ti performance easily.",
      "When it’s coming from Intel AIBs it’s not “speculation” lmfao. If Nvidia was tight lipped about a design flaw and every one of their distributors came out in unison saying the exact same flaw and issue, then it’s not speculation lmfao.\n\nEdit: I think I know who you’re talking about though. Moore’s Law is Dead is not a legitimate source of information, I agree. It is also evident though from their performance figures that it is a scheduling issue. Look at the A380, no matter what you lower the resolution to, the card performs identically.",
      "What happened to \"Rapid packed Math\"? There were like half a dozen games announced years ago when they were hyping up Vega. Are games still using that? Far cry 5 was one of them.",
      "> Look at the A380, no matter what you lower the resolution to, the card performs identically.\n\nDespite what MLID says, [this is just categorically not true](https://www.igorslab.de/en/intel-arc-a380-6gb-in-test-gunnir-photon-total-benchmarks-detail-analysis-and-extensive-teardown/5/). MLID built that entire argument on [JUST the 1% lows on a single test from a single reviewer at a single resolution between an overclocked A380 and an non-overclocked A380.](https://youtu.be/DH2s5HeZzs8?t=528) His reasoning is incredibly poor.",
      "From what I understand it’s still there, you just have to program to use the different data types. If you program an entire engine using 4-byte Floats, then you need to adjust areas where you can go down to 2-byte half floats, but you can’t just convert every single float to a half float because you lose precision, especially on large pieces of geometry. Instead now they use Variable Rate Shading to utilize the included hardware. It’s funny because you saw massive VRS gains on Turing and RDNA2, both of which saw a doubling of FP16 from FP32. Ampere brings it back down to a 1:1 ratio, so it loses almost all of the performance implications…"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Arc A750 (colored dots on the screen)",
    "selftext": "Hello, since yesterday I started using the Arc A750, but today I noticed that some colored dots appear on the screen. I tried to solve it by reinstalling the driver, I checked the cables and some settings, but I couldn't figure out where the problem would be.Hello, since yesterday I started using the Arc A750, but today I noticed that some coloured dots appear on the screen. I tried to solve it by reinstalling the driver, I checked the cables and some settings, but I couldn't figure out where the problem would be, and I wonder if anyone has any idea about this problem.\n\n I want to point out that I didn't have any kind of problem with the previous video card. (GTX 1660 TI)\n\nThank you.\n\n&#x200B;",
    "comments": [
      "Dots coming from a GPU is an artifacting issue that is likely due to a defective video card.\n\nThe problem might be if the GPU is stressed beyond a certain point where whatever is broken inside the video card becomes evident because that section of the GPU or a component of the video card needs to be used or stressed beyond its capabilities. You might try to under clock the video card to see if the issue disappears or you can reduce the work load of the GPU by lower the settings within the software you are using. At this point we do not know if the problem lies inside the GPU chip, a memory component, a circuit weakness, a VRM or capacitor defect, etc.\n\nYou only say dots appear but you give no other information such as when, where, all the time, or only on one game or all games, or during turn on or when the computer is on 10 minutes, an hour, longer, or the temperature, resolution, etc. As I said, You only say you see dots. If it's all the time then the problem is the video card. If the problem only occurs inside one program then the issue is a bug or software issue and not the video card. Problems in video output as to when they occur usually signal what area of a computer is causing the problem.\n\nI suggest you go to an Intel video card user forum to see what other like minded users encounter and request assistance from them.",
      "Thank you!"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "a750"
    ],
    "title": "Intel Arc A770/A750 GPUs get a €248 water block - VideoCardz.com",
    "selftext": "",
    "comments": [
      "This seems silly. I’m glad Intel have made their entry but it doesn’t seem like it would warrant relatively exotic cooling…",
      "Putting a waterblock on an arc card is just a flex. Nothing more, nothing less",
      "Yeah the A770/750 are actually kind of big for their relative performance.",
      "Water cooling has really fairly little to do with performance. EK blocks are too expensive for this card but if someone wants this card in a water cooled pc there are very few options.",
      "the cards dont get nearly hot enough to ever need the use of a waterblock, nevermind one that costs over half the actual card",
      "So roughly it's gonna cost nearly as much as a Rx 6900 xt and u can also get rx6800xt which is cheaper and beats it 😆",
      "It would be a meme for sure. Or you just want to be some (non LN) benchmark champion for this particular model"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "csgo on arc won’t open HELP",
    "selftext": "hello! i made a post last night regarding csgo not opening on my new arc a750, i am coming from a 3070 ti and i just bought an arc to experiment with, i just wanted to make a new post with what troubleshooting steps i did take (game still won’t open)\n\n1) used drivers 4091 and beta 4123.\n2)DDUed both drivers twice and reinstalled after failed attempts of opening game.\n3)DDUed old nvidia drivers.\n4)ran windows update.\n5)disabling things for CSGO like fullscreen optimizations.\n6)performed an sfc /scannow test in cmd.\n7)tried opening game with resizable bar on and off in bios.\n8)flashed mobo bios to latest.\n\nthings that have kinda worked\n\n1)opened the game on an alternate gtx 1650 that is in the computer along side the a750, game opened perfectly fine with that.\n2)put a fresh windows installation on an alternate ssd and put only csgo and arc drivers on it and it opened perfectly fine.\n3)reinstalled game on main OS as a last attempt, the game did open but the minute i tabbed out the game crashed and never to be opened again.\n\nthis card does seem to be defective in some sort because when i use hdmi to one of my secondary monitors, i get really bad artifacting and anomalys on the display. displayport-ports seem to be fine though. i have a feeling it can be a windows build because it is working on another one but the build of windows is only a few months old and has stayed on 22h2 the whole time \n\ni’m just really frustrated and i want to root for intel because this can be a great product \n\nspecs\n5900x\ngigabyte aero x570\n980 pro 1tb\nrm850 psu",
    "comments": [
      "okay i solved the issue, i forced the game to open in 640x480 by manually finding the game and checking that option off under properties, then i put the game into windowed and reloaded with the setting checked off and i was good to go!",
      "Didn't they just release a new driver that optimized CSGO? Weird. Anyway, glad you got it fixed.",
      "Glad that you got it fixed. Mind if i ask about your experience with the card so far?",
      "performance is solid espically on esports titles like csgo, valorant and overwatch, these are really the only games i have played on it but for the price it’s hard to beat. i do wish there was a manual highlight feature with a key bind to record like the past 3 minutes of gameplay like amd and nvidia cards do, but the device is 3 months old so they get a pass, also not having a custom resolution creator is a major miss on this card, i use 1440x1080 4:3 on csgo and valorant and this is such a dealbreaker cause u have to use a stupid 3rd party program do to so, intel used to have it in their graphics settings a long time ago but they took it out. i do have to RMA the card because the hdmi port doesn’t work, every time i use it all i see is a flickering screen, lines, or like half of the display doesnt appear right, also my card struggles running 3 monitors properly but i think it could just be the card cause my 3070 ti has no issues with it. 7/10",
      "Thx for the feedback. I've seen mixed feedbacks about the a750 so i was quite hesitant between it and rx 6600. A750 is about 12% cheaper than an rx 6600 here. \nUsage wise, mine is not much different from yours, mainly eSports title and possibly some non 3a title (my current laptop cant handle it). Often uses davinci resolve for 1080p videos too. From your pov, do you think the 12% price difference worth the purchase?",
      "nah not yet its not ready, maybe if youre are a content creator cause it has an av1 encoder but the software is crappy, itll get better soon tho it seems like intel is putting alot of time in it"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "a750"
    ],
    "title": "i5-10400 + Arc A770 or A750?",
    "selftext": "Hi, I have to upgrade my graphics card. This is my current setup:\n\ni5-10400, B460 MoBo with ReBAR enabled, 2x8gb DDR4 2666, 512gb SSD NVMe.\n\nI'll use the new gpu for 1080p gaming on a 24.5\" 240Hz monitor (games like Far Cry 6, Horizon Zero Dawn, Cyberpunk 2077). Should I pick the A770 380€ or the A750 280€ would be enough?",
    "comments": [
      "In all of those options i would go with a 6650xt, 330 isn’t a bad price, I personally don’t have any experience with intel Gpus. I would shoot for the 6650xt. But for -2 to 5% performance you can save money and go for the 6600xt. But regardless of which you decide, all with provide a very nice 1080p gaming performance. My current set up explicitly is also a 10400f and a 6600xt",
      "Is there a 6750XT in the $370 range? That would be my pick. However, to truly enjoy Maxed out Ray tracing in Cyberpunk, RTX 3060Ti will be your best bet. That's $400.\n\nAlthough I'm unaware how well the Intel cards are at ray tracing at the moment with the new drivers.",
      "Are you needing an arc gpu?, i ask because the 6600xt matches it or surpasses it in most cases. Plus i just got a power color red devil for 230 on eBay. But if you need the intel counterpart i would go with the 770.",
      "A770",
      "So, I've heard the arcs have improved a lot thanks to the new drivers and the A750's become a good deal. Anyway, my current options and prices are A750 280, 6650XT 330, 6700 10gb 360, A770 380. Which one would you pick?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Intel arc a750. No video when installed.",
    "selftext": "I purchased an intel arc a750 to install on my hpz420 workstation, but when i made the swap, there was never any video when it was in the main GPU slot.\n\nThe computer hardware:\n\nPSU: Delta electronics 600W switch mode power supply (DPS-600UBA) (with 1x8 PCI upgrade)\n\nProcessor: Intel Xenon E5-2670 (2.60GHz) 8 cores\n\nRam: Generic 32GB DDR3\n\n(Stock)GPU: NVIDIA quadro 600\n\nHDD: Segate Barracuda 2tb (ST2000DM008)\n\n&#x200B;\n\nHere what I tired:\n\nOS: Linux Manjaro stock BIOS (Arc750 - no signal, Quadro600 - normal)\n\nOS: Linux Manjaro stock BIOS, NVIDIA drivers purged (Arc750 - no signal, Quadro600 - normal)\n\n&#x200B;\n\nOS: Linux Ubuntu 22.04, stock BIOS (Arc750 - no signal, Quadro600 - normal)\n\nOS: Linux Ubuntu 22.04, stock BIOS, NVIDIA drivers purged (Arc750 - no signal, Quadro600 - normal)\n\nOS: Linux Ubuntu 22.04 (22.10), stock BIOS, New kernel installed (from intel's tutorial) (Arc750 - no signal, Quadro600 - normal)\n\nOS: Linux Ubuntu 22.04 (22.10), stock BIOS, New kernel installed (from intel's tutorial), Nvidia drivers purged (Arc750 - no signal, Quadro600 - normal)\n\n&#x200B;\n\nOS: Windows 10, stock BIOS (Arc750 - no signal, Quadro600 - normal)\n\nOS: Windows 10, reset BIOS/CMOS reset (Arc750 - no signal, Quadro600 - normal)\n\nOS: Windows 10, BIOS Updated (Arc750 - no signal, Quadro600 - normal)\n\nOS: Windows 10, both NVIDIA drivers and device pointers removed (Arc750 - no signal, Quadro600 - no signal ofc)\n\n&#x200B;\n\nI have never gotten a signal from the arc a750, However the fans still spun, the lights still turned on, just no signal. All the PCI power connectors were connected. Each test was booted with the different GPUs and each of the output ports was tested, both booted up with it plugged in, and repluged when the machine was on.\n\n&#x200B;\n\nI started noting everything about the latter tests:\n\nThe ARC GPU got hot for the first 3 min of operation, then returned to normal.\n\nThe monitor blinked the \"no signal sign \" whenever I  unplugged or plugged in.\n\n&#x200B;\n\nI am not going to use the arc750 for high performance graphics, it is mostly going to do stream processing, so even wildly inefficient solutions are welcome.\n\n&#x200B;\n\nEdit: more testing:\n\nArc in the main PCI slot, Quadro in the second slot, monitor connected to Quadro:\n\nI would get video if and only if the Arc was not powered. This means that the Arc is recognized, it just won't give video.",
    "comments": [
      "Did you try setting the primary GPU in bios?",
      "I had the exact same issue when I installed my A750, windows would boot and everything would work, but no video, after restarting a few times it worked.",
      "Each test was done with only 1 GPU at a time."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Should i buy arc a750? for a video editor here",
    "selftext": "Hey guys,Should I purchase an Intel arc a750 8gb as a freelance video editor,I would be using Davinvi Resolve as my editor and i cannot increase my budget at all...? Is there any better GPU in this segment? I dont think so...\nAlso i am a passionate gamer and i know how epic this gpu is on gaming.\n\nPlease help mates.Thank you",
    "comments": [
      "https://www.pugetsystems.com/solutions/video-editing-workstations/davinci-resolve/hardware-recommendations/\n\nIf you cannot increase your budget at all, I suggest you try to find a used RTX 3060. Depending on where you live, they can be found for around the same price as a new Arc a750, and you'll have a much better experience using Davinci Resolve."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Intel ARC GPU runs very bad on a game that is only taking 40% load",
    "selftext": "I used to have an Radeon RX 570 before upgrading and it would run Spider-Man (Miles Morales) at 30-60 fps. now I am getting 20-30 fps with at avg load of 40% (not due to any bottleneck) This is the only game I experienced this on.\n\nCPU: Ryzen 5 5600x    GPU: Intel ARC A750 (8gb)\n\nCan anyone help?",
    "comments": [
      "did you turn on resizable bar",
      "Try restarting. Try both the latest whql and beta drivers. Try DDU to wipe all amd and intel drivers in windows safe mode.",
      "Getting the same problem on Horizon Zero Dawn. ~40 fps at 1080p max on the benchmark--I was getting over 90 with a 3060. GPU rarely goes over 40% utilization. The HZD benchmark gives separate CPU and GPU FPS: GPU says 99 (in line with what I'd expect) and CPU says 47. I would think it's a CPU bottleneck but my CPU doesn't go over 60/70% utilization. \n\nR5 3600, Win 10, ReBAR on, GPU running in PCIe Gen 3",
      "yes."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "intel arc a750 fps drops",
    "selftext": "i recently bought a intel arc a750. I really enjoy it, but I get huge fps drops in Warzone 2. Does anyone know how i can fix this problem?\n\n(I enabled resizable BAR and updated the drivers.)",
    "comments": [
      "Have you waited for full Shader Compilation?\n\nIf so, the advice is sad, but: Radeon or NVIDIA."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Intel Arc A750: Turn off the logo light?",
    "selftext": "That's a very bright light! I would rather like to turn it off. \n\nApparently you can control the logo light on the A770, but the A770 LED control software refuses to run without detecting a A770.\n\nAny chance to get rid of that light without taping it over?",
    "comments": [
      "I personally have taped it over after a few days. I didnt know about the LED control software tho",
      "I figured that much by now. It's really a shame, just a simple DIP switch on the side of the board offering maybe 0%-33%-66%-100% would such an appreciated quality of life improvement.\n\nI don't mind the branding, just don't make it a glowing Sun. It's taped over with some transparent but really dark packaging film I had laying around now (subjectively blocking 90%+ of the light): https://i.imgur.com/FPixW0M.jpg I like it - it's fine.",
      "The A750 doesn't have controls for its LED lighting. Black tape is my suggestion.\n\nOtherwise, you'll have to open up the card and pull out the LED cable thats powering it.",
      "I agree. I mean, what's the point of putting LEDs on a gpu if you won't let users control it. Or at the very least an on/off toggle."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "can i use a intel arc a750 whit i3-10100f?",
    "selftext": "hi guys, i want to buy a new pc (i have a old one whit c2d e8400 and gma 4500 \\[very bad\\]), whatcing some videos and stores i have find an arc a750 and a i3-10100f, next i try to search some bottleneck test (nothing found). Yesterday i found a video about the arc a380 and they says that for installing the drivers yu have to use a IGPU, so i have to use a igpu for installing the drivers of a arc a750 whit a i3-10100f? \\[sorry if my english is bad\\]",
    "comments": [
      "Yea you can use it perfectly no need to worry",
      "Yes. You don't need another GPU just for drivers, microsoft has a default driver for video cards. It makes your dGPU run (but poorly) so you can get a copy of GPU driver and install it."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "[TechPowerUp] Intel Arc A750 & A770 Unboxing & Preview",
    "selftext": "",
    "comments": [
      "More of a preview than a review, I didn't know what flair to use. \n\nI love the Arc branded toolkit they're giving reviewers for teardowns",
      "Looks like the gamers nexus toolkit.",
      "Looks promising but lower end than I was hoping.  If it was on par with a 3070 I'd jump.  Still might simply because I can probably afford it!"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Intel Arc A750 Flickering",
    "selftext": "PART 2) Owner of an Intel arc a750, been trying to figure out how to stop screen flickering on games? Any tips on how to stop the flickering?",
    "comments": []
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "a750"
    ],
    "title": "Intel 11700K + What ARC?",
    "selftext": "Hi there!\n\nI'm looking at buying an ARC gpu to have an all-blue setup. I have a 11700K and I'm wondering if it could handle the A770 16gb or if I should go for the A750 8gb instead. What do you guys think?",
    "comments": [
      "What are you planning on using the card for? Gaming or productivity?",
      "Should be fine.",
      "Assuming gaming, if you're doing 1080p A750 will be perfect, you won't need an A770  \nIf you do 1440p gaming, bumping up to A770 will help.",
      "Ik it's late. I have a i7 11700k and it had a 2060 super in it and my cpu hardly ever goes over 35% usage on any hard gameing max settings 1440p 75hz. AAA titles. I'm going to buy a arc intel a770 16gb soon. Did you buy one? How did it do if so. Also I belive our cpu can handle like a 3090ti.",
      "Both actually, and I would say less gaming. I have my own business and do a lot of video editing, but also 3d modeling, graphic design, web development, etc.",
      "I would say less gaming and more things like video editing, 3d modeling (Blender, Solidworks), graphic design (Adobe CC), web development, etc. I have a 34\" 1440p ultra-wide screen, so then maybe the A770 would be the best fit? (meaning workload + occasional gaming)"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Intel Arc | Intel Arc A750 Limited Edition Graphics Card Showcase - VRR/HDR/HDMI",
    "selftext": "",
    "comments": [
      "> Any adaptive sync certified display will do the same and should work with Intel Arc graphics, but we’re validating 100+ top VRR displays to make sure you have an amazing experience when the Intel Arc A700 family of cards launch.\n\n> Finally, we can discuss HDMI standards and what you can expect on Intel Arc products. While the A-series of GPUs supports HDMI 2.0 natively, partners and OEMs can build in support for HDMI 2.1 by integrating PCONs that will convert DisplayPort to HDMI 2.1. Our Intel-branded Limited Edition cards, both the A750 and A770 variety, will support HDMI 2.1 through this method. Other add-in cards, and notebooks, will support it if integrated.\n\nhttps://game.intel.com/story/intel-arc-graphics-vrr-hdr-hdmi/",
      "So now I'm wondering if my use case would still be a good one for Arc. HDMI 2.1 is a must have so I can get 4K120, but I would also like FreeSync over HDMI (which my monitor *does* support officially).\n\nIt has to be HDMI because the \"monitor\" is actually an OLED TV which doesn't have DisplayPort.",
      "Folks should keep in mind that Intel is likely only supporting the official standards for VRR. This means for Intel Arc, \"FreeSync over HDMI\" monitors that **do not** have HDMI 2.1 - or used with an ARC card without the converter - may or may not work with VRR.",
      "Now that ARC reviews are released, do you know if HDMI 2.1 forum VRR is part of the official release spec for A750/A770? Has anyone one tested this yet?\n\nUse case is for 4K high refresh displays with only HDMI input (e.g. OLED TVs)"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Intel Arc on HDMI 2.1 TV - Blank Screen",
    "selftext": "I just got my Arc A750, and I’m having a weird problem with the HDMI out that makes me think Intel might have been lying about the HDMI 2.1 support on LE cards.  \n\nI have a Samsung S95B, and with Input Signal Plus turned off it works fine (albeit with detecting HDR as unsupported and capping at 60hz). But when I try to use Input Signal Plus to get 120hz and HDR it’s a blank screen. It flickers like it’s trying to get a signal but ultimately doesn’t come back. \n\nShot in the dark considering the cards are a week old, but has anyone else run into this?\n\nEdit: it seems like the card doesn’t like my HDMI switch. I would attribute the issue to the switch entirely, if not for it having no issues with a PS5, Series X and 6800XT all using HDMI 2.1.",
    "comments": [
      "I am planning to do an HDMI 2.1 test on my card when I get the chance",
      "Do you see any warning/error messages in the Windows event viewer related to Intel?",
      "Is that a HDMI 2.1 rated  hdmi cable?",
      "Nothing that I can see.",
      "The one I switched out wasn’t, but the others are. Even with both being 2.1 certified (and working on three other HDMI 2.1 devices) I still got disconnects with the switch.",
      "Yes intel is lying it's probably hdmi 1.0."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Games keep crashing with ARC A750 and get KERNEL_MODE_HEAP_CORRUPTION someone pls help",
    "selftext": "",
    "comments": [
      "Whatever happens, just know that we are proud of your sacrifice to have bought an intel gpu.",
      "https://www.intel.com/content/www/us/en/support/contact-intel.html#support-intel-products\\_80939:227960"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "EK Launches Water Blocks for the Intel ARC A750 and A770 GPUs",
    "selftext": "",
    "comments": [
      "https://gfycat.com/coolfluffyboaconstrictor",
      "When you spend more on your waterblock than on your GPU.",
      "Maybe I'll try it next year. I am up for a challenge water cooling a Formd T1.",
      "Looks really pretty, maybe get a bit of extra performance out of it. Mostly for aesthetics though. Can also see some potential for ultra slim form factor builds."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Have any of you played Squad with Arc a750?",
    "selftext": "",
    "comments": []
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Arc A770 Available 10/12 for $329",
    "selftext": "",
    "comments": [
      "329$ is pretty cheap for *modern days standards* specially if this competes even closely to the 3060ti. Hopefully they just can get their drivers sorted out.\n\nAnyways nice to see finally a 3rd competitor in the gpu market.",
      "Pat's rocking that \"dad at Disney World\" look to contrast with Jensen's \"uncle who thinks he's cool but actually isn't\" look.",
      "Team **R**ed vs Team **G**reen vs Team **B**lue\n\nRGB 🤔",
      "I mean... you can already get an RX 6700 for $360 which competes with the 3060 Ti.\n\nAnd actually works well with OpenGL/DX11 and below. And it also has known functional drivers",
      "To be fair Nvidia's 3060 \"MSRP\" doesn't exist, the cheapest one thus far (excluding the very limited number of MSRP models at launch) has been approximately $370 for a low tier MSI model. We'll have to see how Intel Arc pricing plays out in reality and whether we get a card at $329. I agree with the your AMD point, their pricing is very good right now.",
      "HOLY MOLY WE HAVE A DATE!\n\n&#x200B;\n\nAND A GREAT PRICE!?",
      "And uses less power.",
      "great price, just keeping fingers crossed for performance",
      "Truly is.\n\nI don't get the people saying \"its overpriced\" \"the drivers are bad\" \"this is awful not buying it\"\n\nLike this is amazing actually, Intel joining the competition is nothing but perfect for us consumers. We get better and cheaper GPUs in the long run. With the way Nvidia is looking right now it's a good thing that eventually they will be forced to make things cheaper. Only a matter of time before Intel, AMD, and Nvidia are neck and neck\n\nCompetition is good",
      "Great news for everyone.",
      ">​AND A GREAT PRICE!?\n\nNot realy, 13% faster then a 3060 with the same MSRP does not work well with the drivers being at their current state and higher power draw then both AMD 6600xt and nvidias offering the card is DOA. 259-299$ would have been a maybe but at that price it's a flop. Maybe they can discount it then it has a chance",
      "It was meant to be.",
      "and is less experimental phase. Yeah, I would absolutely pay extra for either RX 6700 or 3060 Ti. The value proposition doesn't seem to be there based on my understanding of the expected performance of Arc.\n\nThat said, do we have any benches yet?",
      "In some other timeline Intel released this one first, with better drivers, and effectively created a modern RX580, which would have been a great way to enter the market.",
      "Yeah, this is still a bit too much, with 6600 XTs widely available at $300 now.",
      "No, Intel ruined the naming scheme. I was kinda hopeful they wouldn’t.\n\nR = *R*adeon\nG = *G*eforce\nB = … *A* rk",
      "and stability",
      "Inteeeelllllllll gpuuuuuuu",
      "Guys don't order it. I need to get my order in first :).",
      "They are old tech geeks.  Seems quite appropriate.  I'm sure I'd look just as daggy trying to cool on stage in front of the world..."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "arc a770 defo won the sexiness test so far",
    "selftext": "",
    "comments": [
      "I really hope Intel's GPUs succeed, if only so we can finally have options for aesthetically appealing GPUs",
      "Why would you buy one to replace a 3070 though?",
      "I hope they are persistent on fixing bugs and improving performance in next 1-12 months that will decide fate or 2nd gen arc gpu.",
      "Looking impressive! I am not a fan of RGB. But I will make an exception with Arc. This is how RGB should be done. Also the clean look. Wow!",
      "A 3070 without a backplate? Didn't know those existed.",
      "So far benchmarking wise the arc is identical to the 3070 on a 1440p ultrawide",
      "Hopefully Battlemage will turn out great",
      "EVGA Black edition it was only available at launch kinda was bummed about it but I couldn't really complain at least I had got a GPU then",
      "The 3070 beats it in gaming, pretty big difference",
      "It was evgas msrp card they had a lot lower stock of them",
      "I have been running actual games I was getting around 106-135 fps on the Witcher 3\n40-65 fps on cyberpunk 2077\n139 fps on GTA V\n200-540 Fps on CSGO\n1440p Ultrawide",
      "not completely true in cyberpunk 2077 my 3070 would get 75 fps on high settings and the  arc would get 40-65fps so really the drivers need to be ironed out",
      "Oh damn that means they already fixed a bunch of stuff. Csgo used to run really badly.",
      "I bought to tinker with and test with for a little bit might switch back to the 3070 in month or two",
      "Tastefully refined LEDs to the rescue!",
      "Idk why you're being downvoted.\n\nLike, look at that diffuser. Its terrible. You can literally count each LED.\n\nWhats the point even having a diffuser at that point?",
      "I’ve heard the Arc cards are extremely good in 4K, but you should be running actual games, synthetics are a completely unrealistic best-case scenario for Intel.",
      "Because hes an enthusiast",
      "why change a 3070 for a 770?  \nis a downgrade for you",
      "True, though they have incredible thermals the only reason I buy them, sad to hear they dropped out of the GPU market"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Arc A770 16Gb matching top Ampere GPUs in Hogwarts Legacy",
    "selftext": "",
    "comments": [
      "This article has the Arc A770 16Gb slotted in between the RTX 3080 and 3090 in **Ray Tracing** performance.\n\nLooks like Intel made sure to be prepared for the release of this title. This is the first time the A770 16Gb has really lived up to the potential of it's hardware. This title also has a really good implementation of Xess as well.\n\nEdit to specify I'm talking about RT performance.",
      "Promising results. Intel went and put the hundreds, hell probably thousands, of hours into optimizing for this one game knowing it was really the first hot release since the cards came out.\n\nNow, the question is can Intel consistently get big Wins like this with new games? That remains to be seen.",
      "Performs the same as a 3090 in 4k RT. Crazy good scaling, and XeSS will make that playable.",
      "Its worth noting that the performance its getting is in Ray Tracing scenarios only, in a raw raster scenario its still placing between a 6600 and 6700 xt, and well below the 3080 and 3070, but in a RT scenario it leaves both of its AMD competitors in the dust. Heck, its leaving a 7900 XTX in the dust in a RT scenario. Pretty wild.",
      "Yup, that's the most important question, 1 game means nothing.\n\nTBH it seems that they massively improved since launch, but there is still lots to do, seems like they are making a progress.",
      "That's true. I guess I was just blown away by seeing Arc perform anywhere near 3080 / 3090 in any scenario and I didn't look at the fine print.\n\nAccording to this [video](https://youtu.be/beT2EBXDPpY), the Xess implementation is doing a good job of boosting the raw performance too.",
      "This tracks with some of the early benchmarks which slotted the A770 up with the 3080 in edge cases, and the original Intel expectation they had a 3080/3090 competitor on their hands before driver quality gave them a dose of cold water.\n\nAs the drivers improve these could legit become 3080 class cards, or at the very least, 3070Ti.",
      "Intel is new to the GPU scene and is already fucking AMD in the ass.",
      "This makes me excited to see what Battlemage is like.",
      "question based on your flair:\n\n do you have the RTX 3070 and the Arc A380 in the same system? I was thinking since Intel iGPU has been around forever that their co-existence would be just fine in the same system, for encoding/streaming purposes, etc :-)",
      "Its very impressive, it means that per dollar, Arc is getting the most performance in raytracing there is on the market right now. I'm not sure many people value RT that highly yet, but it shows how well the Alchemist architecture is handling modern games and technologies.\n\nWhats wild is at its price point, 350 dollars, there is no other GPU in its price range that is creating what I would consider a 'playable' experience in 1080p raytraced, (absent dlss/xess ect), able to consistently stay above 30.",
      "I'm not sure, but I think it has to do with building credibility.\n\nLet me jsut shoot my shot here.\n\nIf Intel had some in with a 600-700 dollar GPU that launched running as poorly as the a770 and a750 did, I think they would have been written off as a waste of money.\n\nBut with something as cheap, especially now, as an a750, I think it opens people up a lot to trying something different out, giving them a chance, and if it works really well, thats awesome, if its really bad, well its improving. It appeals to a different market, but when they release an enthusiast series GPU with battlemage, its much better to have people go 'The Alchemist GPUs are actually pretty good, now its worth jumping in' for that bigger ticket item.\n\nOr maybe its a total cope and they just didn't yet have the experience to make an enthusiast gpu they were comfortable releasing, idk.",
      "i’m telling you, if intel keeps this up with their graphics cards division, i may have a team blue system when it’s time to replace my card",
      "I mixed a 2080 and a770. No problems there",
      "yeah, I may redo this thread so I can specify I'm talking about RT performance in the title.",
      "AMD's raytracing is tragic.",
      "Getting a little too excited here!",
      "Yeah, same system. Nothing uses AV1 yet though, but quicksync is pretty good and helps take load off the 3070. Can't say the drivers play too nicely with each other though.",
      "In a 2 slot form factor also, not 2.5 or 3 slots. This is really impressive for the size of the card compared to the competition.",
      "Yes they did - but there’s a side effect that optimization often is more general and helps more than one title. A rising tide lifts all boats type of thing"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "a770"
    ],
    "title": "“Dark ARC” 12600K | a770 build",
    "selftext": "Sharing my latest build. Enjoy.",
    "comments": [
      "Really good looking clean build",
      "This is super clean! I'm building one with an a750. It's my first build so I'm pretty stoked!",
      "What AIO is that?",
      "This makes me want to get rgb fans.",
      "Deepcool LT720",
      "The 750 is a great card! Happy building",
      "Deepcool LT720 infinity 360mm",
      "It’s absolutely stunning. The camera cannot pick up all the variations in the colors. It’s more wow in person",
      "Ultra sexy. Very nice",
      "DeepCool LT series, I think the LT720",
      "Love the color theme",
      "I love this build! Very tasteful RGB and the modern Intel case badges are a nice minimalist design.",
      "Glad I'm not the only one still using the stickers!",
      "That's really clean. Arc GPUs look sick",
      "Ty. I’m a Systems Integrator, most of my work gets sold. I build more than I get to enjoy- most of the time. It can be torture sometimes because I basically want to keep every build for myself",
      "CPU:\n Intel Core i5 12600K 10-core/16-thread @3.7GHz/4.9GHz Boost\nRAM:\n32GB DDR4-3200 Kingston FURY Beast (CL16) \nMotherboard:\nMSI Z690-A PRO Series WiFi DDR4\nGraphics:\nIntel ARC a770 16GB Limited Edtition\nStorage:\n1TB Samsung EVO 970 \nOperating system: \nWindows 11 Professional w/key activated \nPower:\nSegotep GM 750w 80+ Gold ATX 3.0 (12vHPWR ready)\nCooling:\nDeepCool LT720 360mm Infinity All-in-One CPU cooler\n5 Asiahorse 9002 PRO HSP aRGB fans\nConnectivity:\n2.5G Ethernet, Wi-Fi AC6 & Bluetooth on-board\nAccessories:\nBlue ATX/PCIe power cables\nExtra 12vHPWR PSU Cable, Extra PCIe Cables for future upgrades\nCase: \nHYTE Y40 Panoramic case (Black)",
      "Very clean build. Enjoy :)\n\nI enjoy building rigs for my friends and myself more than actually playing on them 🥲😅",
      "Clean af looking build bro 😎 cool that people can now have complete intel builds! interested to see benchmark results",
      "What's that cooler?",
      "please share full parts list man"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Alleged Intel Arc B580 Battlemage listed in shipping manifest, Arc A770 drops to $269",
    "selftext": "",
    "comments": [
      "… Does it outperform the 6650 XT now? It needs to have a clear win over that card to be worth it.",
      "it's not a duopoly, nvidia has been beating tf outta amd in gpus for a while now. Nvidia has almost a 90% market share in the gpu space and rdna3 was so bad it lost amd market share.",
      "Sometimes, but other times nope. Even if it was a bit faster than the 6650 XT in every game, for $70 more then what it goes for nowadays I still wouldn't buy it.",
      "in my humble opinion, the one percent lows are the most important metric",
      "There should be something like a B770: [https://www.guru3d.com/story/intel-arc-battlemage-graphics-cards-expected-to-launch-in-december-ahead-of-ces-2025/](https://www.guru3d.com/story/intel-arc-battlemage-graphics-cards-expected-to-launch-in-december-ahead-of-ces-2025/)",
      "I wish that intel cards were better, breaking the duopoly would be sick.",
      "269$ for 16gb of vram is wild.",
      "No. It’s a budget card. Lowest performance.",
      "B770 looks like it could be juicy. I finally have a monitor that is Freesync rather than GSync, so now I can pick and choose from any manufacturer for my next upgrade. As long as Intel can figure out their HDR support they could be considered.",
      "For the coming few generations, yes. AMD has said that they will cater to the low to mid range to build market share.",
      "Is this the highest sku they gonna make, if it is that's disappointing",
      "What? \n\nJust because a product sells well too doesn't make it good, mindshare is definitely a thing (and arguably a greater factor for Nvidia than Intel).",
      "Idk why you insist on putting words in other peoples mouths and making false equivalences lol. Where did I claim AMD made the better product?",
      "For a first attempt they were okay (okay is not good enough in the current market). They certainly didn't hit their goals but no1 does in the first attempt.\n\nThe question is if they decide to stop the whole consumer line and just focus on enterprise, considering their current financials. Ideally you want both for full integration but that's reality, and then they can maybe steal share from AMD that's already like 10-15% of the market. Tackling Nvidia is another thing entirely......",
      "Not everyone cares about those features.  In my view the only problem on amd gpus is the terrible rt performance.  I don’t care about dlss downgrade tech.",
      "To be honest: no clue. I do have a few suspicions though. Right now they rare still working on their consumer gpu chiplet design, and it wasn't fully working, so they decided to go back to 'monolithic' for the time being, until they work it out.",
      "Wtf? AMD midrange and overhyped cpus? The 9800x3d is the gaming king at this moment. Maybe intel leads in productivity sometimes slightly, but more often than not AMD beats them aswell because of the lackluster performance of Intel Core ultra 200. \n\nAMD doesn't pay people. It's just the fact that Intel is down now, and AMD released a gaming beast that people are excited about. That's why they leave positive reviews, and on Intels parts negative ones. That's been happening since zen 2. That's when AMD really started to pop off.\n\nAMD is taking up more market share by the quarter (up from 23.3 to 24.2). They are the more bought, more performant, more efficient, easier to cool cpu. It's just plain simple. \n\nIdk about the NVidia cpu part, but I reckon it's going to take a few generations to catch up again (just take a look at Qualcomm, they haven't fixed their x86 emulation for arm yet.)",
      "I want a flagship 😭",
      "Yeah amd is really getting slammed. Only really good if you are on a budget and never use new features. I have an amd gpu and the driver issues and fsr are both big downgrades from an nvidia card. Literally have to use a driver that is 3 versions old because amd cant fix their shit.",
      "What’s “bad” about it?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "New Build (Core i5-11600k & arc a770 16gb)",
    "selftext": "",
    "comments": [
      "Why would you buy a new 11600 at this point in time",
      "If you're mainly concerned about the price, why didn't you buy a 12400F? It's likely cheaper and performs even better than the 11600K.\n\nGoing with AMD's ryzen 5600 will be even cheaper, and again perform better than the 11600K.",
      "You unfortunately missed out on a huge generational performance increase. And you only needed to pay an extra 50 euros.",
      "Damn, that is pretty close to the price of a 13400F. If you already had a motherboard for 11th gen then I can see why it's a good choice for you, but otherwise I don't understand why you didn't go for a 13400F or 12400F. Buying a \"higher end\" model of an older CPU is usually a very bad idea because you can get a \"lower end\" model of a new generation that is even better and also cheaper.",
      "Intel chips all still supports DDR4, it's not necessary to use DDR5 and you only lose some few percent of performance by not using DDR5. It's 14th gen chips that will be DDR5 only.",
      "This was poorly informed and you should do your own research not just listen to mates",
      "Finally an i5 and not a 7 or 9",
      "11th gen may be maligned, but the i5 variants were pretty good relative to 10th. it was only the i9 that was a waste of sand/regression vs its previous i9.\n\nCould you have done better? probably.\n\nBut you could have done far worse. It's not a *bad* cpu.",
      "Don't wanna ruin your day but a 12400F or a 5600 shits all over the 11600.",
      "He could have a 400 series chipset on his motherboard, those support 11th gen intel with a bios update.",
      "Enjoy the build 🙌",
      "Mostly pricing xD",
      "He bought a new motherboard, a B560, and paid €170 for it.",
      "thats when you buy a 5 pound gpu",
      "Which motherboard did you get and how much was it? Personally I would've bought a 13400F and the cheapest motherboard I could find since I care only about performance/price, but if you really really need to have the integrated graphics and you want a \"good\" motherboard then the 11600K is not a bad choice.\n\nI don't really get that integrated graphics argument though, you can buy a very old used GPU for about €10 to €20  if you ever need a backup gpu for any reason.",
      "The 5600 is significantly less expensive. I’m pretty sure the 5600X is also less expensive now after its price has dropped, but I wasn’t even talking about that cpu.",
      "In case you didn't realize, 12400 and 13400 non-F SKUs exist just as the 11400 does. So if you wanted an iGPU, you can still have it with a budget Alder Lake/Raptor Lake part.",
      "Your friends gave you poor advices here. The benefits of having the gpu in the CPU is absolutely negated and destroyed by the performance difference you gave up. Besides you can get a 20$ gpu if and when you need. Which in my experience is extremely rare for 99% of people.",
      "Ryzen 5 5600x is worse than 11600k, not better. And it's more expensive.",
      "5600g is slower than the 5600x in gaming."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Intel Arc A770/A750 mark two-year launch anniversary",
    "selftext": "",
    "comments": [
      "I hope they will come up with the next gen GPUs soon",
      "You don't have an Arc GPU if you say this, the improvement has been amazing, the stability and compatibility has been improved so much since launch.",
      ">And they did nothing about the software, Linus in one of his videos told that Intel promised a big revamp, nothing! Liars!\n\nBack to basics. Drivers are software.",
      "Yea they totally care about those 3 people who use linux when fixing drivers of a gaming product (majority of gamers are on windows and the linux statistics are inflated by steam decks)",
      "Can somebody explain the possible reasons they cant launch Battlemage yet?\n\nhardware was ready months ago. And the software is already kinda running on Lunar Lake?",
      "\"hardware was ready months ago. \"\n\nIs there any evidence that this is true?",
      "welp....Ive seen LOTS of people talk nice about the improvements of the drivers.",
      "If Arc could one day meet the performance of the 70 or 80 series Nvidia GPUs, I'd consider going a full Intel build on my next gaming PC.\n\nThey've got a long way to go.",
      "The architecture is, but that's not the same thing as the whole BMG themselves.",
      "And A became B",
      "It will be out soon. We are using it internally",
      "well thats a fair Point.\n\nbut isnt it already running in Lunar Lake?",
      "Why? Statistically at least 15x more people use windows. They'll obviously put all their resources into making functional drivers on windows before making functional drivers on linux. That's like saying app developers should still consider blackberry or windows phones.",
      "My only gripe about my a750 is the lack of VR support.",
      "Arrow Lake using that sweet 2 year old alchemist based igpu.",
      "Update your knowledge",
      "Soon(tm) is somewhat imprecise, but ok...",
      "It will be end of this year",
      "Like you just use Arc control everytime like it is a game, yeah, sure it is annoying that the driver updater not works. But that's pretty much all. I just install the drivers, open Arc Control, change one setting and leave the rest alone.",
      "I do hope Intel delivers Battlemage. Even as Nvidia fans i admit Nvidia have been selling such an overpriced GPU. I don't even doubt if RTX 5000 series will be even more overpriced."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "a770"
    ],
    "title": "Intel Arc 2024 Revisit & Benchmarks (A750, A770, A580, A380 Updated GPU Tests)",
    "selftext": "",
    "comments": [
      "Arc is starting to look seriously impressive. If the A750 beats the 6600 XT in 90% of games and the latter is still $240, the former is starting to look like the best value card for *everyone* in this price range, not just enthusiasts.",
      "This is really impressive, I thought the a770 was closer to the 3060/6600. Gj intel",
      "you are not missing anything with starfield. To say that this game is mediocre will be praising it. Focus on good games not 5/10 games.",
      "Why do you have such allegiance to a company that doesn't care about you? \n\nAMD, Intel, Nvidia just buy whatever seems better for you",
      "Based on AMDs slow progress to increase performance over the years.",
      "A good video, and a fair comparison.  \n\nIt’s a bummer Stairfield still appears to be having issues on ARC.  \n\nHonestly probably better if Battlemage isn’t launched until another 3-6 months of driver development time has passed overall.",
      "Can we not with the fanyboyism? More competition is good for everyone.",
      "Really pulling for Battlemage. We need a third party option.",
      "Looks like ARC is racing to the top fast. BattleImage will leapfrog AMD and get very close to Nvidia.",
      "But but but, BIG NAVI",
      "What do promises have to do with anything? You can see current performance then realize they are making faster chips that will have more cores as well.",
      "The problem with Amd they also still have massive driver issue after years. If Intel keep progressing their driver, it's very possible for them beat Amd in gpu market especially with Battlemage.",
      "Amd drivers are pretty mature and their driver issues (Intel's too) are very overblown. Their drivers today are about as stable as Nvidia's and are still more reliable than Intel's. I really hope that battlemage will moreso give Nvidia a reality check at the high-end and upper mid-range.",
      "Remember \"RIP Volta\"? What happened after that? AMD shooting themself in the foot with their meme marketing LOL",
      "I will happily if you can point to anywhere I did that.",
      "I agree - but it is one game that a lot of well known sites like to benchmark, and it certainly left an impression on many that Intel wasn't ready at launch.   Games hyped that much should be given prioritization for drivers.. \n\nI think we need to see Intel stay ahead on more AAA launches in the future. \n\nI'm optimistic..",
      "I think the driver issues are still common enough that the rx 6600 would be the go-to entry level GPU.",
      "Nvidia getting out of the GPU space would be more likely than that",
      "Lunar lake will have integrated graphics based on Battlemage",
      "I don't understand why Starfield is even used as a benchmark though. It's not even in the top 100 most played games on Steam right now. If they're  using that game to benchmark GPUs, they should use Cities Skylines 2 to benchmark CPUs."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Flagship Arc Battlemage specifications leak with reduced clock speed, 75% more shaders vs Arc A770 and increased die size than previously rumored",
    "selftext": "",
    "comments": [
      "Old article, but I didn’t see it here.  \n\nFlagship Battlemage will retail for around $449 and will give you roughly 4070ti performance.  If intel can do this, I’m ready to dump Nvidia.  The market needs a new competitor.  \n\nAnyone else looking to get one of these?",
      "> Anyone else looking to get one of these?\n\nIf performance is close to what these rumours say and drivers are stable, then yes I'll think about getting one. But if they can't reach clock speed targets then it may not get there",
      "Yes, but the release date matters a lot. If these won't appear until a quarter or so from the 50 series, it would be unpragmatic to not wait for a 5060/70. I can't wait for a Celestial -80 challenger, and maybe a Druid -90 challenger.",
      "My concern is power consumption, if it's doing 4070ti performance but at 300+ watts 2 years later that's a bit of an oof. And I would like XeSS to start getting adoption rates the same as DLSS. But I'm running an A770 for now so I'll see how my experience goes over the next year or so.",
      "Midrange 50 series is still well over a year out. Releasing 6 months before high-end is 9+ months ahead of midrange.",
      "Intel doesn't need a 4080 or 4090 killer. It just needs to be good enough to play most games decently at a decent price.\n\nIf it can meet the 4070 and the AMD equivalent performance for under $500, it's going to be a winner. I think everyone's tired of the ridiculous prices for GPUs and it's going to take something like this to shake the market up.\n\nI'd like to get back into desktop PC gaming, but I just can't afford it now at the current GPU prices, where a decent on costs as much as my CPU, motherboard and RAM combined.",
      "?? There’s plenty of games where even the 4080 can struggle to hit 4K 120 FPS at maxed out settings. And that should be the target if you’re buying a card that’s over $1000.",
      "my a770 has thrown me for a loop incredible for the price and honestly feels faster than a lot of cards i’ve used. 3060, 3050ti(m), 3070(m), 3080 ti, 4060 is it better no at some thing i felt that it did better job but 300.00 it is amazing can’t wait for battlemage!",
      "> and by all accounts AMD has been more price competitive than intel with RDNA2 GPUs\n\nHave we been watching the same market? Don't get me wrong, amd has much higher top end products, but the 770/750 are well priced against the 6750/6700/6600. Intel is just still held back on drivers which is super expected. This entire exercise hinges on their drivers evolving faster than their hardware, but it's only possible if they get more and more arc field data to get this exercise going.",
      "If the flagship Battlemage isn't on par with the RTX 4080/SUPER in games that will be disappointing in my opinion.",
      "Bruh if they build a 4070 ti at the same cost or less than Nvidia, IM NEVER GOING BACK.. Now that I know you guys are \"cool\" we need intel to start talking about frame generation or they are going to get left behind.",
      "Arc in laptops should see to the adoption of XeSS. Arc will eventually become the most used graphics solution overall since it'll be in like 90%+ of all laptops and office PCs going forward. Should make adoption of software technology basically guaranteed. \n\nThe actual hardware itself is the harder part to get right, but if it's priced right people can overlook an inefficient architecture.",
      "What? there are plenty of games which even max out a 4090, at least at 4k. Of course, you do need a fast CPU and rest of the system for that.",
      "I have a 3080, so this wouldn’t be it for me. If they could compete with a 4080 in gaming I’d buy one.",
      "I do not see 4070ti performance for $449 happening\n\nFirst gen ARC GPUs haven't undercut Nvidia by that much, and aren't especially competitive with AMD. I don't see them undercutting more with their second gen\n\nAMD hasn't undercut Nvidia by that much this gen, and by all accounts AMD has been more price competitive than intel with RDNA2 GPUs",
      "They need to push BM out before H2 2024 however I see them have to wait till the end of 2024 and at that point it will be another Alchemist \"late to the party\" scenario...",
      "lol wtf?  There’s plenty of VR stuff I play that would gladly use 2x or 3x what my 4090 can do",
      "\"30-40 million people use VR daily but because I don't own one it's not mainstream\" got it",
      "Well if I didn’t have a 4090 I’d consider it, depending on release date. If it’s 3 years late to the party like arc was… well nvidia and amd will have some better cards out",
      "As long as Intel is keeps the VRAM-frequency locked down overclocking isn‘t worth it as I oced my A750 to 2512mhz Core clock and the VRAM is currently the bottleneck."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Intel's Taped & Glued Arc A770 GPU: Tear-Down & Disassembly of Limited Edition Card",
    "selftext": "",
    "comments": [
      "Thanks Intel.\n\nTwo big takeaways:\n\nThis thing was really expensive to design and assemble, but with no good reason or benefit. It's just inefficient and dumb. Where did Intel get the people who designed the assembly for this card?\n\nThe February manufacturing date on that plate. There were lots of reports that Intel had tons of cards manufactured and sitting in boxes doing nothing for months over the summer. I guess those were true. That card (or at least most of it), was manufactured back in March/April/May and has been sitting in a warehouse all this time.",
      "The sitting on a warehouse thing might be explainable if they didn't have the software side ready. As even now drivers seem hit or miss.",
      "This is probably the most nitpicky video I've seen from steve\n\nThen again, his nitpickiness is based in much more knowledge and years of experience than I do so his opinions are probably valid\n\nIt's probably rooted in the fact that he knows the potential that intel has to make a great product, and he wants them to do better because they can be a serious competitor in the space if they iron out some big kinks",
      "They tried. They decided to delay until they had something decent. I respect that more than shipping it anyway.",
      "I agree that this is very nitpicky. This isn't a product people are going to tear down because it's a mid range card.\n\nFor example the fact that the backplace is taped on isn't a big deal because it looks like it's still sticky even after being removed. Even if it wasn't sticky you don't actually need to tape it back on because the cards works and looks fine with it removed.\n\nAlso most of the pain in the ass when disassembling and the \"complicated\" design is because they added that extra board with the RGB controller. This means the ARC cards that don't have the RGB stuff are likely going to be easier to open which is something that he doesn't mention.",
      "I like Steve, but sometimes he’s truly obnoxious lol",
      "Oh no Steve is so biased against Intel there's no way he'd make content like this about, say, the RTX 2060 FE.",
      "The regular user should at least be able to replace the fans. Fan failure is the most common RMA. \n\nA sapphire fan replacement takes 2 minutes with a couple screws. This one it seems like you have to go through the whole disassembly process to get to the fans. It’s an anti-consumer design.",
      "Because the 4000 series utterly trounces it, at a wildly incomparable price point, and the 7000 series hasn't been fully revealed yet. The 6600 and XT are on the market, right now, and are Intel's direct competition. What is your point?",
      "The problem is rn it's kinda a 6600/6600xt with less features and worse drivers (for now) which is why people aren't recommending it.",
      "So was amd when they released their first card and it was ass\n\nNow look at them\n\nThis stuff takes time",
      "Its so over engineered to the point I want one.",
      "I feel like this review came from a guy who made up his mind before he reviewed it.  He must have said \"screwless design\" like 5 times, whereas the intel slide he showed clearly said \"screwless shroud\" -- an aesthetic choice which is obviously going to have some ease of disassembly trade offs. \n\nNow, I think his right to repair/ease of service arguments are valid, but they're more valid coming from someone who at least can appear to be giving something an unbiased look.  He was whining when the interior contained screws... which *obviously* it is going to, he would have bitched if it hadn't contained screws, because that would have basically meant zero serviceability and would have been a terrible decision.  Damned no matter what.  Barely acknowledged at all in the whole video that the shroud was an aesthetic choice and was going to have some trade offs for function.\n\nPersonally I'd prefer serviceability just like he apparently does, I would have loved to see magnets instead of tape to hide screws.  However, it's hard to watch a video like this and assume that you're getting a fair unbiased take of any positives or trade offs that might exist, the whole thing just reeks of confirmation bias made manifest, not journalism.",
      "Decent dx9 support",
      "Could be by the guys who design their NUCs, which is why its so overengineered. Probably will be a lesson learned for their next interation.",
      "Toyota is one of the biggest auto manufacturers in the world, but I bet if you asked them to make a tractor the first one would be so-so.\n\nLoads of experience in a closely related field doesn’t translate to a perfect first-gen attempt.",
      "I very sincerely doubt that he is being paid for anyone, he is a highly credible voice in the tech space\n\nThat being said, I have noticed LTT (with the exception of his latest video being the review of the A770) and GN being hyper hyper critical of intel lately",
      "Sometimes?",
      "FR, all I want to know is when I can drop an A770 or two in a Plex server and how many transcodes from 50Mbps to 10Mbps can each handle concurrently.\n\nSome see Steve as being nitpicky but the number of *different* fasteners alone is worthy of his ire.  Personally, I think of the design, fit, and finish as truly deluxe but the PITA involved with a disassembly and reassembly is enough to probably enough to open a gyro shop.",
      "Issues would be when you need to clean or repaste in a year or so. \n\nYou kinda want it to take apart easily and come back without needing any adhesives.\n\n[EDIT] I cant believe having ease of assembly/disassembly would be a controversial or even bad idea for some people...."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "My ARC A770 cooling :)",
    "selftext": "",
    "comments": [
      "How did it get that hot. That cooler is overbuilt for the workload. Something might not be right with the card itself.",
      "Lol bro cooled his intel card with amd cooler",
      "because the card is 90 degrees",
      "Why?",
      "Or his case airflow is garbage. One of those fashion over function designs. It's possible.",
      "I would RMA it then, my overclocked card has hit a high of 81c. Although, my case is fully meshed.",
      "need to use intel heatsink for extra -5c",
      "I like that along the front part of the GPU the light is still blue (Intel) while it suddenly turned red (AMD) on the rear where the Ryzen stock cooler was placed",
      "Hell no. The back plate is plastic",
      "But, why on the cooler?  Are you thinking conduction on the cover plate?  Is it even Aluminum?",
      "This is a joke…right?",
      "Look at the orientation of the cooler pins/screws... it's upside down ontop of the GPU backplate.",
      "Disgraceful. You used the wrong stock cooler.",
      "They would never know, that's like thinking Intel or AMD have never given warranty replacements for gamer's overclocked processors, unless they are psychic and know what people do with them.",
      "That does nothing",
      "What settings are you running that your card is that hot? I’m pushing mine hard and barely hits 70*C",
      "My question is...does this work?",
      "Upside down?",
      "This seems a little unnecessary. The backplate coolers were primarily for GDDR6X cards, specifically the 3090, since it had memory modules on the back of the PCB.",
      "Pat Gelsinger is very disappointed in you"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "a770"
    ],
    "title": "Intel Arc B580 Meta Review",
    "selftext": "- compilation of 12 launch reviews with ~3660 gaming benchmarks at 1080p, 1440p, 2160p\n- only benchmarks at real games compiled, not included any 3DMark & Unigine benchmarks\n- geometric mean in all cases\n- standard raster performance without ray-tracing and/or DLSS/FSR/XeSS\n- extra ray-tracing benchmarks (without DLSS/FSR/XeSS) after the standard raster benchmarks\n- stock performance on (usually) reference/FE/LE boards, no overclocking\n- factory overclocked cards were normalized to reference clocks/performance, but just for the overall performance average (so the listings show the original result, just the performance index has been normalized)\n- missing results were interpolated (for a more accurate average) based on the available & former results\n- performance average is weighted in favor of reviews with more benchmarks\n- power draw based on numbers from 13 sources, always for the graphics card only\n- current retailer prices according to Geizhals (GER/Germany, on Dec 15) and Newegg (USA, on Dec 16)\n- performance/price ratio for 1080p raster performance and 1080p ray-tracing performance (higher is better)\n- for the full results and some more explanations check [3DCenter's launch analysis](https://www.3dcenter.org/artikel/launch-analyse-intel-arc-b580)\n\n&nbsp;\n\nRaster 1080p|7600|7600XT|6700XT|3060-12G|4060|4060Ti-8G|4060Ti-16G|A770-16G|B580\n|:--|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n&nbsp;|RDNA3 8GB|RDNA3 16GB|RDNA2 12GB|Ampere 12GB|Ada 8GB|Ada 8GB|Ada 16GB|Alchemist 16GB|Battlemage 12GB\nComputerBase|99.3%|-|113.0%|-|106.2%|126.2%|-|84.5%|_100%_\nGamersNexus|93.3%|-|108.9%|80.0%|98.5%|120.4%|-|88.7%|_100%_\nHWCanucks|90.0%|96.9%|-|90.1%|105.4%|-|-|84.5%|_100%_\nHardware&Co|87.7%|100.8%|-|80.4%|95.7%|120.0%|119.8%|81.2%|_100%_\nKitGuru|87.1%|96.4%|-|77.7%|91.9%|114.7%|-|85.4%|_100%_\nLinusTT|85.3%|-|105.9%|78.4%|87.3%|-|109.8%|79.4%|_100%_\nPCGH|87.2%|100.3%|-|78.3%|92.9%|-|-|90.2%|_100%_\nQuasarzone|96.2%|100.4%|-|84.6%|101.1%|121.8%|-|85.5%|_100%_\nTechPowerUp|87%|96%|106%|83%|95%|121%|122%|86%|_100%_\nTechSpot/HUB|92.2%|94.8%|97.4%|81.8%|93.5%|119.5%|120.8%|80.5%|_100%_\nTom's HW|79.3%|100.9%|-|-|99.7%|-|-|83.1%|_100%_\nTweakers|-|99.4%|-|81.2%|96.6%|119.4%|-|83.7%|_100%_\n**avg 1080p Raster Perf.**|**89.2%**|**97.7%**|**105.3%**|**81.9%**|**95.8%**|**119.2%**|**-**|**84.6%**|**_100%_**\n\n&nbsp;\n\nRaster 1440p|7600|7600XT|6700XT|3060-12G|4060|4060Ti-8G|4060Ti-16G|A770-16G|B580\n|:--|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n&nbsp;|RDNA3 8GB|RDNA3 16GB|RDNA2 12GB|Ampere 12GB|Ada 8GB|Ada 8GB|Ada 16GB|Alchemist 16GB|Battlemage 12GB\nComputerBase|83.9%|-|108.4%|-|95.8%|118.4%|-|88.7%|_100%_\nGamersNexus|94.0%|-|114.5%|82.2%|96.5%|118.7%|-|97.2%|_100%_\nHWCanucks|83.2%|89.6%|-|86.0%|95.7%|-|-|87.7%|_100%_\nHardware&Co|77.6%|95.4%|-|76.8%|91.0%|115.0%|113.6%|82.9%|_100%_\nKitGuru|81.4%|91.8%|-|75.2%|86.4%|108.6%|-|88.0%|_100%_\nLinusTT|76.3%|-|98.7%|75.0%|81.6%|-|102.6%|80.3%|_100%_\nPCGH|79.7%|94.5%|-|75.1%|81.5%|-|-|89.8%|_100%_\nQuasarzone|88.9%|93.4%|-|80.2%|92.9%|114.5%|-|87.4%|_100%_\nTechPowerUp|80%|91%|103%|81%|92%|116%|118%|88%|_100%_\nTechSpot/HUB|82.5%|89.5%|94.7%|77.2%|87.7%|110.5%|114.0%|82.5%|_100%_\nTom's HW|68.6%|96.4%|-|-|92.0%|-|-|84.7%|_100%_\nTweakers|-|94.2%|-|79.2%|91.3%|116.0%|-|86.0%|_100%_\n**avg 1440p Raster Perf.**|**81.2%**|**92.1%**|**102.7%**|**78.7%**|**89.1%**|**111.3%**|**-**|**86.7%**|**_100%_**\n\n&nbsp;\n\nRaster 2160p|7600|7600XT|6700XT|3060-12G|4060|4060Ti-8G|4060Ti-16G|A770-16G|B580\n|:--|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n&nbsp;|RDNA3 8GB|RDNA3 16GB|RDNA2 12GB|Ampere 12GB|Ada 8GB|Ada 8GB|Ada 16GB|Alchemist 16GB|Battlemage 12GB\nPCGH|70.6%|88.9%|-|73.5%|71.2%|-|-|91.5%|_100%_\nTechPowerUp|62%|84%|97%|78%|84%|98%|106%|87%|_100%_\n\n&nbsp;\n\nRayTracing 1080p|7600|7600XT|6700XT|3060-12G|4060|4060Ti-8G|4060Ti-16G|A770-16G|B580\n|:--|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n&nbsp;|RDNA3 8GB|RDNA3 16GB|RDNA2 12GB|Ampere 12GB|Ada 8GB|Ada 8GB|Ada 16GB|Alchemist 16GB|Battlemage 12GB\nComputerBase|81.1%|-|111.5%|-|97.8%|114.4%|-|89.9%|_100%_\nGamersNexus|77.1%|-|89.8%|85.6%|108.0%|135.7%|-|91.5%|_100%_\nHardware&Co|32.8%|63.1%|-|80.2%|101.6%|129.4%|131.0%|81.5%|_100%_\nKitGuru|61.3%|76.4%|-|85.7%|103.1%|129.4%|-|85.9%|_100%_\nPCGH|58.0%|81.0%|-|84.4%|87.7%|-|-|86.5%|_100%_\nQuasarzone|44.9%|74.9%|-|91.1%|117.6%|138.1%|-|83.9%|_100%_\nTechPowerUp|45%|71%|79%|81%|97%|124%|128%|86%|_100%_\nTechSpot/HUB|64.4%|64.4%|57.8%|-|124.4%|166.7%|168.9%|95.6%|_100%_\nTom's HW|54.3%|68.8%|-|-|90.9%|-|-|79.5%|_100%_\n**avg 1080p RayTr Perf.**|**55.4%**|**73.0%**|**78.2%**|**86.7%**|**102.2%**|**128.9%**|**-**|**87.6%**|**_100%_**\n\n&nbsp;\n\nRayTracing 1440p|7600|7600XT|6700XT|3060-12G|4060|4060Ti-8G|4060Ti-16G|A770-16G|B580\n|:--|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n&nbsp;|RDNA3 8GB|RDNA3 16GB|RDNA2 12GB|Ampere 12GB|Ada 8GB|Ada 8GB|Ada 16GB|Alchemist 16GB|Battlemage 12GB\nComputerBase|58.3%|-|106.1%|-|87.7%|102.1%|-|87.4%|_100%_\nPCGH|52.8%|80.3%|-|84.8%|83.1%|-|-|91.0%|_100%_\nTechPowerUp|38%|65%|73%|78%|78%|93%|123%|86%|_100%_\nTechSpot/HUB|59.4%|59.4%|53.1%|-|115.6%|162.5%|165.6%|103.1%|_100%_\nTom's HW|44.4%|65.1%|-|-|84.4%|-|-|85.7%|_100%_\n**avg 1440p RayTr Perf.**|**~50%**|**~70%**|**~75%**|**-**|**~90%**|**~113%**|****|**~91%**|**_100%_**\n\nNote: Due to the small number of figures, the performance index was rounded to whole numbers in this case.\n\n&nbsp;\n\nRayTracing 2160p|7600|7600XT|6700XT|3060-12G|4060|4060Ti-8G|4060Ti-16G|A770-16G|B580\n|:--|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n&nbsp;|RDNA3 8GB|RDNA3 16GB|RDNA2 12GB|Ampere 12GB|Ada 8GB|Ada 8GB|Ada 16GB|Alchemist 16GB|Battlemage 12GB\nPCGH|45.5%|79.9%|-|88.9%|81.1%|-|-|99.6%|_100%_\nTechPowerUp|30.5%|64.0%|71.4%|74.8%|76.5%|92.0%|113.2%|87.6%|_100%_\n\nNote: I excluded one game from TechPowerUp's results (Alan Wake 2), because it destroys the index with it's super-weak performance of the Arc B580 there (0.3 fps).\n\n&nbsp;\n\nAt&nbsp;a&nbsp;glance|7600|7600XT|6700XT|6750XT*|3060-12G|4060|4060Ti-8G|A770-16G|B580\n|:--|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n&nbsp;|RDNA3 8GB|RDNA3 16GB|RDNA2 12GB|RDNA2 12GB|Ampere 12GB|Ada 8GB|Ada 8GB|Alchemist 16GB|Battlemage 12GB\navg 1080p Raster Perf.|89.2%|97.7%|105.3%|~111%|81.9%|95.8%|119.2%|84.6%|_100%_\navg 1440p Raster Perf|81.2%|92.1%|102.7%|~108%|78.7%|89.1%|111.3%|86.7%|_100%_\navg 1080p RayTr Perf.|55.4%|73.0%|78.2%|~82%|86.7%|102.2%|128.9%|87.6%|_100%_\navg 1440p RayTr Perf.|~50%|~70%|~75%|~79%|-|~90%|~113%|~91%|_100%_\nTDP|165W|190W|230W|250W|170W|115W|160W|225W|190W\nReal Power Draw|160W|190W|219W|221W|172W|124W|151W|223W|163W\nEnergy Eff. (1080p&nbsp;Rast.)|91%|84%|78%|82%|78%|126%|129%|62%|_100%_\nMSRP|$269|$329|$479|$549|$329|$299|$399|$349|$249\nRetail GER|266€|342€|_EOL_|329€|274€|294€|388€|313€|294€\nPerf/Price&nbsp;GER 1080p Raster|99%|84%|-|99%|88%|96%|90%|79%|_100%_\nPerf/Price&nbsp;GER 1080p RayTr|61%|63%|-|74%|93%|102%|98%|82%|_100%_\nRetail US|$250|$310|_EOL_|$369|$280|$300|$410|$230|$250\nPerf/Price&nbsp;US 1080p Raster|89%|79%|-|75%|73%|80%|73%|92%|_100%_\nPerf/Price&nbsp;US 1080p RayTr|55%|59%|-|56%|77%|85%|79%|95%|_100%_\n\n* performance just interpolated based an older benchmarks (6750XT is nearly constant +5.1-5.4% faster than 6700XT)\n\n**Interestingly, the Arc B580 achieves a significantly stronger performance/price ratio in the U.S. than in Germany.** Apparently, Intel prices in the USA are lower, but nVidia prices are higher.\n\n&nbsp;\n\nList of Arc B580 reviews evaluated for this performance analysis:\n\n- [ComputerBase](https://www.computerbase.de/artikel/grafikkarten/intel-arc-b580-limited-edition-test.90585/)\n- [Gamers Nexus](https://gamersnexus.net/gpus/intel-arc-b580-battlemage-gpu-review-benchmarks-vs-nvidia-rtx-4060-amd-rx-7600-more)\n- [Hardware Canucks](https://www.youtube.com/watch?v=kumOux-zD_A)\n- [Hardware & Co](https://hardwareand.co/dossiers/gpu/test-de-l-intel-arc-b580-le-renouveau-des-cartes-graphiques-sous-les-300-e)\n- [KitGuru](https://www.kitguru.net/components/graphic-cards/dominic-moass/intel-arc-b580-limited-edition-review/)\n- [Linus Tech Tips](https://www.youtube.com/watch?v=dboPZUcTAW4)\n- [PC Games Hardware](https://www.pcgameshardware.de/Arc-B580-Grafikkarte-280998/Tests/Benchmark-Review-Preis-vs-4070-A770-Limited-Edition-kaufen-1461374/)\n- [Quasarzone](https://quasarzone.com/bbs/qc_bench/views/92504)\n- [TechPowerUp](https://www.techpowerup.com/review/intel-arc-b580/)\n- [TechSpot](https://www.techspot.com/review/2935-intel-arc-b580/)\n- [Tom's Hardware](https://www.tomshardware.com/pc-components/gpus/intel-arc-b580-review-the-new-usd249-gpu-champion-has-arrived)\n- [Tweakers](https://tweakers.net/reviews/12768/intel-arc-b580-grote-stap-voor-intel-kleine-stap-voor-gamers.html)\n\n&nbsp;\n\nSource: [3DCenter.org](https://www.3dcenter.org/artikel/launch-analyse-intel-arc-b580)",
    "comments": [
      "I check the website now is sold out, no stock\nLooks like intel finally make it into gpu. It is cheaper as well more vram.\n\nThe performance it is still not final, usually intel will release the drivers we can get 10-20% perf gain from current.\n\nHope next years intel release the gpu to take 4090 or 5090 with cheaper as well more vram since nvidia 5090 rumors is 32gb vram",
      "A bit faster but also cheaper and has more VRAM. That's what people like about it.",
      "We found one of MLID ALT accounts.",
      ">you’re just overpaying for bad drivers and increased power\n\nBad drivers I agree somewhat, Intel Arc still performs in games than in benchmarks. Power efficiency is good though? Not as good as Nvidia, but Nvidia is the best in dGPU while Intel is a newcomer, you can't catch up in power efficiency that fast(2nd gen). Intel already beats AMD in power efficiency for Battlemage GPUs, which holds true for the iGPU version(Lunar Lake) too(Lunar Lake consumes less power than AMD RDNA 3.5 for same performance). This is already excellent, it shows Intel is catching up. Or at least, AMD is worse than Intel in power efficiency, which is disappointing for a long-time GPU maker",
      "Has anyone made reviews of B580 running on DX9 to DX10 games? So far I have only seen TES IV review and it seemed to run well but it is only 1 game so far.",
      "Do you expect more games to integrate Xess2 in the coming years? If so, then you can expect the B580 to increase its benefits over time. \n\nDo you like spending more money for the same performance? NVIDIA and AMD cards may have been on the market for a while, but they are still on the market. Consumers are choosing between them and it doesn’t matter to them who best who to the market. It’s about what is the best value for their use case.",
      "Sadly, this is not sustainable for Intel, chip is too big for that amount of performance with the node they are using, this ain’t making them money. I understand that they had to do this for their share holders and fulfill their agreements but I hope their next gen is more competitive, lord knows a third competitor would be great.",
      "Thanks a ton for this review aggregation!",
      "Thanks, I'm very curious to see what the results would look like for DLSS, FSR, or XeSS, especially in terms of super resolution and frame generation at 4K",
      "How do these new intel GPU's do with things like Autocad and Revit?",
      "Vex was seeing 10-15% higher performance with an oc so at 1440p you're legit getting a 12gb 4060ti/2080ti. It's also way more comparable to Nvidia cards than if it was an AND one seeing the feature set is much closer unlike amd. Hardware Xess is much closer to dlss than fsr, xe low latency is already in more games than anti lag 2 so a better competitor to reflex (and their driver level variant of XELL for games where it's not implemented seem to be the best out of the 3 actually) , ray tracing is way closer as well and video encoding/decoding is actually slightly better than even Nvidia on Intel unlike amd which is far behind them.",
      "Hopefully, HUB makes their 45-game test as well for B580.",
      ">Sadly, this is not sustainable for Intel\n\n\nTransistor count is about the same between the RTX 4060 and B580.\n\n\nThe main reason the B580 utilizes such a large die is because AD107 (RTX 4060 die) is about 42% denser than BMG-G21 (B580).\n\n\nWhich is kind of crazy when you think about it. Since they're using a similar node.",
      "Doesn’t that make it worse though? Using a lot of expensive silicon wafer and not achieving the same level of transistor density - whilst also using a more expensive process node?",
      ">Doesn’t that make it worse though?\n\n\nOn an architectural level? Absolutely.\n\n\nBut you don't buy products because of their architectures, you buy them because of their price to performance ratio.\n\n\nPeople don't buy a Toyota Tacoma and wonder why it can't hit the same speeds as a Bugatti Veyron.",
      "Average consumption is one part of the picture. Power consumption while idle,vsync, video playback, etc. are another part of the picture.",
      "4060’s are in stock and ready to ship. Can occasionally find one for $279. B580 out of stock and Xess 2 nowhere to be found other than F1 2024. DLSS 3 is mature and has widespread game support. The B580 was hyped to the moon.",
      "They're in the room with Digital Foundry, Daniel Owens, Techpowerup, and basically everywhere I looked. Myself, I already have a problem with Intel's integrated solutions. Can't use portrait mode correctly for my side monitor. A big pain in the ass. So the last gpu vendor I would ever trust is Intel. Dropping settings, buying used, and paying a few extra dollars is worth it.",
      "Keyword: would. Being less than 5% faster than a card that launched almost 2 years ago is not enough for the vast amounts of glazing going on. $250 cards will be few in the wild. It’s just a title grabber. They won’t even be in stock until next year.\n\nSure, you’re just overpaying for bad drivers and increased power for the same performance 😝",
      "> 4% faster than 4060, 22% over 3060 \n\nWith the way it’s being treated, you would think that the B580 is literally twice as fast. Luckily for everyone, the $250 models are nonexistent, and will basically start at $270. I can probably count on one hand the number of reviewers are actually fair towards the cards instead of trying to craft a story. The mods here are a joke pinning some techtuber on the front page."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Intel Arc A770/A750 Graphics Card Review Roundup | VideoCardz.com",
    "selftext": "",
    "comments": [
      "I want to buy an a770 not because I want an a770, but because I want a more reasonably priced 50 series and 8 series.\n\n\nCheering for you arc",
      "i'm pretty impressed with the card given its their first attempt, in a few games it was only ~5% behind the 3070, which says that the hardware is at least good. amd and nvidia have had years to optimize drivers for their hardware, so i think intel is almost definitely leaving a lot of performance on the table here due to how behind they with optimization.",
      "Just a PSA, according to Linus the 16gb A770 LE is going to be available in very limited numbers (probably due to low margins). So if that's the GPU you want you need to be ready to buy on launch day.\n\ntimestamp: https://youtu.be/6T9d9LM1TwY?t=701",
      "770 destroy the 3060 in RT at 1440p, and better when xess, go intel go ! \\*in digital foundry review",
      "It’s not about the card, it’s about sending a message",
      "The drivers stack though is brand new from the begining of the year. They tried to port from their mobile drivers and that did not work. Had to start from scratch. This will get better.",
      "Why hasn't anyone tested these cards with dxvk for the older titles?",
      "the performance is there, it can compete with its targeted NVidia, Amd counterparts. but without a serious discount why would customers buy intel instead of more tried , proved Nvidia,AMD?",
      "They'll probably be able to fix it - or at least make it better - with driver updates sooner rather than later.",
      "We need heroes like you",
      "Voting with my wallet / to support the brand mainly.  \n\nBut actually usable ray trace performance is a bonus and I think these cards will get faster over time.  \n\nHer card is also the vanilla 6600 so A770 is faster all around.",
      "The fast sum up is that the hardware is mostly ready but the software is not. My guess is if you get an A770 you’ll get additional 10% performance on average when drivers are more mature. But Intel needs to fix stability issues for the top 100 games played on steam.",
      "Buy cheap now, wait for driver upgrades while hitting some bumps in the road for a few months, have a card with superior price performance in the near future.\n\nPeople with more money than time can spend two or three hundred dollars more for some guaranteed gaming hours a week. Not everyone has extra dollars and for these people who are looking for maximum performance per dollar and advanced tech like RT and AI optimization, this could be a good buy if they are willing to endure some bumps in the road.",
      "Uhhh, why?",
      "But dxvk translates dx calls to vulkan, and intel gpus are supposed to be alright with vulkan",
      "As someone on the market for a new GPU, the thing that scares the shit out of me is the rumors that Intel is considering pulling the cord out of its dGPU, Drivers improve with time if effort is put on it, but that is the catch.",
      "Is there a link yet to where it will be sold so I can bookmark it?",
      "I am buying an ARC A770 Limited to replace my wife’s Radeon 6600.   She’ll have an all-Intel build (i3-12100, Intel 660p) :). \n\nThe only thing I (really) don’t like is the high idle power consumption..",
      "The i740 was not a GPU, it was a graphics accelerator. The term GPU didn’t apply until hardware transform and lighting was added to the 3D chip, starting with the GeForce SDR. Larabee was not a GPU and lived on his Xeon Phi. The Arc A7 is the first gaming GPU from Intel.",
      "Intel A770 is only 20-30 use cheaper than Nvidia 3060. At this price its not worth the troubles."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Core i5 13600k + Arc A770 go big blue 🤌🏼🦾",
    "selftext": "",
    "comments": [
      "Since the new driver i cant say anything bad. Works like a dream for the price! 🦾",
      "i really hope they manage to bring great stuff  to the market, gpus cost to mutch these days maybe the competition will help...",
      "How's the Arc treating you so far?",
      "Awesome! I think Intel will see great future success in the GPU market.",
      "whoa... is this an AIB Arc GPU? I thought there was like only one style of Intel GPUs?",
      "It is from ASrock, i did not mange to find a referenc card...",
      "Probably for the better, since your AIB looks to have much better cooling vs the reference card.",
      "I don’t really know how big is the difference between them. You can go now whit the 12600k and in a few years upgrade to a 13700k when you save some money 🤷🏻‍♂️",
      "ASRock, Gunnir, Acer, and Gigabyte are all Intel AIB (Add-In Board) partners.",
      "Skip the 13600k unless you're chasing frames\n\nThe 12600k will be around 10-15% slower in maximum theoretical gaming performance, but it also uses less power.",
      "Is that the good old NH-D14?",
      "12600K is gonna be enough for your needs and it has igpu which doesn't hurt to have",
      "But for 80 canadian dollars maybe you are better of whit the 13600kf or k so yo have the integrated gpu 🤷🏻‍♂️",
      "Might I ask why this could be an issue? I have mine on the wooden vinyl floor.",
      "Same. I have the A770 LE and it’s like 👌",
      "Is it worth getting a 12600k for $299(canadian) or 13600kf for $379",
      "sorry i have a small desck it will live down there for now...",
      "13600K is $420 😬….i think a 12600k should be good for 1440p 144hz for a good while. Ill overclock too",
      "Yea for the price i really recommend it 🤘🏼",
      "I just got a new pc w/12600k + rtx3070 and can certify that the 12th gen part works like a charm - no complaints so far. I play at 1440p and am looking for anything between 60-144fps (depending on the game obviously) and this fits right in with all games I've tried to so far from APT: Requiem to RDR2 almost maxed out and new titles like High on Life. Very happy."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Acer Arc A770 Predator custom GPU with 16GB VRAM is now available for $349 - VideoCardz.com",
    "selftext": "",
    "comments": [
      "16gb of memory and a decently good GPU as well for $350 is a steal.",
      "GREAT card for Hogswart’s Legacy.",
      "I have so little faith in Radeon that I think Intel ARC, even with the drivers the way they are now, are more likely to eventually catch Nvidia than Radeon is.\n\nOnly time will tell though. If Intel sticks with it they’ll definitely beat Radeon eventually. Don’t know if they’ll stick with it, or just put it on a skeleton crew though.",
      "If the 6700xt didn't exist and this was facing off against the Rtx 3060ti only it would be such a win.\n\nExcept... the 6700xt does exist unfortunately for Intel.",
      "Offering less features and lower quality features was fine, when they were selling RX 480 8GB for $229 vs Nvidia’s GTX 1060 6GB for $299. That was ~30% price difference.\n\nNow that they’re basically trying to price match Nvidia, that crap doesn’t fly anymore. At least not with me.",
      "Damn. If only it had linux support...",
      "AMD have had their GPU division for the last decade yet the drivers are still awful, Intel has had less than 2 years in the dedicated gaming market yet are improving massively, give it time and they'll surpass the Radeon team.",
      "...glances at my Fedora 37 box with A770...\n\nAlthough support will be better OOTB with March/April distro releases.",
      "Watched them get their asses kicked for over a decade, more or less. They used to at least be significantly cheaper, but for three generations now they’ve just slotted into Nvidia’s lineup.",
      "Yeah rdna 3 is a big disappointment. The 7900 xtx isn't even that much better than the 4080. And for ray tracing the 4080 is better. Price wise they're way overpriced.",
      "AMD's reluctance to fully embrace RT/AI at the hardware level is going to see them lose #2 position if intel continues to improve.",
      "> I have so little faith in Radeon that I think Intel ARC, even with the drivers the way they are now, are more likely to eventually catch Nvidia than Radeon is.\n\nWhy is that?",
      "gaining ground where? even 3080s sold more than the cheaper 6000 series cards",
      "It's supported well on newer kernels but yeah you need a newer kernel for that and can be terrible for something like Debian unless they've backported it to an older version",
      "> The 6700xt is definitely better than the 3070 as of today, the opposite was true in 2020. The 3080 10gb is starting to look really crippled, so the 6800xt is making ground on that too.\n\nPlease tell me that you are not drawing these conclusions based on how that broken mess of Hogwarts Legacy is performing right now.",
      "Yep, can’t wait until it has food debian server support, because I will pull the trigger on it just for AV1. Lucky man with a rolling release distro though…",
      "That's not what I saw in benchmarks. Can you post a link?",
      "Lol except when you turn on ray tracing then that 6700xt is no where to be found",
      ">AMD GPUs still have no hardware accelerated ray tracing\n\nWhy tell such an obvious lie?\n\nYou are really gonna tell everyone with a straight face 7900XT achieved [over 90% RT performance ](https://freeimage.host/i/HEUZmwN) of 4070 Ti with zero hardware acceleration.\n\nSo by your logic Nvidia's 3rd generation \"accelerated\" RT on 4070 Ti is so bad it's only less than 24% (normalised for non-RT performance) better than 7900 XT with no hw acceleration. Nice.",
      "I don’t care about the underlying architectures. I compared R9 300 to GTX 700, not Kepler to GCN. If you release a product, even a rebranded product, I expect a minimum amount of support for it to make it worth the money.\n\nIf AMD named it “R9 291” instead of “R9 390” I’d cut them some slack, but they didn’t so 🤷.\n\nI personally got burned by this because I recommended the R9 390 to someone back in the day."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Intel Arc B580 Limited Edition tested in 3DMark, outperforms RTX 4060 and Arc A770/A750",
    "selftext": "",
    "comments": [
      "This is hardly surprising since Intel Arc A770 already outperforms GeForce RTX 4060 in 3DMark Time Spy",
      "i care about game fps comparison more",
      "Now let's see gaming performance.",
      "Huge if true, could be a new budget build king.",
      "Wow 3dmark.\n\nWorthless test thru",
      "Is the embargo not over until launch day?",
      "Since it might be around 250 bucks. Like there are not many good budget cards eight now. The 4060 is not that bad, just too expensive for its performance (should have been aroumd 250 bucks max)",
      "Since when we feel excited for a new card outperforming a terrible card such as 4060? Even NV's ancient 3060ti already done so.",
      "For the price 249$, Intel is king of budget now",
      "https://redd.it/1hbrxdg",
      "Hey SilasDG, your comment has been removed because we dont want to give *that site* any additional SEO. If you must refer to it, please refer to it as *LoserBenchmark*\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/intel) if you have any questions or concerns.*"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "[HUB] ARC A770 & A550 Review and Benchmarks",
    "selftext": "",
    "comments": [
      "A lot of what I'm seeing here is immature drivers. AMD and Nvidia have a super refined driver that has squeezed plenty of of the gains out. Intel will likely see much larger gains overtime vs the competitors as they refine the driver.",
      "How are they can potentially destroy AMD since Intel's 3070 competitor is losing to low end AMD from 2020? Intel right now is 2 gens behind. Unless you count AMD only targeting gaming in which case by this logic to multithreaded apps AMD has killed Intel already.\n\nAMD provides much better value and is destined to take on series 40.",
      "Seriously some of what intel has shown the hardware seems to punch well above a 3060. I’m curious if it won’t be getting compared to the 3060ti-3070 within 12 months. Looks like specific driver issues causing the relative perf issues in both modern and old titles alike depending on specific instructions used. Imagine if they fix all that and suddenly games can use the full ability of the ARC architecture. I’m just wondering how GTA 6 will run. Oh and starfield.",
      "A reviewer shouldn't give a fuck how much it costs to develop the product.",
      "Wauw your pretty clueless what AMD as a company has done for GPU's....\n\nFirst to HBM  \nFirst to Compute GPU's  \nFirst to Mantle(DX12)  \nFirst to AUDIO acceleraterd GPU\n\nFIRST chiplet GPU incoming",
      "Also:\n\n- AMD (or ATI back then) was also first to GDDR\n\n- First to Tessalation,\n\n- First Terraflop GPU\n\n- First Eyefinity (or scaling rendering across monitors).\n\n- First Re-bar support.",
      "\\+1 for effort.\n\nBut for gaming only it's a not a good card vs competition. 6nm, larger die and higher memory bandwidth plus higher power consumption it only matches a 3060 that was also a bad card at launch. Can't beat the better price per perf 6600xt. Only faster in select games where it is \"optimized\". Yeah it has better RT performance than AMD but at this price range, raster performance is a better indicator than RT.",
      "On average, this is *okay* value, and would’ve been good six months ago. (Hell, these cards have a better launch price than most cards this gen.) The thing is, the 6600, 6600 XT and 6650 XT are so cheap right now that there’s really no reason to go Intel.",
      "For what it offers it's pretty impressive honestly.",
      "If they had released a year ago, I would've given them the benefit of the doubt even with crappy drivers. But at this point, by the time they figure out their drivers, we'll be looking at GeForce 4050 and or Radeon 7500 cleaning the floor with these. \n\nTheir redeeming quality isn't gaming. AI dev? Go ahead. Media encoding? Go ahead. Linux? Go right ahead. Gaming? Only for 100 bucks less at the top end.  \n\nThe only card that entices me is the A310 for less than 100 usd. It's been a while since we had a usable and cheap discrete GPU.",
      "So the Finewine^(TM) argument. What happened to we shouldnt buy things based on the future?\n\nIve always been indifferent to the finewine argument personally and I do want Arc to be successful *enough* to allow Intel to continue with it as I believe, in a couple of gens, Intel GPUs can be truly competitive and more competition is great, however, I see great irony in the narratives being spun in this sub in particular. It feels like the logic flip flops depending on whether Intel is the underdog or the market leader. I thought Finewine^(TM) was a meme ie mocked here? I also thought GN was the most trusted reviewer and LTT is considered trash in this sub? \n\nCan we admit that this sub is no better than AMD and Nvidia subs? Although I do like how the rules are actually enforced here.",
      "I mean I doubt it, but here I am trying to get an A770. The fact is that you’re underestimating AMD a tad much.",
      ">AMD has always been lazy when it comes to GPU innovation.\n\nRDNA 3 is a revolutionary chiplet design. Nvidia doesnt have that.",
      "The \"fine wine\" argument doesn't really make much sense anymore at this point. Arc GPUs were delayed basically forever, while the hardware was in developer's hands for a long time already. If they haven't figured things out by now, they probably won't figure it out in the near future either. Maybe they never will.",
      "> Potentially even completely destroying AMD if they stay with competitive prices.\n\nYeah nope. AMD has atleast a decade head start on the overall dGPU, regardless of AMD's shortcomings on its drivers. \n\nAnd there's a reason why Intel includes Nvidia only on their marketing speak about Arc. Because AMD is still superior in terms of price to performance ratio.",
      "Also IMO HUB is already more \"lenient\" compare to others like GN.\n\nYet people just assume HUB is somehow biased because Steve don't always say what they want to hear.",
      ">The fine wine argument is relevant for people with less money to spend who want to have the best hardware they can get within their budget.\n\nThe justification has already begun. Was AMD not the budget option before when Finewine was argued? Why was it ridiculed then? Because it is hopes and dreams and pretends that all AMD or Intel need to do is fix drivers and that's it. It's putting fantasy over reality because current reality does not fit with desires and expectations. \nYou can not guaranteed that finewine will make any GPU worth its launch price sometime in the future. It might be that by the time those improvements are seen, the whole industry has moved on to new performance levels and prices and it will then be too little too late.\n\nThese GPUs are being launched at the end of current gen for AMD and Nvidia and the only reason the prices are competitive against 2 year old tech is because Nvidia decided to go with scalper prices for next gen and not planning to release 4060s for sometime. AMD is still a question mark. \n\nThese GPUs are like 6-8 months too late imo. That's the objective reality. They should be priced even lower to stay a legitimate option for budget for the next year.\n\nThis is all besides the point though. The irony of this sub lending legitimacy to the finewine argument after ridiculing it for years and arguing for considering it in purchase decisions is not lost on me. \n\nRemember, when finewine was a thing AMD did not have a competitor to the 2070ti and their GPUs were the value option if you were willing to deal with worse drivers and software.",
      "I mean, I hope you're right, but I still wouldn't recommend someone get a card with insanely all over the place performance and stability when alternatives exist because it MIGHT not perform terribly some day.",
      "Yeah, exactly. I had such a bad experience with the 5700xt that I got rid of it and told myself I'll never buy an AMD card again.\n\nThat card had a lot of problems but it was functional, those Intel cards don't even work half the time if you watch the GN review you would know.",
      ">I might still get it because being in on the ground floor might be interesting.  And i want to support a 3rd player.  But it's not the best decision for most people.\n\nExactly. As an enthusiast, this I understand. I wanted to get in on the ground floor of 5800x3D for this reason as well even though I dont really game as much these days."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "MKBHD builds a new PC with an Intel Arc A770",
    "selftext": "",
    "comments": [
      "Because they get crossover viewers this way from MKBHD’s massive audience",
      "What does LTT get out of sponsoring this guy? Why not do it themselves?",
      "They’ve collaborated before and it expands the advertising reach of the screwdriver and LTT Store in general.\n\nMKBHD doesn’t do PC builds and focuses more on phones and Apple products. However, his videos don’t necessarily shy away from technical details when relevant and I suspect there’s some overlap between audiences as a result.",
      "Kind of out of left field. MKBHD doesn't really do PC gaming related stuff sans maybe monitors every now and then.",
      "It's kind of perfect. MKBHD has a clean million more subs than LTT, and he's techy, but just out of sync  in focus enough that if you're his core audience, you might not be Linus's, unlike channels like HUB, GN, J2C, Paul's Hardware, Bitwit, etc.",
      "They're buying time to fix the drivers lol",
      "Release the fucking gpu already",
      "Well that's 95% of what he covers, so I would expect that. Not much of a reason to know about anything else.",
      "The most \"PC\" thing I've heard him talk about is *monitors*.\n\nHe couldn't tell you if a 6600 or a 3080 was the better gpu.  Dude doesn't play computer games or deal in the PC/component/Windows world.  Which is fine, thats just not his wheelhouse.\n\nAfter listening to his internal struggle earlier this year over throwing away his couple-year-old $40k Xeon Mac Pro for a $6k glorified Mac Mini, seeing him pimp an Intel GPU of all things is really quite an ironic combination.",
      "You want an A770, not an A770M.\n\nBut I agree, I'm actually kinda keen to try one out and see how far it overclocks.  That said the 4000 series announcement expected tomorrow will likely scuttle this cards chances before launch sadly.",
      "Forza Horizon 5 is shown briefly in the video, set to 1440P and the High preset... but with dynamic detail enabled. Due to the way that the game changes the internal render resolution and LOD to try and get close to the target framerate, the figures reported are rather meaningless as a performance comparison point.\n\nThe game's perfectly playable (with a few small pauses in the clips shown), but there's no new info here for anyone looking for any sort of meatier \"first impressions\" video.",
      "Nice. thanks for sharing. Maybe it is only me, but I would love a A770M all in Intel blue color. But maybe I am partial to blue shrouds https://imgur.com/UXBs2xZ",
      "Compared to other ratcheting screwdrivers it’s really not overpriced (still expensive though)",
      "Because MKBHD is as big if not bigger and has way more normies watch his channel.",
      "I don't know, ask his 16M subscribers",
      "He’s also got a great golf swing for an amateur in some golf YouTube videos.\n\nNot to mention his ultimate frisbee skills.",
      "One opinion you don't agree with doesn't suddenly make them stop being one of the most respected tech media outlets. I was worried that there might be an actual scandal there for a minute.",
      "they mentioned in one of their podcasts that they were starting to sponsor creators to help expand the brand.",
      "It's Forza Horizon 4 not 5",
      "Ah yeah that makes sense then. I never heard of this guy before today."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "a770"
    ],
    "title": "Hitting the Shelves: Intel® Arc™ A750 and A770 GPUs Release Today!",
    "selftext": "",
    "comments": [
      "OOS @ newegg",
      "Yea this launch has been kind of a mess so far. The A770 and A750 went up earlier than they were supposed to on Newegg and the A770 16gb sold out immediately. The A750 came in stock again on Newegg but now it is also out of stock\n\n**EDIT: THE A770 CAN BE BACKORDERED: https://www.newegg.com/intel-21p01j00ba/p/N82E16814883001**",
      "Aaand .. it's gone",
      "Such a whimper of a launch. They must have made one whole box per country.",
      "True limited edition cards",
      "Complete mess. Release a budget friendly card with all this hype....but don't tell us officially where or when we can buy it until an hour AFTER they sold out of the a770?",
      "Had the A750 in cart and by time I went to check out it went out of stock. Bummer",
      "its my bad, i bought them all",
      "Werent a ton of these things supposedly sitting in warehouses for months?",
      "Man… what a joke of a launch.",
      "https://youtu.be/-DT7bX-B1Mg",
      "Unironically, I think a *lot* of these LE cards are going to end up as shelf art.",
      "If they had all this cards since February.... and they are gone... that meas this is it, there should be no more stock.  \n\n\nThis is the first GPU in history to be designed as a collectible item.",
      "maybe demand is high?\n\nSounds a little like copium",
      "Dumb thing to scalp, frankly. These are the higher end models, and there are already better nVidia and AMD cards in the used market. Prices on those and new old stock of 3000-series cards keep coming down. \n\nNo one is going to pay much above MSRP on an Intel Arc card today 😂",
      "So where's that discount code from the HPG scavenger hunt :/",
      "Scalpers gonna scalp…the shoe market is slowing and PS5’s are in stock.",
      "Fucking backorder won't go through. Just keeps putting me back at the secure checkout page.\n\nEdit: It's aggravating. I work on a PC. I had auto-refresh set every 30 seconds since this morning. Missed the first drop. I got up to use the bathroom ONCE and of course that's then they come back in stock lol. Ugghhhh\n\nEdit 2 hours later: Still won't let me add the backorder.\n\nEdit: 5 minutes after previous edit I finally got the backorder to go through. Had to use the phone app (I was able to spam the confirm order much faster over and over). Worked on the 5th try.\n\nETA on the backorder is 10/19....if of course they get the stock in for it.",
      "They delayed these cards for months in preparation for this launch.   How did they mess this up so bad???",
      "Newegg made them available at 12AM Pacific. Their Chinese overlord’s probably made them do it….but it’s also Newegg so they could have just F’ed up per usual"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Is the ARC A770 a good card in 2023?",
    "selftext": "My RX 570 just bit the dust and I’m looking for a new, ~300 dollar graphics card. The ARC A770 seems like a pretty good pick (especially since it comes with MWII), but all the reviews say it doesn’t perform super well. I know there have been some driver updates that have helped tremendously with performance, so is it a worthwhile card now? \n\nI have an i5-12400F with ReSize Bar enabled so that’s not an issue.",
    "comments": [
      "that title lol you ask like it was released in 2014 or somethin\n\nanyways I'm joking but yes it's a fine card for the right price, though you will have to be aware of comparable cards and the limitations of intel graphics cards, like apis n such",
      "Good card for the price.",
      "A770 is a good GPU that is slowly getting better over time.\n\nHowever either get a 6650xt/6700/xt for better performance and drivers.",
      "It's a good card, I have one in an older system, Intel 9900ks (undervolted) Z370 MB, one that supports resizable bar and it works just fine. I use it mostly for picture and video editing, Topaz Video AI and Handbrake/MakeMKV, and it also doubles up as a Plex server but I have gamed on it. It gets about 75ish% the performance of my AMD 5800X3D/Nvidia 3080 system. '\\*edit. based on 3DMark score and not actual gameplay\\*'\n\nThere's a few quirks though that I hope will be sorted in the future, it idles at 48° and draws 35 watts which compared to Nvidia cards is probably double the temp and power draw, for idle at least. MSI Afterburner doesn't support it yet so I hope that changes in the near future too.\n\nOn a plus side running 3DMark Timespy it never exceeds 71° and while gaming it's usually much lower, it's also a pity you have the 'F' skew of the 12400 processor as the Arc utilises the CPU GPU in some scenarios.\n\nOverall I am very happy with the Arc.",
      "A770 preforms pretty well IMO, obviously far off from high-end Nvidia/AMD but yea; I play in 4K and most games are 60+ fps at mid-high settings, including MW2\n\n\n\nI got it like 1-2 months ago iirc, drivers have (seemingly) improved substantially in that period, went from super buggy for me to rarely encountering a driver issue",
      "Games that run in dx12 or Vulkan run best on ARC. Dxvk is very viable for older games.",
      "Because I don't game as much as I used to and didn't want to spend a bunch of money for a replacement when the 1080ti I had was just fine from a performance perspective.",
      "> MSI Afterburner doesn't support it yet so I hope that changes in the near future too.\n\nMSI Afterburner last I heard may be defunct.",
      "I view the ARC A770 as somewhat of an investment because they’re more than likely going to be collector’s pieces in 5-10 years. Especially the 16GB model.\n\nOther than that, I think it’s a safe purchase if you’re patient. Intel supports their products for a long time, unlike AMD that has dropped support for GPUs after only ~5 years (RIP Fury X, RIP R9 390). Performance and VRAM amount is pretty good at the $350 the 16GB model is currently priced at.\n\nThis is coming from someone than ran AMD GPUs from 2011-2017 though, so I’m more used to dealing with dog shit drivers and other bugs lol. Remember to take care of yourself.",
      "I replaced my 1080ti that I had bought new as a nvidia fe preorder that was dying slowly with an a770 16GB.\n\nIt’s more than enough for 1440 gaming (a bit quicker than a 1080ti) for me and the only game I have had real issues with is darktide where it basically just doesn’t work.\n\nDrivers have come a long way since launch so the real question is if you are ok with it’s level of performance with maybe some bugs here and there.",
      "I'm confused why you would opt for a product with such similar performance after nearly 5 years\n\nIs there some specific productivity application that is better suited to an Intel GPU?",
      "Vulkan runs fantastic on Arc. It's older DirectX versions that struggle",
      "You could do worse for the price.  It's terrible with older titles (DX10 or under), but does okay with DX11 and DX12.\n\nDrivers are still quite unfinished, so the card is very much still a work in progress.  Some stuff will run fine, other things will run like ass, or have lot of graphical glitches or issues.",
      "Personally I think your money could be better spent . Intel is forsure going to crack into the GPU market . It’s not gonna be with this card though .",
      "Intel Finewhiskey?",
      "MSI released a statement that they haven't abandoned Afterburner.",
      "Yeah development is dead on it now.",
      "Yes",
      "If you run AAA titles on Dx12 or Vulkan, it’ll be a really good card (equivalent to 3060 Ti in raster & 3070 in RT performance). \n\nIf you run a lot of older or niche titles that run Dx9, Dx10 or Dx11 you’re not going to have a great experience.\n\nAlso a CPU that supports ReBar is basically a requirement, without that don’t bother.",
      "[The most recent leak shows Battlemage not arriving until 2024](https://videocardz.com/newz/intel-arc-desktop-gpu-roadmap-leaks-alchemist-in-q3-2023-battlemage-in-2024), with a strange Alchemist refresh just before then."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "ACER Arc A770 BiFrost graphics card with 16GB VRAM is now available for just $282 - VideoCardz.com",
    "selftext": "",
    "comments": [
      "the 3060 is the literal worst possible pick for that price range when you have the RX 7600, 6700 XT, and now the A770 16GB.",
      "That is insanely cheap for that much VRAM.",
      "What makes you say so?",
      "vram is cheap, but some companies may give you less in order to lure you into upgrading sooner rather than later",
      "Come on man, no need to sugar coat. Just say its Nvidia.",
      "Looks pretty damn cool, but I don’t really trust Acer products much",
      "I've been using it in my build for a good few months now, and have had zero complaints. My recommendation is to get it if you're looking to buy.",
      "They do well when they have good thermal management (thicker laptops, this GPU, etc)\n\nHistorically its been their thin-and-light laptops with bad thermals that fail",
      "that's not what they said though. They didn't speak about power usage at all, they said that the 3060 has \"more consistent performance\". Based on what would this be, since you determined it correct?",
      "For $280 the A770 is an excellent choice though. An extra 30 bucks for double the VRAM ignoring everything else is a good deal.",
      "I have 2 of their monitors for 4 years now, zero issues. They are a mainstream brand, with hits and misses, I doubt this GPU is a bad buy in any way.",
      "It's miles better than at launch but still nowhere close to AMD/Nvidia level of maturity.",
      "From what I've seen XeSS actually looks better than FSR @1080p and 1440p. It's the same as DLSS @1440p but  very slightly worse at 1080p.",
      "Why? Only Acer products I've bought are their monitors, but they are top tier.",
      "😎👉 **NVIDIA**",
      "If you're interested in the 16gb model. This is the deal to get.\n\nJump on it before the sale is over!",
      "Going after Content Editors and AI Developers?",
      "Only 280, damn that is cheap",
      "I'm inclined to agree for the a770. A750 is a decent choice tho.",
      "And XeSS has become pretty damn good compared to DLSS and FSR"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Installing my Arc A770",
    "selftext": "Just installed my newly arrived A770. Replacing an old GTX 960 and temporary RX570.",
    "comments": [
      "Just letting you know, once you set the rgb patterns to your liking, you can unplug that and it will keep whatever was set to :)",
      "Seriously?? Ty for the advice, dunno if I’ll unplug it or not. Even if I will certainly let Rainbow mode forever.",
      "Friendly remember about activating resizable bar.",
      "This card looks amazing imo!!",
      "You’re leaving performance on the table then, but you do you.",
      "Nice. Make sure to enable rebar!",
      "Maybe he's a behind the scenes tech guy instead of a performer.",
      "Make a reactor from it",
      "Awesome! Enjoy it, glad to see people getting it\n\nIm still trying to find one in stock, I got a 3080Ti but just getting one for fun",
      "Yep, already activated it before buying and aspm L1 too. I made all the steps before it’s arrival but the card at idling keep eating between 37-38w. Need a driver for that.",
      "It is, looks really solid, doesn’t bend while on the PCI, and rgb is nice.",
      "Arc was really interesting me as a sort of \"Painful to use now, but watch it age like fine wine\" sort of gamble, but after seeing how LTT's careful optimism gave way to absolute disdain after having to use it I really don't think it's worth putting up for - *especially* for CSGO. If you were only playing modern DX12 AAA single player games then maybe, but for CSGO and anything else older and multiplayer literally anything else will be worth it more.",
      "I already want one it's just I can't FIND one. \n\nI finally found it though after waiting over a month",
      "No, their overclocking is dumb. Performance bar is so stupid, not a lot of difference between any of the numbers.\n\nPut the performance on 30\n\nPut voltage on .95\n\nPut power on max.\n\nLook at temps, if you see it too hot, bring the 228W back to 210W.\n\nThe voltage does not add that much heat, the power does.",
      "Does anybody have good A770 OC/Undervolt settings via Arc Control?",
      "Don’t even need the rgb cable. I didn’t even pull mine out of the box.",
      "My AIO needs that USB, only one.",
      "If you have money or want a second build why not?\nActually a lot of people bought one, budget gamers (750) or full Intel build gamers (770). When I called to shop who had some in stock, they said they sold the last one 15 minutes before my call… \nWould be glad to see those steam survey or intel sells to see the current marketshare",
      "Max boost is 2725 for me, temp reaches 84c when 100% utilisation though. Doesn't need undervolting, stays in the low/mid 70c at stock.",
      "Already activated it before buying aswell as ASPM L1"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Got a 16gb Intel Arc A770 LE at Microcenter!",
    "selftext": "Nice design, I wonder if it will end up being a collectors item down the road as it doesn’t seem like there’s a huge amount produced. (Regardless of performance discussions)",
    "comments": [
      "You lucky bastard!",
      "Yeah and I like the way the box smells",
      "Want to test it out, plus I liked the design. It won’t be my main gpu",
      "What made you want to take the plunge?\n\n(not trolling)",
      "They have a 4090 so money may not be an issue. I've heard the stock for them was also kinda low so they may have just bought what was there.",
      "What city u in? You bought it this morning (Oct 19)?",
      "It is a 4090 - doing 10,240x1440p with a triple monitor setup for racing sims 😁",
      "Ok.  So A770 really is just for fun!  Nice.",
      "No I got it a few days ago, NY",
      "Can I ask what your main card is and what resolution/speed you run?  Still debating this option in my head but fairly sure it won't push my 1440p UW.",
      "Congratulations on the purchase. One day, I may talk myself into purchasing the 750. Just wish it was a tad cheaper. Enjoy the card!",
      "It’s actually a red video camera, and I did a screen grab from the video with a 24mm canon cine lens",
      "> RED video camera\n\nAh, that explains it haha. Thanks for the response.",
      "That’s me!",
      "Wasn’t the 750 the one to buy?",
      "What camera did you use to take the photos? They look fantastic",
      "Hey do you run the Classical Technology YouTube channel or your username is just a coincidence? Congrats on getting the card! Hopefully you can redeem the free games.",
      "I hope Intel has a top team sorting out the drivers\n\nThis is the card I would buy if I was upgrading",
      "Yep",
      "LTT did a video about running it beside a main graphics card."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Intel CEO confirms first batch of Intel Arc A770 GPUs is getting ready for retail - VideoCardz.com",
    "selftext": "",
    "comments": [
      "More photos of Arc from Intel, seems like the only thing mortals can get :)\n\n[Here's another one](https://i.imgur.com/i5RzYdB.png)",
      "I hope they are $249-$299.",
      "I don't think it makes much sense for it to be priced the same as a 6600 XT/6650 XT when performance will suffer so severely in older games. Tier 3 pricing, right? If it truly was the case then it ought to be $250 or lower. But will that happen? Doubt it. Regardless, I think we all should hope for better than the bare minimum, because it's just too cynical not to.",
      "for 200-250$ it’s ok. If drivers still suck I would only spend 150$ to be an alpha/beta tester.",
      "They are lucky Nvidia will probably announce only the super high end stuff for the time being.",
      "That’s also likely why they haven’t shipped yet. Trying to get the drivers improved as much as possible so they can charge more.",
      "What's the adjective for this whole affair now that officially (unless you can buy one in the next two hours) of them being finally for sale after the 4000 Nvidia announcement today and thus being compared against that line at launch?\n\nIs it *ironic*, *sad*, *unfortunate, emblematic*?",
      "yeah they lost my interest after the whole Q1 disaster",
      "As far as I know, the existing cards in 30xx series already smokes it, but nobody expected anything else. Just getting a product out the door is amazing in itself, let alone getting 3060 like performance on first try.",
      ">at this point who cares?\n\nidk probably anyone thats actually interested in the computer hardware hobby\n\na third player in GPUs isnt a bad thing and it doesnt matter that intel is having a rocky first gen when the other vendors are firing on all cylinders \n\njust remember time is a thing and the future will have a different set of problems and issues unlike we have now. there is a chance nvidia or amd could stumble and next thing you know were in another 5 year gpu slump. this would be a lot less likely with three major vendors",
      "They always announce/release the high end first but it will take away publicity.",
      "What are you even on about.\n\nAlso the reason why people went for macs despite the performance was the software. No one's going to be buying an intel gpu for the software besides av1...which since they took so long to release that nvidia's new gpus are already getting av1 next month. Drivers are literally the worst part about arc gpus. Don't bother me with the fine wine saying either, tech gets superseded faster than the software can mature.\n\nNvidia will still dominate in the workstation/server space too.",
      "As far as maketing stunts go, this one isn't great.",
      "\"getting ready for retail\"...that could mean literally, 1 month, 1 quarter, 1 year. I hate the vagueness that is used. Especially when you listen to a quarterly earnings call \"tracking well\", \"on track\", \"ramping in 2H\", \"sampling well\"....  Just tell us in what month or even quarter(with a year attached to that) consumers should be able to purchase one at a physical or online retailer and be done with the vagueness shenanigan's. I get it that when the physical hardware is in early to mid stages vague is a necessity, but we should be well beyond vague at this point.",
      "Even still, why risk an unknown over the Nvidia/AMD equivalent? Even if they fix the drivers to a workable state it's still less efficient than the Nvidia equivalent on a much better process so you'll waste money yearly just having the PC on.",
      "Depends what you mean by efficient. Performance per watt nvidia wins but power consumption intel wins. \n\nI wouldn’t buy an intel gpu for my gaming pc yet but it would work fine in my other box which I use for daily driver and causal gaming on Linux.  Right now i have an amd 570. \n\nIntel gpus also make sense for content creation workloads based on early benchmarks. \n\nNvidia has a hefty price versus to competition. I upgraded the gaming pc from a 1080ti to a 6900xt last year and no regrets. It runs a lot cooler too.",
      "my 100 USD off from their event is still waiting... yet haven't heard anything from them since Spring. I guess it's competing with regular 6700 costing 400euros. nvidia launch just cemented series 30 prices.",
      "if its 300$ its DOA",
      "In 2-3 years Intel will have 1 card specifically they say is for gaming and then they’ll focus on “Accelerators” down the line-dual purpose cards that will drive your display but possibly also capture input and/or accelerate encoding with next gen codecs. There’s significant hope in the streaming and broadcast markets that Intel may have struck gold with their platform regarding encode/decode/transcode workflows and I guarantee you’ll see them entering this space in a targeted way with multi-linked cards. To do that effectively they’ll have to keep cards at under $400 always. Intel may not become a huge player in the gaming market but they can disrupt other sectors which is a good thing still.",
      "The pictures?\n\nInb4 intel sells GPU NFTs due to poor yields and intransigent driver issues."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Intel Arc A770 16 GB Review and Benchmarks by LTT Labs",
    "selftext": "",
    "comments": [
      "Battlemage is going to be exciting. Much better architecture.",
      "Looks like a pretty good entry level alternative now.",
      "AMD is cooked",
      "imagine giving it the power and cooling of a 4090, equipping it with the fastest memory  and clocking it like a flagship processor...",
      "Soon. Post celestial should be Intel. Falcon shores for data center is first Intel node, Intel gpu",
      "The year where LTT Labs is adding products to their database, and the year where the A770 is still Intel's flagship GPU.",
      "I hope so because damn Intel has been getting its butt kicked lately on the market and they need a win",
      "That interactive (rotatable) Lumafield CT scan feature will never get old. So cool.\n\nAlso, their specs for the memory speed are wrong. The A770 LE 16GB runs at 17.5Gbps (2187MHz), not 16Gbps (2000MHz). The latter is for the A750 and 8GB A770 variants (or a few A770 16GB models from Gunnir).\n\nTechPowerUp never fixed it in their database either, drives me nuts.\n\nWould also be nice if they mentioned the support for 10-bit HEVC 4:2:2 encode/decode, which isn't found on any other current AMD or Nvidia cards. Makes it possible to smoothly edit footage from some Sony and Canon cameras that utilize the format.\n\nInteresting that they've included GravityMark tests already in their synthetics section.\n\nOk, I'll stop adding crap to my comment now. Glad they're putting more tests out, but do wish they were more comprehensive.",
      "Just contact us in any way when you find issues with our database and we‘ll get it fixed",
      "Here ya go: :)\n\nhttps://www.reddit.com/r/IntelArc/comments/1blz05i/comment/kwapd3t/",
      "your looking at it wrong. that wouldnt be a product. that would be a one off tool.",
      "rinse beneficial coherent engine disarm serious tease vanish fine connect\n\n *This post was mass deleted and anonymized with [Redact](https://redact.dev)*",
      "What year is it?",
      "It's a great GPU. It still have problems with some niche titles, but very often you can fix it with dxvk. I could even play Cyberpunk 2077 with some raytracing options on 1440p. Ofc it was with XeSS enabled (Intel's DLSS alternative), but I wouldn't expect any card for the price to be able run raytracing without super sampling",
      "I am always interested in news about Intel GPUs! :3",
      "I have this card in my new rig. I probably won’t be gaming at all, but feels good to have the option.",
      "Arrow lake iGPU has HEVC decode/encode for  422 10 bit, should be good for those with Nvidia or AMd GPUs",
      "Why?",
      "LTT is slowly adding products to the lab page. That's all...today they added the A770.",
      "password hash \"checker\" or something"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Dell now offering $1,949 Alienware Aurora R15 desktops with Intel Arc A770 GPU - VideoCardz.com",
    "selftext": "",
    "comments": [
      "At $1949, that in an absurdly bad price for an A770 PC\n\nEven by prebuilt standards",
      "Who even still buys Alienware in 2023? Dell is out of touch with what people want, just shut down Alienware already",
      "Plus the airflow in that case looks like a nightmare with all that plastic and metal blocking fans. I bet it throttles under load.",
      "Their monitors are pretty good. That’s about it",
      "Gamers Nexus has had some fun deconstructing Alienware cases and dunking on how supremely bad they are all around.",
      "$2,000+ all said and done for a 13th gen x700 chip and an Arc GPU, jesus man.",
      "Dumb people who do 0 research before spending $2000+",
      "Yeah. IMO, a 4070Ti or 4080 (or 7900XT) should be in that thing.",
      "That should be like $1500 tops, but it's an Alienware.",
      "People who buy Alienware currently deserve to be ripped off.",
      "I can build a whole 4080 rig for that much cash",
      "There's some prebuilds that it's actually hard to match the value of with a home build due to economies of scale \n\nBut this ain't one",
      "Yeah I'd argue you'd have a better time with an XPS desktop.\n\nBut I'd argue even harder to pick a different SI to go with.",
      "But how is dell supposed to make any money that way?",
      "Nuts",
      "That’s gonna hurt their rep….unless something has changed Arc drivers usually need more then one day 1 patch for most games coming out not to mention how older games usually have intermittent issues or crashes. It’s not the best experience from a customer perspective.",
      "The Alienware name deserves better than this. The laptops are actually pretty good. They just insist on reusing the same tired chassis design for the desktops.\n\nI'd love to see the level of engineering they put into dressing up an office PC case go into making that Alienware look actually functional.",
      "It’s a crime what they did the the XPS name imo, there’s nothing extreme about them",
      "I owned an xps laptop, put me off on ever buying a dell again.",
      "north automatic possessive handle snow station threatening overconfident hunt silky\n\n *This post was mass deleted and anonymized with [Redact](https://redact.dev)*"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Intel GPU Chief confirms Arc A770 is getting ready for launch - VideoCardz.com",
    "selftext": "",
    "comments": [
      "> , i don't believe the latest rumors saying intel will cancel their gpus, because it would be a waste of money really.\n\nTo be fair, they cancelled their last dGPU effort codenamed Larrabbee despite having poured a ton of money into it.",
      "More talk please, we are all thrilled to hear more talking.",
      "Resizable Bar is nothing new",
      "Not really, a card can be unpopular at $500 and super popular at $150, but if it makes a loss at $150 it's unsustainable. still better to get $150 back for every chip you've made so far rather than throw them away and take even more loss, but if they deem it unsustainable they'll cancel the project.\n\nBut overall that's just this gen, the real question will be if Battlemage (their second gen) is seen by Intel to have closed the gap significantly in efficiency or if it's still so far away they can't see making profit in several more generations. Either it makes small enough gains they can't see profit for the next several generations with similar gains or it makes big enough gains they can see the project churning out profit within another 1-2 generations.",
      "Intel Gpu's for data centers are absolutely not getting cancelled. Those are going full steam ahead with a full future roadmap and support via oneapi. If Intel cancels Arc for consumers/gamers it will be a net negative for that segment. When the inevitable next boom cycle of cryptocurrency comes along these are the same people whining for cheaper Gpus.",
      "Releasing A770 has absolutely no link to if the project is cancelled or not. No one cancels a project with 100s of millions or maybe even billions in stock inventory of a product thereby killing most sales immediately as people won't want a chip they think won't get future support.\n\nIf Intel was to cancel the Arc project it would 100% be after A770 launches, probably 6+ months later if not more like 2 years. They'd just let news of Battlemage drift further and further apart with less and less detail before trying to quietly kill it. That would allow them to shift as much inventory as they can and make as much cash back as they can.",
      "Not really. It turned into their Xeon Phi line and made them a lot of money.",
      "There is little evidence it made them a lot of money, it was largely cancelled due to lack of demand with only 2 actual xeon phi generations with ultimately very little progress (from like 240 cores to 280 cores the following generation despite a node drop). The third gen was cancelled largely due to lack of demand. Super computers are pretty much advertising tools and often have at cost of free gpus/cpus so the companies that provide them get talked about in regards to the most powerful computers in the world. So having supercomputers powered by Phi indicates absolutely nothing in relation to actual demand by customers who would pay for those chips.\n\nBut a project being cancelled doens't mean products can't be sold. Xeon Phi was largely Intel's attempt to salvage something from Larabee but with high R&D costs and low volume sales it's a reach to say it made them a lot of money, in fact we almost certainly know it didn't. If it made them lots of money they wouldn't have cancelled it because no one cancels a product line making them lots of money.",
      "They made products and sold them to companies, primarily for machine learning iirc.",
      "I've got my money on Intel getting things a lot righter for battlemage.",
      "ah another one of those, releasing very soon articles at least this time if from raja. But then again yall in the GPU world dont have a real good opinion about him",
      "If Intel wants traction in this market they need to create the next RX580. Maybe they already have, we’ll see. \n\nThe 580 was the definition of cheap and cheerful. It punched above its price point at launch, and continued to be relevant for a long time first at the mid range, and later the low end.",
      "Sooner than you expect! - 2 months ago",
      "For $249.99?",
      "https://i.imgur.com/MueLuxa.png",
      "I think there are chunk of folks praying that Intel keeps failing for ever. So these rumors keep swirling. If you look at Pat's strategy, computing is no longer just CPU. So they will definitely not give up on GPUs. I think what could get cancelled are anything tangential to computing. If they dont have long term sustainable business model, then they go away. We will get Battlemage/Celestial etc not just for consumer but also DC GPU like Rialto Bridge and XPU(Falcon Shores). Let us see where things are in couple of years.",
      "Whelp I guess the future of arc will depend on how popular the arc cards would be",
      "I do hope there are more choices in gpu consumer market while Intel should take this opportunity to fight with Nvidia and AMD. Let's wait and see.",
      "Just don't give up on the dream. Keep working on the drivers and Intel may have something. Would be good for the market if they can do well in the GPU space. Make it happen, Raja and team!",
      "Not sure how true that is. See optane dcpmm 3rd gen. They announced cancelling the roadmap all things optane, but still releasing 3rd gen optane dcpmm."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Im having some problems with my Arc A770 in sleep mode...",
    "selftext": "",
    "comments": [
      "Report it to intel support, help them and help yourself.",
      "Did you try Ctrl + LShift + Win + B by chance?\n\nEdit: [Also, it's a known issue in the driver report.](https://downloadmirror.intel.com/764788/ReleaseNotes_101.4032_WHQL%20.pdf)",
      "\\>having some problems\n\n\\>Arc A770",
      "Awww it’s having sleep paralysis",
      "I originally had an A770 and loved the thought of supporting it. I can troubleshoot just about anything. Eventually got tired of having to deal with these problems and just random things that would be annoying. Sold it to someone else who wanted to support it and ended up with my first AMD card since the Polaris gen and it’s been great. It just works, every single day lol.",
      "\"fixed\" the same issue, try setting PCI express power management to Deactivated in advanced energy options. It's the same setting that reduces idle power consumption, but for me it introduced this behavior.",
      "This is to be expected with the A770 it's gonna have problems, I would contact Intel and try and submit a bug report this is the only way it's gonna get better!\n\nBut thanks for being one to test the cards. I would but not for current prices.",
      "it's dreaming",
      "Hello Jensen Huang.",
      "Thanks for reminding me. Just updated it.",
      "thats what happens when you use it with its rival processors!!",
      "Its playong atari games",
      "CTRL LSHIFT WINDOWS B should fix it (restarts display drivers)",
      "What's that device that shows up at 0:05(s) of the video, with some lights, one blue, another red... ?",
      "Sleep mode in Windows is an awful feature all around but ARC does have bugs that compound the issue. For instance, my PC will boot cycle a couple times when waking it or turning it on, occasionally and I get a screen saying that the gpu adapter has no vbios, lol. Then it just works. *shrugs*",
      "Your flair is still A770",
      "Flat out wrong. \"With Linux 6.2 there is also now at least the DG2/Alchemist support out-of-the-box, no stability issues were observed, and it's running off an open-source upstream driver stack similar to the AMD Radeon driver stack that is much loved by the Linux gaming/enthusiast community after years of work there.\" https://www.phoronix.com/review/linux62-intel-radeon/5",
      "That's why you don't buy a prototype GPU, good luck.",
      "It’s planning something - Some quote about skynet",
      "Well you got a limited edition. That must mean you get limited graphics.\n\n-\tClosed (by design/won’t fix)\n\n^/s"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Intel officially discontinues Arc A770 16GB Limited Edition - VideoCardz.com",
    "selftext": "",
    "comments": [
      "i guess its people just not drawing connections in the world with this.\n\namd and nvidia only make a certian number of cards a generation and discontinue thier lines too. there just isnt articles about it.\n\nnothing to see here.",
      "No that’s correct. This happens all the time with Nvidia Founder’s edition and AMD Reference cards - they end production to free up space for the next generation cards to begin manufacture. Other models of the A770 such as the Acer 16GB and Asrock 8gb are still being manufactured. Only Intel’s ‘Limited Edition’ reference model has had production end.",
      "I was informed by a multitude of reviewers that \"Limited Edition\" didn't mean that they would end the SKU, just that it's their nomenclature for the card. Was that a lie?",
      "There's an AIB model called the Acer Predator BiFrost and that has 16 GB.",
      "There often are articles when they discontinue chip production for a certain card. Discontinuing production doesnt even mean a whole lot. They could have already produced all the inventory they think they need before battlemage for all we know.",
      "This is only the Intel made \"reference\" card. Like another poster pointed out, Acer has their own A770 with 16GB Vram.",
      "This version of the card has reached the end of production line. There are still AIB A770 16GB cards out there.\n\nI have an A770 LE exactly because I expected this to happen. Wanted to keep a piece of history I suppose. I would hope this means they are paving the way for newer, more powerful cards.",
      "Repost.",
      "Serious question. \nLooking for 16GB card, at least.     \nMy problem was that i was bit afraid that Intel would \"cut\" also GPU division (not considering dedicated igpu).     \nNow i see this information, any one have more insight?        \n8 more GB is like 20$ more cost on card manufacturing.",
      "Yes, missed that ... it was ... ok ill get this Intel, and this is first thing that showed up.",
      "So the headline implies something that is misleading. Wow that's never happened before.",
      "But... that's your only option.",
      "OP did you miss this post from 3 days ago?  This is old news.\n\nhttps://www.reddit.com/r/intel/comments/14fb4ju/intel_discontinues_intel_arc_a770_le_16gb/?utm_source=share&utm_medium=android_app&utm_name=androidcss&utm_term=1&utm_content=share_button",
      "Really wanan buy one . 305$ CAD … but having a hard time deciding if it’s better then my 2060s",
      ">there just isnt articles about it.\n\nYou mean there's no article like [this](https://www.notebookcheck.net/NVIDIA-is-no-longer-selling-the-RTX-3080-Founders-Edition-or-RTX-3090-Founders-Edition-directly.497768.0.html) and [this](https://wccftech.com/amd-to-end-radeon-rx-6900-xt-6800-xt-6800-reference-model-production/).\n\nColour me surprised.",
      "Pretty sure TAP himself said in one of the videos that Limited Edition didn't literally mean limited. They probably stopped production now because they weren't selling enough of them or they are getting ready for the next generation cards.",
      "Yep.",
      "> stopped production\n\nAll of these were produced, boxed, and warehoused in 2022, awaiting the driver team making progress towards a stable retail driver. They've sold through all or most warehoused stock now, and can discontinue the sales support etc. (at least for Intel. Acer's 16gb, etc  may be sitting on tray GPU packages awaiting solder to boards and thus still in production)",
      "> You think whenever AIB makes a card they just go out to open market and buy the GDDR6 module for that card?\n\nYou're making the point you should make, but in the wrong direction.\n\nIf spot price is so low, then surely whatever bulk pricing a major OEM can secure would be *lower*, not *higher*.\n\n> Do you just buy memory chips and it just gets magically superglued to your card directly from fabs?\n\nIt's bought in rolls and fed into machines. The cost to solder a few more chips to a board already on the line is minimal. You think humans are toiling away doing this shit by hand?\n\n> Do you understand that little pesky thing called SUPPLY AND DEMAND? \n\nDo you understand that little pesky thing called *economy of scale*? The more something sells the cheaper it gets *at bulk OEM scales*",
      "Closer to a 3060 isnt it"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "a770"
    ],
    "title": "MW2 running on Arc GPU",
    "selftext": "Hey Intel users,\nI’m running into a minor issue that turn very annoying when playing MW2 with my A770. Idk if some also have this even if you have another GPU from Nvidia/AMD.\n\nEverytime I go into a multiplayer match its laggy to the point of being unplayable during the first 2min (class selection to middle objective in domination for example)  and after that minute or 2 it’s completely fine. It’s like the game was loading a lot at the start and reducing graphics especially shaders to low and textures to med didn’t change the lagginess. I even stopped the better textures downloading, but still laggy. \nIt’s just annoying that you can’t play the first minutes of the match a little competitively. I think that the bigger the map, the longer it’s laggy.\n\nPS: Laggy in the sense of stuttering. Installed the game on a 7200rpm instead of 5400. Tried a match and it’s still stuttering but maybe little less.\n\nSOLVED: bought a Kingston Fury 1TB SSD and it works like a charm, no more stuttering when in game and I can finally play calmly.",
    "comments": [
      "Its going to be a more prominent issue going forward, more and more games are designed for SSD.\n\nYou should invest in an m2 SSD, they're pretty affordable now, you can get a solid 1t for 100 bucks, HDDs are probably going to be relegated to big media drives in the future.",
      "Yea that's the problem the game is stuttery mess in hdd",
      "Do you have the game installed in ssd or hdd? My brother had the same problem with rx 6700 and when he moved the game to ssd the stutters gone!",
      "Sure, but they're not any more expensive than a SATA SSD, and in some cases they're cheaper, and its futureproofing, because Directstorage will come, eventually.\n\nYou can get a solid 1tb NVME for the same price as a 1TB sata thats also solid, so there's no reason not to go NVME",
      "F, I don’t have any ssd for games and didn’t plan to buy one. Never had any prob for games on hdd like this.",
      "I now have a NVMe SSD and it works fine, no more stutter and its smooth af with ultra settings",
      "Nice",
      "DX12 had a unique way of loading shaders which causes stuttering the first time you load an area. Usually you get a few stutters early then it smoothes out. But if there is some kind of driver issue it might not load textures and have prolonged stuttering. That's just my guess without looking at the bench marks.\n  \nAlso, is resizable bar turned on?\n  \nHave you run DDU and fresh installed drivers?",
      "If you’re in the states they’re going for 50-60 bucks for a 1Tb\n\nJust head over to buildapcsales and you’ll find ton of nvme/sata ssd drives for sale",
      "They should make it explicitly clear that there will be issues when the game launches if it detects slow read and write speeds.",
      "honestly nowdays there's not that much value to partitioning anymore so I would just get a solid m2/NVME and put as much as I can on it.",
      "Realistically the amount of rewriting you would have to do on a modern SSD to seriously degrade it would be pretty unreal. For a solid 1tb NVME drive, you would have to write like, over 600-1000TB of data onto it for it to begin to degrade.\n\nThe lifespan of an SSD is probably better than an HDD nowdays, realistically. Its just not something you're going to have to seriously worry about in terms of lifespan for your system, a modern NVME is going to last longer than most other components in your system by a fair amount.",
      "Samsung 970 Evo Plus is 1tb with dram for 100 bucks\n\nIn comparison, the Samsung 870 Evo SATA SSD 1tb is like, 90\n\nFor 10 dollars in difference, you get insanely faster read/writes on the 970 because its an NVME.",
      "When you say laggy, do you mean like rubber banding or choppy and stuttering?",
      "Hdd of 4tb, my ssd is dedicated to windows",
      "Stuttering",
      "Hdd or full spec? The game is on a 5400rpm 64mb cache wd blue of 4tb. Rest is I7-10700f, A770 LE, 2x8 cl16 ballistix 3200mhz ddr4.",
      "Definitely the hdd indeed, more and more recent games do not cope well with an hdd.",
      "Sounds like DX12 stuttering, but like a million times worse than most people experience. You've probably already tried updating drivers, but that's my first suggestion. I would update them across the board, not just GPU. I'd also run a benchmark after updating them to see if your components are performing in the expected ranges. UserBenchMark is a free option that you could try.\n  \nRandom thought, is your GPU in the first pcie slot? If you installed it lower, it might not perform to specs.",
      "First pcie slot as it is the reinforced one. I dunno, why would dx12 stutter just for the first 2 min of a match then disappear? Stuttering would likely occur everytime and in solo if it was dx prob?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "a770"
    ],
    "title": "Intel Arc A750 & A770 Meta Review",
    "selftext": "- compilation of 11 launch reviews with ~2240 gaming benchmarks at all resolutions\n- only benchmarks at real games compiled, not included any 3DMark & Unigine benchmarks\n- geometric mean in all cases\n- standard rasterizer performance without ray-tracing and/or DLSS/FSR/XeSS\n- extra ray-tracing benchmarks after the standard rasterizer benchmarks (at 1080p)\n- stock performance on (usual) reference/FE boards, no overclocking\n- factory overclocked cards _(results marked in italics)_ were normalized to reference clocks/performance, but just for the overall performance average (so the listings show the original result, just the index has been normalized)\n- missing results were interpolated (for a more accurate average) based on the available & former results\n- performance average is (moderate) weighted in favor of reviews with more benchmarks\n- retailer prices and all price/performance calculations based on German retail prices of price search engine \"Geizhals\" on October 9, 2022\n- for the full results plus some more explanations check [3DCenter's launch analysis](https://www.3dcenter.org/artikel/launch-analyse-intel-arc-a750-a770)\n\n&nbsp;\n\n1080p|Tests|6600|6600XT|6650XT|3050|3060|3060Ti|A750|A770LE\n|:--|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n[ComputerBase](https://www.computerbase.de/2022-10/intel-arc-a770-a750-limited-test/)|(10)|-|-|_124%_|81%|114%|143%|100%|107%\n[Eurogamer](https://www.eurogamer.net/digitalfoundry-2022-intel-arc-7-a770-a750-review)|(8)|-|116.4%|-|-|101.6%|131.2%|100%|108.5%\n[KitGuru](https://www.kitguru.net/components/graphic-cards/dominic-moass/intel-arc-a750-limited-edition-review/)|(10)|95.1%|_110.8%_|-|-|_97.6%_|128.0%|100%|108.4%\n[Le Comptoir](https://www.comptoir-hardware.com/articles/cartes-graphiques/46698-preview-intel-arc-a770-le-16-go-a-a750-le.html)|(10)|93.8%|-|_115.5%_|-|101.8%|135.3%|100%|109.2%\n[PCGamer](https://www.pcgamer.com/intel-arc-a770-limited-edition-review-performance-benchmarks/)|(9)|99.8%|119.3%|-|78.4%|106.8%|-|100%|109.9%\n[PCGH](https://www.pcgameshardware.de/Intel-Arc-Grafikkarte-267650/Tests/A770-A750-Test-Benchmarks-Preis-Release-1404382/)|(20)|-|112.7%|118.0%|72.9%|100.3%|-|100%|107.1%\n[PC Watch](https://pc.watch.impress.co.jp/docs/column/hothot/1445247.html)|(10)|-|-|-|-|_104.2%_|-|100%|110.9%\n[PCWorld](https://www.pcworld.com/article/1341464/intel-arc-a770-a750-graphics-card-review.html)|(11)|98.7%|-|-|-|99.3%|-|100%|106.0%\n[TechPowerUp](https://www.techpowerup.com/review/intel-arc-a750/)|(25)|100%|116%|-|76%|104%|132%|100%|106%\n[TechSpot](https://www.techspot.com/review/2542-intel-arc-a770-a750/)|(10)|99.7%|112.1%|119.1%|75.3%|104.7%|130.6%|100%|105.8%\n[Tom's Hardware](https://www.tomshardware.com/reviews/intel-arc-a750-limited-edition-review)|(8)|95.4%|111.5%|113.7%|72.6%|98.8%|128.4%|100%|111.9%\n**average 1080p performance**||**98.4%**|**113.8%**|**118.4%**|**74.6%**|**102.5%**|**131.6%**|**100%**|**107.9%**\n\n&nbsp;\n\n1440p|Tests|6600|6600XT|6650XT|3050|3060|3060Ti|A750|A770LE\n|:--|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\nComputerBase|(10)|-|-|_112%_|74%|107%|137%|100%|109%\nEurogamer|(8)|-|104.6%|-|-|95.8%|126.0%|100%|108.7%\nKitGuru|(10)|86.6%|_102.4%_|-|-|_93.6%_|124.5%|100%|110.9%\nLe Comptoir|(10)|85.0%|-|_104.2%_|-|97.1%|130.6%|100%|110.1%\nPCGamer|(9)|92.3%|111.5%|-|74.8%|103.7%|-|100%|112.6%\nPCGH|(20)|-|104.2%|109.6%|69.5%|97.0%|-|100%|108.8%\nPC Watch|(10)|-|-|-|-|_101.7%_|-|100%|114.4%\nPCWorld|(11)|86.9%|-|-|-|94.2%|-|100%|108.2%\nTechPowerUp|(25)|87%|103%|-|69%|96%|125%|100%|107%\nTechSpot|(10)|86.6%|98.3%|105.2%|68.7%|94.4%|123.8%|100%|106.9%\nTom's Hardware|(8)|85.7%|102.0%|104.1%|69.1%|95.4%|126.7%|100%|112.7%\n**average 1440p Performance**||**88.4%**|**103.3%**|**107.8%**|**69.4%**|**97.0%**|**127.2%**|**100%**|**109.4%**\n\n&nbsp;\n\n2160p|Tests|6600|6600XT|6650XT|3050|3060|3060Ti|A750|A770LE\n|:--|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\nEurogamer|(8)|-|93.4%|-|-|92.9%|124.3%|100%|110.2%\nKitGuru|(10)|75.8%|_89.0%_|-|-|_96.8%_|132.0%|100%|120.5%\nPCGamer|(9)|80.9%|99.0%|-|68.9%|97.2%|-|100%|112.6%\nPCGH|(20)|-|96.5%|102.2%|69.4%|99.8%|-|100%|117.6%\nPC Watch|(11)|-|-|-|-|_104.5%_|-|100%|123.6%\nTechPowerUp|(25)|74%|88%|-|64%|92%|122%|100%|109%\n**average 2160p Performance**||**78.5%**|**93.3%**|**~98%**|**67.0%**|**96.4%**|**127.3%**|**100%**|**114.6%**\n\n&nbsp;\n\nRT@1080p|Tests|6600|6600XT|6650XT|3050|3060|3060Ti|A750|A770LE\n|:--|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\nComputerBase|(4)|-|-|_84%_|74%|115%|148%|100%|111%\nLe Comptoir|(10)|60.1%|-|_73.7%_|-|101.4%|138.9%|100%|107.3%\nPCGH|(10)|-|80.2%|83.8%|73.7%|103.5%|-|100%|119.4%\nTechPowerUp|(8)|67.1%|78.5%|-|67.2%|93.2%|120.7%|100%|107.6%\nTom's Hardware|(5)|62.1%|73.9%|76.1%|65.2%|93.0%|125.0%|100%|114.3%\n**average RT Performance**||**66.5%**|**76.7%**|**80.5%**|**70.3%**|**100.1%**|**131.8%**|**100%**|**112.3%**\n\n&nbsp;\n\n&nbsp;|6600|6600XT|6650XT|3050|3060|3060Ti|A750|A770LE\n|:--|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\nGen & Mem|RDNA2 8GB|RDNA2 8GB|RDNA2 8GB|Ampere 8GB|Ampere 12GB|Ampere 8GB|Alchemist 8GB|Alchemist 16GB\n1080p Perf|98.4%|113.8%|118.4%|74.6%|102.5%|131.6%|100%|107.9%\n1440p Perf|88.4%|103.3%|107.8%|69.4%|97.0%|127.2%|100%|109.4%\n2160p Perf|78.5%|93.3%|~98%|67.0%|96.4%|127.3%|100%|114.6%\nRT@1080p Perf|66.5%|76.7%|80.5%|70.3%|100.1%|131.8%|100%|112.3%\nU.S. MSRP|$329|$379|$399|$249|$329|$399|$289|$349\nGER Retail|290€|380€|380€|300€|380€|470€|~350€|~420€\nPrice/Perf 1080p|119%|105%|109%|87%|94%|98%|100%|90%\nPrice/Perf 1440p|107%|95%|99%|81%|89%|95%|100%|91%\nPrice/Perf 2160p|95%|86%|90%|78%|89%|95%|100%|95%\nPrice/Perf RayTracing|80%|71%|74%|82%|92%|98%|100%|94%\nofficial TDP|132W|160W|180W|130W|170W|200W|225W|225W\nIdle Draw|4W|5W|~5W|9W|13W|10W|40W|46W\nGaming Draw|131W|159W|177W|129W|172W|202W|208W|223W\nEfficiency 1440p|140%|135%|127%|112%|117%|131%|100%|102%\n\n&nbsp;\n\nSource: [3DCenter.org](https://www.3dcenter.org/artikel/launch-analyse-intel-arc-a750-a770)",
    "comments": [
      "Tbh Props to Intel for making a card better than a 3060 as their first gpu",
      "Last gamers Nexus benchmarks the A770 was right behind the 3070 in some cases. So once Intel fixes the driver issues I  really want to see how it shines",
      "Better seems like too much of a blanket statement, especially with the long list of caveats for the A770.",
      "These cards were manufactured Q1 this year (based on the GN teardown video). Drivers have been worked on since then (if not earlier). And the reason these were delayed as much as they were was because of the drivers. I'm gonna go out on a limb here and say that \"a few months of driver work\" have a high chance of amounting to nothing. It could go either way. I am hopeful, but don't count on it.",
      "So all in all, A770 is just beating RTX 3060 and the RTX 3060 Ti smacks them both around.\n\nSounds about right to me. Hopefully they're able to get drivers better, but I don't have any hope for non-DX12 games.",
      "Getting it into the hands of users is the key to making \"game-ready\" driver updates.\n\nSome things will not fix the issues with pre-DX12 games/engines. However, I will at least give Intel props on this - they've been clear they're looking forward with this platform. \n\nDoes that hurt adoption rates in the short term? Yes. But Intel has been pretty clear that these cards aren't for everyone, but that the development of the platform and the drivers is a forward-looking project.",
      "With a few months of driver work it'll FineWine(tm). In some games it's almost 3070 levels and in 1 or 2 compute tests it was hitting 3080 levels.\n\n16GB variant could be an ML monster for the price.",
      "Even if the drivers get sorted, I don't think that solves the old titles issues? I know they are emulating directx9, so I imagine that will take more than driver optimizations to sort out. Next intel cards might be out before that is fixed.",
      "Holy shit, the idle draw. I've missed that part up until now. That's a no thank you from me. That and a bit too much power draw in general.",
      "Yeah [just look at what 2 years did for their DX11 driver!](https://media.discordapp.net/attachments/682674504878522386/999402021474009198/unknown.png?width=1595&height=897)\n\nOh wait.",
      "same, I noticed it recently too. I thought it's just inefficient at gaming and for casual use it would be ok. Can't buy with this idle / multimonitor draw. If you're European then when long term running costs are considered, it's straight up 6700xt/3070 price point competitor.",
      "True. Maybe 50% fps disadvantage for Arc A700 on older games. But still way over 100 fps.",
      "I don't necessarily think it's locking out older gamers. A decent-spec modern PC with a higher-end Arc card should hit 150+ FPS in CS:GO. Keeping in mind most people also don't run a monitor with a refresh rate higher than 144Hz, I don't think this will make the card completely out of reach for budget/mid-range gamers.",
      "Well now they're under market pressure with a real release out and \"many eyes\" reporting bugs.\n\nTake a look at how stable and fast they are under the open source linux drivers, for example. That's the driver where some tests were hitting 3080 levels.",
      "Valve's own Proton compatibility layer operates in similar fashion to whatever Intel is using, and Proton is sometimes even capable of out-performing native support. I'm 100% confident Intel can make improvements, it just takes time.",
      "As much as I want Intel to succeed in the GPU market, and as much as the feature suite is extremely compelling and fully competitive with Nvidia, there’s really no reason to buy either of these cards when the RX 6600, RX 6600 XT and RX 6650 XT are all such amazing $200-300 options.",
      "Right, and I think it was Steve (GN) that said alot of the older games where it falls behind, it's still plenty fps avaliable.",
      "from the same page no, 6750xt jumps from 7w to 39w (6700xt 33w). it's recent chip so recent drivers too. yet still below arc. video playback is 20w for 6750xt, while arc is at 50w...\n\nhttps://www.techpowerup.com/review/intel-arc-a770/38.html https://www.techpowerup.com/review/asus-radeon-rx-6750-xt-strix-oc/35.html",
      "Right, I am i Europe. So I'd much rather run an undervolted 6700xt for efficiency.",
      "Both Ryzen 1000+ and 8th gen Intel support Rebar. Alder Lake represents a fair leap over ol Sky Lake, and each Ryzen generation improved quite significantly."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "I love Acer's Intel Arc A770, but its driver updates are killing me",
    "selftext": "",
    "comments": [
      "It seems like most of these are related to HDMI (which is explained in the article). I use the DisplayPort and the Intel website  drivers and have had none of the issues reported here.",
      "That would make sense, as ARC's HDMI support isn't quite native.",
      "quasi blower cooler for people with restrictive airflow; like SFF users; so at least half the gpu's heat is getting immediately kicked out of the case. Open case coolers in smaller cases can over heat and/or cause the cpu and other parts to run hot too.",
      "How about a DP to HDMI adapter?",
      "It sounds like this isn't Acer-specific",
      "What's the deal with the double fan situation?",
      "more or less the same idea as the founders edition cards from nvidia",
      "It does native HDMI2.0 but 2.1 uses PCON.",
      "I imagine that's fine",
      "What's even the point to get arc anymore when you can get a Radeon that has higher performance for cheaper?",
      "oh, damn, I bought the Acer version and I'm using LG OLED TV, which is HDMI exclusive, double damn.",
      "I am agree, the software package, incl. updates... need some work, and I have the Intel LE. E.g. the best way to get updates for me are the posts here on reddit.\n\nSerious Intel?",
      "This is why I gave up on Acer years ago, much better vendors out there besides them.",
      "That's why I returned my A770, beautiful looking but the drivers are a mess.",
      "Full disclosure, I tried, and Intel branded arc a770, a year ago. It ran great for a day. Then I upped the refresh a little, instantly got the most horrifyingly loud coil wine I’ve ever heard and it never powered on again after that.\n\nHowever, it is common knowledge in PC hardware from the top experts that Intel has made astounding, leaps and bounds in their drivers in the shortest amount of time in history, including team, green and red. Credit or credit is due. They are doing an amazing job.",
      "Then why do you love it?\nJust get a GPU with proper drivers. \nA GPU isn’t something you want to beta test."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "intel arc a770 le everything runs fine out of the box with i3",
    "selftext": "",
    "comments": [
      "Welcome to team blue!\n\nBe sure to visit us at r/IntelArc",
      "What demon possessed you to still be running Windows 8?\n\nEDIT: I should’ve kept scrolling to the CCleaner screenshot before leaving a comment. I stopped at the benchmark lol",
      "Seems like almost a side grade from a 6600xt",
      "Its okay since it blows in wrong direction lol",
      "Ah that makes sense if you have a use case that overwhelms the memory bus.What made you go 770 over say a 6700xt",
      "Looks like the u9s chromax 90mm tower cooler. Definitely far more then the i3 needs but it's pretty small",
      "it is not because 128bit bus was not good enough for armored warfare game which I play a lot",
      "10th as you can see on ccleaner screenshot",
      "I don't know \njust look at it :)\n6700xt was available for a long time and I have upgraded from 5700xt to 3060ti strix oc on my main rig so for this second one, used for traveling I wanted something I can tinker with and learn something new too",
      "this case was used for several builds in my life as it is compact and not just plastic. \n\ngt740\ngtx950\ngtx1050ti\n1060 6Gb\nrx580\ngtx970\n5500xt\n6600xt\na770\n\ncurrently since 5500xt specs:\nZalman T5 case \nmsi b460m mortar wifi  \nall noctua blackout fans 80mm-120mm\ntrident z 3200Mhz cl 16 actually running at 2666Mhz cl 15 1T\ncorsair 650W modular psu \nsamsung 980pro 250Gb\nwin10 64bit pro\n\nupdated bios \nactivated resizable bar \ndownloaded driver\ninstalled gpu\ninstalled driver \nall is running without issues so far even games I'm playing, so hoping for good fps in upcomming games compared to 3070 with only 8Gb vram",
      "Ah that's fair. I always enjoy tinkering with things to a fault sometimes.",
      "zalman t5",
      "I have always wanted to see how space engineers runs on arc cards",
      "it is strange but ryzen 5 3600 with 5500xt was running better than i3 10100 with 6600xt and this a770 is a surprise I'd say. Transition from 1050ti to 6600xt wih DDU was not without issues and now from integrated graphics it is flawless so far, at least for my use case",
      "What case is this? I really like the side panel",
      "Excited to see what the future holds for intel GPUs! With how shtty nvidia and AMD have been with pricing I would love to see Intel give a highend GPU for a cheaper price and since my CPU is already intel I'd love to have an intel GPU too and go full on TEAM BLUE!",
      "it is due to the clearance and max temp I have ever seen was 56C",
      "oh ok",
      "This is my ideal scenario to try out the new tech if the budget allows - keeping a secondary system to try out new hardware. \n\nI have another system that’s capable and currently running integrated graphics, though sadly I have not the budget now to buy another GPU.",
      "1-2C"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Should I buy an Arc a770 ($350) or wait for Battlemage?",
    "selftext": "Hey all!\n\nI'm planning on building my first pc, but have a bit of a conflict. I want to build an all Intel build, but don't know if I should pull the trigger now on an Arc a770 LE for $350 dollars, or wait for Battlemage to come out next year.\n\nI don't need a crazy build as I don't play games all that much, and would like to keep it under a thousand dollars.\n\nThanks for the feedback!",
    "comments": [
      "Honestly, I wouldn't buy ARC for above $300 anymore. You'd be better off spending $329 on a Radeon 6700XT.",
      "why not settle for A750, you get 92% of the performance at 1080p",
      "Looks like they are all up at like $350 right now. We are starting to enter a dead zone where RDNA2 that was on clearance is drying up ahead of RDNA3. Even RX 6600 has backed off the really nice $180 price point.\n\nRX 6700 is still $280. It's a 25% increase in price to get to $350 for something like 10% performance increase. If you're buying today then RX 6700 may be the move.",
      "You want a $350 GPU in a $1000 build?\n\nJust wait for Battlemage if you insist on all-Intel, Alchemist just doesn't have the raw performance.",
      "Yeah I put together a sub $1000 build with the a770. Hadn’t the Arc series improved by a lot?",
      "Even at $350 a 6700XT is a much better idea than ARC a770 IMO\n\nOr they could increase the budget and get a RTX 3070 ($380) or 4060ti ($399) but I wouldn't recommend getting an 8GB card in 2023 unless you're going to stick to 1080p",
      "He said he wants an \"all\" Intel build so Radeon anything is not Intel so he would not be doing what he set out to do. So that doesn't work with his goals.",
      "It'd probably have a better case if it also had a stronger GPU core. Right now the extra VRAM just a niche benefit for a handful of games at certain settings. In the future games will use more VRAM, but they'll also be more demanding computationally. The A770's performance in non-VRAM limited scenarios is around a 6600XT. That's not impressive for $350, especially when the 6700XT exists.",
      "Yeah it's basically A750 or bust for arc. And even then 6600 is an option at that price range.",
      "I'd imagine the a770 will last longer given it has double the vram",
      "Seems like a long time to wait if you don't have a graphics card to game on already. A750 hit $180 recently which seems like a good deal but A770 has held its price while RDNA2 has come down a lot while RDNA3 is rolling out. Comparatively speaking that makes A770 kind of a bad deal at the moment unless you want a creator card with high VRAM at an affordable price.",
      "Then he's waiting for Battlemage lol",
      "The A770 performs like a 6600 XT.",
      "Honestly depends if you want ray tracing or not. I went from a 6700XT to an A770. DX 9/10/11 games are worse a little. DX12 is slightly faster. RT is a lot faster.\n\nNote that my 6700XT ran at 2800MHz core / 2050 VRAM+if and the A770LE runs at 2750MHz @300W so not settings on either.",
      "Now it’s more like a 7600 or 4060, so maybe like 8% faster than the 6600XT",
      "I don't wanna be rude or anything but you're completely uninformed on Intel Arc 😅\n\nA750 is on par with 3060, even performs better in new titles. A770 is on par with 3060 ti, same story. In terms of compute power both cards are a lot stronger than their counterparts. XeSS is pretty good. In terms of price/performance, A750 is on top of GPU value list. They release new drivers almost weekly and their driver support is unheard of in 30 years of discrete graphics cards industry. \n\nNow those are facts, let's skip to the possibilities. Both A750 and A770 are still not at optimum performance. What does this mean? Intel created a hybrid architecture and even they themselves don't know how to unlock the full potential of it, yet. They're however on the right path, I'm expecting some kind of eureka moment from drivers team, the hardware is simply a beast on Arc.",
      "I personally intend to wait for Battlemage, will use the RTX 2070 I have until then.  \n\n\nAnyone have a reliable source for the latest Battlemage news that they recommend btw?",
      "I heard Battlemage may have software and corresponding hardware to run psth tracing better than amd or nvidia; but do your own research on that one.\n\nAlso im certain you can do better than 350 ive seen them go for 300 a while ago without even looking",
      "I had an Arc A770 for like half a year I think (swapped back to nvidia now); I would not reslly recommend it for your first pc\n\nThere were many issues that were somewhat difficult to diagnose/solve and I am pretty experienced with PC building; those issues have generally lessened with driver updates, but they were not all gone when I swapped GPUs",
      "unless you are collecting them, other wise stay away arc for now. \nI have a a770, it is a great toy, not good if you use it on main pc."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "ARC A770 v RTX 3060",
    "selftext": "At this current timeline, which GPU should I go? \nI am liking the fact that arc is 16gb ensuring future safety. But also so the fact that it requires driver update regularly. \nWhere else RTX drivers are more advanced and going on for a long time. But is 12gb. \nI have seen both are at similar levels, which one should I go for?",
    "comments": [
      "In the long and short term, the RTX 3060 12 GB card will out compete the ARC Alchemist A770 16 GB card for stability and overall quality of gaming, because the first generation discrete Intel video cards have a design flaw that will randomly crash the system and there is no indication when the event will happen.\n\nThe Intel card will have a significant advantage if you compress and decompress videos which is a better quality product that saves business money, and the hardware has at least 16 GB of memory that does better in artificial intelligence programs.\n\nIt depends on your use case as to whether your top priority of use will determine if you can be happy with the hardware you select. I would likely use the RTX 3060 12 GB for gaming while using another computer using the ARC A770 to stream and create videos for the game session. This approach makes the best assets of each video card highlight the advantages of both. The video cards do not exactly occupy the same purpose or use case segment of the market.",
      "100% 3060",
      "RX 7600 8GB vs RTX 3060 12GB - Test in 10 Games [https://www.youtube.com/watch?v=JsvSRSmoi4M](https://www.youtube.com/watch?v=JsvSRSmoi4M)\n\nAccording to steam, the top 9 most popular GPUs currently are all GPUs offering 8GB models. So the time when 8GB is not enough for 1080p gaming is not yet for the next 4.5 years.\n\nI recommend these four RX 7600 models for 1080p gaming.\n\nASUS Dual Radeon RX 7600 OC Edition 8GB GDDR6 V2  \nPowercolor Fighter AMD Radeon RX 7600 8GB GDDR6  \nXFX Speedster QICK 308 AMD Radeon RX 7600 Black Edition  \nPowercolor Hellhound AMD Radeon RX 7600 8GB GDDR6",
      "Finding ARC to be lucrative, but not trust worthy. I’m planing for gaming mostly",
      "If you play solo games and games that can accomplish goals in less than 15 minutes without upsetting people you play with if your game stutters or crash, then you are okay with the first generation ARC cards for gaming. You can use the ARC cards for large games, but the games need to be primarily one person games and linear plot stories, so, that anything wrong with your particular gaming session does not interrupt other gamers experience.\n\nYou will not have a problem using an ARC A770 for games such as Borderlands single player mode, but you are gambling that the game won't crash if you are in a boss mode fight. (Usually this won't happen, because if any stalls happen it's in a normal situation run and it won't feel bad at all.)"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "My Intel Arc A770 build !",
    "selftext": "",
    "comments": [
      "Hey guys ! \n\nJust wanted to showcase my new build, featuring a A770 \n\nThe complete build : \n\n* NZXT H510 Flow\n* Ryzen 7 5700x\n* Intel arc A770\n* 2x 16 Go DDR4 Corsair VENGEANCE RGB PRO SL 3600MHz\n* NZXT Kraken Z53\n* 1To Crucial P3 NVMe M.2 SSD\n* Cooler Master MWE 850w Gold V2\n* Be Quiet MC1 M.2 SSD Cooler Heatsink",
      "I think this is the first time I've seen anyone pairing an Arc with a Ryzen CPU.",
      "It's not proprietary. It's part of the PCIe spec.\n\nhttps://www.intel.com/content/www/us/en/support/articles/000090831/graphics.html",
      "AMD has rebar after zen 2",
      "This is so incredibly wrong.",
      "That looks clean AF. Love it! Hope you enjoy the Arc!",
      "It’s cool how you routed your AIO tubes behind your 24 pin. Seen thousands of PCs on here, YouTube, etc and never once seen that done. 👍🏻",
      "Nice, my new build also has one.\n\nCase: Lian Li Q58\n\nPSU: Corsair SF750\n\nMobo: MSI Z690I Unify\n\nCPU: Intel 13700KF\n\nCooler: MSI S280 AIO\n\nGPU: Intel Arc A770\n\nRam: Corsair 6600MHz DDR5\n\nSSD: Corsair 2TB",
      "As long as the tubes are above the pump, it's ok, no problem there.\nJaystwocents and Gamernexus explained that very well on their channel on youtube.",
      "Not propriety at all.",
      "Hopefully the gpu doesnt get too dusty. Heard they are a pain to take apart. Really sick build though",
      "Great! How's the GPU treating you so far?",
      "What GIF is on your cooler? I need it if you don't mind",
      "Hi, what mobo are you using? You forgot to include it in the list.",
      "Hey I'm interested in this build. How much did it cost and where did you buy the parts?",
      "yea man \\\\m/ here is mine: https://preview.redd.it/dfmxwrla8w8a1.jpg?width=1536&format=pjpg&auto=webp&s=229b1a0110c1b832997947e02c84699cc13ec31c",
      "Why is it every time this is mentioned, people completely miss the takeaway point of the GN video?",
      "I've been going through some benchmarks and to my surprise arc works better with ryzen than with intel cpu",
      "this is 22 days late but thats the NZXT N7 B550",
      "Very nice build, but here is a tip: the AIO liquid cooling tubes need to be on the bottom of the cooler if mounted on the side of the case. It will improve the acoustics and longevity of your cooler. Here is explanation from AIO manufacturer: [Corsair](https://help.corsair.com/hc/en-us/articles/360049358271-How-should-I-mount-the-radiator-of-my-AIO-cooler-)"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "ASRock launches Arc A770/A750 Challenger SE graphics cards",
    "selftext": "",
    "comments": [
      "article says they will be available in japan on the 12th. Other places \"mid july\" so yeah, wouldn't expect them to be available anywhere right now.",
      "doesn't look like it. The source for it posted it on the 5th, and asrock will be shipping the cards starting this month.",
      "yeah, these are new it seems. It makes sense to think it was old though since a770 and a750 are old hat at this point. The article starts out with \"with no battlemage in sight...\". It doesn't sound good to me to have the next generation nowhere to be found and for partners to be launching new versions of the existing stuff this late in the stage.",
      "Can’t be found anywhere, except all the articles talking about the performance gains. They probably bought left over chips for a discount and released some budget friendly cards.",
      "Oh, i probably confused it with some other ASRock arc cards",
      "Isn't this old news?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "GUNNIR launches Arc A770/750 Photon \"Elden Ring\" special edition GPU - VideoCardz.com",
    "selftext": "",
    "comments": [
      "Underpowered, sure. But overpriced is wrong. They have very competitive pricing and for the money the performance is actually pretty good.",
      "man this looks so cool",
      "Competitive is a strong word. Over on newegg there's fewer A750 models for sale than there are 6600s, and the most of them are more expensive.",
      "On the eve of Alchemist's replacement it's barely moving the needle in terms of price to performance while still being less reliable than AMD. An a750 for $200, a 6650XT for $220.\n\nCompetitive is a strong ass word. All I can say is I hope Battlemage is a stronger showing.",
      "The A750 is closer to the 6600XT than the 6600.",
      "GUNNIR cards look cool, if only they weren't underpowered and overpriced intel alchemist cards I would have liked to buy one."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "[ARC] Is ASRock a Good Brand for Intel Arc GPUs?",
    "selftext": "Hello! i've recently been looking for an upgrade for my 1060 3GB, posted multiple posts in different subreddits, looked at RTX 3000 series, 4000 series, RX 6000 series, 7000 series, but then i found an A770 which had a pretty nice price.\n\nThough the main question is - is ASRock a good brand for Intel Arc? since the A770 i found is an ASROCK Intel Arc A770 Phantom Gaming OC Edition, and i'm not very familiar with ASROCK or Intel Arc for that matter.\n\nWhat are your opinions regarding ASRock? is it fine? Thanks in advance!\n\nNote : I have a 12th Gen Core i3 if that matters at all, probably does since Arc ReBar is a beast",
    "comments": [
      "I've had ASRock motherboards in the past and they've been fine. I'd consider them a mid-tier board partner. No frills, budget focused, but decent quality. I haven't heard too many complaints about their GPUs.\n\nThat said, just be aware that the Phantom Gaming A770 only has 8GB of VRAM, while the reference card and the Acer BiFrost have 16GB. I'm not sure how much that will realistically impact performance. Especially if you're playing at 1080p or lower. But a lot of recent games have been getting bad press for high VRAM requirements. The big draw to the A770 is that it is an affordable 16GB/1440p card. So it's an odd thing to cheap out on. Especially considering how overkill the cooler is that ASRock is using. And without the deal Newegg is running right now, it's not that much cheaper than the 16GB models.",
      "I have two of the A380s from them, for about 3 months now with no issues. ASRock in general is a budget brand but seems in the past few years have upped their quality.",
      "It's funny because ASRock was the cheapy-brand that spun off of ASUS.",
      "ASRock is a good brand. I've used their motherboards exclusively for the last 9 years and never had a single failure. Even had a cat piss on one and it survived.",
      "With EVGA leaving multiple markets, ASRock and ASRock Rack have become my new favorite brands.\n\nA380\n\nDeskmeet\n\n5405m low power boards\n\nI've been very happy with all of it",
      "It's OK.",
      "\"OK\" is enough for me, especially since the FE version of that card isn't even available here, thanks!",
      "Which benchmarks are you basing this on? It’s been incredibly hard to find good review data for Arc since its performance is so volatile, but I haven’t found any reviews suggesting that the A770 is anywhere close to the RX 6700 over any large sample of games.",
      "Cat: Also mine.",
      "If AV1 isn't a must, the 6650XT makes the most sense out of all of those (compared to the A770, it offers a bit more performance at 1080p, similar but slightly less at 1440p, but better drivers), unless you want the 2GB extra vram the 6700 offers, although performance increase arguably isn't enough to justify that 27% higher price.\n\nA750 is *kind of* an option, but arguably not with the 6650XT so close in price, unless you want AV1.",
      "No issues on my ASRock A750 besides terrible idle power consumption @ ~35-40w with 2 monitors plugged in with the \"fix\", but that's an Arc thing.\n\nAlthough the cooler on my A750 was larger than I was expecting coming from an RX 570 (243mm for my RX 570 vs 271mm on my A750), I didn't even think of checking the size, but my case was large enough for it.\n\nI'd only suggest Arc if you're fine being a Beta tester, that's what it's going to feel like, March drivers were mostly fine (Ignoring performance related issues, the only thing that was noticeable was some weird flickering on youtube videos when pausing and hovering over them), after that I've been having issues, current beta drivers (31.0.101.4514) half the time I can't use quicksync with OBS (Sometimes I can get it back by restarting drivers a few times) and it's completely gone in HandBrake, previous driver I was having random flickering, that was getting pretty annoying.\n\nWith all that said, I'd not suggest the 8GB A770, if you're in the US it's 28.6% more expensive ($270 vs $210) for, iirc, [~6% more performance](https://youtu.be/xUUMUGvTffs?t=578), and you aren't even getting more vram, if you're gonna get an A770 it should be the 16GB version since it's just not enough of an improvement over the A750 for the price.\n\nIf you're gonna spend $270, you could get the 6700 non-xt, it's faster (At least according to older drivers, but there hasn't been a huge change in performance overall, usually just a handful of games at a time), better drivers, and 2GB more vram, the only downside is that it doesn't have AV1.\n\nAlternatively, the 6650XT is also an option, same 8GB of vram, but faster (At least for 1080p, [it's *barely* slower for 1440p](https://youtu.be/xUUMUGvTffs?t=598)) and better drivers, however it's cheaper, although it lacks AV1, just like the 6700 non-xt.\n\nIf you want AV1, the 7600 is an option, it's a few percent faster than the 6650XT, although it does cost ~12% more comparing the cheapest of each, which makes it pretty bad value for around this price range.",
      "I have an ASRock z690 Mobo, ASRock phantom gaming monitor and ASRock 6750xt. They all work great!",
      "My old computer that I built in 2014 with an ASRock Z97 OC Formula motherboard was still going strong when I built my new computer like 2-3 weeks ago. It was pretty expensive cuz it's made for overclocking, which I don't even do, but I figured the capacitors and everything were higher quality since it's made for overclocking.",
      "There was a time I would have avoided them like the plague, but they've been stellar in my books for a while now. I'm really impressed with their progression in quality, particularly while some other notable brands *cough* ASUS *cough* seem to be back sliding so hard.",
      "Those deskmeets and deskminis are a hoot to play with.",
      "I'm from slovakia, the A750 is about 250 euros, the 6650XT is 260 euros, the 7600 is 300 euros, the 770 8GB is 310 euros, and the 6700 is 330 euros ( and the 770 16GB is like 400 euros )",
      "I wasn't there when it happened but from what I heard the cat pissed on the top of the vented case and onto the board (and video card but that was thankfully only on the backplate). Let the board sit for a while and used some contact cleaner and it worked as if nothing happened.",
      "There's a few redditors that design custom 3D printed chassis for Deskmeet and Deskminis. I'm sure they'd be more than happy to design one with active cooling. \n\n[I use this for my Deskmini X300.](https://www.reddit.com/r/ASRock/comments/yy3i33/3d_printed_alternative_chassis_for_asrock/) The guy uploaded the STLs online for free.",
      "Is 6700XT significantly more expensive? I second that 6650xt looks best of all for the price here so far. Is 6600XT much cheaper tho? :-)",
      "6700XT is about 360-370 but I cannot go above 330, that's the highest I can stretch my original budget of 200€\n\n6600XT isn't available, the 4 choices are:\n\nASUS Rog Strix RX 6650XT OC Edition V2 - 262€\n\nSapphire RX 7600 Pulse OC Edition - 290-299€\n\nXFX Speedster SWFT309 RX 6700 Core - 332€\n\nASRock Intel Arc A770 Phantom Gaming 8GB OC Edition - 316€"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Should You Buy an Intel Arc A770?",
    "selftext": "Includes extensive benchmarks at 1080p and 1440p with the latest drivers.",
    "comments": [
      "What does that mean? This is a discrete gpu.",
      "Given the price, Intel for sure unless I really need day one drivers or CUDA for work.",
      "At this point unless you absolutely need a cheap card and are willing to test the waters it's not worth getting an Arc card really. Battlemage is supposed to launch in a few months, it is targeting 4070S~4070Ti performance and should come with decent amount of memory and affordable price. It's a no brainer if you want to go Intel. A750 is a great budget card, but it launched too late to be worth it, pretty much every halfway decent Battlemage GPU is gonna smoke it.",
      "That’s a reasonable assessment however I am happy to confirm that the drivers work well now, so you are no longer testing the waters. The A770 is a good card, just too little, too late for this round.",
      "They are getting way better about day one support for new popular games.",
      "Rule 5: AyyMD-style content & memes are not allowed. Please visit /r/AyyMD, or it's Intel counterpart - /r/Intelmao - for memes. This includes comments like \"mUh gAeMiNg kInG\"",
      "I was mostly thinking about the A750, apparently there are some minor issues in a few games that occur only when using an A750, the same games run flawlessly on the A770 and the other Arc cards. Maybe it's because this card came out later and Intel didn't have enough time to fix its drivers. \n\n\nEither way the sensible thing right now is to wait for Battlemage or at least get an A770, in my country the price difference between the A750 and A770 is relatively small and it is definitely worth the extra money.",
      "Which rumors are you talking about? I hope it's not from MLID or RGT because these guys are clueless clowns.",
      "odd\n\neverything I saw online was showing Battlemage would be a Q3/Q4 launch",
      "There’s no reason battlemage will release in 2024, all the rumors are showing us another late arrival that will be eaten by  NVIDIA Blackwell",
      "From the rumor mill in general all intel is doing is elaborating on their \"labor of love\" which means don't get your hopes up. Im not saying its been cancelled or even delayed but if you read between the lines Nvidia has shown they are prepared to release Blackwell (2024) and intel has said they are still working on it (battlemage) whatever that means.\n\n&#x200B;\n\nEDIT: FYI intel has still not shown us their Frame Gen technology since it was announced, I would expect to hear more about this first before we see battlemage."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "a770"
    ],
    "title": "Intel Arc A380, A750 and A770 8GB GPUs price slashed, A380 now listed for $120 - VideoCardz.com",
    "selftext": "",
    "comments": [
      "Man, I realy wish to have your prices in Europe",
      "Never buy a promise. The A380 *might* be better than that by now, but there’s no 2023 review data for it, so we really don’t know. At any rate, it has a long-ass way to go before it hits GTX 1650 Super performance, which is really the minimum you should get for this price in 2023.",
      "The A380 still isn’t even remotely cheap enough, considering that it loses to a GTX 1650. That class of product should really be, like, $70 by now.",
      "Wish there was a arc a380 low profile",
      "Cheapest gtx 1650 on newegg is $170 and amazon for $160. A380 at $120 is not bad at all, especially with more RAM, AI cores, and a way better encoder with AV1 support.",
      "It’s nonsensical to blame increasing tech prices on inflation when tech markets have found ways to exponentially improve price/performance regardless of inflation all throughout their history. The A380 is at best *identical* value to a GTX 1650 Super from four and a half years ago. That’s abysmal.",
      "So far this is true, but nobody knows what tommorow will bring.",
      "Uk prices have been comparable with the cheapest European markets (ie Germany & Netherlands) for years  and they still are. Nowt to do with Brexit. The relative strength of the dollar due to their interest rate policies are why prices are higher in europe, including the UK.",
      "meanwhile in brexit britain A770 16gb acer bifrost is gbp 427 = 484 euro = 518 usd .....  or could just get an asus strix oc RX6750 instead.",
      "Brilliant!",
      "will only buy it under $50",
      "Let’s just remember that the GTX 1650 Super launched for $160 *four years ago*, and outperformed the A380 by ~35%. That’s the only benchmark worth comparing new budget cards to, because everything else that’s available in that segment right now is trash.\n\nI’m in full agreement that the A380 might well be the relatively best option in the hellhole that is the post-2020 sub-$200 market, but when compared to the market we should have, it’s a detestable waste of sand. Sub-$200 price/performance hasn’t improved in *four and a half years*.",
      "And if they dont? It's also perfectly possible performance could go down with more stable drivers.",
      "yes indeed. even worse when you compare it to the rx 480 which launced for around €200 7 years ago, and still should be around 25% faster on average.  \nthe rx 400 and rx 500 series gpu's also where insanely good at raw performance/compute tasks compared to other gpus. since while in gaming the difference is only around 25% back then a single rx 480 could easily beat and sometimes even double the performance of a gtx 1080 in cad software and similar compute heavy things that wheren't optimized for a speciffic set of hardware and instead relied on raw performance.  \n\n\nso actually the last 7 years there hasn't really been much advancement in gpu's in some cases the ai or raytracing can be usefull however. but ofcource we have to see how well it works on low end cards, since if it works bad then the raw performance of the old 480 might still manage to beat it in such things.",
      "I mean, Alchemist seems similarly compute-heavy, but point taken.",
      "perhaps it is indeed, I didn't yet see as many benchmarks from it outside of gaming and don't own one right now.  \nif that fully is the case, there might actually be a lot of improvement in price per performance next gen, or alchemist gpu's mught be capable of much more performance(probably won't really see that for most people, but some might experience it).  \nit makes sense since typically raytracing cores can be used quite much like cuda on nvidia, so they are typically capable of quite some raw performance.  \n\n\nso sad about performance per price not going up, but intell arc indeed seems like quite much a good trend in the gpu market, since they push the prices less insanely high. perhaps next gen or such might be a lot cheaper per performance since after all this was the first gen, and so it likely had by far more reasearch and producion cost into it, due to much more reasearc being needed for a completely new line of products, also early on optimizations for reducing cost are also limited. so I by far am more angry at amd and nvidia now.\n\nopenly I hope that Intell actually uses this, since amd and nvidia increased prices so insanely much that even their first gen was a quite good or the best competor on the market despite the pricing being as high as  7 year old gpu which performed the same. this could reduce the amount of money loss on the first gen or possibly even generate a lot of proffit and name loyality so also driver support, which might make a next generation much better in price for performance. I hope.",
      "Yeah, I think Arc has a really high ceiling in terms of raw compute performance, but I doubt it’ll ever get particularly close to that ceiling in games. It feels like a Polaris-type architecture, with more features.",
      "Your arguments make no sense, even not taking features into account. And zero goalposts were moved except by yourself.",
      "You manifestly could get something equal to or better than an A380 in… hm, probably early 2020 for $100. Also, GTX 1650 Super, for $160, 35% faster than the A380, available everywhere. We still haven’t beat that, you know, and cheap cards are supposed to be *better* value than expensive ones.",
      "And if the drivers improve?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "ARC A770 performing worse than an RX570 on the same system",
    "selftext": "First, I acknowledge my system is very old and the processor is weak. I bought the A770 LE some months ago just because I really wanted it (it was being discontinued) and because I already planned a system upgrade by the end of the year.\n\nMy gaming rig comprise of a Xeon E5 1620v2 (slightly overclocked at 4.4 GHz via base clock), 16 GB of DDR3 ram running in quad channel at 1866 MHz and, now, an ARC A770 LE.\n\nBefore the Arc I had an RX570 4GB.\n\nWhat I noticed is that the performance of the ARC gpu is much worse than the one of the RX570, specifically in Overwatch: while with the old RX570 I was able to lock on 144 fps without issues, with the A770 I see fps as low as 80 in some scenarios (usually when there are multiple characters on screen).\n\nAlso the usage of the CPU stays at around 50-60%, while for the GPU is 40-50%.\n\nI really cannot think of an explanation for it: why would a much more powerful GPU perform worse on the same exact system and settings? Could it be a DX11 driver optimization issue? (in Cyberpunk, that is DX12, the ARC is performing as expected).\n\nI was expecting to have at least the same performance while waiting to build the new system...\n\n**PS: thanks to the ReBarUefi project, I have Resizable BAR active.**",
    "comments": [
      "Intel themselves said that running ARC on systems that don't support resizable bar is not recommended and will not give optimal performance.",
      "Try DXVK wrappers for games that dont work.\n\n[https://www.youtube.com/watch?v=wktbj1dBPFY](https://www.youtube.com/watch?v=wktbj1dBPFY)\n\nAnd look at this video maybe it will help you if DXVK wont work.\n\n[https://www.youtube.com/watch?v=hUmwhLMdLk4&list=WL&index=5](https://www.youtube.com/watch?v=hUmwhLMdLk4&list=WL&index=5)  \n\n\nAnd i hope you uninstall old AMD drivers using DDU in safemode.",
      "Yeah, I'm not that brand-loyal either but sometime I'm a bit of a \"collector\": I bought the ARC A770 Legendary Edition mostly because it was an interesting piece of tech and it was getting discontinued. xD  \nObviously it had the kind of performance and functionalities I was looking for but maybe I could've gotten a better deal overall by going for another brand (see performance issues I'm having).\n\nStill happy to have it tho.\n\nAs for Unreal, I've tried making a game many times in the past but I always stop at the point where I start to need some 3d assets and other content to continue: I'm no 3d artist sadly.\n\nNowadays I just have some small projects I do for fun, the last one I started was to try and make an Hack'n Slash combat system on the style of Platinum Games (Metal Gear Rising, Bayonetta, Nier Automata, etc.)",
      "I have resizable BAR enabled tho..\n\nEvery PCIE 3.0 system has the hardware to support ReBAR/SAM, the only thing missing is the UEFI driver for it in the UEFI bios, luckly there's a project that allows users of old platforms like me to add said driver to the bios and have ReBAR working.",
      "Thanks! I didn't know you could use DXVK on windows too, I always thought it was mainly thought to run games on linux. I will give it a try, the only fear I have is if Overwatch detects the modified dlls as cheats...\n\nYes, I uninstalled the AMD drivers with DDU before putting the ARC in.",
      "I bought ARC A770 16G LE because my GTX 1080ti was dying, cant run games over 70% of core usage on it.\n\nI was really interested in first gen intel GPU, new oportunity to learn new things and try new things, i am computer enthusiast.\n\nI found and report few conflicts with drivers and software and now intel use them as advice on support :D.\n\nAt start i was not happy with the GPU, DXVK tweaking for ARC was in diapers, after comunity begin to trying it, it start to be much better.\n\nI learner so much things using ARC GPU and now i love it because it work on 70% of games that i tryed and it is fun to do tweaking with it, and the comunity is really interested and resourcefull.\n\nAnd of course it will be a piece of history and after i replace it with something more powerfull it will be my colectible, it is first gen, it looks nice and it is limited edition.\n\nI was hoping you were making games :D,  i am really interested on small project from independet developers or small studios it can be pixel art games or something more complicated, if it looks good and looks fun i am buying it :D",
      "Hm didn't know that, that's pretty cool. Honestly I've got no idea, hope you fix it.",
      "Borrow a PC from your friends for a weekend and try the GPU there.\n\nIf there's a problem with the GPU (for example it's overheating due to not sufficient die contact) then return it or RMA.\n\nIf it works then look at upgrading your PC.\n\nGo for an X3D CPU from AMD, or wait for the new desktop CPU from Intel next year because they're going to switch motherboard sockets again.",
      "I run ARC card before my new ryzen system on I7-9700k, i had Asus MB and they release BIOS that supports ReBar, it wasnt that great, it shows that is isnt supportet by Intel but it work at least.",
      "Do you know how to use them properly because not every game works, i know how to use basic DXVK but dont know how to implement async DXVK, it need setting file to work properly.",
      "Thanks man, hopefully using DXVK will improve the situation. In the end I just have to survive until I build a new system.",
      "I will only try basic DXVK since async-DXVK seem to have cause some bans in the past due to how it interacts with the game's rendering.\n\nI will probably make a throwaway account anyway just to be safe.",
      "What are you building in future ? :) just curious.",
      "Not have defined it yet really.\n\nIf I go for AMD I will probably choose the 5800x3d to save on RAM: I don't really see DDR5 giving an advantage that justifies the price difference right now.\n\nWith Intel I was looking at the i5 14600k.\n\nIt is true though that those are both pretty much dead platforms. If I went for a 7800x3d from AMD I would have a future proof system at the cost of paying more now.\n\nSo, yeah, I'm still weighting my options.\n\nFor sure I want at least 32GB of RAM cause 16GB are quite tight when I work with Unreal Engine. xD",
      "Looks like you did your research and know what you can afford and what you need.  \n\n\nI am not the tipe of person who will judge for brand choises, i build my systems what is best for the best price in that time for me.  \n\n\nWhat are you doing in Unreal Engine ? if you are making game then i am really interested :D",
      "How much faster is it relative to the 1080ti?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "True Limits of Intel ARC A770 16GB. Extreme Overclocking (XOC)",
    "selftext": "Introduction\n\nI recently acquired an Intel ARC A770 16 GB and I wanted to XOC it as it something that not many have done. I will be talking about my experience and results XOCing an Intel ARC A770. Any mods, software used, and software tricks are NOT endorsed by Intel. Use at your own risk if you want to reproduce anything described here.\n\n&#x200B;\n\nPreparation/Mods\n\nThe first step was of course prepping /modding the card. The first standby mod is to shunt mod the resistors from the power connectors to increase the power limit of the card. However, Intel used an interesting resistor. The resistor has a full metal shroud and is not possible to stack another resistor on it as it would short. I did not want to remove the resistor and add a different at the time, so I just left it as it. Instead, to unlock the power limit, I used the Acer Predator Bifrost trick and increased the limit to 400W. In testing, the card never needed to draw more power than that (this is due to the voltage limitation, more on this later).\n\nAnother mod is soldering cables to the I2C buses to control the voltage through an external controller such as the Elmor EVC. There two I2C interfacing on the A770 and they are on two separate buses. As a result, it required attaching two different sets of wires. The external voltage controller recognizes that there are MP2979 controllers, however when I tried to modify the values they did not appear to do anything. I assume that Intel has some sort of lock on it. If anyone has more information on it, write it in the comments.\n\nSince, nobody makes a LN2 pot or a plate adaptor for the ARC series graphics card; I had to manufacture my own using a 3D printer. Despite being plastic, the plate worked wonderfully and survived the experience to be used again in the future.\n\nFront of PCB: [https://i.imgur.com/O0F4y5T.jpg](https://i.imgur.com/O0F4y5T.jpg)\n\nBack of PCB: [https://i.imgur.com/XnBgLC1.jpg](https://i.imgur.com/XnBgLC1.jpg)\n\nLN2 Pot Mount: [https://i.imgur.com/c40ylPY.jpg](https://i.imgur.com/c40ylPY.jpg)\n\n&#x200B;\n\nARC Temperature Bug???\n\nWhen the A770 dropped below 0˚C, HWiNFO showed that the GPU core temperature reached a whopping 255˚C. This temperature would keep decreasing, despite the error, as the card kept getting cooler. So, if the Pot was at -38˚C, HWiNFO showed the GPU core 221˚C. It seems like there is some kind of overflow error and the firmware on the card does not know what to do if the card goes below 0˚C. I am not sure if this was Intel’s intension or a bug, I am leaning towards a bug though. If someone knows more write it in the comments. If it does end up being a bug, hopefully they can fix it in the future.\n\nTemperature Bug Below 0˚C: [https://i.imgur.com/ErQGYZU.jpg](https://i.imgur.com/ErQGYZU.jpg)\n\nTemperature of Card at -38.4˚C on Pot: [https://i.imgur.com/hykBwXb.jpg](https://i.imgur.com/hykBwXb.jpg) [https://i.imgur.com/0SfmHVM.jpg](https://i.imgur.com/0SfmHVM.jpg)\n\n&#x200B;\n\nFrequency VS. Temperature VS. Voltage\n\nAs many popular overclockers have stated in the past, the ARC series drops frequency as the voltage increases. However, there is another part that no one has seemed to mention yet. The A770 will also drop frequency based on the temperature. During the XOC session, if the card dropped below 8˚C, the frequency would drop 50MHz and this trend would continue as the temperature decreases. When I set the desired frequency to 2900MHz and cooled the pot to -70˚C, the frequency dropped to 2200MHz. Another interesting relationship is that if the voltage was dropped and the temperature kept the same, the frequency would increase again. But the card would fail to run any benchmark due to low voltage for the specified frequency.\n\n&#x200B;\n\nSetting a Record\n\nWith the power limit of the card unlocked and using ARC OC TOOL (NOT AN INTEL APPROVED SOFTWARE), the frequency was fixed to 2880MHz and the voltage was fixed to 0.941V, which is approximately 1.010V actual. Any more voltage and the card would down clock the frequency. Also, due to the Frequency VS. Temperature behavior, the temperature had to be controlled so that the GPU core would not go below 8˚C to maximize the frequency. It took a few tries to make it through with these settings; however, it eventually did make it through a Port Royal run. The final Port Royal score was 8,277 with a 2,845MHz, which is a world record for this benchmark and card combination. Unfortunately, this is all I could achieve without the ability to add more voltage, cool it more, and no ability to overclock the memory on the card.\n\nRecord Link: [https://www.3dmark.com/pr/2334224](https://www.3dmark.com/pr/2334224)\n\n&#x200B;\n\nThoughts and Conclusions\n\nThis was a fun and interesting experiment XOCing an Intel ARC A770. I wanted to write about it for a couple reasons:\n\n1. To see if the “temperature bug” is a bug or by design.\n2. To give some exposure to XOCing an Intel graphics card and hopefully the next series of cards they will be more flexible on what can be controlled and modified.\n\nIn conclusion, I hope that Intel allows more flexibility in XOCing their graphics card in future like they currently do with their processors. Thank you all for reading!\n\n&#x200B;\n\nSession Pictures:\n\n[https://i.imgur.com/a6jbnxI.jpg](https://i.imgur.com/a6jbnxI.jpg)\n\n[https://i.imgur.com/Uhqg0NO.jpg](https://i.imgur.com/Uhqg0NO.jpg)  \n \n\nThe system used for testing:\n\nCPU: Intel i5-12400\n\nRAM: DDR5 32GB 6000MHz\n\nMobo: Asus ROG Strix Z690-F\n\nGPU: Intel Arc A770 16GB\n\nGPU Driver: 31.0.101.4314\n\nGPU Pot: KINGPIN Cooling TEK-9 ICON EXTREME V5\n\nOS: Windows 10 22H2",
    "comments": [
      "That is a really cool idea. Congrats on achieving a WR! That 255° C temperature bug is hilarious though, lol. Does something like that happen when XOC'ing Nvidia or AMD GPUs?\n\nIn any way, I hope Battlemage turns out amazing - Intel could dominate the budget segment with cheap CPU+GPU combos.",
      "Try posting this to r/overclocking too, I think they'll be quite interested in these results",
      "I think the numbers looping is because it uses 8bits of memory for showing the temperature so when going negative it loops round to the maximum value. 8bits of memory has 256 possible states shown as 0-255.",
      "Nice, more people trying new thing like this should always be encouraged. Well documented, and keep up the good work!",
      "Nvidia and AMD GPU temperature readings are different in XOCing. Most Nvidia cards stop reading the temperature once the core hits -40°C and will just hold it there even though they are actually colder. For AMD, at least on the 6900xt, in a De8auer video the temperature read 65521°C, which is funny. At some point in the future, I do plan on XOCing a 7900xtx.",
      "It seems your core isnt very good. I could score 2800 at max vf points in all the heavy benches on stock cooling just low ambients. (~50C loar temps) and judging by other peoples results its not that good of a core. \n\n\n\nBought the card for the similar reason - to play around with it, so finding out that it only provided 2 evenings of OC shenanigans was quite upsetting. Voltage lock is the biggest shame.",
      "That ARC Temperature bug is such an Intel thing (in a funny but positive way in my head) - they used an 8-bit positive integer for temperature.. lol \n\nOP - so awesome you did this experiment!  Thank you for sharing",
      "Fantastic writeup and looks like a really fun bench session!\n\nI really wish these cards had more adjustability, but they're so friggin' locked down that it sucks a lot of the fun out of them.\n\nVoltage, power limits, clock scaling, fixed memory clocks...\n\nThere's a little bit of frequency adjustment for the core and a small amount of power curve/PL headroom, but they're *very* conservative and you're still essentially at the whim of the VBIOS. The fact that you threw LN2 at the thing and still only beat the second place score (done at ambient) by *66 points* paints a picture at just how stubborn it is.\n\nThe multiple I2C part is pretty funny, I'd never really looked into the PCBs too much but now I kind of want to. Between things like that, the wonky OPROM, bolted on HDMI PCON and the functionally separated RGB controller basically treating the card like an ARGB fan, it's kind of amazing that it functions at all - but I'm glad it does!\n\nSide note: Does the Predator OC panel trick work with any drivers other than 4314? I tried to get it to trigger the PL glitch unsuccessfully when I tried a while back, but I didn't spend too much time tinkering with it and am pretty sure I was on an earlier driver version at the time. If it worked without issue for you maybe I'll give it another go to test some power scaling stuff.\n\nETA: This is my top PR score for the moment, done under the stock 228W PL (A770 LE) and unfortunately held back ever so slightly by the 5700X, which was interesting. Normally PR doesn't give a hoot about CPU, but it still seems to make a tangible difference with the A770.\n\nhttps://www.3dmark.com/pr/2239288",
      "I will!",
      "Thank you for reading!",
      "Hahaha yeah.",
      "Did it run Crysis though?",
      "Yeah that would make a lot of sense.",
      "Thank you!",
      "Yeah, it was 2 evenings for me of OC shenanigans as well. The core did not seem too impressive and with a better core I could see someone beating my score. Voltage lock is a letdown, but also the fact I could not cool it more to make up for it.",
      "Yeah the card is really locked down and, in the end, it really comes down to the bin on the card as you can't do any tricks to get beyond it.\n\nOn 4314 I ran into some issues using the Predator trick. But then I read somewhere that you should not set the limit beyond 400W, or it will glitch out. So, I set mine at 398W and I had no issues. Have not tried it since 4314 however. \n\nHmmmmm that is interesting to know. I normally do PR since a CPU does not matter as much like you said and that fact that I currently do not have a higher end CPU on my bench.",
      "Oh so one could say that Intel has the best temperature reading mechanism for XOCing. After all, you can just calculate 356-(temp reading) and that number in negative is your actual temperature 🤣."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "a770"
    ],
    "title": "Intel Arc Graphics A750 + A770 Linux Gaming Performance Review",
    "selftext": "",
    "comments": [
      "They seem solid. We just don’t get insane frame rates on ancient games.",
      "Pulling for Intel, currently have an all AMD build but I plan on upgrading once I get a 4K monitor that meets all my personal requirements.. hopefully by then (a few years) Intel will have a high end card out and their drivers are fully sorted out, I’d love an all Intel build.",
      "I'm really rooting for Intel, and it feels good to see some competition, as the 3060 is mainly crushed and being sold for way more. However, AMD has the high ground on ~$300 with the 6600 and 6600 xt's, that's some crazy value for the money that can hardly be beaten.",
      "That performance in dirt rally is weirdly high",
      "I don't think anyone buying a budget card should care about raytracing.",
      "Unless you actually like Raytracing. That's one place the ARC series seems to justify its cost vs AMD",
      "Wednesday the 12th from the looks of it",
      "tbh,i think the cards could go even higher than 6600xt.",
      "Buying parts with the promise of future updates is almost always a bad idea \n\nHowever, in the case of ARC where the biggest issue seems to be drivers I would imagine these cards will age well. But we’ll see. \n\nMaybe they’ll copy AMD’s old FineWine technology lol",
      "launch day is october 12th",
      "I’d like to know how dx9 titles run as on windows it uses an translation layer IIRC so how would something like proton compare?",
      "Raytracing is not a gimmick - it's not an Nvidia creation, it's a lighting technique that's been around for some time (e.g. CGI film creation) but only relatively recently been possible to render in real time. \n\nWhether it's good value or not is entirely different, and certainly less relevant to lower priced GPUs, but it is something that is here to stay and will be improved upon by all 3 companies over time.",
      "I was watching the LTT Livestream, and it did give me hope, but it did also highlight issues with frame consistency.",
      "Then it wouldn't be constructive criticism. The article seems to answer some questions I had, therefore it was valuable to me. I have issues with Phoronix's website though:\n\n1. site stopped loading for me on page 4. Honestly, it could be Internet or my phone. But I suspect the website.\n2. I in the past I tried to give Phoronix some money, but their website makes it a hassle to do so. You have to do a bunch of steps, that I just aint going to do. I like things like Patreon sites. I recall I set up a forums account but ran into issues.  And loss interest in figuring it out.\n\n![gif](giphy|sDcfxFDozb3bO)",
      "Also the AV1 codex encoding support seems to be a selling point for a handful of people who want to stream.  I believe the RTX 3060 decodes but not encodes. The RTX 40xx also encodes. I imagine AMD's next card will also.\n\nOf course, i didn't read the whole article to see if Phoronix tested this with Mesa (page wouldn't load). It would suck if you buy if for that and it doesn't work.",
      "Considering AMDs OpenGL compatibility has been a mess through the entire time I have owned my rx580 I don’t have particularly high hopes for older APIs",
      ">Unless you actually like Raytracing\n\nRay tracing is only relevant at the high-end unless you enjoy playing games at 20fps. \n\nMore realistic, AV1 support is a nice thing to have.",
      "on linus tech tips they ran some games on a770 recently and TF2 and Half Life 2 were same performance as RTX 3060.",
      ">With the current market, it doesn’t make sense for buyers of mid range cards to care about raytracing.\n\nEspecially on AMD'S cards, where it might as well be broken.",
      "Raytracing is a gimmick from Nvidia to sell things for higher price. With the current market, it doesn’t make sense for buyers of mid range cards to care about raytracing."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "[GN] Intel Arc A770 16GB Limited Edition GPU Review & Benchmarks",
    "selftext": "",
    "comments": [
      "They *did*, that's the problem. These card should have been out in *January*, but were held back to fix and rework the drivers.",
      "I'd take MLID with an enormous grain of salt. He seems biased against Intel in some way in regards specifically to Arc, and I speculate he's trying to manipulate stock prices to some end.\n\nIt's late to market, but they legitimately launched a 3060 competitor, at a low price.\n\nFor their first real attempt at a GPU, that's a win.\n\nOnce the drivers are cleaned up, they just have to scale the compute up, and it's smooth sailing.\n\nAlso remember that DIY AIB is *not* their target market for Arc. Their target market for Arc is replacing nvidia in laptops so they can sell cpu/gpu bundles to OEM's. DIY and desktop is such a small % of the market to Intel.",
      "They should have delayed this, but it's at least a start for a new branch of cards in the market.",
      "MLID has ruined any and all Arc discussion whatsoever.",
      "It makes literally 0 sense for Intel to ditch this project not even 1 release in. One of the major points of Arc is to make sure Intel doesn't get edged out in the laptop market further by AMD bundling a CPU+GPU. Them giving up on Arc is literally them losing laptop marketshare.",
      "As a developer working specifically on graphics, it's interesting and I intend to grab an A750 or A770 Limited if I can find the latter. Considering some DX12/VK titles punch near 70-class, I'm inclined to believe that's the \"true\" performance of these cards and they're being held back by drivers. Definitely want to throw some Vulkan stuff at it and try to tune things to see how they behave. I'd also like to see some of the DX11- titles on DXVK. I actually used DXVK on my 6900XT to get better performance in God of War and Baldur's Gate 3. Curious what it does for Arc.\n\nThat being said, it is unfortunately true that these are not ready for the average consumer and most would be better served by an RX 6600 or RTX 3060. I do hope Arc isn't dead after these like some say. If Intel comes out swinging with Battlemage, things could get interesting.",
      "Check out OneAPI if you haven’t",
      "> I wonder how bad it was a year ago.\n\nMinesweeper sub 60fps",
      "The launch was for March 30; the invasion started February 24. If you want to believe 5 weeks would have saved them, that's a choice.",
      "I like GN since he uses 4K benchmarks. A real GPU bottleneck test.",
      "Hard to belive that such early alpha level was released... even for benchmarking review - truly embarrassing...\n\nI can understand all level of dev.; testing of HW, SW, drivers tuning etc., but more should be done before release, while still huge homework to do. \nAnyway, I am keeping fingers crossed, it will be interesting to see Intel as competitor for Nvidia and AMD. Good luck!",
      "They delayed it for almost a year. This is the result. I wonder how bad it was a year ago.",
      "Also didn't help a significant amount of Intel's driver team was based in Russia and the Ukraine invasion nuked any sort of Q2 launch anyways....",
      "I'm okay with the performance. I knew these weren't going to be the best. I just need them out already.  Let them get out there and see what problems we can improve on. I like how they look so going to get one.",
      "There's no way Intel abandons GPUs, if only because of their necessity to remain as a viable competitor in the data centre space.\n\nAnd likewise, there's no reason for them not to make consumer-grade GPUs in that case, because they desperately need a way to bring in revenue to fund AXG and to do that only through data centre/HPC requires Nvidia-levels of marketshare, which they obviously don't have.",
      "They will continue to improve it.",
      "These is not for General Audience for sure. I think Raja should setup a War Room with Driver team and keep working on all open issues. Basically those who buy this are doing testing for Intel across different setups :-)\n\nI am looking forward to raptor lake reviews.",
      "check first gen amd and nvidia gpus and you will realize what intel indeed achieved. In near feature I expect intel to be real nvidia and amd competitor.",
      "Not sure about windows, but the GPU isn't that great when it comes to Vk either. https://www.phoronix.com/review/intel-a750-a770-arc-linux/4",
      "There's a lot of nuance when it comes to Vulkan. Tons of ways to accomplish the same goals. I'm curious to see how it performs with some custom code. Just out of curiosity really."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Intel Arc A770 16GB and Arc A750 8GB available now on Newegg",
    "selftext": "Looks like the listings are now active, and the GPUs can be added to your cart and checked out. Have fun, everyone!\n\nA770 16GB is $349.99, with the A750 8GB at $289.99.\n\nEdit: It's now showing out of stock for the A770, but I'll leave this post in case they bounce back in and out throughout the day.",
    "comments": [
      "Grabbed one!",
      "Already sold out for me.  I hate this so much.",
      "I think 750 would be the card to get if you wanna support intel. However, if you are on a tight budget and want the most from your pc, 6700xt is 360 now, and 6650 is 270.",
      "They made millions of units.\n\nTBD if they just sold through them all or if it's a trickle.",
      "https://www.anandtech.com/show/17263/intel-arc-update-alchemist-laptops-q1-desktops-q2-4mil-gpus-total-for-2022\n\n\"Intel is expecting to ship over 4 million units/GPUs for 2022\"\n\nThat was published in feb, the same month TSMC production ramped for these. Meaning it's probably an accurate accounting sourced from Intel.",
      "([Availability showing on my end](https://imgur.com/a/db6j5mH))\n\nLink to Newegg landing page, if you didn't grab it from the other posts:\n\n[https://www.newegg.com/promotions/intel/22-1809/index.html](https://www.newegg.com/promotions/intel/22-1809/index.html)\n\n[Link to A770 16GB](https://www.newegg.com/intel-21p01j00ba/p/N82E16814883001?Item=N82E16814883001&Tpk=14-883-001)\n\n[Link to A750 8GB](https://www.newegg.com/intel-arc-a750-21p02j00ba/p/N82E16814883002?Item=N82E16814883002&Tpk=14-883-002)",
      "No Canadian Newegg joy for me :(",
      "I'm seeing the same thing. It's possible that they're going to go in and out of stock, as I have a hard time believing it would have completely sold through this fast. Unless the stock consisted of whatever Ryan Shrout could fit in his car's trunk to drop off at Newegg HQ this morning. There are also the ASRock A770 and A750 cards which have not yet appeared in listings, although the LE cards look much cleaner.\n\nETA: ASRock cards are listed, but not live yet.\n\nA770 **8**GB: $329.99\n\n[https://www.newegg.com/asrock-arc-a770-a770-pgd-8go/p/N82E16814930077](https://www.newegg.com/asrock-arc-a770-a770-pgd-8go/p/N82E16814930077)\n\nA750 8GB: $289.99\n\n[https://www.newegg.com/asrock-arc-a750-a750-cld-8go/p/N82E16814930078](https://www.newegg.com/asrock-arc-a750-a750-cld-8go/p/N82E16814930078)",
      "It makes no sense to me either. I mean, there can't be this much demand, especially when Nvidia/AMD cards are all in stock. Something doesn't seem right, unless they launched with 10 cards in inventory.",
      "I can see the A750 and that hasn’t gone out of stock at all.  I was able to add A770 to my cart but went out of stock again.",
      "Had one in my shopping cart but it was gone before I pulled my credit card out of my wallet.",
      "I was able to place a back order too but the 19th has come and passed with no change to my order’s status. Any luck for you?",
      "To be fair that was including both the laptop arc cards and their add in cards for prebuilts(like NUCs). I really don't think there is really as much volume left over for DIY as we would hope.",
      "How is this out of stock??? I've been checking since 8:15 est and did not see the a770 listed as available even one time....\n\n Intel didn't even officially announce where they were selling these units unitl 9am est this morning.....\n\nWhat a joke of a launch",
      "I managed to place an order for an A770 when it was listed as a back order. Newegg took my order.  My order says \"release date: 10/19/2022.\" Maybe that's when Newegg thinks they'll get the next restock.",
      "Are there any retailers up here who have this up yet?",
      "I agree with you. I woke up this morning a little late (Intel never said what time it was launching) I think it was around 11AM and there was nothing. No way these cards are that popular. Intel probably made 100 total for the whole US.",
      "My suspicion is either they have yield issues, which would explain why there seemed to be many more A750s available, or they’re purposely slowing supply so they have more time to improve the drivers.\n\nThey won’t be slammed with as many bad reviews and tech support problems if they release the supply slowly.",
      "Yeah, there's some BS going on. How is it sold out, can't be that popular when there are tons of AMD/Nvidia cards in stock.",
      "Canada Computers has it listed, but OOS for now: https://www.canadacomputers.com/index.php?cPath=43\\_557\\_5769"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Unfortunate experience with Arc A770",
    "selftext": "Now, to preface this, I have to say I still really love the software and the Arc cards in general. I would like to still use an Arc A770, provided I can get one that actually works this time.\n\nFor the first time in my life, I've got a fried graphics card on my hands.\n\nGot my A770 a week or two ago after being a longtime Nvidia user, and came to love the card's value and software. However, when I installed it, I had two problems; either games would crash outright, or the computer would go to a black screen and just restart in the middle of gameplay. This was extremely annoying. I thought that maybe it could be a power supply issue, but my Seasonic Vertex GX-1000 is more than enough for the card.\n\nI checked, made sure Rebar was enabled, Above 4G encoding was enabled. I thought that maybe my old Ryzen 3900x was not compatible, so I installed a Ryzen 5500 in my board. Still had black screen shutdowns and game crashes.\n\nI thought, maybe Windows 10 was not compatible. I bought an SN850X and installed Windows 11 on it (didn't want to get rid of all of my old data on the old NVMe). Still had black screen shutdowns and crashes.\n\nI said, alright, I wanted a motherboard upgrade for a while anyway. I bought a new MSI Pro Z790-A Wifi DDR4 motherboard with a 13600K processor, because maybe my old board was bad, and I figured an Intel processor would pair better with the card anyway. \n\nReinstalled Windows 11, made sure Rebar and Above 4G were enabled, and I still had black screens and game crashes. I also noticed something interesting about 20 minutes ago.\n\nDuring my gameplay of the RE4 demo when the card was functioning, the framerate was rapidly decreasing and I couldn't figure out why. I tried reinstalling drivers, but it didn't help. I checked the card temp with the overlay. It was 90 degrees! How is this possible, I thought. I have a Phanteks Enthoo Pro 2 full of Noctua A12 fans, there is no way it's coming close to overheating.\n\nI looked in the case and saw the RGB on the card no longer functioned, and the fans weren't spinning at all. I smelled the card and it smelled of burnt plastic. It still functions, but has no cooling or RGB. Pulled the card out and it was burning hot to the touch.\n\nHave to submit the RMA, which I'm sure won't be a big headache. I just wish I was able to play RE4 Remake on launch day, definitely not happening now. Oh well.\n\nLike I said, still love the card and the software and I will gladly use another, provided the next one is functional.",
    "comments": [
      "I RMA'd my A770se more about 2 weeks ago because the aRGB stopped working. I didn't have any of your other problems though in the 2 months of ownership. Still haven't heard back if it'll be fixed or replaced.. bought from eBuyer here in the UK. I did get an email 10 days ago saying they had received it, I guess I'll just have to wait.\n\nGood luck with yours.",
      "Why was this downvoted? lmao",
      "Because ranting about drivers and reviewers when it's clearly a hardware issue is off-topic. OP just did an unfortunately bad job at diagnosing a broken GPU. For the record, I didn't downvote it myself.",
      "Because reddit",
      "I just had exactly the same problem. When I got the card I installed the rgb cable and turned off all colors. Then I removed the cable and uninstalled the rgb software. 6 weeks later the fans stopped and the card was in the 60s with no activity. Just for kicks I reinstalled the rgb cable. When I tried to install the rgb software it told me it could not find an ARC card. ARC control showed the card. It seems that some controller for the fans and rgb went bad. I also submitted an RMA request to intel",
      "Wow that's pretty unlucky. Sounds like you've spent a fair bit in recent times. \nHopefully they can rma and get you a new card. \nWhile i didnt but an arc card, id strongly consider it in the future. I hope they can improve and become dominant in the market",
      "tease subtract intelligent reminiscent jellyfish tender innate cover birds wide\n\n *This post was mass deleted and anonymized with [Redact](https://redact.dev)*",
      "Why did you pair it with a 4th gen Intel lol. It doesn't support ReBAR, so even if it worked, you'd get extremely low FPS",
      "I had suspected that my motherboard was at fault because it previously had many blue screens trying to enable D.O.C.P. for my RAM, and I wanted to upgrade to something newer and larger anyway because the board was an ITX board in a massive case, and it was showing its age. I was going to upgrade it anyway at some point so I figured now is the perfect time.\n\nI couldn't rule out how the board performed with Above 4G and Rebar enabled on my other card either because those settings were completely foreign to me until the ARC program told me to enable them, and I had already sold my buddy the old card.",
      "The problem is likely with your motherboard. Check for the most recent bios to see if it has an update for Intel GPU. I doubt it though.",
      "Something that i have learned (not sure if its similar) older intel motherboards (i had the same gens paired with a gtx 980) seem to prefer DVI. My newer board (same card) will prefer HDMI. maybe as your card doesnt have DVI it wont give a signal at all",
      "It was not intentional, my existing GPU was showing issues that could be thermal/age-related so I wanted to see if the card would output in my existing environment.",
      "Thank you for your experience with this, I too have an Arc and will be fitting it into a new system as I wish to upgrade everything as my hardware is ageing.\n\nMy issue is that I tried to use the DP to connect to a newly purchased monitor as part of a 2-monitor setup and the monitor does not receive a signal. I put the A750 into a LGA1151, Z97 motherboard with a 4th gen Intel, and when the monitor reported that it didn't receive a signal, I did basic troubleshooting. I tested the cable on an independent environment (which was good) and the card ports (which also worked) so either that is the DP port of the 2016 monitor (a Prolite XUB2492HSN) OR it cannot operate in a dual monitor setup using HDMI and DP (which I haven't tested yet as I wanted to get the DP port on the card to function)\n\nLike you, the green and red options do not appeal, and I suppose that we have to live with issues using bleeding edge stuff.\n\nThanks for sharing, I would be interested to know how your situation improves.",
      "thanks for the suggestion - will do. \n\nHowever, I might just wait until I get a new MB / CPU.",
      "Very interesting possibility as my current card uses and operates 2 monitors on mini-HDMI and DVI.\n\nIs it possible that to use DP it must be activated on the MB - and if so, I cannot seem to find the category that would be in. Any ideas?",
      "Try maybe using the iGPU (could have to be activated in BIOS). I have tried changing the default and haven't had success.",
      "Your experience is pretty typical. When professional reviews with a million + hours of testing and troubleshooting under their belt struggle with drivers, what did you expect? Everybody from Gamers Nexus, Hardware Unboxed,and PCWorld all have issues with their drivers. All I can say is the best play is to pick up a rx6700xt for cheap and run with that. Alternatively, set your expectations lower and expect some of your games to not work and try to stick to ones that do until they work out the driver issues.\n\nOP, it's obvious that your problem is 2 part. Software problems and physical problems, Dead card. After you RMA, just replace it with something else and sell your RMA card afterwards or use it in your backup build. What's the point of getting a card that fails to run games some of the time due to driver problems. The whole point of a discrete you is to run games.",
      "And has had that before also or up the power slider first!",
      "Bump the voltage a bit probably undervolting stock!"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "How do I buy the now intel arc a770?",
    "selftext": "",
    "comments": [
      "Centralcomputers.com has them now",
      "Hi there! Thanks for shouting out to us! We have Intel ARC A750s and A770s available now for sale! Feel free to check it out on our website! \n\n&#x200B;\n\nhttps://www.centralcomputer.com/catalogsearch/result/index/?cat=55&q=Intel+ARC",
      "Currently, our only MW2 Game promo is with the purchase of an Intel Core 11th and 12th Gen i7 and i9 CPUs. However, we are currently in talks about securing future promotions with Intel. We will definitely update our website and social media channels when this happens.",
      "No prob! Love going to your stores!",
      "Newegg have A750 in stock right now.",
      "Does purchasing from CentralComputers include the MW2 game promo?",
      "Thanks, I’ll check it out",
      "Hey there! Thanks for letting us know about the price difference, typically we do price competitively against our competitors but we will certainly share this with our products team and go from there. As for not being an Authorized Dealer, we can reassure you that we are an authorized Intel reseller and have been for a very long time.",
      "Hey there, I have gone ahead and notified our products team about the pricing difference between us and our competitors. As for now having the other bundles, we are in talks with Intel right now to see if we can be part of future promotions from them.",
      "I’m not complaining personally. If I wasn’t able to get an a770 on launch day I would have no issue paying the extra to get it locally from you guys!",
      "Appreciate the support! FYI, we don't even make $50 on these cards. Margin is low. Well under 10%.",
      "With money, usually.",
      "https://game.intel.com/story/intel-arc-graphics-release/",
      ">Centralcomputers.com\n\nBut the price is $50 higher than newegg and not bundled with MW2 + Adobe",
      "I wanna get the best in the series because I have more then enough for it, thanks anyways",
      "Okay where can I buy",
      "Central has always had a small price hike… but they are my local store… so they get a shoutout",
      "I have both models, it's literally single digit gains between them, a770 not worth imo.",
      "Computer shop.",
      "Thanks again for shouting us out! We do typically price competitively with our competitors, however, I will go ahead and share the price difference with our products team and see where it goes from there. Thanks again!"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Purchased an Arc graphics card, pleased with the results!",
    "selftext": "Backstory is that I wated to build a Minecraft PC for my nephew from spare parts, so I had to free up a GPU in the process. I decided to buy an Arc A770 to replace the 1070 in the Media PC, so I could put the 1070 in said nephew's PC.\n\nI've been plesantly surprised by the Arc's capabilities. I had to install the latest Beta drivers to even get it to install (Win 11 on the Media PC), but apart from that I'm seeing pretty good results in most games I have tried. Sure, there is a lot of driver work and improvements to do but so far I haven't seen any major issues, at least from the standpoint of a Media PC connected to a 4K TV.\n\nI think it's good there is a 3rd player in the market and I really hope that Arc becomes a bigger thing as time goes by. Some random photos below.\n\nArc in the Media PC: [https://i.imgur.com/XZP0ZsR.jpg](https://i.imgur.com/XZP0ZsR.jpg)\n\nArc being all lightey: [https://i.imgur.com/o7ZcCji.jpg](https://i.imgur.com/o7ZcCji.jpg)\n\nArc running Stray: [https://i.imgur.com/kL1tCVZ.jpg](https://i.imgur.com/kL1tCVZ.jpg)\n\nIf I had a message to Intel I'd say this is a great first attempt into the GPU market, it represents good price to performance, keep at it. It's extremely hard to make GPUs, and I think perseverence will pay off in the longer term.",
    "comments": [
      "Glad to hear you’re enjoying your new toy! Thank you for sharing 👍 \n\nI’m excited for intel GPU future also.",
      "well, it's not perfect, there graphics glitches in some games, and Resident Evil HD Remake and Resident Evil Zero dont run for me, but other than that it's not that bad as some say, specially with DirectX 12 and Vulkan games.",
      "It’s actually performing great in most games that I play. The only game I had issues with is battlefield 2042, but I wasn’t too fond of that game anyways. So it wasn’t a major loss.",
      "It has fine driver support for most games, and poor driver performance in a few games.",
      "Battlefield has problems with Battlefield 2042",
      "I have 0 issues at 4k resolution on mw2 with Intel arc. Runs smooth as butter. What's funny is some reddit clown bashing something other people are reporting works fine because of a YouTube benchmark. Reddits full of know it all's!",
      "That is one beautiful GPU",
      "I love my A750 and it does perform better than my 1080ti. But the 1080ti is the gpu that never dies. It was/is a really great gpu. If you’re happy with what you got right now just keep the 1080ti. Otherwise if you’re looking for a worthy upgrade I would consider the next Gen cards from either Nvidia or AMD.",
      "It falls right in place in the performance to price ratio. I’m happy with my 1440p full ultra with XeSS set to quality. Really smooth.",
      "Arc A750 16 GB doesn't exist. All Arc A750 cards are 8 GBs. Depending on where you live the Arc is way more worth it. Not everyone lives in the UK. In Canada, the RTX 3060 on sale is $500 CAD + 13% sales tax. In terms of Price to Performance the Arc is way more worth it.",
      "Given how well theyve performed this early on, i think intel is going to be a major competitor. There was never any doubt that theyd cause a disruption, but i think itll be a bug one, which is good.",
      "How does it compare to the 1080ti? I’m still limping along with this 6 yr old gpu",
      "391.97 UK including tax & shipping",
      "Same here. I'd like to know because a $350 upgrade isn't bad.",
      "They did say it was for their media PC, so might not even be used for games.",
      "Big one for sure. Once they get the software and drivers optimized to the hardware it’ll be huge. I’m super happy with my purchase.",
      "Not massive enough upgrade compared to a 1080 Ti to be worth it yet, though with time, drivers should get better to the point it would be worth it.",
      "Classic EA!",
      "LOL what a joke comment.",
      "Find me a single reason why it’s better for the price than a 200$ Rx 6600."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Why is the Arc A770 so much slower than the Radeon RX 6750 XT",
    "selftext": "\nOn techpowerup’s website it says the A770 has similar theoretical performance to the Radeon RX 6800 XT but in relative performance they say it's worse than the Radeon RX 6600 XT. With all the extra cores and other hardware the A770 has I don't understand why it preforms so bad. What am I missing? Is it because the A770 doesn't have any L3 cache or does it and techpowerup doesn't list it? Why is the 6750 XT so much better for gaming? Is the software the only thing holding the A770 back and how?",
    "comments": [
      "Driver and hardware design.\n\n&#x200B;\n\nIts hardware in testing seems to be a monster at parallel compute, scaling well with resolution, but does poorly with latency focused applications.\n\n&#x200B;\n\nDrivers don't help either.",
      "I'm not sure what theoretical performance they're referring to, but even in ARC's best case scenarios it's a 3060ti in performance. It's rumored Intel was aiming for 3070 performance, but failed. It's performance is usually just a bit better than the 3060 and 6600XT.\n\nThey probably could extract a higher level of performance from the GPU, but it would require lifting the power limits to absurd levels. \n\nIt's worth noting that in Ray Tracing, ARC performs better than Radeon in general.",
      "It does not generally perform like a 3060Ti. Those are edge cases.",
      "2x A770 encode faster than a 4090. Nvidia just 'unlocked' better encoding on their latest drivers. just sayin.",
      "Basically, compute tasks and gaming tasks are very different things. The A770 is a very large die, almost matching the 3070 Ti, but there’s some sort of fundamental flaw in the architecture and/or drivers that doesn’t let it use all those shaders consistently for gaming.\n\nAlchemist feels like Polaris all over again - its compute performance is great, but that doesn’t mean its gaming performance will ever be.",
      "Driver and hardware design",
      "I mean aiming for 3070 and hitting 3060ti is like giving up 5% at most so they did pretty well for a first shot.",
      "Fine wine™️",
      "If you don’t mind a technical read, then Chips and Cheese’s [analysis of the A770](https://chipsandcheese.com/2022/10/20/microbenchmarking-intels-arc-a770/) sheds some light as to why it’s not as good as it should be.\n\nBrief summary: struggles compared to the latest AMD/Nvidia GPUs when shader occupancy is low; can’t fully utilise internal bandwidth as well as the competition; worse instruction latencies, too. Drivers play a part, of course, but they can’t make up for fundamental architecture design choices.",
      "Driver.",
      "Wrong subreddit to ask this since you're gonna get the r/Intel apologists, but the reality is Intel has a brand new architecture and brand new drivers without any development. \n\nEven IF their hardware was theoretically capable of high end performance, it would take years for the drivers to mature and catch up.\n\nEven in 2023, AMD is FAR behind Nvidia in driver maturity.",
      "haha, we can all dream",
      "I've heard people say that their A770 performs just as good as their 3070 or 6700xt. It's really depends on the game. \n\nLTT did a full 30 day review (by Linus and Luke) where, after the latest driver updates, they didn't have anything bad to say about the performance. They still swapped back to their Nvidia top tier cards, but A770 wasn't supposed to be a top tier card.\n\nI think in the next few releases, Intel will have, or be extremely close to having a top tier card. I really hope they get it figured out.\n\nI will say that if I didn't already have a 3070. I would have picked up a A770 16GB card. They are almost half the price of my 3070 when I bought it.",
      "When you look at tests it might not perform as well as more expensive cards, but save the money, you don’t really notice the difference.",
      "It's closer to a RX6660XT or an RTX3060TI\n\nBut really depends on the game as sometimes it matches a RX6750XT or quicker than a RTX3070 \n\nAnd most importantly the drivers and driver updates, for example it seems to be really slow in games like CSGO, until the drivers were updated and now it's much much quicker\n\nIntel will give it some fine wine",
      "Yeah the drivers are pretty bad for dx9, so they pretty much used a translator that uses Vulcan instead of dx9. Kinda odd, but it seemed to work.",
      "The a770 won’t be slow for long. Intel just needs to figure out driver stuff and then it’ll be sweet",
      "A770 looks towards the future DX9 to 11 games has weaksauce performance, they did improve it a bit a few months back but buying this you really wanna focus on DX12 performance and consider DX9 and 11 games as secondary.",
      "Unfortunately, due to money woes Intel won’t be continuing their fledgling discrete graphics journey it seems. My guess would be the tech learned will possibly get into better APUs. Even this is a big who knows. If it were me, I’d try to keep at least that part going because AMD wipes the floor with them in this market. Look at Steam deck, PS5, Xbox (both series). Not only have they been the hare from the old story, they let their tortoise get so steady it’s cost them in all markets. Before i get accused of being some AMD fanboy, which honestly I am as well as being an Intel fanboy, my most current system i built around the 13700k. My latest AMD built system is a 3900x i think. The two tech companies i dislike the most are Apple (snobby hardware, no repair, specialized everything that can’t be replaced outside their eco-system which is tons of steps back from the rest of the industry) and nVidia. TBC, I use nVidia cards mostly and enjoy them, but they are total ROI whores now. It’s kinda like they really have the same thing anyone else can but they only sell to high end clients snd have proprietary “Like a Virgin” tech, get my drift? Totally off their rockers.",
      "To add, we really don’t want any of these companies to fail. Intel, especially, can fabricate in the USA which is a big deal, although their nodes have seemed to be problematic for a time now. We could end up in a situation where only Intel sells to us or supports our efforts in the USA and that is more important than anything else if/when."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Intel expected to lift Arc A770 GPU review embargo in early October - VideoCardz.com",
    "selftext": "",
    "comments": [
      "review Oct. 5 but with a release dater of \"very soon™ \" ??? Surprised they can say that with a straight face when they said 1Q22 then Spring then summer and now \"very soon\".  LOL",
      "I mean, it’ll be an interesting product, but I won’t be able to seriously recommend these to people.",
      "Schrodinger's GPU",
      "See you in 2023?",
      "Expect these to be ass. But that's not the point guys. When Nvidia and AMD cards came out..they were trash. With in time, I expect them to get better. The thing is Intel has to stick with it\nThat's going to be the test. Everyone is going to point and laugh at them but within the 3rd or 4th generation I expect a real 3rd competitor.",
      "This doesn't really make any sense. Intel said they'd release their skus starting from the bottom with the a380 then working up to the a770. What of the a580 and a750, will they just release review embargos on those at the same time too then?",
      "> Everyone is going to point and laugh at them but within the 3rd or 4th generation I expect a real 3rd competitor.\n\nThe question is, has Intel the perseverance to sustain such a long haul and financial burden?\n\nOr are they going to cut losses and call it a day? Remember that Gelsinger expressively stated, he not only WANTS but NEEDS to exit business or at least will knife lossy divisions to cut losses. Intel hasn't the money power anymore.",
      "I'm definitely interested in them. I use Linux and currently have an all AMD setup with a Vega 56 that's starting to show its age. I need to see how it attacks up in perf/$ to RX 7000 first of course. I really want to see some one do some in depth testing with DXVK to see if it circumvents their driver issue with older graphics apis",
      "At what prices?",
      "While I'd love for Intel to be competitive in the GPU market, it's obvious that won't be the case this time around unless they are willing to sell these cards at Costco (or even below them). I have a really hard time imagining someone choosing (as in not sticking with the option for your prebuilt) to go with ARC with its inferior drivers in older games and with less features than even AMD, especially with Nvidia and AMD's next gen right around the corner. Hopefully next gen doesn't suffer from the problems ARC has had with its \"Q1\" launch.",
      "Imagine if the a770 gets destroyed by AMD/Nvidia xx50 class next gen card, lol...",
      "Picked up a FE 3070 for $300 and an RTX 3070 MSI for $350  off Marketplace. The used market has dropped substantially and I feel as though Intel completely missed their window",
      "They are going to be very niche for anyone that needs high double precision but can’t afford enterprise cards.",
      "Presumably. I'm guessing the 770 was singled out because it's direct from Intel, not an AIB.",
      "Gelsinger will quietly strangle Arc graphics division to death long before we reach 3rd gen. \n\nThe whole graphics release has been an unmitigated disaster for Intel.",
      "If anyone has the money to burn on a 4000 series, they were never looking at Arc in the first place. But with the 3000 series dropping lower and lower, that kills Intel's window for the 200-500 market. And nobody's sure what rabbit AMD is going to pull out of their hat.",
      "It will, when they release them 1-2 years from now, since they need to move old stock",
      "The x50 class from nvidia/amd next gen will be yet another rx580 with lower power draw or a 2060 at 2060 launch MSRP.",
      "Intel has the money the problem is Intel is run by the damn shareholders.  Business lose money all the time but Intel is always a shareholder first which sucks",
      "The simple answer is “it won’t”, top ARC is basically an RX 6650 XT competitor at this point."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Will intel arc a770 le go on sale below 300$ again?",
    "selftext": "I’ve been looking for different Gpu’s for my build and I wanted to buy the a770 le because of the looks, I remember it being on sale on amazon (even after it was discontinued) brand new below 300, but now it’s not there anymore. It either has to be this gpu or Ill have to go with another brand, unfortunately other versions of this gpu look goofy to me.",
    "comments": [
      "I'm looking at two on newegg right now that are below $300.",
      "Thats too bad. For what its worth I agree the limited edition was the best looking design.",
      "Unfortunately it is discontinued. You’ll need to find used most likely at this point.",
      "What a bummer",
      "Its not discontinued, just the first party ones. You can still find a few board partners. Acer and sparkle come to mind. But I think there's another 1 or 2.",
      "Yeah but not le (limited edition manufactured by intel)",
      "Those two look very immature for me unfortunately",
      "Those two look very immature for me unfortunately",
      "I get not liking the look of the rgb cards. But you don't like the sparkle titan or the asrock challenger?",
      "They don’t even come close to the original intel design from a  mature and simplistic style perspective. To each their own :) I’ll be going with a different gpu unfortunately",
      "They don’t even come close to the original intel design from a  mature and simplistic style perspective. To each their own :) I’ll be going with a different gpu unfortunately",
      "They don’t even come close to the original intel design from a  mature and simplistic style perspective. To each their own :) I’ll be going with a different gpu unfortunately"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Arc A770 16GB ($349) replaces Quadro RTX 5000 16GB (~$2299) in CFD",
    "selftext": "The Intel Arc A770 LE 16GB shows solid OpenCL performance in [FluidX3D](https://github.com/ProjectPhysX/FluidX3D), a bandwidh-bound  computational fluid dynamics software, going head to head with the Quadro RTX 5000 16GB and beating the RX 6900 XT 16GB. Of all devices tested so far, the A770 LE is #2 in price/performance after the GTX 1660 Super 6GB.\n\nhttps://preview.redd.it/4udbdlpgwpz91.png?width=1776&format=png&auto=webp&s=217fb3f2b4229a2b9d0789be72c4f8d6b95e94e3",
    "comments": [
      "What is the source of the results tabulated and your claim regarding price-to-performance? Are these your own values?\n\nFor example, in this chart, a GTX 1080Ti and RTX 2060 Super have higher MLup/s and are cheaper.",
      "A lot less impressive when a 2060s is better than both. Is the quadro much more expensive than the arc? Yes, but that's clearly not its intended purpose since a much weaker GeForce card beats it.",
      "A GPU is a GPU, a vector processor with fast memory attached; they don't have an \"intended purpose\". Quadros have never been faster than their GeForce counterparts, but their (typically) larger VRAM capacity allowed for price gouging. With much cheaper gaming cards now also getting 16-24GB, all but the highest-end 32-48GB Quadros are obsolete.",
      "I've done most of the benchmarks myself, and some are submitted by users, such as the A770 results by [RawMango](https://twitter.com/mango_raw/status/1591451934241980417). This is a purely VRAM bandwidth-limited algorithm (like most compute applications), and on top it's non-cacheable, so FP32 performance and cache are irrelevant for this particular application. Only VRAM capacity and bandwidth matter. For price/performance I assume MSRP prices, see [here](https://twitter.com/ProjectPhysX/status/1591458548437090304) for the full plot.\n\nMLUPs/s means Million Lattice Point UPdates per second, or how many grid points are computed per microsecond.",
      "Studio. There is no more Game Ready drivers for Quadro.",
      "Look closely, the Ti is only \\~5% faster, due to its slightly faster VRAM (1008 GB/s vs 936 GB/s). The top 3 bars are A100.",
      "No idea why people still restrict themselves to a proprietary ecosystem with CUDA/HIP/Metal, just to run into porting trouble on the next (super)computer with hardware from the other vendor.\n\nThis wouldnt be a single % faster if I used CUDA, because it's already at the bandwidth limit. OpenCL is exactly as fast and efficient as CUDA, and on top it is compatible with literally every GPU from every vendor since \\~2009.",
      "512.78\n\nI wouldn't expect any difference between different driver versions though. 80-82% efficiency is already as close to the roofline curve as it gets with that mix of coalesced/misaligned memory access.",
      "Most of them go head to head with the GDDR cards, depending on bandwidth and efficiency of architecture / memory controllers.\n\n\\- GCN: Vega 64 performs rather poor at 50-60% of 484GB/s, Radeon VII is faster at 40-70% of 1024GB/s\n\n\\- CDNA2: MI200 is only \\~50% of 1638 GB/s\n\n\\- Pascal: P100 are at \\~68% of 549/732 GB/s\n\n\\- Volta: V100 is exceptionally efficient at \\~88% of 900 GB/s, Quadro GV100 is only \\~60% of 870 GB/s\n\n\\- Ampere: the A100s dominate everything at \\~60-80% of 1.5/2.0 TB/s",
      "I'm comparing the A770 to the expensive RTX 5000 because both have 16GB. Memory capacity limits the maximum grid resolution you can simulate, so both cards have the same capabilities in CFD. With a 2060 Super or 3060 Ti (both 8GB) you can only do half the grid resolution.\n\nThe only workloads where mid-range Quadro is faster is when the software artificialy cripples itself it detects \"GeForce\", such as nicely demonstrated by [Siemens NX and CATIA](https://youtu.be/z14bPZcwHck).\n\nThe AMD cards just have poor VRAM bandwidth to begin with, and can't handle full load on the memory controllers as a legacy of GCN. The infinity cache can compensate the slow VRAM very well in games, but in non-cacheable compute workloads it's basically useless.",
      "Whoops",
      "This commercial is so incredibly cringe, nice find on that one. Also amazing work writing your own CFD tool. Pretty impressive",
      "True, but I still think the comparison and the way you worded the title is kinda misleading. The arc is faster than a much more expensive card but just because said card is extremely overpriced. It would have been more fair to compare it to a GeForce GPU, like the 2060s for example which is much closer in price.\n\n\nAlso I said \"intended purpose\" just because I assumed such an expensive card would have a significant advantage over gaming cards at least in a few scenarios, otherwise who the hell is going to buy them at that price. But honestly it wouldn't be surprised of Nvidia scamming people, and I still don't know any workload where quadros consistently beat their gaming counterparts (ignoring vram bound situations).\n\n\nP.s. just a curiosity, any idea of why new amd cards (6000 series) perform so badly? I guess it's the fact they have only gddr6 right?",
      "How do HBM2 cards do in these tests? Vega 56's etc?",
      "Quadro on which drivers?",
      "Let me add that OpenCL isn't used as much as other alternatives, specially in the industry, where Quadro cards reign supreme due to nvidia's CUDA technology.",
      "It's not that you restrict yourself, but the software you use does.",
      "How is the 3090 ti so far ahead of the 3090?",
      "Presence of A100 distorts the chart and makes the difference between various consumer GPUs less clear. CPUs, mobile GPUs, GPUs older than 5 years should have been in another graph",
      "Yes, but studio or game?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Acer Arc a770 for $319",
    "selftext": "Over at B&H. I already own a 4080 but a recent gpu with 16gigs of vram at that price is insane. I also heard the drivers for arc cards are a lot better now so I would definitely grab one if I needed it",
    "comments": [
      "Sure, why not especially if you are adventurous person.",
      "I have both versions of the A770 for multimedia production. March/April was a bit rough, but it's all good now. Especially for media work. Gaming, I can't comment, but they do update the drivers every few weeks now according to what people report.",
      "I'll admit I went team Blue and got the A770. After awhile the drivers beta where terrific but then random things would not work, suddenly I cant use the camera like I used to before, now REsizableBAR would not be recognized as enabled even though CPU-Z and my BIOS reflected it did. I decided to flip to the 4070FE and I have not regretted it.",
      "As far as being difficult to work with in terms of gaming, I'd say not very. I got the A770 LE because of the price, built my first CPU with it, and it's been running great. \n\nI've only had two problems with it, and both were easily solved. Fallout 4 had cruddy performance, which was solved by installing a DXVK mod. XCOM 2 wouldn't start on Driver 4514, which I solved by going back to the stable branch of released drivers. \n\nAny game running on DX12, the A770 will be able to run just fine. Older games (pre-DX11) are sort of hit-or-miss, but if we're being honest, those games are hit or miss regardless if your OS is Windows 10/11.",
      "It was on sale of even less at newegg, I think right now it is (I'm not US)",
      "Did you try using a regular driver? Beta drivers are a bit iffy sometimes but switching to a stable one solves the problem",
      "That's not a bad deal.... too bad it wasn't yesterday. There was a powercolor fighter 6700xt for $299 on Amazon, and it came with Starfield",
      "Is it that difficult to work with it? I'm seriously thinking about getting it for like a backup card or just to try it out. The price isn't a problem for me but I just want to know if I should get it or not. It's a beautiful card and I've read the drivers are improving with each release so it's tempting not gonna lie",
      "What's the direct competing card to this from the other two?",
      "If you already have a 4080, what do you need the A770 for? The 4080 is much more capable",
      "Well it would be used for gaming but the bro is right. I don't really need it my 4080 runs circles around it but I'm just hella curious about these arc cards. They look fantastic and the specs are good for 1080/1440p and the drivers are improving so yeah a 16gb card at this price is a no brainer for anyone looking for a gpu to complete their build. It's a cheaper alternative to what's already out there from both AMD and Nvidia",
      "6700 and 3060",
      "Do you need a second GPU? \n\nIf the answer to that is yes, then it is a really great but as it can perform a lot of work in the background.\n\nIf you just want to try it out, try making some videos about your experience and being an active member of the arc beta program by reporting issues.\n\nOtherwise, it's kinda pointless to buy it just cause when you are rarely ever going to use it anyway.",
      "I decided not to get it I already have an awesome gpu and I don't want to be selfish and take away another card unnecessarily from someone needing one. My 4080 should last me for a long time so I'm content with it, for now"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "What is the AMD and Nvidia equivalent (when looking at performance) of Intel Arc A770?",
    "selftext": "",
    "comments": [
      "Depends on the application.\n\nIn gaming, it sits between RTX 3060 and RTX 3060 Ti, or between RX 6600 XT and RX 7600.\n\nIn memory-bound OpenCL compute, the A770 punches well above it's weight, and is about equivalent to an RTX 4070 or RX 7800 XT.",
      "In (practically achieved) VRAM bandwidth they are similar, and that is what matters for compute workloads.",
      "Intel's initial projection of the ARC Alchemist A780 or A770 GPU was to equal or surpass the Nvidia RTX 3070 or AMD's RX 6700XT.\n\nThey never made their video cards come close to beating the RTX 3060ti nor the RX 6600XT.\n\nThere are two aspects of why the Intel video cards did not and/or could never achieve their stated goals. The GPU chips they manufactured has a design latency flaw, and the second problem is that the company did not have enough experience and development of the video drivers used by the graphics.\n\nThe A770 will never surpass the performance of the RTX 3060ti 8 GB nor even the RX 6700 10 GB because of design flaws built into the hardware.\n\nIntel never issued the A780, but they released the A770 and A750 where both cards should be able to beat the RTX 2060 Super and RX 6600 8 GB while gaming with the latest games using RE-Bar (Smart Access memory).\n\nIntel's video cards use emulation software to utilize older programs, because the design and video drivers of the GPU was never created to use older software prior to 2020, which means programs that need to use protocols for DirectX10 and prior are slower than they should be. The performance is acceptable now due to improved video drivers, but some of the modifications will not be liked by some. They increased the FPS rate by increasing the stutter and 1% low frequency rates on many games, so, you sacrifice smoothness for increased overall speed.\n\nIf for any reason you cannot use Smart Access memory for any type of use with the Intel video cards, your performance will be slower by more than 10%.\n\nYour performance experience will be based on what programs you use, and if the video drivers have an optimum adjustment to perform well using that application. On a price basis, an Intel A770 purchased below 240 dollars provides decent value, because you are buying something that potentially performs above an Nvidia GTX 1660 with a goal to do as well as an RTX 3060 12 GB card. As for comparison with AMD, the card games above an RX 6600 but it has a hard time when it can process workloads to perform above an RX 6700XT. There are some programs that the Intel A770 has difficulty using such as any software that uses DirectX 9 and prior. For those programs, your performance will likely be at the GTX 1050ti level or lower (such as the GTX 960).",
      "http://www.3dcenter.org/artikel/fullhd-ultrahd-performance-ueberblick-2012-bis-2023 says between 6600XT and 6650XT for AMD, between 3060 12GB and 4060 for Nvidia at 1080p. At 4K, the 3060Ti GDDR6X and 6700XT should beat it.",
      "Don’t listen to the haters, it’s between 3060 and 4060. Also slightly above the 6600XT. Probably one of the best GPU’s at the price range if you plan on playing newer games, as Intel drivers are less reliable with older ones. \n\nI don’t know why you are asking this, but if it’s for perspective on the card you should buy it’s this. \n\nAMD still reigns supreme in the low end GPU scene, though Intel is a very good alternative if you  primarily play games newer than 2019. It has issues (though mostly resolved) with older games, and is still much cheaper. It’s a constantly evolving NEW platform, so there may or may not be issues but it does seem like they are fixing anything they can. If you’re making a budget build and you are really trying to see where you want to make cuts, I think Intel arc is a great alternative. \n\nI don’t think Nvidia GPU’s are worth it in the budget range unless you buy used. It would take a bit of justification as to why you would need a budget gpu in a system that is also probably lower quality. If you are building a completely new rig and need it to be budget, Nvidia is out of the question a bad pick.",
      "Is 7800xt an equivalent of 4070?",
      "I come from the future and I can confidently say my A770 outperforms my 3060 Ti\n\nThey’ve immensely improved their drivers",
      "You have a zotac Nvidia then.",
      "Same here."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Is ARC A770 Compatible with B450M",
    "selftext": "I'm thinking to buy an ARC GPU but not sure if my motherboard works just fine with it. Is there driver or performance problem with B450M?   \nI'm thinking of buying 6650XT (They are the same price in my country) but 16GB VRAM makes me think of ARC A770. By the way I will buy Ryzen 5500 or 5600 in future. Want to make sure everything just works fine.",
    "comments": [
      "Get the A770, it will only get a better with driver updates. and having 16gb of vram it should last too.",
      "IMHO it depends on what you play and what resolution you play at.  The 770 is best at 1440P, and could potentially last longer due to higher specs and RAM.  The 6650XT does OK at 1440p, but best at 1080p.  YMMV, but I'd go for the ARC.",
      "The board needs to support resizable BAR (REBAR). Do some research on your board to see if it does.  It will work if it does not, but you do not get all the performance of the card.  If it does, you're good.  Otherwise the 6650XT is a baller card for the money.\n\nEDIT:  additional details!",
      "The guy who is selling the card told me to check. He said that it won't work with some motherboards.",
      "I just checked mine and it seems I can enable REBAR. Do you think A770 is worth it? As I have seen on youtube it seems like I will face random fps drops and low performance in older games. Should I just go with 8GB AMD? I'm expecting to use my new GPU next 5 years.",
      "The a770 is borderline PS5 level performance. If you're gonna hold on to your GPU for a while, I think everyone agrees that 8gb is not enough. ARC 16GB is the better option",
      "Wow I wasn't expecting to see such result with RT on. Thank you for your comment. I think I will go with A770 - Ryzen 5600 combo.",
      "Thank you for comment. I checked with GPU-Z and it says supported. I think I will buy A770 but still have some questions like why the card works so hot in youtube videos.",
      "I've had more issues with amd cards than my a770. Rx 570 only used 4 of 8gb vram, 5600xt green screened and black screened, R9 290 straight up died on me. HD7870 was the best amd card I ever had, got a 1060 6gb after that and every time I've used an amd card since then it has been no bueno.\n\nThere's some weird little issues with the arc, but I've been using mine since just before Christmas. It's actually ridiculous that 3 amd cards have caused me more grief than the first gen arc, it was not an opinion I expected to have after 6 months.\n\nIt's as good if not better than my 3070 laptop gpu (don't judge I didn't buy a gpu mid pandemic, I'm the winner here lol) meaning games like control or red dead 2 the arc tends to do better because of the dx12 and high vram, but I played Half Life 2 and it was much smoother than it was 6 months ago, and I made a game with unity in dx11 and that went fine. \n\nBiggest peeve with arc is that it has issues turning on the monitor or will throw a momentary error regarding the hdmi output, but when the monitor says 'there was a problem with your hdmi device' and it works just fine a split second later, you're more confused than anything.",
      "You will probably need a bios update if you are getting a 5600 anyway, so yes, you should be fine.",
      "The fan curves was adjusted (for the better) in the last driver update.",
      "why shouldn't it be compatible?",
      "The card works fine without ReBAR support. I installed an A770 in my girlfriend's PC using her as a guinea pig of sorts to test the Intel GPUs. Her build is AMD and doesn't support ReBAR. She has a 2560x1440 144hz monitor.\n\nWhile you would get a performance uplift with ReBAR, the card does deliver around ~130FPS in games, such as Deep Rock Galactic, with ReBAR off.\n\nThe drivers started off quite rocky but they have definitely improved over time thus making the Arc series a great budget friendly option.\n\nTLDR; ReBAR is a recommendation, not a requirement.",
      "Intel arc a380 user here and i have a b450 msi tomahawk mobo with a ryzen 3 3300x with rebar enabled and the arc380 handles like a champ. I purchased the card for $101 at microcenter with 2 year warranty included and i have yet to see a game stutter or have any issues in 1080p with that card. Forza Horizon 5 in 1080p high settings with ray tracing and this card plays well over 70 fps without intel xess. With intel xess i get maybe 5-7 frames more and really makes the game visually stunning on my oled c1.",
      "' the card does deliver around ~130FPS in games, such as Deep Rock Galactic, with ReBAR off'\n\nPlease always mention resolution",
      "> As I have seen on youtube it seems like I will face random fps drops and low performance in older games.\n\nIf you play a lot of older games, it might be better to go with Radeon or Nvidia. You'll have to use google to see if the games you play are impacted, unfortunately, there's not really a compiled list of \"games that have problems with Intel ARC\"",
      "Sorry about that, it's 2560x1440.\n\nI'll update my comment."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Acer Arc A770 Predator BiFrost GPU with 16GB VRAM drops to just 259 USD - VideoCardz.com",
    "selftext": "",
    "comments": [
      "Still £400 in UK and almost €400 in major EU markets 🥴\n\nVery good strategy to make the 4060Ti and 7700XT look like good value. They're doing the AMD move. Only lower price in US and maybe in a couple months it'll be lowered in other markets",
      "Yeah EU and UK pricing is not very attractive when it comes to Arc cards. The A580 probably won’t be available for another month or so in majority of EU just like what happened when arc initially released. Still not see any listings anywhere.",
      "Really where it should be for an inconsistent rival of a last gen x60/x70 tier.",
      "wtf",
      "For anyone who doesn't know, with last weeks updates it now run starfield moderately well at 1440",
      "Yep, the title says USD not EUR or GBP. Hope that helps."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "a770"
    ],
    "title": "Intel Arc Graphics A580 / A750 / A770 Linux Performance For Early 2024",
    "selftext": "",
    "comments": [
      "This subreddit is in manual approval mode, which means that **all submissions are automatically removed and must first be approved before they are visible**. Your post will only be approved if it concerns news or reviews related to Intel Corporation and its products or is a high quality discussion thread. Posts regarding purchase advice, cooling problems, technical support, etc... will not be approved. **If you are looking for purchasing advice please visit /r/buildapc. If you are looking for technical support please visit /r/techsupport or see the pinned /r/Intel megathread where Intel representatives and other users can assist you.**\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/intel) if you have any questions or concerns.*"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "a770"
    ],
    "title": "Intel confirms upcoming Arc desktop SKUs: A770, A750, A580, A380 and A310",
    "selftext": "",
    "comments": [
      "Did they also leak in which decade they'll be released? /s",
      "No way. It's 3070 level hardware with unpolished drivers. Expect 3060Ti real-world performance.",
      "Good luck guys.",
      "Intel should’ve been very clear about the release timeline. I think they still can be transparent about it.",
      "IIRC the high end is supposed to be up around 3080 tier",
      "This century. Stay tuned.",
      "They already went transparent with it. \n\nThey told that early summer desktop GPUs will be launched as OEM only in china. \n\nThen in late summer launch OEM only worldwide. \n\nThen \"later\" launch as standalone GPUs.\n(So one could expect October/November for this)\n\nOverall it seems like they really really want to have most of the driver issues sorted out before releasing standalone GPUs.",
      "Do we have any idea how they will compare to existing gpus yet?",
      "There are low end GPU out there, it is just the value is so bad that people ignore it. With possible bad driver due to first launch, it will be irrelevant",
      "Existing in may 2022 or April 2024 when they are actually available? 🤣",
      "You expect the first iteration of GPU to meet the halo products of established GPU manufacturers such as AMD/NVIDIA?",
      "the lack of vram makes me sad, though i'm not sure what i should've expected from the bus widths. wish 8-12gb was standard because 4gb is proving not to be enough and 6gb is next",
      "And I honestly agree with their way of going right now. Why do people care so much that it only gets released in China. Makes no difference if they release it worldwide now, or later.",
      "Those were likely early laptop performance leaks at reduced clocks.",
      "Yes. It has enough transistors at like 22 billion and already has RT performance better than NVidia or AMD. It's tier 4 vs Tier 3, and Tier 2 for AMD.\n\nIf you include the games where it crashes due to drivers to drag down averages it'll be like 3060ti levels.",
      "It's because Raja Koduri promised stuff. \n\nHe promised the GPUs will be ready early, in Q1 2022. \nTurned out to be a lie.\n\nThen he promised GPUs in the hand of gamers for cheap - again a lie because they prioritize OEM.\n\n\nPeople say Intel GPU will be irrelevant if they launch after Nvidia launches RTX 4000 and AMD Rx 7000. \nI don't think so because all the Nvidia and AMD products from new gen are gonna be 600$+(real price, not msrp).\nIntel's best GPU is supposed to be under 500$.",
      ">He promised the GPUs will be ready early, in Q1 2022\n\nHe actually said that? When?",
      "I don't know for sure if it was him but there were messages from Intel that arc GPU lineup will roll out in Q1 2022."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Should I get an Intel Arc A770? I heard mixed reviews about them, but apparently it’s on par with a RTX 3080, and I see it at Microcenter for $350.",
    "selftext": "",
    "comments": [
      "Hahahahahahah it’s not even close to being on par with a 3080, this website you are looking at is a joke",
      "What? No. No to all of this.",
      "No - all glued together, cant do maintenance.\n\nhttps://www.youtube.com/watch?v=N371iMe\\_nfA&ab\\_channel=GamersNexus",
      "Honestly bro, not even...",
      "I found it hard to believe as well, probably gonna get a 6700xt",
      "Absolutely not. Intel Arc sucks. Stick to the tried and true. Get Nvidia.",
      "It depends. In some scenarios it's on par with a 3070TI, in AV1 encoding it beats the 4090, and in other scenarios like some DX11 games it gets sub 30 fps at any resolution.",
      "Yeah don't be. It's weird. I got an A770 for free, and I use it sporadically and just give up after drivers inevitably cause some fatal issue.\n\nI used it for a while, then it stopped displaying.\n\nThen I tried again and logging into windows would hard crash the system\n\nThen it BSODed over and over.\n\nThen the driver installer crashed the system so installing drivers was hard.\n\nThen the system just crashed.\n\nThen it was unstable under load... at stock levels. Undervolting helped delay crashing, but never fixed it.\n\nAfter that I've just given up for now. Maybe I'll try again, but it's ridiculous. I want to use it over my 3070TI for playing AAA games at 4K since it's got 16gb of VRAM, but it's so randomly unstable it just doesn't work for me.\n\nOh and despite all the random issues it outperformed the 3070ti in shadow of the tomb raider at 4K. And when I got it to transcode some stuff, it did it twice as fast.",
      "On par with RTX 3060...",
      "wow... reminds me of an apple product. hard, HARD nope",
      "No way!",
      "If your going to spend 1129 on a 3080 just spend the extra 80 and get a 4080",
      "Yeah use case is a huge factor, but overall I'm not set on the Arc just yet...",
      "Only if intel started making gpu few years ago… would gone into crypto market earlier.",
      "Lol nowhere near the performance of a 3080.",
      "There will be a new crypto boom on some new stupid token. There always is.",
      "Not really, more like 1060.",
      "I really hope intel take their 2nd generation seriously. They could claim a part of the market just for budget cards",
      "They trade blows especially on anything with ray tracing very similarly to a 3070 or 3060. It isn't a bad card.   If you want ray tracing, it is better than that than 6000 amds.   It is a beefy card that might get better with better drivers.",
      "or AMD"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "How to enable VRR on my Intel Arc A770 16GB via HDMI 2.1? My Q80A is in game mode and all, but VRR is always disabled and the Control Center shows no info on how to enable VRR.",
    "selftext": "Anyone having a similar problem? I don't know what to do anymore, I tried all the options I could, and it has nothing to do with my Samsung Q80A.",
    "comments": [
      "Is this even supposed to be a feature?\n\nIntel has stated that the HDMI 2.1 output ist not native, but is via DP-to-HDMI conversion with a dedicated adapter, just like all the external adapters. And so far none of them has supported translating Adaptive Sync to HDMI VRR.\n\nAlso Intel has largely equated \"VRR\" with DP Adaptive Sync, so unless Intel stated somewhere explicitly that they support HDMI VRR, I think they might still only support DP Adaptive Sync. And unless some DP-to-HDMI 2.1 adapters come out, that support HDMI VRR you will probably not be able to do that.",
      "Maybe it's a bug, report to intel.",
      "Download intel graphics command center and enable it there.",
      "I'm having the same issue with DP and HDMI.",
      "Having the same issue, using a Dell S2721DGH monitor, can't get VRR to work on HDMI or DP with a A770.  No adaptive sync options at all in the ARC control center and installed Intel Command Center has enabled adaptive sync but it made no difference. Freesync and Gsync works fine with Radeon and GeForce on the monitor.",
      "That's odd. After I installed my a750 my vrr was on. I used the OSD on my LG C1 to confirm VRR was active. I am using the hdmi port and a 2.1 cable that I used with my previous gpu. Maybe this can help, not sure if the option is greyed out for you. [https://twitter.com/CapFrameX/status/1587377690977394691?s=20&t=EaKEf9Gs3wLJdgfJ1s6YNw](https://twitter.com/CapFrameX/status/1587377690977394691?s=20&t=EaKEf9Gs3wLJdgfJ1s6YNw) . I did go in and enable asp, but in the command center it was under preferences global settings for me. [https://gyazo.com/collections/1d34c8ccb9c8d421f5c5a1aa617a4e54](https://gyazo.com/collections/1d34c8ccb9c8d421f5c5a1aa617a4e54) screenshots of it and where in the app you can see if it applied. Hopefully this helps!",
      "I have the QN90B and I'm able to use VRR over \"HDMI 2.1\" aka DP 2.0 to HDMI 2.1 PCON there, although it isn't working flawlessly. I tried out some games and I discovered, that most aren't using VRR at all, which seems pretty weird. The QN90B auto detects 4096x2160@120Hz with HDR and VRR. Games I tested and which seem to work with Arc HDMI VRR:\n\n\\- Metro Exodus Enhanced Edition: VRR not working- Assassin's Creed Valhalla: VRR not working- Riders Republic: VRR not working- The Witcher 3 Next Gen Update: VRR working- Dishonored 2: VRR working- Deathloop: VRR not working\n\nI thought, that it might be the launcher with overlays or something like that (because W3 and D2 are the only games I play on GOG), but I own D2 on Steam too and it's working fine there.\n\nAs my QN90B supports HDMI Forum VRR and Freesync Premium Pro I've decided to connect my primary PC with a 3080 over HDMI and enabled G-Sync (compatible, so it's using the Freesync Feature) and tested AC Valhalla which wasn't working before and now it did without any problems.\n\nW3 and D2 are working as expected so I'd assume that there's something going in on the drivers side, which prevents the card from sending frame-info to the monitor (?). The two games should proof, that VRR over the HDMI 2.1 PCON is indeed possible and that Intel should look into that.\n\nI'm already in contact with their customer service on that case and they've said that they're going to test it internally, after saying, that 4096x2160@60Hz is max for HDMI on the Arc and that I'd need a DP to DP connection in order to use VRR, but I insisted that it seems to be perfectly fine on W3 and D2 and I've sent them some pictures and system reports too.\n\nI hope they can figure something out on that one (well... NVIDIA already did, so the ball is in Intels court now).",
      "Check this out, SHOULD support all vrr techs, but unsure on hdmi\n\nhttps://www.extremetech.com/gaming/338376-intel-arc-tech-talk-focuses-on-vrr-hdr-and-hdmi-2-1",
      "This is weird because my LG CX was displaying it was using VRR upon first boot of the A770 until I until it off in the graphics control panel.",
      "According to the HDMI Forum, HDMI 2.0 has been replaced by HDMI 2.1. Technically all new products are HDMI 2.1. But as this number is basically just the version of a PDF, it says next to nothing about the supported features.\n\nOfficially, manufacturers should have to list all of the optional features of HDMI 2.1, such as VRR, max resolution or for example the maximum bandwidth (48GBit/s FRL would be the maximum).\n\n>You cannot use version numbers by themselves to define your product or component capabilities or the functionality of the HDMI interface.\n\nhttps://www.hdmi.org/spec/hdmi2\\_1\n\nAnd yes, many manufacturers use \"HDMI 2.1\" to indicate DSC, 48G FRL and VRR support, but there have also been others that have not. Customers expect this, but it is not guaranteed by anyone and just a common assumption, just as with USB versions like USB 3.2.\n\nJust as an example, Intel's PR representatives have stated that the HDMI 2.1 output is achieved via dedicated DP adapter and Intels UHBR10 Displayports only support a max. bandwidth of 40 GBit/s. So it is already impossible that the Arc GPUs would support the full 48 GBit/s worth of bandwidth plus DSC, because they already need to use at least part the available compression to fit this through DP.\n\nThe saddest part is, that for Intel's iGPUs the detailed specs are public and list all those details, but for the Arc A GPUs, the only official specs on Intel's website only state the max supported resolution via HDMI as 4K (available natively) , even though Intel itself sells variants of that GPU with better HDMI-ports. So unless there are already other existing DP-HDMI converters that support VRR (to my knowledge, those do not yet exist) I would not assume that Intel has such a thing, if they never marketed the existence of such a new functionality anywhere...",
      "Is it working for you on every game? \nI just found out that it also works with The Division 2.",
      "Situation is still the same, but they are aware of it afaik.",
      "Support rep I've contacted back in January said that they are investigatig this situation based upon my findings. That's all.",
      "nVidia -dunno about AMD- supports VRR via HDMI 2.1, which is to be expected tbh. As for adapters, I could buy one but I fear it might not work and also I rather prefer if they support the feature on other ports, not only displayport.  \n\n\nOther than that, the A770 16GB is one heck of a GPU, specs wise, it's really working surprisingly well with my games. Older games might need some tweaking in some cases, but still... A great gpu.",
      "I got the most recent drivers, the beta ones, and the Command Center shows an option for Adaptive Tessellation, but nothing about Adaptive Sync anywhere.",
      "hope they add this feature, because nVidia and AMD allow VRR to work through HDMI 2.1. I just use Smooth Sync for now, but while it works great, it isn't the same as VRR.",
      "thanks for the help. But alas, it didn't work for me. I event went to install the beta version of the app, but to no avail, although a new option appeared regarding games compatibility.",
      "great info! Good to know that you got in touch with them, because knowing that VRR works perfectly fine with other GPUs shows that maybe the drivers are to blame and Intel might have to work on it.\n\nVRR works superb on my 32\" 165Hz monitor, but on my TV, that's a totally different story.",
      "So I know what's causing problems with VRR.\n\nFirst of all for OP: Are you connecting your Arc via HDMI 2.1 to Input HDMI-Port 4 on your TV? It's important to use HDMI-Port 4, as the other ones don't support 4K@120Hz. Even if you plan to use 2K@120Hz on every other port I could imagine that there isn't enough bandwidth for VRR and that's because:\n\nIntel Arc is using full 10-bit color depth instead of 8-bit dithering. Most people trying to get VRR running are using a monitor with DP 1.2 and a refresh rate over 120Hz, which isn't enough bandwith for VRR due to the color depth that Arc uses. In that case the users would need to reduce the refresh rate manually to 120Hz (maybe 144Hz works, can't confirm, as I don't have DP 1.2 on my monitor). Disabling HDR could also help to reduce the bandwith.\n\nFurthermore:\n\nThe games that I tested have different full screen implementations. I noticed that older titles (like before 2010) all work with VRR but newer ones are hit and miss.\n\nSo I deactivated the full screen optimizations for those particular game. This can be done by right clicking on the .exe of the game and select sth like \"disable full screen optimizations\" in the compatibitlity tab (could be named differently, because my Windows is set to German).\n\nThis fixed my VRR issues on all the games that weren't working properly before.",
      "does it still work for you via HDMI 2.1? 'Cos I don't see the Adaptive Sync option anywhere in the Command Center."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Refurbished Acer Predator BiFrost Intel Arc A770 16 GB - $220.79 [eBay]",
    "selftext": "",
    "comments": [
      "I wonder why they have so much refurbished stock. 67 already sold and more than 10 for sale. Is there a common part going faulty in this model?",
      "Good question",
      "Or were they mining? High stress on VRMs 24/7 etc",
      "Returns more likely. Disappointed customers etc. It is not for everyone."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Arc a770 or Arc a750?",
    "selftext": "I'm looking at building my first pc and I want to go with an intel arc graphics card. I don't need crazy performance, but I want something that can run the latest games and will last for a decent amount of time. \n\nWith that being said, I'm debating between the Arc a750 and Arc a770. I can buy the a750 for $220 on Amazon and the a770 for $460 on Amazon (also have a bid on ebay open box with a maximum bid of $350). What is your recommandation?\n\nThank you!",
    "comments": [
      "The A750, it’s not even a question. You’re getting 92% of the performance for half the price.",
      "Just get a750, there is no point of 770 . a750 is 90% of a770",
      "A770 should be around 350$ and I'd go for it, I already have an A750 and I feel like I'm gonna need the extra VRAM in a not so distant future.",
      "The problem with that is the RX6700XT exists for the price of the A770. The A750 is easier to justify the cost of.",
      "Fair enough, my A750 is unbeatable in price/performance.",
      "If you want longevity and insist on an Arc card then id suggest the 770 over the 750",
      "I have the 750 and like it a lot but the only game I play is a modded GTA V with video quality maxed out. I still get 40fps",
      "I went for the 750 with my Ryzen 7 5700X as it had the two items I was looking for. 1) An 8-core processor, and 2) good 1080p graphics.\n\nWhat is now being raised as an interesting question is how much 8MB VRam is useful in today's GPU - but for low spec gaming - I can't see how that will affect me for the next 2 years",
      "Out of the two, I'd reccomend splurging on the A770. But either way, be prepared for possible issues with different apps, including games. I was an early adaptor if the A770, bought two different models, and had so much trouble I sent them both back. But from what I understand, Intel has come a long ways since then and they're looking like a pretty good option with the understanding that you may run into some snags along the way. Good luck and have fun Mate!",
      "A750 is way better since it's almost all the performance for a much cheaper price. But 16GB of VRAM on the A770 I think is mandatory for a new card in 2023 unless you're planning on playing in 720p.",
      "When you say low spec gaming, do you mean 1080p?",
      "Yes, when compared to 4/5/8K @ 150fps average.\n\nOne thing that I didn't include in the op was my need to be as efficient as possible with the power.\n\nHaving an 8-core at 65W met my requirements for the CPU, however, researching power draw on graphics I found that the higher resolutions require more energy. \n\nI then had to make a choice based on voltages against price - and price won out with the 750."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Intel ARC A770 + Ryzen 7 7700X | 25 Games Tested at 1080P & 1440P",
    "selftext": "",
    "comments": [
      "Not really how it works because it depends on the design, if the design isn't efficient, even with more physical units, you'll have some of them just standing there with no use, same happened with FURY and VEGA cards, that's why they weren't that good for gaming",
      "The A770 is nowhere near the 6800, it is actually WAYYY behind, catching only in terms of ray tracing",
      "having better potential or better computational power never had much to do with gaming, The same can be seen when comparing VEGA to RDNA2. Now, Spider-Man remastered is heavily CPU Bottlenecked in most scenarios at 1080P, so the 1440P and 4K are usually the real GPU tests",
      "This gpu is not still utilized fully by drivers.",
      "There is no consistency in testing. Showing one scene in 1080p and a different one in 1440p tells me nothing about the scaling. You need to isolate for the variables, not introduce new ones. This is why I stopped watching most of these YouTube CPU/GPU comparison videos. If at all they aren't completely faked, the methodology is flawed.",
      "Agreed, but this is not and never was the point of the video. These are gameplay benchmarks, for actual benchmarks with super accurate data I have my GPU comparisons, which will come soon with the A770 as well, including Rasterization, Ray Tracing, FSR and even XeSS tests",
      "Xess is much better than FSR 1.0, and FSR 2.0/2.1/2.2 are aexcellent in most games, they're nothing compared to previous FSR 1.0 and DLSS 1.0",
      "Vega did really well in Doom. Actually scaled with its shaders.",
      "Indeed it is because ARC GPUs love Vulkan, the thing is the DXVK still has considerable stutters with these GPUs",
      "Looking forward to it.  Enjoyed this initial run through.  I am waiting on my ARC GPU i am very curious how it will perform and on XeSS.  Of all these upscaling technologies I have only tried FSR 1.0 once on the Riftbreaker demo as a curiosity on an APU 4650g so this should be quiet the difference.",
      "Why buy Arc GPU riddled with SW issues and inefficient while you can buy a faster radeon GPU for the same price?\n\nSeems so illogical to me.",
      "Good to know",
      "Yah, that's explained in the beginning of the video",
      "Mind linking that Techspot article? I can't find the latest driver result",
      "Not if you use the async version of DXVK",
      "It’s not even the latest ones https://www.techspot.com/review/2542-intel-arc-a770-a750/",
      "It isn’t, when you see specs the A770 has a better hardware than 3070 and even RX 6800 (not die size and transistors but rops and cores for example). Aswell, A770 have a bigger Die and more transistors than the 3070, so fully optimized Arc should be above them or at same level. New drivers apparently already bring the A770 above the 6800 by Techspots",
      "For Spider-man remastered*. Also if you check each hardware, the a770 has better potential.",
      "The more hardware you have the more you can exploit with drivers"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Arc A770 16GB Question",
    "selftext": "I am ready to pull the trigger on the A770 16GB card at Newegg for $349 to replace my GTX 1070. I currently have an 8700K and Z370 Taichi that I just download the BIOS with Resizeable bar support. In a couple of months I will be upgrading to a 13700K and MSI Tomahawk DDR 5board I wanted to. My long winded question is should I get a GPU sag support bar for this card?",
    "comments": [
      "It's not that big of a card. You don't need a support for it.",
      "Don’t listen to people who say it’s too much CPU and such. Buy whatever CPU you want, but if it’s a high end one like 13700k, then it’s only for the better. You will get better 0.1% lows and it’s a guarantee of a less bottleneck for a better GPU you will buy in the future.",
      "Yeah. I do mostly productivity work with multiple VMs. I only do occasional stress relief moderate gaming. I just want to replace a 6 year old 1070. The 13700 is going to happen anyway.",
      "By the sounds of it they don’t seem to be using the gpu to do any actual work except for providing video output.\n\nThere’s not really any “stability” issues with arc.. there’s some minor things like it not turning your monitor on when it wakes from sleep properly but that’s probably the only bug I’ve experienced in quite a while outside of a game.",
      "Idk what people are on about… there’s literally no such thing as “too much cpu for your gpu”. \n\nIf you want a budget performance level gpu, your current setup should work fine with the a770 and the new setup when that comes should be better even if the gpu is going to be the limiting factor. I’m currently using an a770 with a 9900k in a z370 asus board without issue.\n\nAn a770 isn’t that big and heavy, but I wouldn’t say you need a support or not without seeing it in the case and how well it’s supported by the case from the bracket.",
      "Any reason to not get something like a RX 6700xt for better drivers and better rasterization performance?",
      "Isnt arc getting good reviews for blender type work?",
      "There is no sag with this card.",
      "Nope",
      "Thanks",
      "Are you sure you want to use an Arc card in your work machine? It’s fine if you want to use it in a side build as something to play around with, but I don’t think the drivers are in a state where you should be doing actual work on it.",
      "The 13700k is a great CPU for that workload. The more cores, the better.",
      "It’s quite a bit slower than an RTX3060 in Blender\n\nhttps://www.pugetsystems.com/labs/articles/intel-arc-a750-a770-content-creation-review-2376/#Blender",
      "bust out some extra $$$ for the 13900KS. get one A770 and purchase another later for multi-gpu situation. No on else except intel, will be doing multi-gpu going forward for DX12.\n\nso far, I can't find one title, this gpu can't handle over 60fps in 4k (mostly \\~120-240fps in 4k full detail.)",
      "I'm just going to leave this here :\n\n[https://www.techpowerup.com/review/intel-arc-a770/32.html](https://www.techpowerup.com/review/intel-arc-a770/32.html)\n\nUsed 3070 and 3070ti's going for cheap if you look around a bit.",
      "The 13700K seems like too much CPU for the A770, I’d get an i5-13400, i5-13500 or Ryzen 5 7600, and even that’s excessive - probably a Ryzen 5 5600 or i5-12400F would do, of which the former is the better buy.\n\nI’m also not sure how the A770 has aged, but the RX 6650 XT might still be an overall better buy.",
      "13700K is way overkill for a GPU that has basically \"only\" GTX 1080ti performance, which will be considered low end this generation.\n\nYour 8700K is adequate for arc a770."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Intel Arc A770 good for 1440p 60fps in modern games?",
    "selftext": "Current GPU is a 980TI which struggles with even GTA 5 at 4k.\n\nI am looking for a stop gap upgrade until the 5090 comes out in 2024/25. My goal is to get at least a stable 60fps in Starfield and run stable RTX mods on minecraft (for the kids) and keep the budget around $500. Also I am looking for the best card with the most VRAM for the money within this budget as well which is why I saw the A770 with 16Gb Vram.\n\nIf it can get 4k60fps on games like Hitman, GTA 5, Flight Simulator. That would be a bonus.\n\nMy other specs   \nAMD 5900x\n\n32Gb ddr4\n\n&#x200B;\n\nIf you guys have any other recommendations within that budget please let me know. \n\n&#x200B;",
    "comments": [
      "In most games I play at 3440x1440 I can get 60-80fps with ARC A770",
      "Intel Arc are very price competitive and have full support for all new titles released on DX12 and beyond. The important improvements are for DX9 which they've done this past March 2023. No more native DX9 support. Instead they got a DX12 to DX9 translation going on. It looks to be solid. League of Legends and CS:GO titles perform much better now.\n\nhttps://youtu.be/ecfV17wrjfs&t=12m00s the whole video is worth watching but this chart summaries the important bit. Price to performance competitive. So it makes a really good stop gap for you and a GREAT hand me down to your kids and their gaming future.\n\nI would buy one base on the price alone. price competitve ray tracing gaming!",
      "I’d wait until Starfield benchmarks are out before deciding.  They didn’t do a great job in system requirements so there is not a good way to even aim for the target right now (if you really care).\n\nIt may be relatively inexpensive, only work on one vendor, or not even be possible with a 4090.",
      "You can try find a old 3080 for around $400ish if you are lucky. That would be minimum for 1440p 60f for new games. They are just not very optimized.",
      "Not even close in price. That card costs triple what an A750 costs but it's not 3x better.",
      "Problem: Starfield is AMD sponsored",
      "Well, at least FSR is platform agnostic. Hardware XeSS is actually noticeably better",
      "If you're looking to do Minecraft RTX, it'd be best to look at an Nvidia card, like an RTX 4070 or better.\n\nhttps://www.tomshardware.com/news/minecraft-rtx-gpus-benchmarked",
      "If your games supports XeSS then yes. Source: have a750 and it struggles without upscaling.",
      "Star Wars Jedi Survivor has no problem playing at 1440p 60.",
      "XeSS is also available on competitors’ hardware, it just gives less of a performance boost.",
      "4070."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "a770"
    ],
    "title": "Intel Arc A750/A770 will be available at 6 Micro Center locations",
    "selftext": "List:\n\n-\tDallas, TX\n-\tHouston, TX\n-\tChicago, IL\n-\tWestbury, NY\n-\tDenver, CO\n-\tOverland Park, KS\n\nSource: https://game.intel.com/story/intel-arc-graphics-release/\n\nEDIT: Looks like the A750 only, I don't see any A770s even listed at these locations.\n\nEach location seemingly only received seven A750s and no A770s. So the entire Micro Center chain received 42 Arc 7 GPUs lol",
    "comments": [
      "What went so drastically wrong that it seems only a few hundred GPUs were made available? Intel certainly is no stranger to product launches, I wonder if there was pressure just to launch something at this point.",
      "Yep that seems to be the case. The fact that they only had stock at a handful of Micro Center locations is a pretty big red flag. Meanwhile my single Micro Center location had over 120 4090s this morning.",
      "About par for Intel with how the Arc GPU is going. So many people want them to succeed after being fleeced by Nvidia and AMD over the last couple years, but that just can't deliver on the demand that is there waiting... I almost went to my local Micro Center in Fairfax, VA but glad I didn't waste my time",
      "All these cards were made back in Q1, only 4 million arc cards were made overall and that covers the entire stack from a380 to a770 (not sure if that is just discrete cards or whether that includes laptop gpus as well).\n\nIt's pretty much a case of get these cards from somewhere now because they ain't coming back (great for gpu collectors though)",
      "Wow, that's a lot of 4090s...who has the money to blow on a 4090?! I was hopeful with the 770 to finally get a GPU with good specs at a reasonable price. Gonna have to hope my 1070 holds out a little longer.",
      "No 770 at overland park.. i hope there are some",
      "Nice, did you receive a software bundle email from Micro Center?",
      "In the UK, only seen listings so far for just one major online retailer and the big pc one (overclockers) , haven't heard anything from them. There is zero volume but hopefully it's the sort of thing where it'll be available on off for a few weeks on Newegg for you guys",
      "It seems I did. I looked back at the receipt and I got COD MWII for Free.",
      "I hope they have more than 100000 gpu in stock.",
      "I stopped by the NJ Microcenter in Paterson. They said that they just did not receive them yet and to check back tomorrow. I also called their support line and it seems like they don't really have any answers.\n\n\nEdit: Spoke with a store manager and they confirmed that Intel barely sent any stock. They suggested to keep an eye out on the site and to check back in on delivery days (wed,fri). They also said they did not receive any promotional material, what’s going on Intel!?!?",
      "What a 🚀 launch Intel!",
      "I actually just got one. They just received some at the Dallas Store. The funny thing is that no one here seems to want one.",
      ">certainly is no stranger to product launches, I wonder if there was pressure just to launch something at this point.\n\ni heard they didn't want to sell directly to people because there is no margin. The plan is to try to bundle with systems, and cpus to make money.\n\n\\>> The source is moreslawisdead and some one line articles. Couple. Nobody is on the record but is the likely explanation. \n\n&#x200B;\n\nMy take:\n\nSo i'm cool with companies making money and such, but they annoying thing is this isn't' how ti was marketed, and i'm kind of annoyed about it tbh. wasting our time with a marketing gimmick if true. \n\nMy guess: it's all about money though they have cards. they are trying to look like a release, part marketing stunt, and part use it as a loss leader as part of the stunt to sell more computers etc. Right because once you bundle the idea is it can help sell some chips.  I can see the point.   \n\n\nSo weird to have a release of product like this that you don't intel to really sell, so you don't believe it in but your customers do. Never seen anything like this i can think of.",
      "That's a lot of foot warmers.",
      "that's what i feared, i would happily buy one if I could get a 770",
      "Just to put this all into perspective, Nvidia + AMD shipped together something around ~50 million desktop units for 2021. This is likely 4 mil all in, including mobile.",
      "So, if they produced these back in Q1 is production done or what? Surely, we'd see more form AIBS etc later on. I have seen rumors about how the entire gpu division is done for. I am having a hard time reading the market and seeing if they are in this for the long haul or not. I ask bc what's the point of fixing this if they aren't?",
      "No, just walked in and got the card no bundles.",
      "When gamers nexus did his teardown, confirmed that that die was produced in Q1 and the rumours have been of a warehouse full of alchemist dies waiting for the software to catch up.\n\nAs far as I'm aware in terms of alchemist, that initial Q1 run was it, especially considering 6nm is in much hotter demand than it was back at the start of the year.\n\nThe gpu division isn't done for but I think it's going to be a very very long time before we see the arc division on parity with someone like amd where obviously they have the cpu side of the business, while also being able to make half a dozen different gpu dies for their products stack. Current rumours are Intel focusing on data centre and laptop (which makes a lot of sense) and then just keeping some sort of presence in the discrete market, so maybe just one gpu die for the battlemage launch to keep costs low. This is a awful market to be launching a new gpu into when supply is far outpacing demand.\n\nSo long story short, Intel is in the long haul for the graphics business but no one including Intel has a clue about them being involved in the discrete gpu space long term"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Intel Making Moves: Acer Arc A770 BiFrost GPU Review, Thermals, & Tear-Down",
    "selftext": "",
    "comments": [
      "Hey! We got a board partner",
      "well nice to see acer in the race 😎👍🏼",
      "Lol Acer really cant help but put aeroblades on every product. Not gonna complain though, it looks cool, but the rattle and noise are genuine concerns from their laptop range.",
      "With the latest Performance test (actual driver) i will say. For the Price performance it is really more interesting as nvidia or AMD in the mid range.  If i had not buyed last yeard direct on release date my AMD RX 6750 XT Referenz model this GPU will be more a 1st chose as any other brand atm."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Arc A770 display goes nuts during driver install?",
    "selftext": "Hi guys, doing a build for a friend, NZXT Z790 board / i7 13700k and his A770 just won't work. It works perfectly fine up until driver install, than during the install the screen just goes to this mess: https://postimg.cc/zyDfxBjS\nI tried everything, latest bios, chipset drivers and Intel ME firmware. All Intel recommended bios settings for Arc are enabled. Also tried 2 different driver versions. The DP cable works fine on my own PC, so I know it's fine. Any idea what to do? Could the card just have come broken?\n\n**EDIT: The display works fine with HDMI only, but same problem on every DP port. Hopefully it's just Arc not vibing with my DP cable, will test with another cable.**\n\n**EDIT: It was the cable! This card is very sensitive to cables obviously, this same cable has no issues with my RTX 2060. But all good, he is keeping the card.**",
    "comments": [
      "I had one for 4 days, it has worn down my sanity almost to a depression, i ended up returning it for the sake of my sanity. Intel needs to get their drivers worked on as their main goal, too ignored atm.",
      "If the display is fine until you install the driver, then card is not broken, try ddu in safe mode and then let windows install drivers on its own, restart and try again, if this doesn't work, get in touch with intel maybe?",
      "Welcome to Arc. The drivers were doing pretty well until a few weeks ago, when it seems that they broke something. Performance wasn't great and there were bugs, but at least they worked. \n\nI've got a laptop that was kind of obnoxious the whole time, but it's barely worked the last couple of weeks.\n\nSo with the new beta driver all of my problems have been fixed. Maybe give that a shot?",
      "I tried all that unfortunately. I can get into windows with on board graphics no problem, and uninstalling the drivers will make it work again, but then as soon as I try to re-install Arc drivers, same thing. You don't think there could be something wrong with the card? One thing that's odd, is that I also tried HDMI output as well on it and I can actually see the desktop in that case, but it flashes like a grainy overlay every second.",
      "What doesn't seem weird? It's stuck on screen like this forever. I don't mean it just happens briefly during the install (I wish).",
      "Yea, I think my next step is to try it in my own system and see how it goes. His PSU is brand new and pretty high end (Lian Li SP750) so I am thinking a power issue is unlikely.",
      "Ah pic didn't load when I originally posted",
      "Are you guys both on Windows 11? Was just thinking an easy way to test it across different systems and OSes quickly would be to use some of those windows live usb like Gandolf or Hirens https://alternativeto.net/software/gandalf-s-windows-10pe/\n\nThen you wouldn't have to touch your filesystem and you can test anything from Windows 8 to 11.\n\nKeep us updated. I am rooting for Intel here. Thanks for attempting being early adopter! I don't have the money or balls to but we need more competition in the GPU market. They have the money to get those drivers worked out. Hopefully they make an AMD rx 5/6××× style comeback with compatible, solid drivers.",
      "It actually turns out it works fine with HDMI on my monitor. None of the Display Ports work at all, but as I was looking up this issue I found out that Arc is very sensitive with regards to DP cables and other people have had similar problem. My friend hasn't come over yet to get his PC, but when he does I told him to bring his own DP cable, mine is from around 2016 so hopefully his newer ones works.\n\nBut I stress tested the card with some benchmarks and it works perfectly fine otherwise. He doesn't game, it's a work station only build (graphic design and some rendering) so the 16gb vram and price point of Arc is actually really good value for his case use. I'll update my original post when I found out if it's the cable, just so people don't get the wrong idea. I too would like Intel to succeed with their gaming GPU's as well.",
      "Update? Did you guys return it or get it working. I lose sleep over stuff like this, doesn't matter if it affects me or not...",
      "Hey dude! Yes! Display ports worked just fine when he brought it home. It was just the card not liking my cable (which for the record work perfectly fine with my RTX 2060). Glad he could keep the Arc. Here is a photo of the final build if you are curious: [https://imgur.com/a/BHOWAlZ](https://imgur.com/a/BHOWAlZ)",
      "Nice! Glad everything worked out. Clean build man!",
      "This doesn't seem that weird during display driver installs the default windows driver is loaded in between.",
      "Isn't it possible that parts of the card aren't utilized until the proprietary drivers are installed? I am asking, not challenging your idea. I remember like a gtx 260 or something like that I was messing with would not act up until I ran certain physx games or something. It was ages ago so don't remember specifics.\n\nMaybe try like PopOS with steam/proton if you Linux like that. \n\nAlso, maybe try a different PCIE slot, make sure PSU is powerful enough and re-seat the card and power to the card.\n\nObviously an entire different system would be the ideal way to rule out a bad card."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "How's the price of Arc gonna be in EU?",
    "selftext": "The Arc A770 should be around 399$/449$, but how does it translate in euro? \nI'm from Italy and I'm really looking into buying an Arc GPU, but I have a very big question mark on prices... 450€? I might buy it, but 500€ or more, not sure, I currently can find 6750XTs for 450€ :/",
    "comments": [
      "Well $=€ +tax from Italy ~20%? easy math I think you can figure it out yourself.",
      "The A770 needs to be a hell of a lot cheaper than that to be worthwhile, there are RX 6700 XTs going for $360-380 right now.\n\nIntel is basically 18 months late with this one, I wouldn’t be surprised if Arc never gets off the ground.",
      "399$? Maybe from a partner but Intel is gonna make their board cheap. Definitely cheaper than a 3060 ti. Most of us are hoping for below 3060 pricing.",
      "With those prices nobody will buy them in the present state.",
      "Intel is not AMD. They can afford to lose hundreds of millions of dollars. They underdelivered on alchemist, but Intel is not going anywhere. All the tech tubers and leak bullshit about Arc and future dGPUs getting axed was debunked.",
      "Yeah well intel has a big choice ahead of them. Give people a great deal in exchange for basically being a tester. Or give people an equivalent value to the competition and become irrelevant in the GPU sector.\n\nIf intel goes with the first one they can scrape all the budget oriented gamers and PC's market share away from AMD and Nvidia (Although Nvidia doesn't seem to even care about that segment anymore)",
      ">The Arc A770 should be around 399$/449$\n\nThis is still very much in the air. Intel hasn't released any pricing info on the A770 apart from assurances that they will be priced aggressively.",
      "A380 is already in Germany in few stores so you can check what's the EUR end user price vs USD to get the conversion rate for the USD MSRP of the 770. It's still unsure what decision Intel will make as they may for example back down with prices even more.",
      "In italia minimo 1000€, prezzo amico eh",
      "We are talking about regular pricing in Europe, not the prices some US retailers empty their warehouses. The cheapest 6700xt in Europe is around 550€. Edit: no, found one for 484€.",
      "Well, the A380 is actually the perfect server card. A1 & h.265 transcode that provides very good performance at the price range. \n\nShit gaming card at that price, but amazing for a media server.",
      "Understood, tnx for the heads-up, I really hope they'll back down further, 3060ti performance for like 350€ would be really good, also almost inline with a 6600XT",
      "A380 is in German stores? I haven't seen any yet!",
      "They can't sell at parity, they have to sell cheaper as there still are a lot of fixes and tuning to do.",
      "Mindfactory for 189 EUR as per recent news.",
      "Hmm, for janky 1060 performance i just saw. In my mind I thought it was 2060.thanks for looking it up. I was at Netbook billiger, but I'm steered back towards the Netherlands by Google constantly ;)",
      "Nope, it is unlisted now.",
      "I hope the prices will stay far below 450€, but we all know that these companies are not that into charities :D",
      "I hope so, AMD didn't do that with the 5000 series tho 😂",
      "Don't buy an Arc GPU ffs.\n\nIntel missed the boat. These GPUs are still janky, and they don't really offer a compelling value proposition compared to the existing AMD and Nvidia offerings, and AMD and Nvidia are in the process of releasing their next gen GPUs that will deliver even better price/performance options. Couple that with a used market that is seeing prices get crushed by a flood of used mining cards and unsold 30 series new stock.\n\nArc is going to be dead soon, the writing is on the wall. They didn't get Alchemist out soon enough, and it's likely to be the same issue **IF** they decide to do an actual release of Battlemage. I suspect Battlemage won't be released as a mainstream discrete graphics card, I think Intel will quietly smother the whole Arc project and you'll be left with orphaned hardware that receives next to no support from either Intel or game developers."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Arc a770 | PC not booting | VGA debug LED lights up on motherboard",
    "selftext": "I recently bought a new GPU arc a770 and it doesn't work. My PC turns on but wont boot. The screen shows no signal and I can see the **debug LED on my motherboard lighting up on \"VGA\"**.\n\n  \nPutting my HDMI cable on the motherboard didn't change anything (with the new GPU installed)  \nMy previous GPU will boot perfectly fine (GTX 1050)  \nI bought a new PSU because i had a terrible cheap one and I thought that was the problem but it didnt change anything. (the PSU listed is my new and current one)  \n\n\n**Specs:**   \n\\-Intel Core i5 9600K 3.70GHz  \n\\-MSI H310M PRO-VDH PLUS (MS-7C09)  \n\\-2x 8GB Patriot Viper Steel 3200MHz  \n\\-AsRock Arc a770 8GB GDDR6  \n\\-Aerocool Aero Bronze 750M 750W\n\nthanks in advance.",
    "comments": [
      "Try clearing the cmos to see if that will help",
      "Did you update the bios?",
      "I had similar issues with my A750 when I first installed it, my computer was acting like it wasn't even there, it would turn on and boot but no video (I thought it might've been a dead GPU), eventually after many restarts it started working.\n\nSince you have an iGPU, I would remove the dGPU, change the settings in the motherboard to enable the iGPU rather than it being set to automatic, DDU drivers in safe mode, put the dGPU back in, then install intel's drivers, I believe part of the reason I was having issues was that it had no drivers installed and I didn't have an iGPU to fall back on.",
      "I did clear it multiple times by removing the battery but nothing happened",
      "Yes its latest version",
      "Thank you for your suggestions, I tried them both DDU and Integrated GPU, unfortunately nothing changed.   \nDid your issue just resolve itself just by restarting?",
      "Yeah, many restarts later the display eventually worked with the GPU.",
      "Wish i was that lucky ;-;"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Witcher 3 ARC A770 LE Quick Performance Analysis - 120 fps without RT, but RT On is a stuttery mess!!",
    "selftext": "",
    "comments": [
      "> Source ? \n\nMe\n\n>Is this with latest drivers \n\nYes",
      "Isn't the game currently a mess on everything pc atm though?",
      "Honestly impressive then for the price, especially given that it hasn't \\*really\\* been optimized.",
      "Whats the resolution tho ?",
      "Source ? \nIs this with latest drivers https://www.intel.com/content/www/us/en/download/729157/intel-arc-graphics-windows-dch-driver-beta.html that released yesterday with new optimization for The Witcher 3: Wild Hunt Next-Gen Update.",
      "Doesn't support XESS, pretty big for Arc.",
      "2560x1440",
      "It is real performance, but it just fails to mention what the actual rendering resolution is on the chart.\n\n1440p FSR2 Quality is scale factor 1.5, in other words, 960p. Well, 960p with improved temporal AA. 4K FSR2 Quality is actually 1440p.",
      "Need an arc A970 for this",
      "Resolution?",
      "Have you tried running DX11? What is the result there for non rt performance",
      "Witcher 3 RT is destroying even $1000 newly released GPU:\n\nhttps://www.youtube.com/watch?v=HbFNQbXDfmE",
      "It kind of is, in DX12. Stutters galore. DX11 runs about 10% worse than previous patch, but it does have some visual upgrades so it's to be expected.\n\nRT features are also quite heavy, so losing half of your framerate seems more like a given.",
      "w8 - that does not exist... ?",
      "Nah I wish",
      "This RT update adds nothing to the game, probably sponsored by Nvidia . Like all the game works anti-performance effects.",
      "People really need to stop making these comparisons when using DLSS or FSR. It’s not real performance. 1440p ultra with FSR at only 120 FPS for a game released in 2015 and without any of the new ray tracing isn’t anything to brag about. The performance without FSR must be horrible. I don’t take any FSR, DLSS, or XeSS benchmarks seriously."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "With the Intel arc a770 or RTX3060ti I am torn.",
    "selftext": "I want to build a pc and I'm wondering between Intel arc a770 or RTX3060ti.\n\nI mainly want to play games (Apex Valorant simulation game).\n\n&#x200B;\n\nI also want to do some video encoding.\n\n&#x200B;\n\nWhich one would you recommend?\n\n&#x200B;\n\nI'm thinking of going with the i7 12700F CPU.",
    "comments": [
      "Unless you think Intel will improve the video driver software for the ARC Alchemist A770 video card by more than 40% in less than a year (No, they won't), the Intel video card is significantly less efficient for most programs compared to the RTX 3060ti. It has a hard time barely competing with an RTX 2060, RX 6600, or RTX 3060, and the initial testing of the card's performance is slightly above that of an RX 580 or just below a GTX 1660.\n\nThe A770 has hardware and software design problems that probably will never let it compete well enough to come close to an RTX 3060ti. The primary use you can use it for is encoding and decoding video, rendering files, and using tasks that take advantage of the 16 GB of DDR6 video memory. If you play games that are below 2k resolution and less than 120 FPS, then you will not notice many of the defects of the A770 card, but if you use those less demanding programs then why buy either an A770 or an RTX 3060ti, since, you can buy less expensive and equally competent video cards from AMD and Nvidia.\n\nNeither is the best value from either Intel or Nvidia, and currently AMD's video cards are the low cost leaders in every single market segment.\n\nWithout knowing anything about price, model, size, etc. from either the A770 or RTX 3060ti, I would always prefer the RTX 3060ti. The RTX 3060ti works in the same gaming range as an RTX 2080 Super.",
      "Get your self a amd rx 6600xtvand you will be happy",
      "Go with 3060ti Zotac AMP Trinity OC Edition (5 Year warranty) or with the ASUS variant (3 year warranty). This is best value for money = performance card in the market you could get right now, so get it.",
      "Thank you very much.\r  \nThe Palit RTX3060ti had a superior price (at a store near me), but considering stability, etc., would the ZOTAC be better?",
      "Check the difference between all the Zotac variations on the internet for 3060ti. Get the best one, you won't regret that card for atleast the coming 5 years. Hell I'm chucking my 1060 from last 6 years or so. It's good for 1080p gaming.",
      "It is certainly better to have a 5-year peace of mind, since it is an expensive item.\r  \nThank you very much!",
      "Intel GPUs recently received excellent driver updates, but it's still hard to recommend them. A 3060ti is a superb 1080p or 1440p card, and will easily handle e-sports games, especially with the 12700 that you are pairing it with. \n\nUnless you are getting the A770 for half the price of the 3060ti, just get the latter.",
      "I still think Intel GPUs are no good....\r  \nLet's hope for the next generation.\r  \n\r  \nThank you for your detailed information.",
      "\"Unless you think Intel will improve the video driver software for the ARC Alchemist A770 video card by more than 40% in less than a year (No, they won't), \" Oh how that has changed",
      "Thank you for telling us about it.\r  \n　(I'm sorry you had to learn that.)\n\n\r  \nIn my area, rx6600xt is about $40 more than RTX3060, so I will choose RTX3060.\r  \n\r  \n\r  \nBut the rx6600xt also consumes less power, so I'm honestly not sure lol.",
      "He said video encoding. AMD sucks at that. Great gaming cards but they aren't good at anything else really. \n\nAlso, at his price range, the 6700 XT seems more acceptable.",
      "Thank you for the details.\r  \n\r  \nI see that there are still many inadequacies...\r  \nI would like to purchase another Intel GPU when it becomes cheaper in the future.",
      "If you want to take the chance go ahead. The better perspective is to see if there are reviews and people on YouTube and online that have said that their Intel video cards are now equal to what was promoted. \nI stand by my opinion. But go ahead, and buy and take the risk. I notice that no one that has purchased the cards are saying good things yet.\n\nHere's the situation. Why even buy Intel GPU hardware when there are tons of video cards dropping in price every day now.",
      "Lol what he meant to say with his comment is that you're already wrong because they've shown massive improvements in the 2 months since you wrote your false comment....",
      "No. My statement holds even today. And I notice no one that has used an Intel GPU has said anything, and even your opinion isn't putting new data. Did anyone on YouTube test the new drivers to see if the promoted claims that the best cards A750, A770, or the special Edition A770 come close to the RTX 3060ti? Or even the RTX 3060 12 GB? No, not yet. I'm still waiting. You are still falling for Intel's shill propaganda which they have been spewing for the last two years.\n\nThe major improvements they upgraded to was the ability to play more DX9 games at better performance where they couldn't play many of those games at all. The average performance remains somewhere between an RX 6500XT and an RX 6650XT in terms of AMD gaming and when comparing to Nvidia the performance level is slightly above the GTX 1650 Super and the RTX 2060 12 GB card. That jump is not 20%. And I haven't even mentioned the things you needs to do to find out what games you can play and at what settings, because their Intel ARC gaming interface is divided between the Microsoft operating system video settings and Intel's own video software. (This is due to gaming API problems and the DirectX versions you are using for the games you want to play.)\n\nWhy don't you just watch the Gamer's Nexus video about the current status of Intel ARC video cards from https://www.youtube.com/watch?v=b-6sHUNBxVg\n\nMy opinions are NOT false. They never were and they still stand. And the 'massive' improvements are basically allowing games that use older video applications that use DirectX versions before 11 to be playable. That doesn't mean most games that are playable now have improved 20%, not even 10%. You won't like what they did to get the so-called improvements in some games.\n\nI gave you some backup information and even a video reference. That's not a 'lazy' opinion.",
      "here ya go! A video that isn't over two months old... [https://www.youtube.com/watch?v=teXtQ\\_hRXYM](https://www.youtube.com/watch?v=teXtQ_hRXYM) \\--  After recent updates, it's beating out BOTH of the cards you mentioned at pretty much all of the tested games in 1440P and 4K gaming... And this is just the beginning, with every driver update it will become better and better. With RT enabled.... the Arc 770LE absolutely demolishes the cards you mentioned, get your head out of your ass, please",
      "This just keeps ageing so well."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "a770"
    ],
    "title": "Current state of VR support on Intel Arc (A750, likely also applies for the A770 and A380)",
    "selftext": "I hadn't posted about this here yet, but I am a collector of VR Hardware, and when the A750 and A770 first launched in Germany, I was very eager to try out VR performance on as many headsets as I could.\n\nI personally bought the A750, mostly as I intend on mostly using the card for AV1 encoding in my Server, but might as well try VR on it as well I thought. Here's my findings so far:\n\nThe Intel Arc drivers currently **don't seem to support Direct Display Mode devices at all**. This is a requirement for VR headsets that use DisplayPort or HDMI to receive the image they're supposed to display. That means that I wasn't able to test any of these headsets, and I've tried a lot of them: Oculus/Meta Rift CV1, Valve Index, HP Reverb G2, Vive Pro 2 and the Varjo Aero (tho in the case of the Varjo, there was also a software lock-out, as they don't want their customers running into issues with unsupported GPUs, which also includes ones from AMD).\n\nOfficial Link support with the Quest (both with a cable and wireless) also wasn't working. Quest Link refused to find it's connection (the PC dialog never found the headset's USB connection and I never got the Quest Link prompt in the headset itself). and Air Link straight up threw me an error that the Video Encoder was unsupported. I have no idea what kind of magic other people have pulled off to get Air Link or Quest Link working on Intel Arc, I sure wasn't able to.\n\nI was however able to run VR on my A750 through 2 other means. For one Virtual Desktop basically runs on anything that has a DirectX 11 capable video output and a CPU, so that worked with no issues at all on Arc too, and I was also able to use my Vive Pro 1 using the Vive Wireless adapter.\n\nWith Virtual Desktop performance was quite stellar and I didn't really experience any stutters or similar. With the Vive Wireless Adapter however, I immediately started noticing hitching and stuttering every so often. The more CPU heavy of a game I tried, the worse the stuttering got, hinting that large parts of the stuttering may have to do with GPU drivers reliance the CPU.\n\nFor reference, I'm running a 5900X with 32GB of DDR4 3200MHz CL16 memory.\n\nIf anyone is interested in the performance numbers using the OpenVR Benchmark Tool, I'd be more then happy to provide them later as well. I've already run the benchmark, but the results I have not saved on my main data drive...",
    "comments": [
      "Yeah, this is true. Linus from Linus Tech Tips also faced this issue. Intel has responded saying they are in the alpha stage. They should be able to release beta drivers for VR support shortly.",
      ">The Intel Arc drivers currently don't seem to support Direct Display Mode devices at all.\n\nI am curious if the Direct Mode does work on Linux, since [the implementation is shared with AMD.](https://monado.freedesktop.org/direct-mode.html#intelamd)",
      "No unfortunately not, and I was not aware of it and bought a NUC with arc card to play specifically VR....",
      "I’m running the same specs as you plus an A380 (cause why not see what it can do with VR). Oculus/Meta Rift S didn’t throw me any errors, just said display port was disconnected even tho it was connected.\n\nOn the Intel discord, I’ve brought this up and have been reassured multiple times that VR isn’t software blocked nor hardware blocked. So I’ve been assuming that Oculus/Meta VR and Steam VR hasn’t whitelisted them yet for the direct connection headset. I’m thinking they’re waiting for the rest of the 40 series and 7000 series to release before doing a bulk VR whitelisting of said cards. *People with 40 series have been reporting the same problem so I’ve heard*\n\nFrom what I understand, the wireless alternatives ignore the gpu hardware check and goes by what the gpus can run as you said",
      "I know you probably won't respond but, since then have you tried again? And if you did has oculus link still worked?",
      "I'm curious, at a collector of vr hw, you likely have some great opinions.  I'm looking to buy an older lower cost but still serviceable vr computer to drive an oculus 2.  Only need to do something like Alyx at \"ok\" levels.  It's hard to discern where that sweet spot of age vs functionality vs price is.  If you've thoughts on those lines, love to hear them!",
      "still waiting :(",
      "As soon as a Mesa driver version with Arc support ships with PopOS, I will try that out. I'm just not familiar enough with Linux yes to feel comfortable modifying the system itself through the terminal ![gif](emote|free_emotes_pack|sweat)",
      "40 Series does work with all the headsets. Nvidia changed something with the Framebuffer handling in Ada Lovelace that causes heavy stutters in VR at higher resolutions if not accounted for.\n\nThis whole topic was already discussed in great detail on the official Varjo discord, and Varjo has already released a patch to fix 5hose stutters on the 40-Series GPUs.\n\nIntel Arc isn't blacklisted or anything. Direct display mode devices shouldn't show up to the Windows desktop, but they do on Arc, and usually at the wrong resolution too. The Rift CV1 showed up at the right resolution, but I'm guessing, since it wasn't found by the Oculus software, that there's some flag missing, like HDMIs 3D side-by-side display stuff. The Index shows up as a 640x480 display, and looking into the SteamVR web console it actually says that no display with the right resolution was found, but it does list all displays connected to the Arc GPU. And in the case of the Reverb G2, the display didn't show up on the desktop (it's a built-in Windows driver, go figure), and WMR did actually launch as if everything was working right, but the displays in the headset stayed black. My guess is that yet again, the driver wasn't able to initiate the vorrect display mode (resolution, refresh rate, direct display, 2 DP lanes per screen, etc.) on the G2...\n\nThe tl;Dr is, it doesn't seem to have anything to do with the software, at least for the display connections, it seems to be a driver related issue with the hardware not showing up correctly...",
      "I haven't, tho my intention was to at some point do a 30 day Arc trial, using nothing but my Arc card for 30 days and when I do I can try Quest/Air Link again :D",
      "Honestly, that I can't really say where the sweet spot for price to performance is for VR capable PC Hardware. What I can say is that you'd want a GPU with at the very minimum 6GB of VRAM (which at least all of the Intel A7 cards do fulfill), at minimum 16GB of System Memory and preferably 6 CPU cores with HT or more. I wouldn't to older then an Intel 8th Gen CPU (or the generation after if you're going Team Red) and in terms of GPU not older then Nvidia 10-Series or RX6000.\nIf you have a Quest 2 then that leaves Intel Arc also as a GPU option open for you, as long as you're using Virtual Desktop (as like I've said before Headsets that require a Display connection to the GPU currently don't work on Arc and the Oculus software refuses to work completely on Arc)",
      "They have not fixed it yet!!!?",
      "You could try something more up-to-date like Fedora or endavourOS.\n\nI'd be very interested to see whether this works.",
      "Okay, so there has been some change as the driver updates have been releasing. My experience was with either the launch driver of the update after launch driver which just said *headset is disconnected.* I’ll try launching VR on my Rift S and see if I match what you’ve been experiencing",
      "It still says that, but at least with the CV1 a 2160x1200 display was showing up on the desktop.",
      "Damn. Sorry man"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "I7 4790k with arc A770 8gb",
    "selftext": "Hey guys ! \nI am looking for an upgrade path for my first ever pc build. In 2015 i built a pc with an i7-4790k 16gb of ram and a gtx 970.\nNow it s slowly turning 8 years old and while I don t really play a lot of intensive games some hiccups show up. Like Spiderman Miles Morales is running at about 50 fps and so on. Not huge deals but as PC s are a hobby of mine I started looking around. \nI slightly overclocked the cpu to 4.2ghz and in afterburner it says that the cpu is only at 60-70 ish usage while the gpu is at 90-95 ish. Looking around i found the new intel Arc A770 gpu with 8gb of vram at a reasonable price same price i paid for my 970 back in the day. \nSo I guess the question is \nWhat do you think guys is this a gpu that could work well in my pc and keep when I eventually upgrade my cpu, mobo and so on ?",
    "comments": [
      "Arc cards need a 10th gen or better cpu to function correctly. There is unofficial ways to get them to work but YMMV.\n\nIf your motherboard doesn't support resizable bar, then the ARC's performance will be absolutely horrendous.\n\nI'm extremely happy with the A770 but you really really need resizable bar for it to do well.",
      "Do that and buy a 6700xt, not a A770, that should be quite good tbh",
      "Don't bother, get a 6700xt, 6650xt or 6600xt, you will be bottlenecked by the 4790k, but it's a far better way to go if you plan to upgrade the rest of the PC soon.",
      "At no cost? There is not much to think about if it won't cost you anything. Even if it did, it's a considerable upgrade.\n\nI'm still baffled as to how are you considering not upgrading for free... What are you going to be waiting for? Do you expect someone to offer you a 13900K in exchange for your 4790k? It's a great CPU, I loved mine, but come on...\n\nThe only drawback is that it doesn't have an iGPU, but you have a graphics card, so that shouldn't be a problem.",
      "Absolutely do this free swap, you are being gifted basically. Just make sure those parts are working before you do it of course so no one screws you over.",
      "Do you think the processor mobo combo is good enough for one ?",
      "If you are upgrading to the i7 9700f then it will be absolutely fine and that's still a quick eight core processor and should be an ample match for that ARC770",
      "Another Question \nI have the chance to swap my z97 mobo and i7-4790k processor 16gbs of ddr3 for a z390 mobo with an i7-9700F 32 gbs of ddr4 at no cost just a straight swap. Is it worth it ? Should i just wait as these are pc components released in 2019 and are not a huge deal better than what I already have ?",
      "> If your motherboard doesn't support resizable bar, then the ARC's performance will be absolutely horrendous.\n\nIs there benchmarks for that?",
      "Please someone answer this.  8th gen on z390 w rebar enabled checking in here",
      "Ey thanks man \nI guess i just needed someone to point out the obvious. I overthink something like this cause i don t want to mess up a working system",
      "https://www.intel.ca/content/www/ca/en/support/articles/000091128/graphics.html",
      "I would take the free upgrade platform and add the A770 but get the 16gb vram version and the drivers for Intel are improving massively",
      "There is a way to mod a bios for older chipsets to Unlock ReBAR, but I did not do this for my 5775c yet and can't say anything about it, except for internet claims that the mod works as intended. Still much better to upgrade to literally any modern platform.",
      "Doesn’t it work with 9th gen if rebar is enabled?",
      "So you think i m good to buy a gtx 2070 used and rock my old setup for a while longer ?",
      "I know the official specs but I’ve seen people run it on older hardware as Intel pretty much supports rebar on older gen cpus as long as the MB does it"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Stable diffusion benchmark. ARC 770 close to 4070.",
    "selftext": " [Stable Diffusion Optimized for Intel Silicon Boosts Arc A770 Performance by 54% | Tom's Hardware (tomshardware.com)](https://www.tomshardware.com/news/stable-diffusion-for-intel-optimizations) ",
    "comments": [
      "*looks at benchmarks*\n\nI see the A750 just above a 4060, and the a770 behind the RTX 4060ti.",
      "*It's basically a 4090!*\n\nStill a great improvement, though. Nice to see the progress.",
      "i’ll be buying one if benchmarks can substantiate the info.  Wonder what the power usage will look like..",
      "Depends on the power draw if it's a buy or not, price not too bad for 16gb vram in the current market. The performance isn't great in SD, considering older nvidia cards can be faster for similar price.\n\nI'm running into a vram limitation with the 2080ti, though the performance isn't bad with an undervolt draw of 160w. Im definitely looking for 16gb or more vram, but also power efficiency and performance.",
      "Similar story with [FluidX3D](https://github.com/ProjectPhysX/FluidX3D) CFD: the A770 even slightly [outperforms](https://github.com/ProjectPhysX/FluidX3D#single-gpucpu-benchmarks) the 4070 here. This is due to the higher VRAM bandwidth.",
      "Is this on Linux or Windows? Do you need the newest Mesa/Linux kernel? Possibly versions not out yet?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Is the Arc A770 stable?",
    "selftext": "I built my PC in the middle of the GPU crisis, so I have an i5-12600K paired with a Quadro T600 (a cut-down, slower 1650). \nIt is holding up decently, but it is starting to struggle in Premiere, After Effects, Resolve and Blender. \nI'm in between the 3060 Ti and the Arc A770 16GB. The former is more poweful and possibly more reliable, but the Arc has AV1 encoding and twice as much VRAM.\nI'd like to get the Arc, but how are the drivers? Is it stable enough not to cause issues in the programs I use?",
    "comments": [
      "Yeah sounds like ark is completely viable now dudes from gamers nexus and pc gamer recently talked about this.",
      "Can someone answer? I also want to know",
      "My a770 has been great except for some old directx9 games. No noticeable driver issues here.",
      "Had no problem yet with my A770,played various games ranging from dx9 to dx12 to vulcan",
      "Post on /r/IntelArc",
      "Interestingly, Intel recently launched the workstation versions of Arc GPUs such as the Arc Pro A60 12 GB. I just don't see where to purchase them.   \n\nhttps://www.intel.com/content/www/us/en/products/docs/discrete-gpus/arc/workstations/a-series/overview.html",
      "Doesn't crash, programs run as they should, no graphics glitches...",
      "In resolve, if I enable use Intel neural engine \"optimization\", crashes whenever using superscale (and text may glitch when you overlay them, idk if it's fixed now). Planar motion tracking may cause hard lock of the system. Overall ok, but your mileage may vary. I kinda have to use my much weaker Nvidia system to bail arc out sometimes.",
      "I can't say that there are no issues, but the drivers are the best they've ever been. When they fix the sleep mode bug, then I think I'll finally be able to say that Arc is viable for the average user, if still fairly inconsistent.",
      "Define stable, but for you nvidia is best choice ATM."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Is there Intel arc a770 VR support?",
    "selftext": "Hello everyone I’m currently looking into getting the a770 for a build but I want to know if there is any VR support or improvements since it has launched, I have the quest 2, any help or info would be very appreciated thanks.",
    "comments": [
      "No support.",
      "Looks like this guy said it works with virtual desktop only right now (no airlink or oculus link), but said it ran a few games well.\n\nhttps://www.reddit.com/r/oculus/comments/104fs86/oculus_pc_vr_w_intel_770_750_gpu_yes_you_can_play/",
      "How sad🥲",
      "This is still a 'priorities' stage, and system stability and overall performance is more of a priority than VR support. Arc isn't really powerful enough for an enjoyable VR experience anyway.",
      "Stay Tuned™",
      ">Arc isn't really powerful enough for an enjoyable VR experience anyway.\n\nPretty sure a A770 is faster than my 980ti, which was fast enough for most VR content.",
      "Yeah that's pretty surprising",
      "Just like the 7000 series you aren’t missing out much",
      "I would like to ask any recommended VR device to recommend?",
      "No official VR support and not with SteamVR headsets when tested in late December/\n\n[https://babeltechreviews.com/intels-arc-cards-do-not-work-with-native-steamvr-headsets/](https://babeltechreviews.com/intels-arc-cards-do-not-work-with-native-steamvr-headsets/)\n\nHowever, Arc A770 worked with Reverb G2/WMR in January and was benchmarked.\n\n[https://babeltechreviews.com/first-look-at-arc-vr-performance/](https://babeltechreviews.com/first-look-at-arc-vr-performance/)",
      "The ARC 770 is more powerful as my RTX2060super, so it's more than capable for VR (I'm using HTC Vive Pro/wireless)."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "a770"
    ],
    "title": "Intel Arc Alchemist desktop series including A770, A750, A580 and A380 SKUs reportedly delayed till late Q2/early Q3",
    "selftext": "",
    "comments": [
      "Honestly, it doesn't matter.\n\nThey missed their window by not launching during the GPU apocalypse uncontested, so now they have to go against Nvidia and AMD offerings with mature drivers in a health(ier) market.\n\nMight as well take the L at this point and just figure out your drivers.",
      "What they've officially stated is launch in 'summer', compared to mobile 5/7 'early summer'. Anything past 'early summer' puts it in calendar Q3.",
      "They're going to have to seriously discount those cards to sell next to Nvidia and AMD. Can't see many people taking a chance on the first gen of cards especially in the enthusiast market when the top Arc card is only at 3070 level of performance. \n\nAnd then by August and September all of the talk will be on RTX 40 and RDNA3 which will blow these cards out of the water.",
      "This will get steam rolled by Lovelace and rdna3. Should’ve released this in June 2021",
      "Not if it's dirt cheap like polaris. \nBoth lovelace and rdna3 will be starting at minimum 400$ for the lowest model.",
      "Their gonna take an L. The market is getting better and by the time it comes out, amd and nvidia will have their gpus at a few % above msrp. Therefore making these card useless. They won't be faster, they might be a bit less expensive, and their launching gpus close to next gen.",
      "To be honest, even if they launched in June 2021, they'd have probably all been gobbled up by miners.",
      "most of them are probably going straight to OEM.\n\nOEMs will use it as a way to tell AMD/Nvidia to fuck off with their horrid \"incentives\" like priority allocation. Most people buying computers don't know what the hell goes in them so the main problem is drivers. if the drivers continue to be miserable then intel will get nowhere even giving these things away for free.",
      "At least they would’ve sold (Not defending miners). Now they have zero chance of becoming dominant in any market",
      "More competition, more fun.",
      "Well considering the frame stuttering in the arc a350m for mobile that was recently tested, my money is on bad drivers and at this point Intel should just work on the drivers. They missed a critical chance to disrupt the market. Now new gpu prices have fallen down hard. Not yet at msrp but much better. The second hand market will be flooded with rdna2 and 3000series from Nvidia and not mention the rtx4000 series and rdna 3 gpus coming soon. This will be really rough  on Intel.",
      "When will Intel learn that blatantly lying to investors will get them nowhere?",
      "Efficiency wise it's better than GTX 1600, at least the already released laptop 350m which at around 40W has same performance as GTX 1650m at 50W. \nThat's not too bad. Probably on the same level as RTX 3000 as efficiency didn't improve much compared to GTX 1600 and 2000.",
      "Well, thats basically all she wrote for ARC...\n\nI know for a fact Intel won't price them relative to their performance vs other cards.\n\nI hope I am incorrect, as aggressive pricing is the only thing that will save them. That creates an even bigger problem for them though, if they start out low priced, they will remain there for many years. Thats just the way the market works.",
      "They're making them at TSMC so it didn't matter. if these were being made at Intel fabs, well they'd be worse if they were being pushed out on a broken 10nm a year ago or 14nm due to power efficiency.\n\nBeing made at TSMC means when TSMC were starved for capacity these would have been even worse.\n\nUsing TSMC 6nm indications would be that performance is closer to AMD/Nvidia parts that use half the die size. \n\nThe only way these matter in the first gen or two is if Intel does it's Atom/phone/tablet tactic and just firebombs pricing to force their way into relevancy but if the architecture doesn't catch up then eventually when they try to charge real prices they'll be pushed out.",
      "Mmmhmm.",
      "More competition more fun works if wafer supply is ample and performance is competitive. If performance sucks and they eat up supply of incredibly limited wafers then it's terrible for everyone, Intel included. They'll be eating shit selling these at below what they cost to produce and AMD/Nvidia lose out supply to make twice as many gpus from the same wafers and the end user loses out.",
      "Thats just clock gating. Easily fixable.\n\nLook at the GPU speed every stutter, you will see it drop to 1150mhz.",
      "TSMC 6nm is a retooled 7nm, so the density is not that dissimilar.",
      "How is it fixed if u dnt mind explaining?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Intel Arc A770 on an old machine",
    "selftext": "Hello, my PC is rather old and I was wondering if it would benefit me to upgrade to an Intel Arc Card or keep using a gtx 1070ti?\n\nMy processor is an i7 6700k with 32gb of ram and the Motherboard is Asus Z170-A. Windows 10 tells me I cannot upgrade to windows 11 so I'm not sure if getting an Arc card would benefit me more, or just get an AMD graphics card?",
    "comments": [
      "Intel recommends 10th gen or newer for REBAR. Arc really needs resizable BAR.\n\nYou’d probably get more out of a CPU/MoBo upgrade than A770 on an “ancient” 6700K.",
      "If you can’t get REBAR don’t even consider an Arc GPU at this point. Intel officially supports that back to 10th gen CPU but most motherboard manufacturers added it to 300 series motherboards(8th and 9th gen CPUs)",
      "For old CPU like that, it's best to go for AMD GPU, they scale best on slower CPUs compared to Nvidia. And Intel is worst as it needs Rebar .",
      "Absolutely not. If you're not running a pretty modern rig, Intel is definitely not an option.\n\nGet AMD card instead, those work well with weaker CPU's. Probably some discounted 6000-series card is the best bet.",
      "Outside of a few niche cards, the gen 3 to gen 4 difference is like 3%. Hardly enough to get worried about.",
      "Some older boards have hidden settings for Re-bar. If you're confident enough to go down that rabbit hole, you could do that.",
      "Arc is cool and I want to support competition but no, without resizeable bar on 10th gen Intel or Ryzen 3000 the performance would not be worth it and could even go backwards.\n\nDepending on availability in your area a rx 6700 from AMD could work",
      "Well your options are limited. Arc GPUs need REBAR support, and AMD/Nvidia need PCIE 4.0. \n\nYou can of course still hook them up to a PC with PCIE 3.0, but you'll be losing a good chunk of performance. A whole platform upgrade is the best option at this point."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "16GB Arc A770 in Stock at Newegg. Just ordered. Question anyone try Steam Link or Parsec etc... streaming? Does the encoder do well vs AMD, Nvidia?",
    "selftext": "So wanted to post that Newegg had it in stock.  But also figured i would ask about Steam game streaming whether parsec, steam link whatever other ways there are.  And if it worked well?  I know Nvidia is supposedly better at this than AMD because of the encoder but curious how Intel Arc compared.\n\nThanks!\n\n\nhttps://www.newegg.com/intel-21p01j00ba/p/N82E16814883001?Description=arc%20a770&cm_re=arc_a770-_-14-883-001-_-Product",
    "comments": [
      "They've said time and time again that even if the alchemist GPUs flop, they will continue with their plan for future GPU architectures. They've invested billions; they're in it for the long haul.",
      "AMD has the worst encoder out of everyone, and nvidia has the best (considering the highest end option for both AMD INTEL and Nvdia)\n\nThis is from the best to worse\n\n1. Nvenc \n2. Quicksync with a dedicated gpu\n3. Quicksync with a iGPU\n4. Amd encoder",
      "I have a A770 but don't use it for any streaming, however I do hope you're not using the card in a primary system. You're going to spend a lot of time trying to get things to work, and if you do, will have to settle with mediocre performance. I don't mind the tinkering as I have it in a secondary system I play around with but no way would I use it in my main system for gaming- at least not yet.",
      "Hello, thanks for the warning but i saw the reviews and I am aware of the issues.  It will go in my main system but I will work around the issues and i have a laptop for work.  So its mostly a curiosity and to see how it changes over time and to support a 3rd player i guess too.   \n\nI have been playing a lot of 90s adventure games and Vampire Survivors lately so i should be alright ha!  Otherwise i game on Xbox and XCloud mostly.",
      "Just to add to this. This may change with the new AV1 encoder that amd is saying for their new gpus next month. \n\nSo it could change but its unknown at the moment.",
      "And hopefully they aren't in it the way Google claimed to be investing in Stadia for the long haul.",
      "I'll be curious to see if these continue to sell well. I'm sure Intel is paying close attention to Arc sales in determining the future of their GPU sector.",
      "Both the RTX 4090 and 4080 are much more efficient than their 30-series counterparts. For the same or even less power usage, you are getting 50-100% more performance. From purely a power efficiency standpoint, it'd be strange to consider the 30-series impressive but suggest the 40-series isn't.",
      "Thank you for your sacrifice. I want good things from Intel and people like you putting the money into the risks are what will propel that forward.",
      "I've got my eye on a Battlemage series card for sure! I want to see how these do, but NVIDIA has become just so power-hungry (take that however you like). 30-series was impressive, I dont' know why but 40 series kinda isn't. I don't run actual space heaters because of the electrical cost, I don't want to have the same feeling about my PC.",
      "That, and better software to use it. We'll see how it goes.",
      "We need people like this, putting themselves forward to help Intel out with driver issues, hardware issues, and who knows what else. I think that they can genuinely be a great contender if they stick with their low cost GPUs, especially with NVIDIA pulling their shenanigans",
      "No billion dollar multinational needs charity from its customers whatsoever. We don't need end users suffering so Intel can half arse their drivers.",
      "That's pretty typical for Google though"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "111°c On Intel Arc A770?",
    "selftext": "I recently stress tested my New Rig using 3DMark's Time Spy and got a score of 13,450, but my gpu temps were 111°c. Is that abnormal or is that just typical Benchmark/Stresstest temps? Im fairly new to this stuff and Im still learning.",
    "comments": [
      "Score is ok, I highly doubt you temps though. Under load temps for this card are generally around 70-75°",
      "Check the reviews for this card, you will see that 111c is very high.",
      "What's your CPU temp like? Other case components? Is your case properly ventilated? If so, you need to contact Intel tech support because you may have faulty thermal paste in your GPU. I would advise against taking this card apart. If you were to watch gamers Nexus's tear down, you will see how much of a nightmare it must be to put back together. The bifrost probably isn't too bad, but the Intel LE looks pretty scary.",
      "Ik that 111 is high but fir a stresstest idk if that would be typical?",
      "What software are you using to check the temperature? The gold standard is hwinfo64, so I would check with that.",
      "Time Spy (especially the non-Extreme version) is not that heavy of a stress test. If you're hitting 111c in that, then you will have high temps in regular games as well, unless you play really old games.",
      "I don't know what I should do about this. I don't want to take the card apart for fear of breaking something. I know I don't have the best airflow but ever since I got the card I've been looking at temps and it sits at around 70-75. Im using basic fans that came with the Deepcool LS720 360mm in the front and The case fans that come with my current case the Corsair 4000d RGB. I do get a lot of air pushing out the back of the case. I'll update you when I get home.",
      "Was it a visual glitch?",
      "Return it. That’s super not normal",
      "Check your GPU fan speed readings with HWinfo, and check if the actual fans are spinning and working properly, I mean by actually looking at GPU when it is under load.",
      "Damn... my 2021 laptop scores that high.",
      "Same here, gaming is 70-75° tops for me.",
      "Im going to run it again.. maybe the fans werent doing its thing or it was giving me a false read 🫡",
      "IVE RESOLVED THE PROBLEM"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Arc a770 or the RTX 3070",
    "selftext": "Been doing some research on the intel arc a770 with recent updates with the card 4090 update, this card from what I've seen can ultra most games I looked at warzone 2.0, borderlands 3, guardians of the galaxy, and metro exodus, and red dead redemption 2, all these games maxed out, 70 to 120fps varies with these games lol but still thats good maxed out, now since intel is still figuring out their card is it worth to buy now and be a beta tester for the card or just buy a 3070 and be happy lol, I found a 3070 on fb for 600$ talked down to 500$ Canadian money, and the intel arc memory express for 479$ plus a bonus comes with modern warfare 2 for battle net. \n\nWhich one of these cards would u recommend the most ?\n\nSpecs \nRyzen 5 5600 \nGA-A320M s2h \n2x8 corsair rgb ram at 3600\n500 plus 80 evga\n1650 OEM model",
    "comments": [
      "3070 for $500",
      "I would go for the 3070 yet. One problem is I can’t really find good comparisons on YT. In one video it is on par with the 3070 on 1440p in the next it is around 30% slower in the same games. But if you want to give the Arc A770 a chance go for it if you play on 1440p (it should be the best resolution for it), imo it has a great improvement potential with his drivers.",
      "it will boot, but shutdown under load. depending on your cpu at how much load it will",
      "Definitely go for the 3070, ARC GPU's are really new and dont have great driver support yet. And if you can find a 3070 for a decent price I would definitely go for it.",
      "GPU'S should be half their price and we would replace them much more often. They have ramped out of logic in 4 years.",
      "Definitely go for 3070. \nIntels first gen gpus just aren’t worth it for most of us who only upgrade gpus every 2 years or so, next gen of intel could be very promising thought",
      "3070.\nmuch better driver support.\n\n\nnVidia launch driver. 550MB (now 800MB)\n\nIntel. 1200MB.. and they still get their act together.",
      "Used 3070 or new a770 for the same price ?",
      "Intel arc a770 is 349.00 founders edition!?",
      "I found this \n\n\nKey Differences\nIn short — GeForce RTX 3070 outperforms the cheaper Arc A770 on the selected game parameters. However, the worse performing Arc A770 is a better bang for your buck. The better performing GeForce RTX 3070 is 771 days older than the cheaper Arc A770.\n\nAdvantages of Intel Arc A770\nUp to 51% cheaper than GeForce RTX 3070 - CA$477.99 vs CA$972.51\nUp to 51% better value when playing Call of Duty: Warzone than GeForce RTX 3070 - CA$3.32 vs CA$6.71 per FPS\nUp to 100% more VRAM memory than NVIDIA GeForce RTX 3070 - 16 vs 8 GB\nAdvantages of NVIDIA GeForce RTX 3070\nPerforms up to 1% better in Call of Duty: Warzone than Arc A770 - 145 vs 144 FPS\nConsumes up to 2% less energy than Intel Arc A770 - 220 vs 225 Watts",
      "If I were to make a video what would you want me to test?",
      "Would 500 plus 80 be enough for the 3070 as is ? Like will it boot in other words",
      "500 used 3070 and a new arc a770 for like 459$ new"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "What can cause Intel Arc A770 unstable, mostly low gpu usage in some games?",
    "selftext": "In some games my A770's usage is between 20 and 100%, mostly around 40%, I have huge lag spikes and fps drops. The cpu usage is low in every game. I have deleted all of the old nvidia drivers with DDU and did a clean arc driver install. Some games just working fine, even older games, with 99% gpu usage.\n\nMy config: i7-8700k, Z370 Taichi, 16GB 3200Mhz CL16 ram, 2TB nvme ssd. Will the cpu be the problem?\n\nRebar is turned on.\n\n&#x200B;\n\nedit: temperatures are under 65 celsius. I have tried from 1080p to 2160p, every settings, but nothing has changed.",
    "comments": [
      "Some games just run very badly on arc, it's because of the drivers that are far from fully optimized",
      "DX12 and vulkan games should all be alright, dx11 most of them should be ok, dx9 is mixed bag",
      "Yes, it’s in the description.",
      "You require resizable bar on in bios pick dx12 games best!",
      "Could bump power or voltage in oc settings see if it stabilizes",
      "GPU and CPU usage is not a good indicator of bad performance.",
      "I also have an 8700K and Asrock Z370 taichi 32g cl32 ddr4 and 3 NVME M2 drives.Win 11. Have my A770 installed about 2 weeks. I only play medium to low level games but it's been pretty much flawless. The only problem I had was when I screwed around with bios settings trying to reduce the 40W idle power consumption. I crashed windows with a tdr video error. Once I put the bios settings back I have had no problems. Running with the Feb Intel drivers.",
      "Do you have resizeable BAR enabled? If not, performance will suck bigtime!",
      "Is your ReBar really on? When I first got my a770, I had my ReBar turned on in the bios, but according to Intel software, it wasn't. Turns out, I needed to switch from MBR to GPT for ReBar to work properly. \n\nIf that isn't it, could be hardware incompatibility. While it works, it isn't working to the best of its ability.",
      "If you have rebar on, then it's probably either a cpu bottleneck (unlikely most of the time with an 8700k) or buggy drivers with the game you're trying to run (far more likely). For example, I only get around 50% gpu utilization in F1 2020 and F1 2022 with my A370m.",
      "I don't know how, but 2 games now are running much much better. Dota 2 is playable now, but still has some small fps drops, but Valheim runs perfectly (4K, almost max settings with stable 60 fps (without vsync 70+ fps).\n\nI have to download some more games, I played on PC 2 years ago and I don't have games on it.\n\nBut I think cpu is okay.",
      "I wanted to update this thread, but I don’t have time to test the pc. I’m just sleeping, because I got covid. As soon as I will be better, I will update this post.",
      "Yes, I know. Valheim was unplayeble few minutes ago. Between 1 and 80 fps I had everything."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Is the intel arc a770 good/decent at gaming?",
    "selftext": "I’m aware it’s a cheaper gpu but from what I’ve been researching on my own it’s comparable to a 4060/ti while being 100/150 dollars less.\n\n I mainly play games like Fortnite, CoD and other 3rd person shooters/fps and I’d like to play in 1440p at 100hz+. (I’d assume it can do that as my laptop with a 1660ti hits 120fps in Fortnite with my settings in creative and 60fps+ in games with my settings at 1440p) \n\nI’d pair it with the intel i7-13700k so would that be a good pair?\n\nThank you to anyone who helps 🙏",
    "comments": [
      "The main issue with the arc cards is the drivers. Since they’re new and not really in high use, not many games focus on optimizing for them. Intel is frequently updating drivers to improve performance which saw a recent uplift of up to 200% increase in performance in certain games. If you’re getting it brand new for like $250, it’s definitely a good deal, just make sure you have a lengthy return window and don’t void the warranty. In theory it’s a great investment but only time will tell. I recently put an a750 in my old computer to upgrade a 1660 and it’s been solid aside from starfield. Game barely ran but I’ve heard that’s been fixed since. I believe there’s a similar Intel variant of smart access memory so having your cpu be able to access gpu memory if necessary can help for shooters at 1440p. I will say, if you can scale back the cpu to a 13600kf or even 12th gen and get a 4070 instead, you’ll have a much better experience at 1440p as it’s more gpu bound than 1080p."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Arc A770 16GB - OpenCL performance",
    "selftext": "I am running **clpeak** on Arc A770, and I am getting half of advertised half-float performance: \n\n    $ clpeak\n    Platform: Intel(R) OpenCL HD Graphics\n      Device: Intel(R) Graphics [0x56a0]\n        Driver version  : 22.49.25018.23 (Linux x64)\n        Compute units   : 512\n        Clock frequency : 2400 MHz\n    \n        Global memory bandwidth (GBPS)\n          float   : 363.95\n          float2  : 383.93\n          float4  : 385.50\n          float8  : 396.99\n          float16 : 400.12\n    \n        Single-precision compute (GFLOPS)\n          float   : 12338.24\n          float2  : 10562.94\n          float4  : 9856.40\n          float8  : 9476.16\n          float16 : 9204.72\n    \n        Half-precision compute (GFLOPS)\n          half   : 18397.46\n          half2  : 18378.09\n          half4  : 18413.70\n          half8  : 18233.57\n          half16 : 18371.29\n\nAdvertised performance ([https://www.techpowerup.com/gpu-specs/arc-a770.c3914](https://www.techpowerup.com/gpu-specs/arc-a770.c3914)):  \n\n\n    ...\n    \n    FP16 (half)\n        39.32 TFLOPS (2:1) \n    \n    FP32 (float)\n        19.66 TFLOPS \n    \n\nThe GPU is rendering idle desktop during the test but that should have minimal impact.\n\nWhy factor x2 difference? Is the website accurate?\n\n(Debian Testing/kernel 6.3/CPU: i9-12900k)",
    "comments": [
      "I think this is somewhat of a problem with clpeak. I get very similar results with my A750:\n```\nPlatform: Intel(R) OpenCL HD Graphics\n  Device: Intel(R) Graphics [0x56a1]\n    Driver version  : 22.49.25018.23 (Linux x64)\n    Compute units   : 448\n    Clock frequency : 2400 MHz\n\n    Global memory bandwidth (GBPS)\n      float   : 396.12\n      float2  : 396.38\n      float4  : 402.52\n      float8  : 408.13\n      float16 : 410.27\n\n    Single-precision compute (GFLOPS)\n      float   : 11371.21\n      float2  : 9733.42\n      float4  : 9092.19\n      float8  : 8757.09\n      float16 : 8488.46\n\n    Half-precision compute (GFLOPS)\n      half   : 17088.24\n      half2  : 17036.59\n      half4  : 17067.08\n      half8  : 16973.48\n      half16 : 16903.71\n\n    No double precision support! Skipped\n```\n\nBut my own OpenCL benchmark program reports the correct values:\n```\n|----------------.------------------------------------------------------------|\n| Device ID      | 0                                                          |\n| Device Name    | Intel(R) Graphics [0x56a1]                                 |\n| Device Vendor  | Intel(R) Corporation                                       |\n| Device Driver  | 22.49.25018.23                                             |\n| OpenCL Version | OpenCL C 1.2                                               |\n| Compute Units  | 448 at 2400 MHz (3584 cores, 17.203 TFLOPs/s)              |\n| Memory, Cache  | 7721 MB, 16384 KB global / 64 KB local                     |\n| Buffer Limits  | 3860 MB global, 3953458 KB constant                        |\n|----------------'------------------------------------------------------------|\n| Info: OpenCL C code successfully compiled.                                  |\n| FP64  compute                                          not supported        |\n| FP32  compute                                        18.533 TFLOPs/s ( 1x ) |\n| FP16  compute                                        71.712 TFLOPs/s ( 4x ) |\n| INT64 compute                                         0.558  TIOPs/s (1/32) |\n| INT32 compute                                         3.632  TIOPs/s (1/4 ) |\n| INT16 compute                                        12.969  TIOPs/s (2/3 ) |\n| INT8  compute                                        13.201  TIOPs/s (2/3 ) |\n| Memory Bandwidth ( coalesced read      )                        265.82 GB/s |\n| Memory Bandwidth ( coalesced      write)                        257.27 GB/s |\n| Memory Bandwidth (misaligned read      )                        265.45 GB/s |\n| Memory Bandwidth (misaligned      write)                        267.82 GB/s |\n| PCIe   Bandwidth (send                 )                          1.33 GB/s |\n| PCIe   Bandwidth (   receive           )                          1.28 GB/s |\n| PCIe   Bandwidth (        bidirectional)            (Gen1 x16)    1.33 GB/s |\n|-----------------------------------------------------------------------------|\n```\n\nFor reference, here is it with early drivers on Windows 10. The single-precision benchmark in clpeak was broken, but my own benchmark program indicated ~16.5 TFLOPs/s.\n```\nPlatform: Intel(R) OpenCL HD Graphics\n  Device: Intel(R) Arc(TM) A750 Graphics\n    Driver version  : 31.0.101.3802 (Win64)\n    Compute units   : 448\n    Clock frequency : 2400 MHz\n\n    Global memory bandwidth (GBPS)\n      float   : 400.11\n      float2  : 397.44\n      float4  : 404.31\n      float8  : 410.60\n      float16 : 414.91\n\n    Single-precision compute (GFLOPS)\nclCreateBuffer (-61)\n      Tests skipped\n\n    Half-precision compute (GFLOPS)\n      half   : 17103.30\n      half2  : 17056.69\n      half4  : 17079.48\n      half8  : 17038.33\n      half16 : 16890.57\n\n    No double precision support! Skipped\n```\n\n```\n|----------------.------------------------------------------------------------|\n| Device ID      | 1                                                          |\n| Device Name    | Intel(R) Arc(TM) A750 Graphics                             |\n| Device Vendor  | Intel(R) Corporation                                       |\n| Device Driver  | 31.0.101.3802                                              |\n| OpenCL Version | OpenCL C 1.2                                               |\n| Compute Units  | 448 at 2400 MHz (3584 cores, 17.203 TFLOPs/s)              |\n| Memory, Cache  | 6476 MB, 16384 KB global / 64 KB local                     |\n| Buffer Limits  | 3238 MB global, 3316120 KB constant                        |\n|----------------'------------------------------------------------------------|\n| Info: OpenCL C code successfully compiled.                                  |\n| FP64  compute                                          not supported        |\n| FP32  compute                                        10.993 TFLOPs/s (2/3 ) |\n| FP16  compute                                        16.525 TFLOPs/s ( 1x ) |\n| INT64 compute                                         1.156  TIOPs/s (1/16) |\n| INT32 compute                                         3.839  TIOPs/s (1/4 ) |\n| INT16 compute                                        26.797  TIOPs/s ( 2x ) |\n| INT8  compute                                        10.262  TIOPs/s (2/3 ) |\n| Memory Bandwidth ( coalesced read      )                        251.20 GB/s |\n| Memory Bandwidth ( coalesced      write)                        408.41 GB/s |\n| Memory Bandwidth (misaligned read      )                        406.35 GB/s |\n| Memory Bandwidth (misaligned      write)                        441.38 GB/s |\n| PCIe   Bandwidth (send                 )                          6.84 GB/s |\n| PCIe   Bandwidth (   receive           )                          7.13 GB/s |\n| PCIe   Bandwidth (        bidirectional)            (Gen3 x16)    7.95 GB/s |\n|-----------------------------------------------------------------------------|\n```",
      "Thanks, interesting. Is your benchmark app publicly available, or can you suggest any other bench app?",
      "Not yet. Will upload it on my GitHub eventually.\n\nEDIT: It's opensourced now on my GitHub: https://github.com/ProjectPhysX/OpenCL-Benchmark",
      "The A770 is supposed to have 2:1 throughput for FP16 and you're seeing 4:1, I'd have expected to see roughly 36 TFLOPS at FP16, and you're getting a massive 71.712 TFLOPS. What's going on there?",
      "I've opensourced my [OpenCL-Benchmark](https://github.com/ProjectPhysX/OpenCL-Benchmark) utility now. Have fun!",
      "It's 2:1 FP16:FP32 for general compute, but 8:1 for XMX matrix operations. The benchmark does two fused-multiply-add operations on a \\`hlaf2\\` vector in an unrolled loop 512 times. It's possible that the compiler converts this to matrix operations for a 4:1 ratio.",
      "I gave it a try and I do not get as good results as you:\n\n&#x200B;\n\nI am curious how you are getting 71 TFlops of FP16, on Arc A750, while I am still on 18TFlops. \n\nI'll have a look at the code later. Anyway, thanks for sharing it, much appreciated!",
      "This seems to differ significantly between Windows/Linux Arc drivers. Not sure why.",
      ">I am running clpeak on Arc A770, and I am getting half of advertised half-float performance:\n\nYou're very unlikely to ever get the peak throughput, and especially not in a random benchmark not specifically made for your device."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Arc A770 worked fine until...",
    "selftext": "I installed an Intel Arc A770 in my rig running on an i7-9700k CPU set on a Gigabyte z-370 mobo with 32 GB of RAM. So, all was fine until I upgraded to Win 11 Pro. I ventured into Steam to play Cyberpunk and found my game hit play. It took a while to load up, but when it did, it stopped. A message screen came up informing me Cyberpunk has flatlined.  As I said before, I am running this GPU on a 9th gen CPU; it is recommended a 10th or 11th-gen.  Maybe I pushed my luck too far.  Okay, has anyone had gaming issues at all with this intel GPU?",
    "comments": [
      "Before my recent upgrade, I was playing with a 9600k. I can't see a 9700k being any issue. Given that the Intel GPUs still have some growing to do when it comes to drivers, the GPU would be my first guess as being the issue.",
      "If this was a windows upgrade and not a fresh install, I would approach as follows:\n\n&#x200B;\n\n1. DDU the intel drivers and reinstall from a fresh download\n2. if 1 didnt work, consider a clean install of win11",
      "Just clean install Win 11.",
      "Definitely the drivers. I did a fresh install of Win11 and both windows and Intel update assistant didn’t see the Intel card… just the iGFX. I scrubbed the BIOS but nope just had to download the actual gfx driver from Intel (not just the control center) now everything works. \n\nGood luck.",
      "All the drivers are up to date.  Intel Arc Control says the current driver version. 31.0.101.4644",
      "The upgrade was through Windows. I can reinstall with a media creation tool. Thanks for your input."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Intel Arc gpus prices in Europe",
    "selftext": "Does anyone know why Arc gpus prices are so high in Europe compared to North America?\n\n&#x200B;\n\nIsn't Intel Arc A770 supposed to be 350$?\n\n&#x200B;\n\nhttps://preview.redd.it/z0rh87ejbvfa1.png?width=920&format=png&auto=webp&s=f9a09cdf3029e58a58ffc2e4541919a68b792b50",
    "comments": [
      "Likely import duties, and remember the pricing you see in North America is always without sales tax, and in Europe, from my experience, it always includes it.",
      "Prices in the EU always have VAT included, which for hardware is 16-27% depending on the country. An MSRP of $350 translates to 370-410€ + import tariffs.\n\nIn Spain, Pc componentes is selling them quite expensive. They should be roughly 390€. Either the store or some national/local distributor are getting very high margins.",
      "Shouldnt you convert all the prices to a single currency unit to really compare? Nordic prices are they NOK, DKK, or SEK, regardless they actually look the cheapest of the lot",
      "German prices already dropped to 370 for the 770LE",
      "Electronics are almost always more expensive in Europe than NA. I haven't seen a situation where this hasn't been the case for me personally over the past 5 or 6 years.",
      "Because Europe is conglomerate  of thieves.",
      "the A750 is 274€ in Germany, which converted to USD is even less than $250 once you remove the VAT.",
      "Didn't know about that. \n\nThanks for the info!",
      "Still 370-410€ is still too expensive in my opinion.  \nFor example, in PC Componentes the Intel Arc a770 is 462€ when you have RTX 3060 that is going around for 376-450€, that being said going for a a770 is a no go for most people when you have a more \"stable\" GPU for cheaper price.",
      "The screenshot was taken from this [article](https://www.eurogamer.net/digitalfoundry-where-to-buy-intel-arc-gpus-american-and-european-prices).",
      "Good to hear, can you link some german pc shops that sell the card for that pricing so I can keep and eye out?\n\nThanks!",
      "I just hope that the prices of electronics in Europe start to drop, guess I have to wait and see.",
      "Oh, no doubt. I was just doing the math for the direct translation of the MSRP to the EU.\n\n$350 in the US is not equivalent to 350€ in the EU",
      "Its 379 but close enough I guess https://www.notebooksbilliger.de/intel+arc+a770+16gb+grafikkarte+782171"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Acer Predator BiFrost Arc A770 vs. ASRock Phantom Gaming Arc A770?",
    "selftext": "So let me start off by saying, I'm running on a [Radeon HD 6850](https://www.newegg.com/asus-radeon-hd-6850-eah6850-dc-2dis-1gd5-v2/p/N82E16814121419) (pre-dates Blu-Ray) on a Ryzen 7 3700X system.  I've been wanting to upgrade since at least 2019 but as everyone knows a bunch of things hit the fan between chip shortages and crypto mining.\n\nI originally wanted to buy either an RTX 4070, RTX 3080, RX 7800 XT, or RX 6800 XT.  Black Friday came and went and I didn't pull the trigger because the pricing still seems obscene to me.\n\nI've noticed the Arc 770 GPUs that Intel is putting out and I've heard the pros and cons.  I've accepted the fact that there are going to be shortcomings with the Arc GPU, but **ANYTHING** I buy will be a dramatic improvement over what I'm crawling on at the moment.\n\nIs there anyone out there who insists I should wait till the economy craps the bed possibly next Fall and get either nVIDIA or AMD, or do I settle with an Intel to get by with?\n\nIf I go with the Arc 770, I'm looking at either the [Acer Predator BiFrost](https://www.newegg.com/acer-predator-bifrost-intel-arc-a770-oc/p/N82E16814553001) or the [AsRock Phantom Gaming](https://www.newegg.com/asrock-arc-a770-a770-pg-16go/p/N82E16814930102).  My only experience with Acer in the past was buying a value-offering laptop with cheaper build quaility.  I've not bought an AsRock, but I'm aware it's Asus pedigree.  The Acer is $30 cheaper than the AsRock.  What I'm not clear on is which of the two OEMs offer a better cooling system and which has better support for their product?\n\n&#x200B;",
    "comments": [
      "I ended up getting a 6600 for $185 off of NewEgg on Cyber Monday.  I'm pretty satisfied with its performance, but then again ANYTHING would have been better.\n\nI was a little concerned with whether or not Intel would play the long game with their Arc GPUs, which is why I ended where I'm at.\n\nI'll jump up sometime in 3 or 4 years barring any future pandemic or chip shortage.",
      "I had this same battle in my head and actually just purchased the Bifrost A770 a couple days ago and it’s been amazing so far. The build quality and RGB is crazy it’s very well built (better build quality than my 4070 and 6800XT) and thermals between the two are about the same. I was skeptical on the Acer branding too but the Predator stuff tends to have a premium feel and it definitely exceeded my expectations. If you have a microcenter near you I snagged mine for $269 last Saturday."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Arc A770 vs RTX 3060 vs RX 6700XT - The FULL GPU COMPARISON",
    "selftext": "",
    "comments": [
      "We are not here to laugh. While nvidia currently is greedy we hope that by supporting both financially and mentally intel and AMD to do better we can get the gpu market Back to normal",
      "I thought dxvk could fix everything but the scores in ff14 leave a lot to be desired. Also being able to do so well in metro exodus yet worse in cp77 kinda weird outcome, since metro uses RTGI which is a heavier RT effect and 770 crunches through it fine.\n\nMy one hope is someone benchmarks this with DLSS2XeSS, but then again its really bizzare how FSR2 runs better than XeSS quality in spiderman.",
      "Not that unsual as the stock CP2077 is insanely bad...if the stock perf gets improved, the RT one will have a boost as well",
      "This!",
      "Not sure why we're even laughing.  Team blue is patting themselves on the back for paying red prices for blue performance in this video.  Green is still making money on having the best top-end that enthusiasts overpay for.  The same youtubers selling you on pooping on nvidia are simultaneously advertising for them.",
      "It’s going to be some time before the gpu market can be called normal",
      "Yea realized my mistake a month ago,",
      "I didn't even notice the post was 3 months old. lol",
      "Let’s take moment to laugh at team green"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Does Intel ARC have something like Radeon's VSR or GeForce's DSR?",
    "selftext": "basically the question is, **can you force/run a higher resolution using GPU scaling with ARC just like it can be done with RADEON/GeForce?** \n\nI run my 1080p144 monitor at 2160p with 150% scaling in windows desktop and game at native 1080p for games that don't feature FSR 2.0+ and at 2160p with FSR2.0 Balanced (or Performance) for the games that do feature the tech, a friend of mine is interested in an ARC A770 16GB for his creatives tasks (mostly 3D creative apps & video, casually gaming just from time to time) and wants to do the same thing, **this feature is basically THE BIGGEST reason he has been thinking of dropping from A770 16GB to RTX 3060 12GB** since he isn't rich and cannot afford to ALSO replace his current IPS 1080p monitor for a 1440p or 4K display but wants/needs the higher content real state, looking around online we haven't found an answer about this so we are asking here, he really wants that 16GB of GDDR6 and the video encoders featured with the ARC GPU but if he cannot run his 1080p monitor as 4K with ARC he will go GeForce.\n\nThanks in advance.",
    "comments": [
      "Why the old link? It's XeSS and it's not the same as DSR or VSR because... it doesn't work on video.",
      "I'd really encourage your friend to simply upgrade their monitor first. You can get a decent 4k panel for much less than either card, something like this: [Acer VG281K bmiipx 28.0\" 3840 x 2160 60 Hz](https://pcpartpicker.com/product/dP4Ycf/acer-vg281k-bmiipx-280-3840x2160-60-hz-monitor-umpv1aa001), and it will make a massive difference. Downscaling for content creation just seems like a bad idea.",
      "https://www.techradar.com/news/intel-teases-reveal-of-arcs-ai-powered-dlss-super-sampling-alternative-this-week",
      "he doesn't have the budget for that, buying another screen is not an option at this time since that will simply take the money away from the GPU purchase and delay him quite a lot (took him a couple months to save the $400 he currently has), guess he'll have to suffice with the 3060 12GB.",
      "It's actually a [work in progress](https://www.techpowerup.com/305558/intel-outs-video-super-resolution-for-chromium-browsers-works-with-igpus-11th-gen-onward), but it's some months that they are not giving new updates about it",
      "he bought a prebuilt with integrated graphics a while ago and is in the process of upgrading to a dGPU, since we could not find any info regarding ARC offering anything similar to GeForce's DSR he decided to get the 3060.",
      "https://www.reddit.com/r/IntelArc/comments/12tc6pg/super_resolution_in_mpc_video_renderer_supported/\n\nI tried it and I couldn't tell if it's running or not. Nvidia VSR was pretty obvious.",
      "I have done some test, and there are some small differences. It works only with sub 720p streams and videos, and it's not always effective.  Considering that it must work with igpus, I suspect that they set it with a very light preset.  \nI hope that they will add some setting to the driver soon.",
      "Is their current gpu broken or something?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Issue with Arc A770 LE fan speed",
    "selftext": "I've been having some trouble with A770's cooling, specifically the fan curve. When idling, the card runs at 600Mhz, 50C, and around 600 rpm fan speed. When running games with an uncapped framerate, the card would slowly climb to 90C while the fan speed remains at about 1000 rpm. And when it hits the thermal threshold, it would only lower the gpu clock.\n\nI've only seen the fan speed go up to 1700 rpm once, and it was AFTER I turned off the game to let the card cool down. When I launched the game again to see I could replicate it, the fans never went past 1200 rpm.\n\nIs this a driver issue or hardware issue? Has anyone encountered the same problem? My case is small, which may affect cooling, but having the fans not speed up at all is another problem entirely.  I can't even test to see if the fans are faulty because Arc Control doesn't let me control the fan speed.\n\nHere are the specs:\n\nCPU: Ryzen 5 5600X\n\nRAM: Corsair Vengeance LPX 16G 3600 (x2)\n\nMotherboard: MSI b550m Mortar MAX WIFI\n\nSSD: Samsung 970 EVO Plus 1T\n\nSystem: Windows 11 2H22\n\nA770 Driver: 31.0.101.3430 DCH\n\nArc Control Panel: 1.0.4759.0\n\n&#x200B;\n\nUPDATE: My new card doesn't seem to exhibit the same problem, so I guess this issue has been resolved! I know, it could be the latest driver that fixed it, but I don't feel like testing out old drivers at the moment since everything is working fine now and I just want this to be over.",
    "comments": [
      "Sounds like you’re running with the initial drivers. Try installing the Beta drivers. This should fix your issues. \n\nUninstall the driver (Run [DUU](https://www.guru3d.com/files-details/display-driver-uninstaller-download.html)) and install the Beta drivers - [link to drivers](https://www.intel.com/content/www/us/en/products/sku/229151/intel-arc-a770-graphics-16gb/downloads.html).",
      "Thank you for the suggestion. I installed the beta driver, but the problem persists. The card climbed to 90C and throttling kicked in, with fan speed still at 1000 rpm. After running like that for 1 minute, I closed the game and saw fan speed immediately increased to 1200-1258 rpm, which persisted even minutes after the card had cooled back down to 50C.",
      "By all accounts, even overclocked it shouldn't be running that hot. I would get a replacement if I were you.",
      "I don’t have many games to test as I just built the pc. I tried Overwatch 2 and Deep Rock Galactic, covering both dx11 and dx12. And yes, it’s the same situation in both games.\n\nEdit: Oh right I also tested Doom Eternal. The same happened there, with the only difference being I didn’t even need to leave the menu for the card to heat up. Doom’s menu is just more demanding, I guess.",
      "I do have Afterburner, but I installed it after I found out about the fans, I think. I wanted to use it to adjust fan speed, but it doesn’t support the card and gives no reading on anything.",
      "Yeah, I’ll give another try some time. Thanks.",
      "I would say that i has had the same issue. But Today i have solve it!\n\nLast week i shipped my A770 back to the retailer cause it was overheating to 95°C and even on Desktop it was sometimes freezing the system.\n\nToday my new A770 arrived. First thing i did was to run 3DMark with the DX12 Speed Way Benchmark. It directly hits the 90°C. After seeing that, i had the idea to use \"Fan Control\" to force more speed, cause i never heard the fans i was sure they must have more speed to give.\n\nWell, Fan Control don't work with the A770.\n\nAfter that fail, i asked google and i found this post here on Reddit.\n\nAfter reading all the comments, I thought maybe its my Bios Set Up that is making the Arc overheating, but at the same time i remember that my Asus Strix B660-I Gaming Wifi got the latest bios Update with the Description:\n\n......\"1. Improve system performance\r  \n2. **Support Intel Arc graphics**\r  \n3.Many ME updates and optimizations for....\n\nThan i thought, maybe this resolution i used from beginning on: [Intels High Power Consumption on Idle Issue Resolution](https://www.intel.com/content/www/us/en/support/articles/000092564/graphics.html) is still active in bios and its causing some issues against the bios update that supports Intels Arc. \n\nSo i opened my bios, and put everything that belongs to the PCI-E to \"Auto\"\n\nAfter the restart i used 3DMark again. The Arc just hits only the 80°C and the fan goes finally up until over 2000RPM.",
      "I think I ran into a similar issue here. Just got an ARC A770, fresh installed 4032 drivers, and realized that the moment I touched the performance options in ARC, the fans would refuse to spin up past \\~1200 RPM or so. Rebooting fixed the problem for me, but I guess overclocking is just not feasible for now.",
      "As of today, it has not been fixed. Only the LE version of the graphics card will be affected by the performance tuning of the ARC control center. The current temporary solution is to restart the system to restore the default settings of ARC and do not set any options in the performance tuning.",
      "Yeah, I'm considering it, but I'll have to wait until there's new cards in stock. For now I'll just have to run it underclocked.",
      "Glad to hear you were able to solve the issue. I’m a day or two away from getting my replacement card. When I do, I’ll check if your solution applies, if mine still overheats that is.",
      "Same here. If i lower the temp target it will lower clocks and wattage instead of ramping up the fans. I wish we had a fan curve to adjust.",
      "Just curious. Does this also happen when you play a different game?",
      "Interesting. Do you have MsI afterburner installed? Or any other type of GPU control software other than the intel program?",
      "Yeah I was gonna say to uninstall it as it might be interfering somehow.\n\nOne last go run DUU have it shut down. Remove the GPU and install it back in (make sure it’s seated all the way). Install the power cables (make sure they’re properly seated). Load into bios and ensure rebar is on. Load into windows and install Beta drivers. Once installed restart immediately.\n\nIf this doesn’t work it could potentially be a hardware issue.",
      "Sorry forgot to mention. Make sure you run DUU for every GPU you would’ve had installed. Make sure you clear if ALL old drivers before installing the new intel beta drivers."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "NF I2C Slave Device - unkown device from Intel Arc A770",
    "selftext": "There was an archived post on this a few months back. The advice was to download and install the Asrock Polychrome Sync software.  Just installed an Asrock A770 Phantom Gaming 16GB. Same problem. Installing the Polychrome Sync doesn't remove the yellow exclamation mark. Just in case anyone had the same problem, it can be solved by manually updating the driver using the one found in one of the folders created by Polychrome Sync (in my system it was in \"C:\\\\Program Files (x86)\\\\ASRock Utility\\\\ASRRGBLED\\\\ASRISP\\\\AsrNfDrv\\\\\"), and it will install the \"Asrock SPB Device\"",
    "comments": [
      "Thank you, this should be higher!"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Intel Arc A770 issues",
    "selftext": "I decided to purchase the Intel Arc A770 and it arrived yesterday. I tore my PC apart and installed it last night. I did a fresh Windows 11 install and everything was going smoothly until I went to install the Arc drivers. During the installation, the monitor loses signal and doesn’t come back on. I rebooted the machine and now it loses signal once it starts loading Windows. I manage to get into safe mode and rollback the drivers to default. I tried both the current beta and stable releases as of 5/9/23 while getting the same result, no signal from the HDMI port after installing drivers.",
    "comments": [
      "1. Go over to /r/intelarc \\- great support for the Arc over there\n2. Download DDU and an older driver (some report rolling back to older versions like 4255)\n3. Boot into safe mode\n4. Run DDU and reboot into safe mode\n5. Install 4255 drivers in safe mode\n6. Reboot into normal mode\n\nI think that'll probably fix your problem.",
      "Pretty sure your monitor doesn't play well with arc, an older monitor by any chance? I had issues where ARC forces a 1080p resolution on a monitor much smaller (output out of bounds) and only solution was use another GPU/iGPU.",
      "Nope, I gave up and returned the card. I ordered an 4070 instead.",
      "PC specs.",
      "Did you fix the issue?",
      "The monitor I’m using was purchased in 2015 so it is quite old. LG 23MP47HQ",
      "CPU: Ryzen 9 3900X\n\nMotherboard: MSI MPG X570 Gaming Plus\n\nRam: Corsair 32GB kit (cmw32gx4m2e3200c16)\n\nSSD M.2: Samsung 980 Pro 2TB",
      "Previous GPU: MSI Gaming X GTX 1060 6GB",
      "I have an older monitor and I was planning to buy this card too. I wonder if i'll face the same issue...",
      "I have an older monitor and a brand new one, it didn’t work on either one. I find it may be an coincidence for whoever."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "the intel arc a770 16gb le is getting way better!!",
    "selftext": "wow!!!\n\nhttps://preview.redd.it/izzb21grsdea1.png?width=1510&format=png&auto=webp&s=92bc9d4067e284a5dea6fef680ce9286df5fa749",
    "comments": [
      "wow!!!\n\n&#x200B;\n\nexcept benchmarks are not entirely reflective of gaming performance",
      "it doesnt touch the 7900 xtx but for 350 dollars i dont expect it to.",
      "its in the rtx 3070 territory i think a little better",
      "How does this compare to a 7900XTX? I’m genuinely curious, this is still majorly impressive.0",
      "What about the 3060? I know it was competitive with it in newer games and fell behind on older ones. does that still match?",
      "Damn those drivers really are aging like fine wine then hot damn.\n\nEDIT: I thought this couldn't possibly be that big of an increase but it is. The average 3070 score is 13507, aka 3 points less lol",
      "a few more driver updates and i might go ahead of the 3070",
      "How is it with older games? I'm thinking less of stuff so old that it doesn't matter (Ex. Half life, Counter strike, etc.) and more like.. heavy skyrim/fallout 4 maybe bf1/bfv which uses and older frostbite engine even though its DX11?",
      "it works great now, driver update fixed that issue.",
      "hot damn man! A770 looking nice\n\nRip the graphics division",
      "I really want this to work good so I can swap it into my unraid server.",
      "Best value GPU there is in my opinion"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "intel arc a770 16gb vs amd 6700 xt in stable diffusion on windows 10?",
    "selftext": "I have an offer to swap my 6700 xt for a770. Saw on youtube that the a770 generates at 10 iterations per second on windows. Is this true? P.S. I've run SD on different versions of linux, but get black images as a result of generation (possibly a hardware issue). On windows my speed is about 3 iterations.",
    "comments": [
      "I would go for intel day and night. Better card. Better technology. Better support.\n\nHere's an article about it.\nhttps://www.tomshardware.com/news/stable-diffusion-for-intel-optimizations"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "a770"
    ],
    "title": "Will Intel Arc Gpu work on Amd Ryzen 7600",
    "selftext": "I am creating this post want to get feedback, bought an Intel Arc A750 along with Ryzen 5 7600 since the X version was not available am preparing to build a pc, and not yet ready for 4k. Also have the A770 limited edition as plan B, & very willing to try their Intel gpu.",
    "comments": [
      "Yes it'll work. What makes you think it won't?\n\nAlso the A770 is barely faster, only get it if you absolutely need the 16GB VRAM.",
      "First time building it myself, and asking those who know what they are talking about appreciate the response.",
      "As long as your CPU supports Resizeable BAR, you should be fine with Intel GPUs. Which the 7600 does. You may as well use the A770 since it has more VRAM since games even at 1080p these days seem to be gobbling up as much VRAM as you can throw at it.\n\nI actually daily drive a 3700X and A770 and the experience has been decent so far. You're actually way better prepared for future games though than me since PS5/Series X games tend to make CPUs cry no matter the resolution.",
      "VRAM is video RAM, which is how much graphics data your GPU can store. Bigger is better, because it lets you play the game at higher resolutions and texture quality. Doesn't mean that smaller is necessarily worse though. The performance will be about the same at lower resolutions when the compute power of the smaller VRAM GPU equals that of the higher VRAM GPU.\n\nArc 770 with it's 16GB of RAM is good for 1440p gaming and even 4K in some cases. Arc 750 is only good for 1080p gaming.",
      "yes, but if he wants to the silicon to go a little further, the 770 is better for the 16gb along - also, plenty of things will increasing use above 8gb (RTX for example) as time goes on.\n\nWe need to know his price difference."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Drivers not working on arc a770 16gb",
    "selftext": "When ever I try to install the latest drivers for my GPU it works until the end and it says Exit code 1 and that happens every time I try. I've checked the Intel support page on the Intel website and that doesn't help any help is appreciated.",
    "comments": [
      "Try DDUing and reinstalling.\n\nJust the other day I had an issue with installing 101.4900 (I think it looked like it finished installing and then tried restarting, but waiting something like 10 minutes nothing happened), so had to restart my computer then DDU and reinstall again.\n\nI also went to look for that error and it also says to [DDU the driver](https://www.intel.com/content/www/us/en/support/articles/000094237/graphics.html), although the installing they mention isn't just opening/pressing install/etc.",
      "Did you read how others fixed this problem in [/r/IntelArc](https://www.reddit.com/r/IntelArc) ?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "a770"
    ],
    "title": "With the new A770 and the Arc software, can you cap FPS in all games automatically?",
    "selftext": "NVIDIA has a way to cap FPS across all games by the click of a button in the Control Panel.\n\nDoes the Intel Arc software also have this capability?",
    "comments": [
      "Recommend a gpu with 3x the price not have sense.  \nWaiting first for the review.",
      "“I would recommend a flagship model of a GPU that is over 3x the price of the a770 and that being the liquid cooled model, would work great for your 60fps monitor”\n\nwhat…? lol",
      "Is this a troll post? why would you recommend a last gen $1,699.99 GPU?",
      "Haha, I'm going to upgrade for sure but the lesson learned from that horrible mess was that the same issue could happen even with a 144hz monitor because if let's say CSGO runs at 277hz, but my monitor is 144hz, I still get the same problem.",
      "3070 is a tier above the A770. They're trying to compete with the 3060.",
      "144 is a huge upgrade over 60 and worth it. You can use Fast sync (nvidia) or Enhanced Sync (AMD) to compensate for frames over 144. Not sure if ARC will have that same technology- I'd assume it would. Guess we all see in about 30 minutes.",
      "Never heard of vsync?",
      "Hope so. Problem is I'm going to buy a new PC soon and I'm wondering if I should go for the ARC 770 or Nvidia 3070/80.\n\nThat could be a difference maker for me because I've got a 60fps monitor and the only way I managed to fix the lag in my games was to use that fps cap from Nvidia. It was unplayable otherwise.",
      "it\\s turned on.. still lag in games",
      "You should try fixing that issue first... That's not normal. Capping your fps is just as applying a bandaid on your issue. \n\nAlso, considering an arc 770 vs an rtx 3070+ is a bad idea and not even up to discussion, no one should be buying an arc 770 currently, performance is all over the place. Go for the established brand and card.\n\nAdditionally, 60hz in 2022 is really outdated and going for such a good card is a waste of money unless you're on 4k.",
      "Did you see the reviews for the ARC770? it's not close to the 3070/3080 in any way. Plus it has a lot of issues and it's not a finished product.\n\nWatch the [GN review](https://www.youtube.com/watch?v=nEvdrbxTtVo) if you haven't.",
      "The problem is that my work requires a riduculous amount of monitors connected, and in high resolution.\n\nI've done a lot of research on this and it seems like it's crucial to get a card that has at a minimum 16gb of VRAM. \n\nThat's why the 770 is interesting to me, so I don't have to spend too much on the 24gb 3090 card or the 4080 16gb version..\n\nIt's really annoying but that's the predicament I'm in. I can wait until December so hopefully there will be enough improvements in driver support that it seems stable enough. If not I hope to God GPU prices have crashed by then.",
      "Seems pretty bad yea. \n\nI'm just going the market crashes next 2 months so I've got a chance at a good deal on the 3090 maybe",
      "Are you working on an investment bank or something? :D\n\nIn any case that really is a very specific case so disregard what I said before. I read games so I thought this was only for gaming.",
      "I just hope the Arc 770 performs well on software that isn't gaming, or if the optimization problems will be the same in that area as well. If the Arc can't even do render software correctly, there's no way I'm getting it.\n\nThanks for the heads up anyways.",
      "Honestly don't buy a new PC now . Wait for 4000er release . Huge price drops will come . \n\nBut Arc has its appeal . First of all you have something unique and help a healthy competition . But i would check prices first and if the drivers are stable . I know i won't get an arc because I have a slightly weaker gpu , the upgrade wouldn't be worth it and i love retro gaming and it's clear that this is a major issue still. If you don't care about older games then go for it .",
      "You're going to but a $300-$500 video card and can't get a better monitor? You are always better to cap framerate inside the game first before you do it at the driver level. I'd get the ARC and use the rest of your money on a 144hz monitor."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "a770"
    ],
    "title": "What is the current state of ARC GPU's and multiple monitors?",
    "selftext": "I have a weird setup where my desktop feeds multiple monitors across the house using Fiber HDMI/DP. Currently I utilize the 12700k iGPU + a 6600x, but I wonder if I would have better system stability if I replaced the 6600x with an a770 so I could have one driver for both. I know that there were some issues with ARC and multiple monitors upon release and just wasn't sure if that was still the case.\n\nI use this PC for different multimedia tasks like music and photo editing as well as entertainment, I don't use it for gaming.",
    "comments": [
      "No one is going to have exactly the same display combo, just buy it and return it if doesn't work the way you want.\n\nI returned my A380 when it doesn't do HDR tonemapping in Plex, but AMD GPU does even it wasn't in the official supported GPUs.",
      "Keep your AMD GPU.",
      "You shouldn't be having stability issues with both drivers enabled. I've had the opposite setup - Ryzen 7700x + RDNA2 iGPU + ARC a770 and didn't have any issues. \n\nI've also used Intel i9-13900K + Xe iGPU + Arc A770 and it works well with the latest drivers  - although it was a little rocky until recently because of some odd conflicts that occured in previous drivers.",
      "Never had any issues since Nov '22 driving 2 Dell 27\" 4k montiors over DP with an A770.",
      "I have no issues using multiple monitors on my a770. You have a weird setup so I'm gonna go out on a limb and suggest you won't get a solid answer without trying it for yourself. I honestly never really had issues related to dual monitors and I bought mine on launch day.\n\n&#x200B;\n\nThe only issue I have is it still does not wake up my monitor/s coming out of sleep.",
      "I'm only wanting to find out if ARC has issues with multiple monitors and if a single GPU driver is more stable than 2.",
      "Gotcha, no advantage to having single vs multiple gpu drivers?",
      "No one knows, because they don't have big enough install base.",
      "Gotcha",
      "I’m pretty sure it’s fixed now"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "a580",
      "b580",
      "arc b580"
    ],
    "title": "Intel Arc B580 Battlemage GPU OpenCL/Vulkan performance leaks out, 9% to 30% faster than A580",
    "selftext": "",
    "comments": [
      "Just saw a GamersNexus video on this yesterday.\n\nFrom what I understand, the A-series Arc cards had to basically emulate many functions for gaming that were natively supported by other cards. \n\nThat led to driver support being reaaaallly shaky at the beginning.  It has improved a lot since then, and even though I only play a couple of games, my A series cards have performed really well. \n\nThe B-series cards have that capability baked in natively, which *should* give the cards a big performance boost over the previous generation and make driver support much easier.",
      "Which is a great combination at this price range tbh. The 2080 is a DECENT 1080p - 2k card, pair that with a healthy 12gb vram and you got a nice package. Something that AMD and Nvidia fails to deliver.",
      "So this is basically a rtx 2080 but with 12gb vram?",
      "Go to NGREEDIA",
      "Great",
      "Yeah, I got nothing to say to this level of performance as long as pricing is good. If they can figure out the drivers B580 should be the better 4060 for less money.",
      "it's nowhere near as bad as people say, in my experience",
      "\"rendering 3D scenes\" lol... as opposed to what? smell-ovision?",
      "Had No issues yet....there are regular Updates , especially when new popular Games release",
      "3060 ti is probably still faster. Maybe wait for the B770",
      "I am very confused can you guys help me i am planning to buy this card, the only thing worrying me is driver support, is it that bad ? i dont consider myself as a first day gamer when the game release but i have heard even the previous titles struggles with intel driver supports is that true ?",
      "Why are you comparing a productivity benchmark for a gaming card. It's like comparing a productivity benchmark to the 9800x3d. It loses to a 13600k.",
      "Just search for b580 on NewEgg to see all the cards for pre-order that have sold out",
      "https://opendata.blender.org/benchmarks/query/?device_name=NVIDIA%20GeForce%20RTX%203060%20Ti&device_name=Intel%20Arc%20A770%20Graphics&device_name=NVIDIA%20GeForce%20RTX%204060&device_name=NVIDIA%20GeForce%20RTX%202080&compute_type=OPTIX&compute_type=CUDA&compute_type=HIP&compute_type=METAL&compute_type=ONEAPI&blender_version=4.2.0&group_by=device_name\n\nAh look! The blender score shows the a770 with about the same score as a 2080!",
      "Driver support is really good.",
      "Has it been officially confirmed from Intel that they won’t be making the B750 & B770? Wasn’t there supposed to be a B980?",
      "When can I order one of these cards? None listed on Amazon or NewEgg",
      "For now, we don't know what AMD's cooking",
      "An intel insider some weeks ago said the inner working on intel is wishy washy on a ton rn, cutting costs left and right just to return what had gotten cut days or weeks later, so maybe not even intel themselves know for certain which is why the messaging is mixed.",
      "Yeah but like who cares? We know what they meant."
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "a580",
      "b580",
      "arc b580"
    ],
    "title": "Retailers accidentally ship Arc B580 early, mistaking It for A580",
    "selftext": "",
    "comments": [
      "congrats,\n\nhappy for you,\n\nnice,",
      "I can see the pic of the wee lad, in my mind",
      "I thought they had the cards but the drivers aren’t released until embargo day",
      "They aren't talking about reviewers, they shipped some cards to regular consumers that had pre-ordered a B580",
      "Reviewers have had the LE cards for over a week, embargo lifts tomorrow, on the 12th. I believe AIB models are embargoed until the 13th (launch day).",
      "When do the reviews drop?",
      "Lmao good luck selling 4060 once B770 come out and when 5060 are right after.",
      "I'm giddy. Can't wait for the reviews",
      "A lot of times reviewers will be given an early or beta version of an upcoming driver.",
      "It's just retailers sending out cards early. They're paperweights without the drivers so there's not much harm or leaking done here. We all knew what the cards looked like anyway",
      "I read this with patrick bateman voice",
      "On an unrelated note: Retailers seeing a sudden boom in A580 orders. One retailer which remained anonymous states \"We can't keep A580 on the shelves. They're suddenly moving out fast\"",
      "To bad they didn’t price them as the A580.",
      "Unfortunate, it costs 400 EUROS in my country. It's just not worth it. Im gonna buy an rtx 4060, and keep an eye out for that potential b770.",
      "the B770 is very likely to be announced at CompuTex alongside AMD and Nvidia, with similarly a launch date to follow too far away.  Anyway - whatever you are in the mood for, right now is the worst time to buy a card - any remaining 4000 series stock not called 4090 is going to have its price drop soon.",
      "With the world recorder vat, and the usual price upping during Christmas time in Hungary it's unlikely it'll drop much. It cost me 295 in euros and it was 20euros cheaper literally a day before pay day... I just want to game during my 3 weeks off. Hell knows how many ot I'll have to do January..\n\n\nOr maybe I'm just coping",
      "how the hell it works in the US? I see people are getting completely wrong SKUs or entire box of drives instead of just one. in countries where I lived you as a seller just cannot sell it without scanning barcode, only after this you will be allowed to print invoice and ship stuff. and ofc you cannot just put 6 drives instead of one, company will just deduct it from your salary",
      "Lol that’s funny",
      "It's a strange naming scheme",
      "Why are there so many leaks? I started to think they just let some pass to hype us."
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "a580",
      "b580",
      "arc b580"
    ],
    "title": "ARC B580 and Steam VR - VIVE 2 Pro",
    "selftext": "  \nBad luck, we must pressure Intel. Things will NOT work out on itself.\n\nSteam  \n\n\nHello,\n\n\n\nThank you for reaching out to Steam Support about the issues you appear to be encountering with SteamVR.\n\n\n\nThe GPU you are currently using, an Intel ARC B580, is currently not compatible with SteamVR.\n\n\n\nSome users may have been able to get this GPU to function under certain circumstances but Steam Support is unable to troubleshoot issues that occur with this GPU.\n\n\n\nFrom what we can understand GPU driver updates will need to be released for the Intel GPUs. Until then, this GPU will remain incompatible with SteamVR.\n\n\n\nYou can view the system requirements for SteamVR here: [https://store.steampowered.com/app/250820/SteamVR/](https://store.steampowered.com/app/250820/SteamVR/)\n\n\n\nWe apologize for this inconvenience.\n\n\n\nIf you have any further questions, please let us know - we will do our best to assist you.\n\n\n\nSteam Support\n\nMichael\n\n\n\nMessage from you on May 11 @ 10:08pm | 14 hours ago\n\nCard is far more advanced then minimal requirements.\n\nI am sure both Steam and Intel can cooperate in making it to the list.\n\nIts a shame such well priced and popular card from major vendor is left behind.\n\n\n\nPlease let me know as soon as this is corrected.  \nHTC Vive\n\n||\n||\n||Dear Customer, Thanks for contacting VIVE support.   We appreciate your keen interest in the VIVE Pro 2 Full Kit and your proactive inquiry regarding its compatibility with your ASRock Steel Legend B580 Intel Arc A580 12GB OC graphics card.    Given the relatively recent introduction of the Intel Arc series to the discrete graphics card market, and the nuanced nature of VR support with evolving architectures, we wish to provide you with the most accurate information currently available.   Comprehensive and dedicated testing specifically involving the B580 chipset and the Intel Arc A580 12GB OC in conjunction with the VIVE Pro 2 Full Kit appears to be limited. This is due to the ongoing development and assessment of VR support for the Intel Arc series.   While Intel has been diligently releasing drivers aimed at enhancing compatibility with virtual reality headsets and platforms such as SteamVR, the [VIVE Pro 2's officially recommended specifications](https://www.vive.com/eu/vive-ready/) include NVIDIA GeForce RTX 20 Series or AMD Radeon RX 5700 or superior. Although these represent the optimal configurations, graphics cards below these recommendations may function, potentially with a reduction in visual fidelity or performance, particularly in graphically intensive applications.   It is challenging to provide an absolute guarantee of \"100 percent working\" across all possible system configurations, especially with newer graphics processing units. The overall performance can also be significantly influenced by the specific virtual reality application or game being utilized, as well as other integral system components, such as your central processing unit and random-access memory.   Regarding your expressed preference for a high-quality VR kit over a more expensive graphics card paired with a potentially less immersive VR system, we understand your rationale. The VIVE Pro 2 offers impressive resolution and exceptional visual fidelity. However, the overall quality of your experience will ultimately depend on the capability of your graphics card to adequately drive the VR content you intend to utilize.   In conclusion, while the Intel Arc A580 *may* function with the VIVE Pro 2 on SteamVR, we cannot offer a definitive guarantee of complete and flawless operation without specific testing data for this precise hardware combination. The necessity to adjust graphical settings for optimal performance remains a possibility. **Nina** (VIVE) May 12, 2025, 18:34 GMT+8 |",
    "comments": [
      "Intel cards don't support VR. At least not yet. Honestly tho VR is a niche market so I would rather them ignor it and get everything else great before diving into it. It wasn't till recently that AMD's VR performance was good enough to consider as well. Nvidia has had a strangle hold on the VR market sense VR started hitting the market. \n\nI love my Intel card but if I was using VR I would have bought nivida hands down simply because they offer the best reliable performance in VR.",
      "Uh… that’s not news to us tho? Intel never said it supports VR",
      "Intel has been pretty straightforward on not supporting VR: https://www.intel.com/content/www/us/en/support/articles/000093024/graphics.html",
      "I'm not interested in VR but I really want Intel to at least go all hands on it some day.",
      "It is a growing market, but it definitely is niche. It's a very common part of simracing as well. I own a vr head set and most of the people I game with also own one as alot of what I do is simracing and simdrifting and VR in those communities is extremely common (i personally use triple screens in sted of vr). My B580 is a far superior card to my old GTX1660 that I ran VR with, but the b580 doesn't run the VR. \n\nWhen the b580 has some driver issues and needs tweaking in games as common as CS2, Marvel rivals, and COD, which are by far the some of the most common and most played games, VR can wait. \n\nVR is a niche market that isn't for everyone. Not only is it a pricy perifial but there are alot of people that simply can't use it because it makes them physically ill. (Litterly the only thing I can do in VR is simdriving, anything else I sick with in 20 minutes.)",
      "Well, in general products list what they do support, instead of what they don’t. Of products listed what they don’t support, every product would have a huge list of “specs”…\n\nSo, at least for electronics, I always assume they if something is not explicitly listed as supported, then it’s not supported.",
      "Created a ticket with Intel as HTC and Steam responses were handwashing - simply not supported\n\n||\n||\n|Hello Vojin Vidanović,  Thank you for contacting Intel Customer Support. Please be advised we are currently experiencing higher than normal case volume, which may cause delays. Thank you for your patience. Below is a summary of the information you submitted, as well as the number assigned to your case.  Please reference this case number whenever you contact Intel Customer Support about this case.  Best regards, Intel Customer Support Case Number 06579504 Case Subject Steam VR is not working, please work on this! Hello, Thank you for reaching out to Steam Support about the issues you appear to be encountering with SteamVR. The GPU you are currently using, an Intel ARC B580, is currently not compatible with SteamVR.|",
      "Its growing market, check SteamVR to see how fast its library and user share is growing.  \nI do get now they dont (I through it was more unified standard like OpenGL or DirectX).  \nBut that makes my next gfx card surely nVIDIA. Changing card and buying VR set is too expensive.  \nAs I explained in mail to HTC, having budget card that supports VR would be great, as it enables users to buy better sets for price diff, and on paper B580 is not inferior to VR supported cards.",
      "I believe so, but that is the support page \nNowhere in card specs on the box (what I call straightforward) does ot say \"this card does not (and never will) support steamvr or any vr device",
      "No it did not, but is kind of expected for that gen card.\n\nIt would be honest to have no support vr and never will on the box and in card info.\nI found out recently",
      "Thanks, I shall wait for tech to be more mature and standardized",
      "In short vr is still expensive unsupported toy",
      "Created a ticket with Intel as HTC and Steam responses were handwashing - simply not supported\n\n||\n||\n||",
      "Created a ticket with Intel as HTC and Steam responses were handwashing - simply not supported",
      "Created a ticket with Intel as HTC and Steam responses were handwashing - simply not supported\n\n||\n||\n|Hello Vojin Vidanović,  Thank you for contacting Intel Customer Support. Please be advised we are currently experiencing higher than normal case volume, which may cause delays. Thank you for your patience. Below is a summary of the information you submitted, as well as the number assigned to your case.  Please reference this case number whenever you contact Intel Customer Support about this case.  Best regards, Intel Customer Support Case Number 06579504|",
      "Created a ticket with Intel as HTC and Steam responses were handwashing - simply not supported\n\n||\n||\n|Hello Vojin Vidanović,  Thank you for contacting Intel Customer Support. Please be advised we are currently experiencing higher than normal case volume, which may cause delays. Thank you for your patience. Below is a summary of the information you submitted, as well as the number assigned to your case.  Please reference this case number whenever you contact Intel Customer Support about this case.  Best regards, Intel Customer Support Case Number 06579504|"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580",
      "arc b580"
    ],
    "title": "Is the B580 a sensible modern upgrade from a GTX 1660 Ti?",
    "selftext": "I recently upgraded my wife's midrange PC and was left this PC as a personal rig from some older parts.\n\nHere is the prospective build: https://pcpartpicker.com/list/jCQgb2\nRight now I have everything but the new case and the Arc B580, but the B580 was purchased and is on track to arrive before Christmas.\n\nAt this time, I play mostly on a 1080p monitor today, but curious to see if I can get playable frame rates on my 50\" 4k TV. My dream is to own a 32\" OLED 1440p monitor someday, but not this holiday season lol. Do you think the B580 is worth upgrading the 1660 Ti I currently have in this system? Is the 5600x going to bottleneck this GPU, or do you think it should be a good match given the B580 is still a lower end card?\n\nGiven that I already pulled the trigger on buying a B580 while it was still on stock, I'm really just trying to figure out if I should also be upgrading to AM5 now or in the near future. It's negligibly the same cost to me to meaningfully upgrade my CPU on AM4 as it is to invest in a low tier mobo and R5 7600x w/ RAM. I'm not certain whether either CPU upgrade would even be meaningfully necessary with this card or if the 5600x I've adopted is good enough.\n\nThanks for your help!",
    "comments": [
      "My A750 was a good upgrade from a 1660ti, so yes. It performs just fine with two 1440p monitors and the B580 is benching much better.",
      "It's a good 1080p card, and some games you can run at 1440p as well. 4k is way out of its league though.",
      "Upgraded from a 1660 ti to A750 and have been happy. 1660 has no hardware upscaling so started having issues with 1440p. A750 eats my 1440p games for breakfast. Looking for a B750 next.",
      "if you already have an AM4 mobo i would argue 5700X3D as cpu will be more cost effective\n\nB580 is a good match for your current cpu, should be even better with 5700\n\nB580 should be capable of 4K60 with upscaling on most games so if that works for you thats good, however for 1440p its more than capable",
      "Some TVs will allow you set the resolution (in Windows) to 1440 for example, and also can set 1440 in games as well for better performance vs 4k. The 5600x should be 'good enough' at 1440. Just recommend to update the board BIOS, and enable SAM Smart Access Memory/or Resize Bar, and Above 4G Decoding Memory, and XMP, in the BIOS settings after the update.\n\n\nAlso, may double check Windows is in UEFI mode(if Windows 11 already set). Can check on this in the System Information app in Windows. Under BIOS mode, should list UEFI(non legacy).\n\n\nAnother item, recommend to DDU(display driver uninstaller) the previous GPU, so no software conflicts for performance. There's an option in DDU to 'clean and shut down', just before going with the hardware swap.\n\n\nQuick DDU overview(also lots of video how to's online)\n\n\nhttps://www.intel.com/content/www/us/en/support/articles/000091878/graphics.html",
      "I did the same upgrade and yeah super happy",
      "You don't need a new mobo or CPU. Your 5600X will be just fine for the B580 (and probably even the B770 if it comes out). And yes, it's going to be a solid upgrade from your 1660ti.",
      "GTX1660 -> A750 LE - real upgrade. Also in VR for Quest 2 PCVR via VD.",
      "With upscaling it’s fine tho. The internal resolution doesn’t need to change much at all. 1080p -> 4K is a fine upscaling resolution. The ps5 even runs a lot of games at 720p lol. It does break down a lot more below 1080p tho",
      ">4K should be a breeze as long as you're liberal with XeSS and FSR usage.\n\nWith games starting to list it in their minimum requirements you might have to be very liberal with it.",
      "Yep but u need resizable bar",
      "Why would you not use upscaling for this? It would look 10x better than letting your display handle the upscaling.",
      "Yeah I play 1440p with 1660ti and A750 which both can handle reasonably with some tweaks, so B580 should be more than ok there",
      "Yep I built a second pc with A750 so I can compare 1660ti and that on a daily basis - confirm A750 is a good upgrade over it",
      "I do console emulation up through PS3, and then most of what I play on Steam are turn-based JRPGs. The most graphically intensive games I play would be Nier Automata and Nier Replicant. I want to play Baulders Gate 3 at some point when it gets cheap. I don't always have as much time as my partner to play games due to work, which is why I give her the better parts usually.\n\nThere's like two locations in Persona 3 Reload specifically which have ray tracing enhanced reflections and they look beautiful. My FPS and overall experience in this game was overall great at 1080p Ultra using the 1660 Ti except in these two areas of the game. If this card helps to address issues in more modern games like that in games like that one, and if it allows me to continue streaming over steam link at 60fps over my nice router, or from away on a 1Gb Internet connection, I will be ecstatic.\n\nMy son likes Pajama Sam and Putt-Putt, hoping to get some good performance in those lol.",
      "4K should be a breeze as long as you're liberal with XeSS and FSR usage. Don't shy away from upscaling because even with a high-end GPU, you'll STILL need it to get the framerates you'd ideally want.",
      "Yes",
      "Yes.",
      "How dare you. That graphics card belongs in a museum to be enjoyed by all!",
      "I have the same dilemma right now, I own the same gpu as you and I'm wondering if B580 would fit for my 1440p monitor.\n\nBy the benchmarks I've seen, it looks like a promising upgrade, I might finally take the bite.\n\nBut I am still waiting for the B750/B770 to see the price/performance.\n\nOtherwise, we both can't go wrong with the B580."
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "a580",
      "b580",
      "arc b580"
    ],
    "title": "Intel Arc B580",
    "selftext": "I’ve owned an Arc A580 and was somewhat disappointed by the performance and drivers. After more and more updates the card was finally a bit more useable. I decided to buy the Arc B580 on release date and replace my A580. I have this rig set up as my living room console. So far I’ve been impressed, it beats the performance of my 4060 and my 7600. Shockingly it’s more stable than my 7600 (maybe another story for that card) when playing Fortnite and The Finals. Yes, there’s some issues still looming around. I play OW2 from time to time and I’m still getting that issue of micro stutters where the fps will dip to 100fps and back to 200fps. Not perfect but I love seeing intel bring some affordable GPUs to the game. ",
    "comments": [
      "This is my favorite b580 edition, very beautiful 😍. I am also going to buy it 🤭",
      "Yeah, I’ve been eyeing the white version of this card since it was announced. Thank god I was able to order it online the day of release from my local Micro Center. Even the guy that was ringing me up said that he already saw 5 get picked up and only had 2 left over. That morning they had 18-20 cards in stock from different brands, sold out",
      "Really excited about getting this when they restock. I was gifted a prebuilt that has an AMD RX 6500xt and its not the best lol. Hopefully this card will be a huge upgrade",
      "cant wait to get my hands on the asrock b580 lol. too bad its out of stock everywhere and newegg doesnt even have it listed on their site anymore.",
      "I made the same decision and i appreciate this fine GPU. I own a Ryzen 7500F and my Acer Nitro B580 runs more stable in RDR 2 than my old RX 7600.",
      "Performance might improve little by little on certain games to fully optimize them. We’ll probably not see crazy types of uplift performance compared to the Alchemist cards but just have more stable performance. \n\nI haven’t done many benchmark tests as I had a short weekend. So far the games I tested are\n\nFortnite\nOverwatch 2\nThe Finals \nMarvel Rivals\n\nOnly game I had issues was Overwatch 2 as it uses DX11. \n\nThe CPU cooler I used is the Cooler Master 622 Halo White.",
      "Looking good system there. Do you see the performance to get better in the future on b 580 or it has peaked (or closed to it) therefore we're not expecting the same improvement we've witnessed they've broke through with the first Arc genb\n\n Btw, what cpu cooler would that be? Might be a good alternative to Deepcool AK62 I'm eye ing for my next build.",
      "Bro is complaining about 100 fps on an intro grade graphics card I prob read it wrong and it's the studdering that's the issue.",
      "What brands are those fans?",
      "How many fps on Fortnite low 1080p dx12 ?",
      "Cooler master halo for the cpu fans and cooler master sickle flow for the case.",
      "about 150-180fps",
      "what do u suggest with overwatch? cant figure out how to fix it and i get eyecancer playing it right now.\nlooks like 640x480.....",
      "Ok b580 or 4060 for competitive gaming ?",
      "I set mine to low but at 1440p without using FSR. It does okay but I do get a few micro stutters. When I put it at High or Ultra frame rate goes from 100 to my screen cap of 165 and then back to 100.",
      "RTX 4060 will not give you any compatibility issues with what you are playing compared to the B580. So far my B580 plays most DX12 esports games flawlessly but going to DX11 i tend to have some minor issues/hiccups with the frame times. If you can wait, try to see what the RTX 5060 has to offer (i know it may lack VRAM) but if you can't wait get the B580. Also depending what CPU you're pairing it with."
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580",
      "arc b580"
    ],
    "title": "Graphical glitches with B580",
    "selftext": "My friend just got a new Intel Arc B580 for his gaming computer and he noticed some weird graphical glitches happening very randomly, and only when things are in full screen. He says the problem happens while playing something, and when alt-tabbing it goes back to normal, only to then happen again when the game goes back to full screen. And then after a few seconds it completely stops. Here's what it looks like:\n\nhttps://preview.redd.it/ezann1fl9oze1.jpg?width=3051&format=pjpg&auto=webp&s=2f801691658819e613ba2d4e4dfb9b9461ef31aa\n\nNormally I would just say that's a dead card but I've learned that with Intel a lot of the times that is not the case. Here are his motherboard and cpu (before you look, he knows there is a huge overhead issue with the new intel cards, he plans on getting a newer cpu in the near future):\n\nMotherboard: Gigabyte H410M H V3  \nCPU: I5 10400\n\nAny suggestions would be appreciated!",
    "comments": [
      "Oof from my history with computer, i think it's related with the GPU memory health. Try to trade for a replacement and see if its any different\n\nbut it could be just a driver error. Idk",
      "Active resizable bar in bios, That fixed my problem.",
      "The same thing happens to me with the A580 exclusively when I go to stream opening obs",
      "this was my first guess as well. Im also wondering if they used DDU on the old drivers. it could be the gpu vram like suggested but until rebar and DDU'd drivers are confirmed I would hesitate to return it until doing those two things",
      "You know now that you mention it did happen for the first time after he started streaming something to me on discord.",
      "I think someone mentioned RAM issues in an older thread.",
      "Damn, this is even mentioned in the Tech Linus Tips video. I'm fed up. I'm a content creator and haven't been able to work on Kick. Objectively, I'm switching to an Nvidia graphics card 😭"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "a580",
      "b580"
    ],
    "title": "B580 results in blender benchmarks",
    "selftext": "The results have surfaced in the Blender benchmark database. The results are just below the 7700 XT level and at the 4060 level in CUDA. It's important to consider that the 4060 has 8GB of VRAM and OptiX cannot take memory outside of VRAM.. The video card is also slightly faster than the A580. Perhaps in a future build of Blender the results for the B-series will be better, as was the case with the A-series.\n\nhttps://preview.redd.it/ro23ch1z606e1.png?width=1428&format=png&auto=webp&s=299898992f8b13ac31e17c3c2dc8650c20d6047f\n\nhttps://preview.redd.it/ex8y5ncz606e1.png?width=1433&format=png&auto=webp&s=e04573cdb5dd25f36b0ca60707e03fbdcc4122d8",
    "comments": [
      "What does this imply for real world performance? Like what does Blender test?",
      "How are y'all benchmarking since the official drivers will not release until the 13th?",
      "Work tasks related to 3D rendering. I wouldn't compare it to games. Moreover, Blender uses the Intel OneAPI, meaning everything here is quite optimized",
      "From what I understand, benchmarks like this come from reviewers who received advance cards and drivers, but didn't shut off their internet connections before benchmarking. \n\n Hopefully they don't get in trouble for violating the embargo date.",
      "Games have little to do with professional rendering, that's the freaking point.",
      "I saw a leaked vulkan score, looks to be very close to 3060ti/6700XT which would make it an excellent card.",
      "My thoughts as well. What drivers are they using?",
      "Not really. 3D scenes used for Blender benchmark are Monster, Junkshop and Classroom, none of the scenes require more than 4gb of Vram at most. The benefit to 4060’d be a gddr6x vram, which is faster than regular gddr6. But still, the difference wouldn’t be that big, since 4060’s chip is very slow anyway, especially for its price",
      "How does this compare to the A770 I see that has more benchmarks done here 4 Vs 8",
      "Relatively, depending on what your requirements are. We are unlikely to see Intel in Redshift/Octane. But the B770 with 16-18 GB in the same Blender for its price may be a good solution. Also in AE compositing, transcoding, etc. I would still wait for real tests. Even Blender does not behave exactly the same as in benchmarks, if you tried to compare video cards in rendering and benchmarking.",
      "Appreciated.\n\nI don't know anything about Blender, but I'm surprised how the 4060 is smoking everyone else (at least using Optix... why not in CUDA ? Isn't it the implementation of choice for an nVidia card ?).",
      "Unbiased non real time ray tracing speed if using Cycles, biased if using Eevee.",
      "Linux drivers ?",
      "Roughly speaking, these are the same thing, the only difference is that OptiX uses RT cores and is limited by VRAM memory. These are both computing platforms from Nvidia. CUDA as a computing platform is most often used when there is not enough VRAM during rendering. Previously, OptiX was less common, but as RT cores appeared, its rendering speed increased significantly and it became more widespread. And if for gamers ray tracing is a dubious matter, then for content creators it is meta.",
      "How do",
      "That's good to know",
      "I see ! Thank you for this clear explanation.\n\nThus I gather that if the 4060 had 12Gb memory like the B580, it'd score even higher.",
      "The problem with Blender benchmark is that its not that accurate. For example, even on a 4080 card the viewport is quite noisy and laggish, on 4060 its just slideshow. Radeon gpus are not better much, but the gap is nowhere near what those benchmarks and tech blogers trying to imply. In the end, radeon, and intel arc cards will render slower than rtx one, but not so much slower. And the viewport performance is roughly the same between them. And talking about nvidia optix denoiser, its bad literally for anything except static renders. Intel’s OpenImage denoiser saves a lot more details than Optix.",
      "Are they going to release a b770? I'm looking for 4070 ti super speeds.",
      "As more and more of these come out, I can’t help but feel a bit disappointed. \n\nI’m still really excited to see official benchmarks and reviews from YouTubers and reviewers"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "a580",
      "b580",
      "arc b580"
    ],
    "title": "Extremely Unscientific Arc B580 Production Volume / Sales Figures Estimate",
    "selftext": "Hey everyone! \n\nEveryone in this subreddit is familiar with the poor availability of the B580. I've been wondering if this was due to low supply or high demand. Since we won't know the exact sales figures for Arc, we'll have to turn to some extremely unscientific methodology. We know for a fact that the RTX 4060 far outsells the B580 (it's the number 1 GPU on the Steam Hardware Survey), so a far more interesting comparison is against the **AMD Radeon RX 7600 XT.**\n\n**Extremely Unscientific Methodology**: Counting number of Newegg reviews per AIB model. I'm basically treating the number of reviews as a rough proxy for the number of GPUs sold, assuming a similar rate of review writing between AIB models. I'm also using the \"Verified Owners\" filter to limit the number of reviews to those who have actually purchased the card. \n\nThis analysis also focuses on the American GPU market. \n\n# Intel Arc B580\n\nAccording to [TechPowerUp](https://www.techpowerup.com/gpu-specs/arc-b580.c4244), these are the AIB models of the B580:\n\n1. [Acer Nitro Arc B580](https://www.newegg.com/acer-nitro-an-b580-oca-intel-12gb-gddr6/p/N82E16814553012) \\- 4/5, 3 reviews\n2. [ASRock Arc B580 Challenger OC](https://www.newegg.com/asrock-challenger-a580-cl-8go-intel-arc-a580-8gb-gddr6/p/N82E16814930131) \\- 4.7/5, 4 Reviews\n3. [ASRock Arc B580 Steel Legend OC](https://www.newegg.com/asrock-challenger-b580-cl-12go-intel-arc-b580-12gb-gddr6/p/N82E16814930132) \\- 4.8/5, 12 Reviews\n4. [GUNNIR Arc B580 Index](https://www.newegg.com/gunnir-b580-index-12g-intel-b580-12gb-gddr6/p/3GM-0001-00007) \\- seems to be a mostly Chinese AIB, I've only seem scalper prices on Newegg. 5/5, 1 Review. \n5. GUNNIR Arc B580 Photon OC - ditto.\n6. GUNNIR Arc B580 Photon OC W - also seems to be a scalper listing.\n7. [MAXSUN Arc B580 iCraft](https://www.newegg.com/maxsun-icraft-intel-arc-b580-12gb-gddr6/p/3GM-0003-00001) \\- 0 reviews, only 3rd party sellers, ships from China.\n8. MAXSUN Arc B580 Milestone - ditto. \n9. [ONIX Lumi Arc B580 OC](https://www.newegg.com/onix-odyssey-8346-00178-intel-arc-b580-12gb-gddr6/p/N82E16814987002) \\- 4.7/5, 28 Reviews\n10. [ONIX Odyssey Arc B580 OC](https://www.newegg.com/odyssey-arc-b580-12gb-gddr6/p/N82E16814987001) \\- 4.9/5, 14 Reviews\n11. SPARKLE Arc B580 GUARDIAN - release is scheduled for \"[Mid-March](https://www.fudzilla.com/news/graphics/60557-sparkle-launches-new-arc-b580-guardian-graphics-card)\" apparently, no Newegg page.\n12. [SPARKLE Arc B580 TITAN OC](https://www.newegg.com/sparkle-intel-arc-b580-titan-oc-12gb-gddr6/p/N82E16814993013) \\- 4.8/5, 28 Reviews\n13. SPARKLE Arc B580 TITAN Luna OC - seems to be a white version of the Titan OC, doesn't seem to be sold in the US, no Newegg page. \n14. [Intel Arc B580 Limited Edition](https://www.newegg.com/p/N82E16814883006) \\- 4.7/5, 41 Reviews\n\nTotal review count: 131\n\n# AMD Radeon RX 7600 XT\n\nAccording to [TechPowerUp](https://www.techpowerup.com/gpu-specs/arc-b580.c4244), these are the AIB models of the RX 7600 XT: - \n\n1. [ASRock RX 7600 XT Steel Legend OC](https://www.newegg.com/asrock-steel-legend-rx7600xt-sl-16go-amd-radeon-rx-7600-xt-16gb-gddr6/p/N82E16814930120) and Challenger OC - 4.8/5, 33 Reviews, these two cards seem to have a combined Newegg page. \n2. [ASUS DUAL RX 7600 XT OC](https://www.newegg.com/asus-radeon-rx-7600-xt-dual-rx7600xt-o16g/p/N82E16814126700) \\- 5/5, 5 Reviews\n3. [ASUS TUF RX 7600 XT GAMING OC](https://www.newegg.com/asus-tuf-gaming-video-card-tuf-rx7600xt-o16g-gaming-amd-radeon-rx-7600-xt-16gb-gddr6/p/N82E16814126699R) \\- seems to have no reviews? \n4. [GIGABYTE RX 7600 XT GAMING OC](https://www.newegg.com/gigabyte-gv-r76xtgaming-oc-16gd-amd-radeon-rx-7600-xt-16gb-gddr6/p/N82E16814932687) \\- 4.2/5, 26 Reviews\n5. [PowerColor Fighter RX 7600 XT](https://www.newegg.com/powercolor-radeon-rx-7600-xt-rx7600xt-16g-f/p/N82E16814131862?Item=N82E16814131862) \\- 5/5, 3 Reviews\n6. [PowerColor Hellhound RX 7600 XT](https://www.newegg.com/powercolor-radeon-rx-7600-xt-rx7600xt-16g-l-oc/p/N82E16814131861) \\- 5/5, 3 Reviews\n7. [Sapphire PULSE RX 7600 XT](https://www.newegg.com/sapphire-pulse-11339-04-20g-amd-radeon-rx-7600-xt-16gb-gddr6/p/N82E16814202440) \\- 4.7/5, 7 Reviews\n8. [XFX Speedster QICK309 RX 7600 XT](https://www.newegg.com/global/ae-en/product/14-150-888?item=14-150-888) \\- 4.4/5, 6 Reviews\n9. [XFX Speedster SWFT210 RX 7600 XT](https://www.newegg.com/xfx-speedster-rx-76tswftfp-amd-radeon-rx-7600-xt-16gb-gddr6/p/N82E16814150889) \\- 4.4/5, 7 Reviews\n\nTotal review count: 90\n\n# Anecdotal Datapoints\n\n1. Central Computers, a small chain with only 5 locations in the San Francisco Bay Area, [received over 100 B570 cards](https://www.reddit.com/r/IntelArc/comments/1itjpp9/psa_central_computers_bay_area_ca_has_tons_and/) during a just a single restock. It's worth noting that it's certainly possible Intel allocated disproportionately more supply to them, as Intel is headquartered in Santa Clara and CC has been a pillar of Silicon Valley for decades. \n\n2. Hardware Unboxed said that the initial supply was \"[quite substantial. They were \"blown away\" by demand for the Arc B580 – even the pre-orders were exceptionally high](https://youtu.be/fJVHUOCPT60?t=1199).\"\n\n# Important Caveats\n\nObviously this methodology is not very scientific at all LOL\n\n1. It doesn't take into account all retailers (maybe the RX 7600 XT sells more on Amazon vs Newegg, or through brick and mortar)\n\n2. It assumes that users of both GPUs leave reviews at the same rate, which is probably not true as it's quite possible Arc users are more likely to leave reviews since they're more likely to want to support an end to the AMD/Nvidia duopoly. \n\n3. However, it is worth noting that it's not like the B580's reviews are inflated by people review bombing due to driver issues, the LE's review average is sitting at 4.7/5, compared with 4.2/5 for the highest volume RX 7600 XT model, the Gigabyte Windforce Gaming OC.\n\n4. But it's also worth noting that the B580 has been on the market for a much, much shorter period of time, as the 7600 XT was released all the way back in January 2024 compared to December 2024 for the B580. This extra year of sales will tilt the numbers in favor of the RX 7600 XT. \n\nI don't think the data points towards the B580 being a \"paper launch,\" but I also don't think Intel is pumping out millions upon millions of B580s. They're definitely in third place in volume compared to the RX 7600 XT and the RTX 4060, but they're not as far as you might think; demand just seems to be super high. \n\nIf I have time, I might look at some other retailers (B&H maybe) or another card like the RX 7600 or the B570.\n\n**Once again, please don't take this too seriously LOL**",
    "comments": [
      "Yeah for some reason it's just super super bad in the US",
      "People who screamed paper launch should take a look at Nvidia 50 series, especially 5090. A halo product is already expected to have low supply but somehow it's even worse, even with way more AIB partners than Intel did.",
      "Pretty much the US gpu market is cooked because I’ve read numerous posts where people living abroad don’t have trouble finding the B580 close to msrp with lots of inventory. Im pretty sure no one is touching the 4060 because it has 8gb of vram, if it had 10gb like the B570, it too would be struggling to stay on the shelf because the 7600 (non xt) is also an 8gb card and is readily available",
      "I can easily find b580s MSRP plus tax on Turkey. Maybe they focused on countries where they have more chance but still b580 sparkle titan was cheaper than normal version.",
      "Just got a \" used \" b580 for 260€ ( open Box ) from Caseking in Germany with 24 months warranty im pretty Happy",
      "I checked Shopee Taiwan and searched for the RTX 4060. Looking at different sellers' sales numbers, the highest was 379 units, followed by 209, and the third was around 79. Roughly estimating the total sales from all sellers with visible numbers, it adds up to around 1,500 units.\n\nThen, I searched for the B580. There were four sellers with visible sales numbers: the highest sold 95 units, followed by 52, then 25, and 18. In total, that's 193 units.\n\nConsidering that the RTX 4060 has been on the market for almost two years, I think the B580's sales numbers are quite impressive. If we extend the timeline to two years, it could potentially surpass 700 units or even more. Especially if future driver updates bring a 5-10% performance improvement, this price-to-performance ratio could make the B580 a big seller.\n\nThe RTX 4060 is the best-selling model from NVIDIA and the entire GPU market, making this comparison even more interesting.\n\nI also searched for the A770 and only found a few listings from different sellers, but the sales numbers were all zero. Then, I searched for the A580, but the results mostly showed listings for the B580 instead.",
      "I can get B580 for 300€ (250€ MSRP +22%VAT) today if i want to. Also other models are also in stock in several eu countries.",
      "Well the prices for NGREDIA and AMD  in most of the european countries are bad as hell, saphire pure 9070xt is 850€ and saphire pulse wich is supposed to be a MSRP model is even worse and is around 950€, also cheapest 5070ti is 1300 in stock and cheapest 5070 is 1100 and in stock.\n\nEdit: it could also be that people are unwilling to try intel because its new in GPU market. Even thoug i myself am considering it for my build, since i am building a highend pc but can wait a year or two on GPU, because i am not upgrading to a 4k monitor for next two years.",
      "i checked rn and there's tons of prebuilds (1 retailer has 13 different prebuild with b580 other one has 19 and this is just 2 retailers ) and retailers still have many b580 i dont know why there's none at us market.\nFor proof (there is so many more i put just 2 retailers)\nPreebuilds\n\nhttps://www.incehesap.com/gaming-hazir-sistemler-fiyatlari/ozellik-95818/\n\nhttps://www.itopya.com/HazirSistemler?gpumodel=intel-arc-b580-q7598\n\nOnly gpu \n\nhttps://www.incehesap.com/intel-arc-b580-12gb-gddr6-192bit-ekran-karti-fiyati-76606/\n\nhttps://www.itopya.com/intel-arc-b580-12gb-gddr6-192bit-limited-edition-ekran-karti_u27591\n\nhttps://www.incehesap.com/sparkle-intel-arc-b580-titan-12gb-gddr6-192bit-ekran-karti-fiyati-76874/?srsltid=AfmBOoomMi_5NMORH1gY9jio3hvXt2Jg1cX6j-9IpeqTXA-c7mErX1_Z\n\nhttps://www.sinerji.gen.tr/asrock-intel-arc-b580-challenger-oc-12gb-gddr6-192-bit-ekran-karti-p-54280",
      "That's the fascinating part too, like you read all these posts from Europeans etc. saying how there's plenty of supply... meanwhile in the US there's literally not a single listing that has stock that's not from scalpers or resellers.",
      "Yeah, I'm really not sure whether that's due to higher supply outside the US or lower demand.",
      "Link?",
      "Interesting, thanks for the research! Looks about right. How were the RX 7600 and 7600 XT sales? Intel obviously has no chance to overtake Nvidia in the GPU market, but they could get closer to AMD."
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580",
      "arc b580"
    ],
    "title": "Intel Arc B580 Xe2 HPG (Battlemage) launch imminent?",
    "selftext": "",
    "comments": [
      "As mentioned on the Tom's Hardware comments section, the winner of more competition is the consumer.",
      "Shhhh baby, it'll happen when it happens.",
      "Pretty much the main reason intel needs to get their shit together, unless we want AMD to also start jacking prices up eventually for both their GPUs and CPUs.",
      "On the CPU side they're kind of already there. For some reason they refuse to launch the 7500f in the US so the cheapest AM5 option for a while was the 7600, now it's the awful 8400f. It doesn't take a lot for these companies to get complacent.",
      "This is the same screenshot that was linked to a Twitter post by [Tomasz Gawroński](https://x.com/GawroskiT) \\- I have not heard of him until now.  I'm engaging with him on X.  I'm finding more and more people that are outside the US, who are supporting Arc.  The numbers are growing every day, we will see...\n\nIt is not easy as X has become a paywall nightmare with more and more features not being free.",
      "I really wish they had a trade in program for the folks who invested in their first gen.",
      "From what they told us before, Battlemage shouldn't get murdered by nanite and Unreal 5 as a whole, lets wait how that pans out",
      "I’m very much looking forward to what they have to offer in this next gen. I’m building a budget living room pc so my fiancé and I can play pc games together (aiming for MHWilds) and really considering getting an arc to replace my 3070. The living room pc will have the 3070 if the battlemage performance is good enough.",
      "Looking into technical write ups on it, nanite relies on indirect draws and of course Execute Indirect. More and more it's clear that emulating the function was a bad call. They've addressed this in battlemage but no driver update seems like it will fix the alchemist.\n\nhttps://www.trickybits.blog/2024/04/20/nanite.html",
      "> It is not easy as X has become a paywall nightmare with more and more features not being free.\n\nPeople are switching to BlueSky en masse!",
      "My 3070 will be passthrough and prime for Nv only games. Going dual GPU.",
      "I hope so",
      "Performance at RTX 4060 (with 4GB extra VRAM) at a580 price?",
      "[Every GPU is getting murdered by nanite. ](https://youtu.be/M00DGjAP-mU?si=aHh_UuZRSV5y4Dn7)\n\n[Silent Hill 2](https://youtu.be/07UFu-OX1yI?t=129) (some people are seeing massive gains, like 4080 owners)",
      "Really high geometry count (tessellation)",
      "Intel is only doing discrete to create a brand and to get experience on drivers that actually run games. Until they have money again they aren't going to make anything above a x060, maybe x060ti."
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "a580",
      "b580",
      "arc b580"
    ],
    "title": "Retailers accidentally ship Arc B580 early, mistaking It for A580",
    "selftext": "",
    "comments": [
      "Well there you go, congrats to those who got their B580 GPUs early.",
      "Source: Reddit, VideoCardz\n\n👍",
      "C.  Following the English alphabet.",
      "First gen was Arc (Axxx), now Battlemage (Bxxx), then Celestial (Cxxx). I've seen \"Druid\" thrown around for the 4th gen name but who knows.",
      "celestial is coming out next year around dec too?",
      "I see. I only saw Druid mentioned in the article, and they skipped Celestial so why I was asking. Make sense.\n\nThanks for sharing that!",
      "IDK. 12 months seems quite quick to me, 18ish is more normal.",
      "So what's the naming scheme for the first letter in the model?\n\nWhat's the next one after 'B'?"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "a580",
      "b580"
    ],
    "title": "B580 as a replacement for a 1070 ti for 1440p gaming and video editing - worth upgrading?",
    "selftext": "So, my PC has a old 1070 ti in there. At least since I upgraded my monitors to 1440p ones, the card struggles to deliver stable 60 FPS in some games on medium or low settings - on one game even with FSR 3.0 (but that game is not really optimized).  \nI don't play the most demanding games out there and I don't need the highest graphic settings and prefer a smooth frame rate over the highest graphic settings.\n\nI have a i7-8700k plus DDR4-RAM of 16 GB, all of that runs with a 550 W power supply. Would the upgrade fry my PSU (honestly, I've been planning to upgrade that grandma of a PC for a while now for various reasons)  \nWould a B580 be an upgrade in terms of FPS? One more thing that is important for me is Video. I know it can encode and decode AV1 which is awesome, but how good were the Intel Arc video encoders on the A series? Like AMD in earlier days (way behind Nvidia) or is it roughly on par? I know, even upgrading to A580 would probably be a big upgrade, as the 1070 ti does produce a really blocky video on lower bitrates.  \nWere the A series compatible with DaVinci Resolve editing? With my 1070 ti, I tend to have some stuttering when playing back video (hardware accelerated decoding is turned on).  \nOr would it be a better idea to just scrap the entire PC, upgrade to DDR5, a somewhat current CPU and a PSU that has headroom for upgrades in the future?\n\nAlso, one or two more of off-topic questions to the Arc card owners: Does YouTube display AV1 videos automatically (for me, it doesn't, I checked some videos with yt-dlp, it makes sense, as my old GPU can't decode AV1)? YT tends to save a bit on bitrate on 1080p and as we all know, AV1 performs really well, even at relatively low bitrates. I saw someone testing the Arc A series cards with 500 kbit/s von 1440p60 and it did look really watchable (sure, there were a lot of artefacts, but holy lord, for that low bitrate at high resolution it looked amazing and AVC, VP9 and HEVC would've probably failed miserably).",
    "comments": [
      "Probably not. 8x is a lot more than you might realize. Only when you go as low as 3.0 x4 that you'll start seeing noticeable performance hit due to bandwidth limitation. For example, my motherboard's 3.0 x16 slot started malfunctioning a few weeks ago, and now I have to use the x4 slot. Certain games like Honkai Impact 3 seem to be very sensitive to that as frame time spikes increase tenfold, while others like Star Rail are just very slightly affected, with hardly noticeable spikes.",
      "First, check if you can enable ReBAR on your motherboard, since it's a must for all Intel dGPUs. Pre-Alder Lake motherboards may not work very well with ReBAR so take that in mind (for example my B460 Aorus Pro AC keeps resetting its CSM setting every once in a while and disables ReBAR during the process because ReBAR cannot work with CSM on).",
      "If the motherboard is only PCIE 3.0 will the 8x lanes on the B580 be affected in a large way?",
      "550W is well enough for A770 and B580, unless havin some i9 to pair it.",
      "From what I can find the B580 is about 5% better than the 1070Ti. I'm sceptical about my own findings though and I hope I'm wrong, because I'm also looking for an upgrade for my 7 year old card without spending a fortune",
      "If you can enable ReBAR on your motherboard then it's worth a try using your 8700k with the B580. The CPU will likely bottleneck but it's worth a try if you're on a budget.\n\nThe Arc cards do work with Resolve and the B580 seems to be an excellent budget choice. However i think your CPU will limit you. It sounds like you should do a new build! A lot depends on your budget though.",
      "I did a little more research and you're correct, impact is negligible :)"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580",
      "arc b580"
    ],
    "title": "i5 12400F & ARC B580 - A Decent Budget Gaming Combo? - 1080p and 1440p Tested",
    "selftext": "",
    "comments": [
      "Many people will probably be running this combo so it is good to know. Very underwhelming performance with the 12400f. I have an arc a580 with that cpu and the improvement I saw from those benchmarks does not seem too big of a jump so far."
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580",
      "arc b580"
    ],
    "title": "Hey did anyone benchmark the intel arc b580 for rendering in blender maybe? Because I cant find any data on it.",
    "selftext": "",
    "comments": [
      "https://opendata.blender.org/benchmarks/query/?blender_version=4.3.0&group_by=device_name \n\nB580 scores - 1805.36\n\nA770 scores - 2145.39\n\nA750 scores - 2175.69\n\nA580 scores - 1695.75\n\nFor reference a nvidia 3060 scores - 2132.1",
      "Thx!"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580"
    ],
    "title": "Is b580 a good choice for Editing and Blender?",
    "selftext": "I might buy this for my editing and blender work!",
    "comments": [
      "Battle mage currently has issues in blende so can't tell yet as the scores are terribly low even compared to a series. But it's likely the 4060 will be better in blender since generally optix crushes everything else, though I hope I'm proven wrong."
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "a580",
      "b580"
    ],
    "title": "Are people being too optimistic about the B770?",
    "selftext": "This is going to be a controversial topic but I am seeing alot of people saying they are going to wait for the B770. \n\nThe B580 is phenomenal for its price, and they are expecting or hoping for the same with the B770. \n\nHowever there are a few things that all tie together which I think everyone should be taking into consideration, with the first being the performance difference between the A580 and the A770.\n\nTechnical City has the A770 at an average of 11.4% faster than the A580. \nhttps://technical.city/en/video/Arc-A770-vs-Arc-A580\nAssuming a +/- 2.5% similar difference in performance between the B580 and B770 would put the B770 at 19-24% faster than the 4060.\n\nThe next thing is the MSRP price increase of the B580 over the A580.\n\nThe A580 debuted with an MSRP of $189 while the B580 has debuted at $249. This is an increase of ~32%\n\nThe A770 retailed with an MSRP of $349, applying the same percentage of price increase for the B770 would put it at ~$459 for an estimated MSRP.\n\nAlso Tom Peterson who is from Intel and heavily involved in their GPUs, has stated in an interview that Intel is losing money pricing the B580 as aggressively as they are, which I think means we can't expect the B770 to be priced any more aggressively in comparison when it's launched.\n\nThen there is timing. Nvidia and AMD will be releasing their next gen cards soon, and the generational uplift for Nvidia's 60 class of cards over the last 3 generations ranges from 15-40% and averages out at 25%, this means is very reasonable to expect the 5060 to be 15-20% faster than the 4060, with AMD's entry level card somewhere in the vicinity and likely with more VRAM.\n\nKeep in mind this is just speculation but all in all I think by the time the B770 is released it won't be nearly as competitive with AMD and Nvidia's similarly priced cards in the way the B580 is.",
    "comments": [
      "I mean, until we actually get to see the b770 in specs and price, we dont really know if its going to be good or not.\n\nAnd considering that the b580 is already out of stock in many places (fuck scalpers), waiting is pretty much the only option left, especially for those of us not in the US.",
      "If the 5060 has 8gb of vram like we saw on some leaks then its pretty much dead on arrival from anyone who actually has any braincells.\n\nAnd you can almost guarantee that its going to be more expensive than the 4060 at launch, simply due to the fact that Nvidia really has no reason to give a fuck anymore, considering how much money they make from datacenters already.\n\nIntels biggest competitor is going to be AMD, depending on their entry level new Radeon cards, which could be very competitive.\n\nI still have high hopes for intel, not the company itself, just their GPU department.",
      "If you are waiting for a B700 series card I would also argue you should be waiting to see what Nvidia does with the 5070 as well as AMD alternatives.\n\nI am quietly optimistic that the reason Intel didn't announce the 700 cards is because they have confidence they may be competitive.\n\nIn all honesty though I don't think we should be naive in that the reason Intel announced and released the 500 cards prior to Nvidia and AMD is probably because they only have a short period to be relevant.\n\nAs much as I hate to say it if the Nvidia 5060 releases and is just as good as the B580, then consumers will go with the stronger support of Nvidia. The main hope Intel has is that Nvidia remain arrogant on their brand power and keep or even try to increase prices.\n\nWith all that said I believe people where expecting Nvidia to release the 5060 mid way through next year, so Intel may have some time to make this work for the B500 and hopefully B700 cards.",
      "I think architectural deficiencies in alchemist resulted in the card not scaling as anticipated with additional core count. It is hard to imagine that would not have been an area of focus for improving the architecture in this generation. Perhaps the performance delta between the B580 and B570 may give some clues as to that, although there are probably diminishing returns at the higher core counts. If a b770 is released (assuming Intel doesn't try to rush out Celestial), I can't imagine it scaling as badly as alchemist, but whether it reaches targets remains to be seen.",
      "It will probably be priced out of being a good deal outside the us. between higher sales taxes and companies charging higher prices. If it's more than 300€ you will just get an amd card or a 4060",
      "Intel promised and the B580 delivered. \n\nNow that's a good start.\n\nThere is nothing wrong with being optimistic.",
      "Atm with initial drivers the B580 is almost on par with 4060ti 16gb, if the B770 is another 20-25% over that im gucci for sure.",
      "I bet the 5060 will have 8gb ram and cost $350 and people will still lap it up",
      "\\>If the 5060 has 8gb of vram like we saw on some leaks then its pretty much dead on arrival from anyone who actually has any braincells.\n\nExactly my sentiment. It was originally the card that I planned to upgrade to from my 1060, but I've lost hope ever since Nvidia did the 4060 dirty (by making it a 4050 in disguise). Now the 5060 with 128bit and 8GB VRAM further ensured that I won't be buying any low end Nvidia card. 2025 is around the corner and 8GB has no place for $250-300 price range.",
      "No, if you compare the chip sizes you'll understand why. B580 is outclassing A770 with a smaller chip.",
      "Honestly, my guesses are evenly split between \"B770 in 3Q 2025\" and \"they won't release the B770 and will just move straight to Celestial 1st or 2nd Q 2026.\"",
      "if peeps will buy 4060 over rx6700xt then they will buy 5060 over b770 even if its better lol, peeps just follow brands even in the pc market",
      "For sure, like I said this is just speculation based off the data currently available and assuming similar trends. One thing I forgot to mention was they also cut down the B580 compared to the A580. If they decide not to do that with the B770 there might be a larger performance difference between the B770 and B580 then there was with the A770 and A580.",
      "Considering we should be expecting a jump from 20 Xe cores to 32 cores, this is quite the leap. The card should solidly be a 1440p card that I assume would have 16GB of GDDR6. Since we saw a pattern of higher res equals more performance deltas on the B580, I’d be optimistic about it doing well in a 1440p/4K shootout. \n\nOn the otherhand, Tom Petterson straight up said Xe 3 was finalized on the hardware level at this point, so they may just skip to that.",
      "https://www.techpowerup.com/gpu-specs/arc-a580.c3928\n\na770 is 23% faster than A580. It is a 1/3 bigger card, and the scale seems about right.\n\nA B770 would be a slightly slower 7700XT with 16GB VRAM and better RT. I am willing to pay 400 for it over the 7700XT. But of course, AMD and Nvidia is launching next gen, so my decision is withheld until then. I'd have to compare price of that generation. See my math below.",
      "I don't think that there will be a B770, regardless I will wait for Celestial. I am impressed by the generational uplifts and Celestial should be a cracker series!",
      "Theres hardly a point for it to be faster because of the 8gb vram was holding back the 4060 so imagine what it will do to the 5060 if its 15% faster",
      "Made a mistake. It's not 15 -40% over the last 3 generations, actually is 15-20% with an average of 18%\n\nIt is still reasonable to expect the 5060 to be 15-20% faster than the 4060.",
      "I genuinely like intel GPUs,  unlike CPUs, for its relatively fine specs for a good price. I bought my a770 (16 gb version) for around $300 and i like it. For the same price i could take 4060, but 8 gb vram, meh, not enough these days even for 1080p, especially doing some llm stuff.\n\nI think Intel's main goal is to fight for low end/ mid range gpu market and they are doing pretty well, considering it's only 2nd gen being released.",
      "The 5070 will not be in the same price bracket as the B770. The B770 will probably be $399 to $450. The 5070 will probably be priced like the 4070 at best. ($599)"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580"
    ],
    "title": "How is life without DLSS?",
    "selftext": "I have a fairly old 2060 and I've been looking to upgrade for probably over a year now. I skipped the 40 series cus it was $$$ and the 50 series isn't looking much better. \n\nThe AMD 9070 looks good, but it's still double the price of a B580 and quite power hungry. \n\nThe thing I like about my 2060 is DLSS. The new transformer mode is insanely sharp. I can run CP2077 at high, with balanced dlss, 1440p at 50-60fps (no RT). Looks great, runs smooth.\n\nI know Intel has XeSS, and it looks pretty nifty. But how many games actually support it? How are you finding life with your Intel Arc? Do you miss DLSS? Or is it barely an issue?",
    "comments": [
      "FSR works well and is pretty widely supported. I've never used DLSS so I can't say I miss it, but the alternatives seem just as good from what I've seen. XESS is great when it is implemented. It remains fairly rare though.",
      "DLSS is great but I refuse to pay the Nvidia tax for it. XeSS is not in a ton of games, though it is in MH Wilds which is pretty cool. But thankfully you can use FSR on any GPU.\n\nGetting a great graphics card for $250 is more magical than DLSS will ever be.",
      "I have a b580 and a 4090 (in different machines in my home, b580 is in my kids machine) - the b580 at 1080p and 1440p is pretty brilliant.  But I also can say that DLSS is unmatched and if you like it, you should probably go for a 4-series Nvidia card.",
      "I guess it depends on the older game. As for examples…I’ve played Heroes III (1999), Skyrim, and Witcher 2 (2011) with no issues on the battlemage architecture. \n\nXeSS also looks as good as DLSS IMO, it’s just not in as many games as DLSS of course.\n\nEdit: Since you mentioned DLSS in Cyber….XeSS looks reallly good in Witcher 3 and Cyberpunk. I have the Claw 8 handheld with battlemage graphics and a 4090 desktop PC. I wouldn’t be afraid of buying a B580 after using the Claw 8.",
      "If you do get one, be sure to utilize the sharpening feature in the main intel graphics settings if there’s no slider for that specific game. You can set specific profiles for each game too. \n\nIt can help make XeSS pop and clean up a lot of blur if you have to use FSR. Made Witcher 3 look amazing with XeSS and helped RDR2 look much better when FSR Quality is on.",
      "90-95% of new AAA games are released with XeSS support I believe (either out of the box, or added later). Steam's XeSS enabled game list is relatively long\n\n\nhttps://steamdb.info/tech/SDK/Intel_XeSS/",
      "A 3070 or 3080 are still quite good ya know. If you look the right place they aren't scalped as hard",
      "Only 4060/4060Ti 8GB cards available here in NZ. Kinda lame.",
      "50 series dropping support for 32 bit Physx means you can't even play all your old games anymore on current Nvidia hardware. 50 series is a disgrace. Nvidia is dropping the ball for gamers, but they really don't care about that market anymore.\n\nFSR4 is sadly now AMD only. Hopefully the community can continue to build on FSR3.",
      "Certainly an option. I just wish they had more than 8GB.",
      "At some point gamers need to hold game companies feet to the fire and refuse to buy poorly-optimized games. Upscaling shouldn't be required on a new GPU to play a new game at a pleasant framerate and resolution, it should be sometime to fall back on 3-4 years down the road to extend the life of the GPU. FSR 3.1 is fine for that.\n\nUnfortunately looking at how well MH Wilds sold despite having months of evidence that it was diabolical levels of poorly-optimized, that point isn't coming any time soon. Gamers will buy anything and pay any price for it.",
      "I have the B580 and haven't had any issues. I've only gone as far back as 2010~ but haven't had any issues specific to older titles. There's a YouTube video where someone tested their whole library on an alchemist card and something like 95% worked near flawlessly.",
      "Thanks, that gives me a bit more confidence.",
      "Yeah. I've experimented a bit, and XeSS looks really good.  About the same as DLSS.  I wish it was in more games.",
      "https://steamdb.info/tech/SDK/NVIDIA_DLSS/\n\nhttps://steamdb.info/tech/SDK/Intel_XeSS/\n\nhttps://steamdb.info/tech/SDK/AMD_FidelityFX/\n\nXess little more than fsr but dlss is far ahead  for availability",
      "Intel all the way.. I’m rocking with a770 but can’t find a580",
      "If your game only has DLSS or FSR, just use optiscaler to enable XeSS.\n\nhttps://github.com/cdozdil/OptiScaler",
      "XeSS is implemented in more games than FSR",
      "Dang.  That’s a shame.  How is the AMD card availability?  Can you get a 7800xt or something like that?",
      "What games can you not play without physx?"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "a580",
      "b580"
    ],
    "title": "My sparkle A580 does this every damn time. Often even refuses to post in my system ",
    "selftext": "I already replaced the motherboard (now an ASRock X870 Pro RS) because I was experiencing similar issues with my A770 on the old B650 Tomahawk. That issue is completely fixed now, but my A580 is still having the same problem, crashing the system after a few seconds in windows, after restarting no video but pc posts. Strangely, it worked once right after I installed the new motherboard, so I thought the issue was resolved. However, I wanted to create an A580 vs. B580 comparison video, but that’s clearly not happening anytime soon.\n\nFor context, I used to run everything on a B450 Strix, and the A580 worked fine there. A full Windows reinstall didn’t help, running DDU didn’t fix anything, and resetting the CMOS didn’t resolve it either. What makes this even more frustrating is that this is my second Sparkle A580. The first one caused similar system crashes, but only while running games. To make things more confusing, all other Arc cards and even Nvidia cards work perfectly fine in this system.",
    "comments": [
      "You swapped motherboards, but what about video and power cables or even the PSU? Are you using any PCIe risers? Is everything properly plugged in and grounded (I know, I know, but got to confirm the basics when weird things like this happens)?",
      "Ahhh the annoying bothering problem I had with like 3 or 4 of my setups ... I never figured it out until recently ... I found out the solution that worked for me and maybe it could help you too. \n\nThis usually happens when there is shorted circuit or faulty ground connection which causes electrical noises to directly impact the visual output .\n\n First look for any shorted circuits in the case itself and then the setup like connections devices sometimes even a usb hub can cause such problems.\n if all were good then Try removing or at least separating any connection of your setup to the floor or ground .that was what caused my PC to go funny and play hide and seek with me .\n\n If that didn't work for you , try checking if the cable is faulty or if it is properly plugged by like pushing it in more or applying a very very light force on the cable to see if it affects the problem like if it makes it worse or just stops that from happening . \nThis happened on my PS4 with the non original hdmi cable it had . Maybe replacing the HDMI cable could help if that is the case with yours as it was with mine . \nHope this helps.",
      "I tried without extension cables and the psu from my old system where the card did work. No risers",
      "Yea, looks like a power supply problem. These posts seems can't wait to associate Intel Arc with keywords like \"issues\" before properly describing what they are experiencing...what is going on...",
      "There is not short circuit anywhere. Tried a dozen different cables all same issues. I did manage to to get into windows long enough to launch some games but as soon as games start it crashes the pc. My A750, A770, A380 and B580 work without any issues in the pc so short circuit is very very unlikely.",
      "If it does this on a more inferior monitor than yours (like a 1080p display /tv) then its not your cable bandwidth (my current bet is here is on old sh!tty cables, as its your common denominator between what you've mentioned; No need for an $80 HDMI cable, but, the cable *does* need to be new enough to cover the spec needed by the newer bandwidth demands).\n\nIs HDR disabled?",
      "Happens on both DisplayPort and HDMI but I’ll try different cables and hope for the best\n\nEdit:\n5 cables or so all same issue.",
      "A. Already disabled\n\nB. Rebar is turned on and CSM off\n\nC. Windows install is already GPT \n\nD. Can’t even get far enough to open settings app in windows.",
      "Would a cable force the pc to restart though? It happens on both hdmi and DisplayPort, also just tried a different monitor with different hdmi cables and same exact issue. I’m pretty sure it’s the card itself.",
      "Some modern boards allow you to pass the video processing even if you are connected to the motherboards inbuild graphics. Have you tried the display port of the motherboard?",
      "Guess I’m going to have to RMA again. Every time I want to use this card it’s hours of troubleshooting and I don’t want to waste my time with that",
      "I tried like 5 different cables all experience the same issue so it highly doubt cables are to halen. Started the RMA process",
      "If something else would be faulty all my cards would have the same issues but none do. A750, A770, A380, B580, RTX 3080 all works with 0 issues.",
      "My A770 does something similar in the bios with only one of my monitors. It drops the video signal constantly. I've heard it could be due to certain cables. I know I had an HDMI cable that just would not play nice with my A770 and any of my monitors that did work with my old 980TI",
      "I had a lot of issues with my 380.\n\nSome things I remember I did: \n\nRemove all video drivers including Nvidia and amd, there is a special tool for this \n\nWhen installing the Intel driver select custom and only install the driver itself. \n\nCheck with driver easy if all drivers are up to date",
      "When this happens, if you try ctrl+shift+windows+b does it help?",
      "Can you enter Windows safe mode or try with live Linux image?",
      "Disable ASPM in BIOS, if it isnt already.",
      "My A750 was having the same issue although momentarily, I gave it to a friend until his 4070 came back from warranty, he didn't have this issue somehow.",
      "I had similiar problems with my A770. No boot, intermittent black screens, reboots, artifacts. After RMAing the card and turning off ASPM the card finally works fine.\nIt really was defective. Turning off ASPM fixed a few monitor standby issues."
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "a580",
      "b580"
    ],
    "title": "Hypothetical discussion of Battlemage performance levels",
    "selftext": "Hi all, as with most of you I am excited for battlemage as well, so after the rumors of battlemage arriving before the end of this year and yesterdays post of B580 getting listed on amazon, I wanted to check what the new lineup would look like based on some fairly realistic assumptions of how intel could improve its gpus gen on gen\n\nEasiest way to do this would be using TPUs gpu database chart and I want to show 3 levels of each (25% - 35% - 50%)\n\nAccording to TPU current alchemist lineup looks like this, \n\nA380 (~RX6400/GTX1650)\n\nA580 (~RX5700/RX6600)\n\nA750 (~RTX3060/RX6600XT)\n\nA770 (~RTX2070S/RX6650XT)\n\nI'm not gonna argue about if gpus are performing to this level, if you used arc you know it depends on the game but generally their ranking seems fair\n\nSo lets look at where the new lineup would land if we get an improvement around 25%\n\nB380 (~RX480)\n\nB580 (~A770)\n\nB750 (~RTX3060Ti)\n\nB770 (~RX6750XT)\n\n\n\nAnd if we get an improvement around 35%\n\nB380 (~RX5500XT/RX580)\n\nB580 (~RX7600XT)\n\nB750 (~RX6750XT)\n\nB770 (~RTX3070)\n\n\n\nAnd if we get an improvement around 50%\n\nB380 (~RX590/GTX1660)\n\nB580 (~RX6700XT)\n\nB750 (~RTX3070/70Ti)\n\nB770 (~RX7700XT/RX6800)\n\n\n\nThis is of course based on the assumption that gpus will keep the same core count, however as you know there are also many other things that affect performance like bus width, bandwidth, core clocks",
    "comments": [
      "Based on Lunar Lake, I would expect somewhere between 33% to 50% performance uplift.",
      "If they make a bigger gpu with higher core count than a770, why not? It could happen with battlemage\n\nHowever i think 4080 is still a bit too far for intel imo, matching 4070 is more likely",
      "Dang I have a 3070 and I was hoping for some RTX 4070-4080 performance with the battlemage series to finally upgrade without taking out a loan. Maybe celestial (if/whenever that comes out) would be the move but that probably won’t be until late 2025 right?",
      "Lunar lake igpu I think maxes at 1.95-2.05ghz; a770 2300-2500 (2400-2500 is an overclock, run mine around 2500) and I’ve read Battlemage will be closer to 3ghz\n\nCould be very strong",
      "I'm not so sure about your performance comparisons. They said 50% I thought I read.",
      "The leak showed the b770 being between a 4070 and 4070ti but it's a leak so who knows what actual performance will be since they're often fake.",
      "I’d def settle for a 4070-level performance if it was sub-$500. I’m upgrading my pc right now and I was surprised (not really) at how high the 4070 variations still are",
      "I can see that A770 performing better or performing like how it should in DX12, however not every game is DX12\n\nA770 = 6650XT isnt my claim but considering all of the games in tpus game benchmarks its where the card sits according to them, its not a comparison between fsr and xess or comparison between rt performance, \n\nI'm aware from a580 to a770 theyre actually 256bit cards with decent memory bandwidth, a750 performs almost like a 4070 in topaz video ai, which ive found out happily\n\nBut all in all, you cant basically ignore arcs shortcomings, hopefully battlemage will fix them like you said",
      "B580 listing showed 2800mhz for it",
      "Been definitely considering AMD for my upgrade but I’m feeling weirdly drawn to Intel hahaha",
      "I hope the top end B770 has 16 GB of VRAM, performs the same or better as the 4070 and sells for $350 max. That would make it appealing.",
      "Intel's Xe2 has 50% IPC on previous gen.\n\nDue to a better node, approx 30% better clocks then approx another 20-25% due to other architectural improvements.\n\nB770 will be around RTX 4070 level.",
      "Nothing is certain until we see the benchmarks from reviewers\n\nThis is just guesstimating",
      "Recently I have found leaks to be pretty accurate.",
      "Yup, hopefully Intel does what i said above. That's the only way they're going to gain market share and have a shot at the next gen making decent margins.",
      "7700XT performs bit slower than 4070 in raster games however its significantly cheaper",
      "My focus is PCVR. I'm getting an Omni One (VR omni-directional treadmill) delivered soon. The current rumor is specs on par with 4070 super only with 16g VRAM instead of 12g (\"ti\" is necessary to get up to 16g) for one of the models. If they can put that out at the same price as a 4070S I'll likely snatch it.\n\nI'm just as excited as I was for the 285k (for the love of intelligence fix the name and call it 15th gen) because I expect all the issues to be worked out on launch. the chipset has so much potential. I am really hoping some amazing numbers in some areas for gamers.\n\nSpecifically so I really hope the efficiency & display interface they hyped on in the video they put out a few months ago shines through    https://www.youtube.com/watch?v=1LSF-II0l-4...  Quest 3 (which is what i use) caps at 120hz. If it can render VR at that refresh rate for most games for me it'll be a roaring success.",
      "The leak showed the b770 being between a 4070 and 4070ti but it's a leak so who knows what actual performance will be since they're often fake.",
      "I can see the B770 having at least 16gb since the B580 amazon posting showed 12gb",
      "PCI-e only cards are definitely not where the market sits, however theyre usually efficient and cheap enough for what they are\n\nFor htpc market and like entry level 1080p gaming they still matter imo, how profitable they are is intels question"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580"
    ],
    "title": "New graphics card arrived!",
    "selftext": "I'm gonna be swapping over my RX 6600 for this (Arc A770 16gb), mostly because this was at a decent price during the holidays and I couldn't find a B580 since they were out of stock at the time.",
    "comments": [
      "Idrk if this is a substantial upgrade over the RX 6600- will the card atleast be played in 1440p? Or utilize lots of programs that need that 16GBs of Vram?",
      "Congrats on the new Arc A770! It's quite a good price for the GPU performance, but from the RX 6600 to the Arc A770, it's not a significant improvement but it's still good.",
      "I would say yeah, but just know that some games still aren't optimized by Intel's drivers as they are relatively new to the GPU market. I hope you enjoy your new GPU though",
      "I mostly got it since I've heard it was decent at 1440p, and I wanted to be able to use my monitor at its native resolution for the games I play. That and the 16gb of vram I feel like will be useful in the future.",
      "How much did you spend? I got an open box A580 in microcenter for $110 yesterday. Upgraded from 1060 3gb for $185 back in 2016 🤣",
      "I need to upgrade my CPU and motherboard before I can use my sparkle titan 😭😭😭(I got it early because I was afraid of it selling out and scalpers)",
      "Oh, well for now, just enjoy the 6600 and have the last moments before you replace it.",
      "$240 for A580? It usually goes for $180 on Amazon before taxes.",
      "Ah ok. A770 is around $270 on Amazon so $240 is a great price.",
      "Nice! he can utilize your old hardware so it's not e-waste",
      "Nice!! I bought the same card for my son's rig. Went from a 1080ti with a 5600x. To this card and a 7600x also made the jump to 1440 as well. It's been a good upgrade so far.",
      "Its a good card just not for gaming. If u ever need to head down the content creation route the media encoders on that gpu is unrivaled with.",
      "Good looking card.",
      "I think it would've been worth the wait. Unless you really wanted 16gbs of vram instead of 12 gbs",
      "It's totally an upgrade over that 8gb card. \n8gb cards suffer nowadays. \n\nI upgraded from a 6650xt... never looked back.",
      "I am planning on getting a new PC case eventually down the line, thinking on swapping I've to the Jonsbo TK-1 as I think it's an aesthetically gorgeous case, it fits my board (im using an matx board), it'll probably look good with the new graphics card, and it's small and compact so it'll give more room on my desk.",
      "How much are they usually? I was able to get one on Neweggs eBay page for $240 and free shipping.",
      "I was thinking on going for a B580, but that was a little bit after we made the purchase on eBay, and they were all sold out where I lived. (Edit\" fixed some spelling errors)",
      "Yeah I've heard about that, hopefully the games that I play will be fine.",
      "It'll be in good hands, I plan on giving this to a buddy of mine someday, he want to build his nephew a PC."
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "a580",
      "b580"
    ],
    "title": "Which Arc card should i get?",
    "selftext": "Before anyone says \"don't, just wait for Battlemage\"\n\nWell my budget is at max currently £200.\n\nIt would seem most people here believe that the B770 is going to be $400+ which is around £320+ but we'd have to add tax onto that so probably closer to £400 ($500).\n\nThe B770 is going to be out of my price range.\n\nThe B580 may be in my price range, but if the B770 is going to come in $70+ higher than the A770 ($329 before tax) then that would be the B580 would probably be around say $229 (before tax) ($70 higher than the A580 launch of $179). I could potentially run to this, but when the A580 never released in the UK it feels like theres a good chance the B580 won't either.  \nStock is starting to dry up fast here of Alchemist GPU's, and i'm not convinced their going to restock if the next line is about to launch, but that also doesn't mean their going to release in the UK any time soon either.\n\nThere is one official retailer currently still selling Arc GPU's.\n\nI wanted to get the Asrock a750 challanger, but its just listed as preorder and i'm not sold that they'll actually get it back into stock. Tried to check with them and they just gave some gas about not being able to confirm an estimate.\n\nThe options are:\n\nSparkle A750 ROC Luna £179\n\nIntel Arc A750 limited edition - £169\n\nThese both come with a game (Assasins creed Shadows)\n\nI've heard that both cards aren't as good as the Asrock. Thats a shame, is that true?\n\nShould i just go for the cheapest? I'm feeling that might be best.",
    "comments": [
      "A770 16GB when the price drops after the new cards come out.",
      "Arc a770 3 fan 16gb OC all the way",
      "A770 16GB or battlemage top of the line.  since your gaming the video card is your most important part along with your screen (240hz).",
      "Yes that's basically the deal I'm looking at to\n\n\nI'm thinking I might just get the limited edition a750 for 169 plus AC shadows, then I've got a nice upgrade from my a380, a free game to look forward to and I can save for battlemage. \n\n\nMaybe I should spend 10 more and go with the a750 roc Luna though.",
      "THe A750 is a beast.",
      "The arc a750 LE is a great card. \n\nThat's a good price as well. \n\nGet it and enjoy",
      "Try to find A770 but A750 is still great choice",
      "I can get one for £239 atm, which is on a discount. Do you think they'll drop the price much more than that?  \nIts also tempting because part of the deal is Assasin's creed Shadows which the offer might be pulled before the next line are out.",
      "A770 Luna has been around that price lately, I got mine a month ago for $240 (around £190)",
      "ah luna a770 is out of stock, would the non luna variant of the sparkle ROC be the same but basically in white?\n\nHow have you found fan noise?",
      "At this point, if it’s not urgent, I would wait to see how battlemage pans out. In hindsight, I could’ve waited too, but I also got AC:Shadows+ Ubisoft Classics (for 6 months), which sweetened the deal; also, the card is white and matches my build.\n\nBtw, since they may pull out the offer as you said, I have already redeemed the game and it shows up as a preorder on the ubisoft launcher, ready to download when it comes out on Feb. 14th.",
      "I owned the a780, it was awesome but yes - it was hotter than the competing Nvidia card - however, ran games very well and the heat wasn't a deterrent",
      "Just got the ASRock Challenger A750, very quiet card and the cooler works quite well, keeps the temps at 60C at load. Very good considering it's one of the cheaper Arc A750 cards.\n\nIt may or may not come back in stock, Arc card stock doesn't seem to be amazing, likely Intel is prepping to focus on Battlemage.",
      "If your Budget is somewhat Tight, then I would go for the A580.\n\nIt is a 1080p Starter Card for Gamers so you shouldn't have much issues. I'm entirely sure if it can run 1440p unless you don't plan to go play at Higher Resolution.\n\n\nI have an A770 but I believe an A580 will be Consistent on Workflow for you.",
      "Retail pricing is back to circa two years, ago. A750 ~170-185, a770 ~ 230-245. So, it's really not on sale, and the current limitations isn't fairing to well in modern games--particularly UE5 engine games with any kind of lumen or nanite. As a first gen card, best to wait for the b580 reviews at least, or buy pre-owned for 100-150 range, IMO.",
      "If your budget is tight I would also look at the used market. Last gen AMD cards are getting pretty cheap.",
      "I love my ASRock 770. I can vouch that it's an extremely capable card. I stream and game simultaneously from it. A little game tweaking here and there and I can actually do well in a lot of games in terms of stability and framerate. I've been streaming SH2 at a solid 60 fps with XESS enabled. My main game is dead by daylight, and again XESS enabled give me a solid 120 at all times on the game's ultra settings. I'll say that call of duty is very weird to play though.",
      "I'm stunned with SPARKLE's A750 8GB OC.\n\nDoing great in new games and also in multimedia(rendering photo/video)\n\nCurrently, no regrets at all.",
      "Price in the UK is 330 for a770  and 239 for a750 when not on sale, so at £169 with a game its definitely on sale",
      "I've just ordered that exact card, great price really for what you're getting"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "a580"
    ],
    "title": "My first Intel Arc build.",
    "selftext": "So this is the card that got me interested in Intel GPUs. I bought the Arc A580 for $170 from Amazon during one of the Prime deals. \n\nPC Specs Pairing\nRyzen 5 5600 \n32GBs of Ram clocked at 3600mt/s\nMSI B550-A Pro\n750w PSU\n1TB nvme SSD \n\nI typically like working with low end or budget hardware compared to high end. Yes I can set everything to max and play at 4K without any trouble but I prefer troubleshooting budget hardware to make games playable on them. Spent endless hours trying to play games at 1080p at low, worst headaches I got and I loved it 🤣.  But I learned something along the way, card runs more consistent in 1440p. Fortnite on competitive settings (everything low with view distance far on DX12) averaged 90-100fps at 1080p. At 1440P with with some upscaling with some setting set to med-high with lumen on I managed to get a locked 60fps on my 4K TV. So in the end this was my gaming rig for my living room (until I upgraded to the B550)",
    "comments": [
      "I actually got the B580 recently as well. 😄",
      "Too bad its an A580 not a B580. \n\nNice choice.",
      "i saw that above and then made maybe the biggest typo possible lmao, sorry",
      "I see alot of people saying the blue B580 is ugly but I really like the color. I wanna convince one of my friends to do a B580 7600X build with a blue theme haha",
      "Holy fuck ur cpu cooler fan is hot asf!",
      "Did a bios update, enabled Rebar, DDU old AMD drivers, updated Intel drivers, used different CPUs (5500, 5600, 5600x, 5800x) Always averaged 100fps, even did a comparison of the RX 6600 and got better results there in regular Battle Royale. My old 2060 even had better results. I’ll do a retest to see if drivers fixed this issue.",
      "Those typos screw up the Google searchers. C580. See? Hello Celestial people from the future.",
      "Fornite tests! Woo!\n\nOk, so, wait, my baked in graphics (Radeon 780M) on DX12, everything low, view distance epic) I can get 165hz locked on my 1080p monitor (via USBC out).\n\nSurely something is amiss if you're unable to get above 100fps at 1080p. Rebar enabled in BIOS?",
      "Life is too short to play games on Low.",
      "is the b580 better than an a750?",
      "It’s doable but very limited. There’s some motherboards that have blue accents which go with the card. Color scheme is pretty much blue and black.",
      "check again - this is an A580 btw",
      "A580, not B580.",
      "yes",
      "Way better. More VRAM and more powerful cores.",
      "The card used is the A580.",
      "Aw nuts."
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "a580"
    ],
    "title": "Should i get rtx 2060 or arc a580",
    "selftext": "I have ryzen 5 5600, b450, 16 bg ram now should i get rtx 2060 or a580 now which should be best upgrade for me now ",
    "comments": [
      "people be saying get the b580 but theres none on stock lol",
      "A580 or b580?",
      "That's kind of a big jump for someone asking about what sub $180 GPU to get",
      "B580",
      "None. Both are a bad choice 'cuz-\n•2060 might be a better option than the A580, though it is a really old and inefficient card (You know what I'm trying to say here)\n•The A580 is a big No-No as the Alchemist series of Arc GPUs has been a disaster in the terms of driver maturity+you're here talking about the low end variation of a first-generation product.\n \nHere, buying an AMD GPU or a B580 should be considered.",
      "Force one into existence",
      "Not to mention that the b580 is at least 50% more expensive than the A580",
      "Neither: get a B580. And a Ryzen 5700X3D.",
      "a580, the rtx 2060 is very underwhelming.",
      "I ran a 2060 super until I changed over to the b580\nI used it to play on 1440p and it ran well! Depends on what OS op is using I guess",
      "I am not comparing the two. I am saying that it is strange that so many people default to saying that OP should buy the B580 instead, when they are in two different price categories and have two different levels of performance",
      "Yep, that's right. I guess they just misread the post and in the current hype around B580, they automatically reacted with B580.",
      "A580 unless you can get an A750 for a similar price.",
      "b570 or the a750 nothing else is worth from intel . b580 isn't available but if you can get one do sure get it",
      "Rx 6600 since it's more performance, and slightly more expensive. (I wouldn't 1st Gen arc cards)",
      "the 6600 is prob best $200 card right now. I have seen a few B580 or 6600 xt  for not much more. I would get those instead if possible and still in budget. they're about 25% better than the 6600 but $30-50 cheaper last time i looked",
      "You can get a A580 new now for $169 and is a very good value if you can't swing a B580:\nhttps://www.techpowerup.com/review/intel-arc-b580/33.html\n\nIt looks like a used 2060 is $140 and being ~30% slower is not a great value.",
      "I have an Intel A770 16GB card. My son has the Nvidia 3060. His card crashes all the time and mine is stable as can be. Mine outperforms his as well so id definitely get the B580. My only complaint about my A770 is that last I knew it wasn't VR compatible with my Meta Quest so I bought Virtual Desktop and it runs fine that way. It's been awhile and maybe it supports VR now.",
      "The A580 is actually kind of ok for 1080p. The 2060 is weaker",
      "A580 is not the lowest end of Alchemist, and drivers have come a long way. I would still steer OP toward a used radeon card either way though."
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "a580",
      "b580",
      "arc b580"
    ],
    "title": "rare case where B is better than A",
    "selftext": "https://preview.redd.it/a1qus2j0a42f1.png?width=1758&format=png&auto=webp&s=1e1f65251aad1ade18d9668dab2e3453bf703d72\n\ni decided to buy the asrock b580 oc after...um...missing the chance to pre-order the nintendo switch 2 in japan...the a580 card i'm using was bought new last september from a chinese brand i can't spell for around $150 on amazon japan.\n\nCurrently, thanks to the rising value of the yankee, the price of imported electronic components is decreasing, including graphics cards, the price of the intel arc b580 has dropped from \\~52000yen to \\~44000yen and may drop further if the yankee continues to appreciate.  \n",
    "comments": [
      "[Amazon.co.jp: GDDR6 8GB Graphic Board with Intel Ark A580 (Genuine) AR-A580D6-E8GB/DF : Computers ](https://www.amazon.co.jp/-/en/dp/B0CMXGG77T?ref=ppx_yo2ov_dt_b_fed_asin_title)",
      "store link? I can absorb the import cost if I could get it around $150 too. \n\n\ncurrently one retailer in my area are scalping B580 to $400 level, particularly Steel Legend variant. Any B580 priced at $300 and below will instantly vanish. currently no one buys from that retailer when the price crossed $350 mark",
      "You mean 'decreasing value of yankee'",
      "[https://www.amazon.co.jp/-/en/dp/B0DNV4NWF7?ref=ppx\\_yo2ov\\_dt\\_b\\_fed\\_asin\\_title](https://www.amazon.co.jp/-/en/dp/B0DNV4NWF7?ref=ppx_yo2ov_dt_b_fed_asin_title)"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "a580",
      "b580"
    ],
    "title": "A310, A380, A580.",
    "selftext": "Did these three cards ever get sold by Intel? And not third party? Reason I ask is because I’m trying my hand at building an Intel Arc LE collection.\nI have an A770 and a B580 but I wonder if I can even get the rest of these cards. I’ve only ever seen Sparkle, Asrock, Gunnar, and other companies.",
    "comments": [
      "A7670 is the best card honestly the next best card probably",
      "No LE cards of these.",
      "No. They only sold 770 and 750.",
      "dont forget DG1!",
      "i have an arc a580 with no markings from a third party because it was made by a Japanese company. it's the sparkle version without any of the sparkle branding on it so it looks like stock",
      "There are also a crap-ton of DG1 and DG2 dev boards that would make great collection pieces. Many have the wavy silver prototype shrouds.\n\n\nE.g.: https://forums.anandtech.com/threads/intel-xe-dg1-sg1-dg2-prototype.2610568/\n\n\nLot harder to find, though. :)",
      "No, only B580.",
      "ofc im joking bro, get the a770 or the b570 if you find it around 240$",
      "LE look alikes are tbh difficult to find",
      "Do they have a B570 Le?",
      "Hold onto that",
      "That’s my fault I didn’t realize the typo.",
      "i guess thats what makes them collectible... although not necesarily valuable, unless intel cards do really take off and displace second, or even first place in the GPU world",
      "Where would I ever find one of these?!",
      "That’s what I’m looking for next, gonna get an Arc A750 and then start looking at trashed prebuilts to see I can’t find any LE look alikes"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "a580"
    ],
    "title": "Taking out the Arc A580",
    "selftext": "Took out the Arc A580 to see if there’s any performance improvements after some driver updates that were released. Surprisingly yes! I saw improvements on some of the esports titles that I play the most. The Finals I saw go from low-50-60fps to med 80-90fps. OW2 since its DX12 beta release game went from 120 with stutters to 200-220fps with no stutters. Fortnite seems to be the same 130fps on performance. Marvel Rivals, 80-90fps on low. \n\nThinking of using this for a week and see how it works with more games. ",
    "comments": [
      "Nice! Still think it's a card worth buying for anyone on a stricter budget and only cares about 1080p gaming.",
      "First line up of the Intel cards. They were so bad at launch that they weren’t really worth buying, even under the msrp and discounts they were hard to sell. I got mine for $150. It’s worth getting one today if your able to find one even the A750 or A770",
      "Is it worth an A770 if I have a 6900xt?",
      "Yup. It’s slowly becoming more usable but still not user friendly when installing drivers. But I’m slowly starting to recommend it more.",
      "It's also capable of image generation, and even video, albeit it's not so good for the latter",
      "i don't game but I really like the idea of using a B580 for blender or compute stuff on a budget!",
      "Not worth it, the 6900xt is compatible with more games and you’ll take a performance hit.",
      "Thanks",
      "Didn't know there were an a580"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "a580",
      "b580"
    ],
    "title": "i5 11500 or 5700x to pair with intel arc a380 gpu",
    "selftext": "i can't afford latest n high end stuff rn, I'll try to Keep the cost as low as possible with optimal performance gain\n\nTo give u an mrp idea in my region,\ni5 11500(6c,12t)-125$\nRyzen 5700x(8c,16t)-125$\nIntel arc a380-138$\nG. aorus elite b450m-95$\nG. b550m k/Biostar b550mxc pro-110$\n\nRx 6600-240$\nA750-260$\nA770-300$,350$\nB580-350$\nA580 isn't available\n\nI want to learn video editing,may do some programming/info/loom video occasionally, maybe with basic colour grading/motion gfx/animation on davinci/AE\n\nI've heard of Intel's Quick Sync/Deeplink on video editing,do I lose something if I go with Ryzen 5700x since it has 2 more core,isn't all the necessary video editing stuff included in the arc a380(like av1) if I go with this combo?\n\nI've got a MBA M1 8gb/256gb,can i edit 720p/1080p videos on it?\n\nOptional -\nPlay valorant/PS2/PS3 emu games,gran turismo 3-4 sometimes(really loved GT5 when I had an i5 12500H Rtx 3050 laptop)\ni5 11500 has avx512 encoding but dk how great that would be in rpcs3 compared to 5700x/my previous 12th gen laptop exp.\n\n27\" 1440p 180hz display(275$,grabbing it ig, it's crisp n big)\n\nIs GPU even needed in my usecase?\n\nCurrent pc specs-\n i3 10th gen\nAsus 510m mobo(supports 11th gen,capped at 144hz 1440p,hdmi 2.0)\n2x8gb ram \n256gb gen3 nvme SSD \n1tb HDD \n450w generic psu\n21\" 1080p 60hz",
    "comments": [
      "get a lower end B760M and an i5 12400 imo, so you at least have some semblance of future proofing, 11500 is alright, but it's gonna become obsolete real fast, and you can't upgrade it to next gen on that mobo, but if you think your financial situations are gonna improve and you will replace the board and cpu altogether, then it's fine. but if you had to choose between the 5700x and 11500, then it's a blind buy for the 5700x. I understand you're on a budget constraint, so I'm not gonna try and coerce you into buying  a better gpu, but any of the CPUs you mentioned are perfectly fine to drive an a380, ans if you pick up the 11th gen processor or the 12400 you can Leverage the deep link tech too. But why would you get a 1440p 180hz Monitor for this set up is baffling me, because this is an alright 1080p build. Unless youre trying to future proof to some extent and are gonna pick up the 12th gen cpu, I'd advice prioritizing a better gpu over a 300$ Monitor. And youve already got a 144hz 1440p Monitor.",
      "Buying an old LGA1200 motherboard seems silly when you already have an old LGA1200 motherboard. Either use the one you got or jump to the new generation AM5.",
      "My parents buying me these :),and I wanted to replace my 10yr old monitor,so 1440p 27\"  is decent upgrade,I didn't want a ultra high refresh rate,I just needed a big monitor and it came with 180hz,while others have 165hz,100hz same price haha, different brand n warranty period that it"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "a580",
      "b580"
    ],
    "title": "Arc A-B series \"off-on\" fan behaviour",
    "selftext": "I am overall very enthusiastic about the new B-series and hope to pick one up soon. But there's something that stuck out I wanted to raise a discussion on.\n\nNew reviews of the B580 LE note that it has annoying fan ramping behaviour:\n\n>https://www.techpowerup.com/review/intel-arc-b580/41.html\n\n>\"While there was no idle fan-stop on the A-Series, Intel has made this capability standard with the B-Series—the fans will turn off in idle, desktop productivity, media playback and internet browsing. Unfortunately, the fans do spin up every few seconds, which can be quite distracting, because the human ear is much more sensitive to changes in noise. I'm not even sure why this is happening, it looks more like a bug than a feature, hopefully it's something that Intel can fix.\"\n\n>\"[thumbs down] Fan keeps switching on in idle\"\n\nYou can see this clearly in their graph of idle fan speeds:\n\n>https://tpucdn.com/review/intel-arc-b580/images/clocks-and-thermals.png\n\nEven the marketing for all the B580 cards seems to be proud of this!\n\n>https://www.asrock.com/Graphics-Card/Intel/Intel%20Arc%20B580%20Challenger%2012GB%20OC/\n\n> 0dB Silent Cooling\n\n> Spin For Cooling, Stop For Silence.\n\n> The fan spins when the temperature goes high for the optimal cooling, and stops when the temperature goes low for the complete silence.\n\n...\n\n>https://www.sparkle.com.tw/en/products/view/6893fe373180\n\n>0db Fan-Stop\n\n\n\nNow isn't this already a well-known dodgy issue for the A-series cards? The Sparkle cards in particular have had many complaints, and I thought it might have been a Sparkle-only issue. Or maybe Sparkle just has noiser fans that make it a lot more obvious.\n\nhttps://www.reddit.com/r/SparkleComputer/comments/1g0wqai/intel_arc_a_750_sparkle_fans_ramps_up_and_stops/\n\nhttps://www.reddit.com/r/IntelArc/comments/1gq6vyj/sparkle_intel_arc_a310_fan_issue/\n\nhttps://www.reddit.com/r/SparkleComputer/comments/1dngosy/sparkle_a310_eco_fan_revving_upd_and_down_issue/\n\nhttps://www.reddit.com/r/SparkleComputer/comments/1bohrb5/intel_arc_a310_eco_revving_fan_issue/\n\n**Old review from 2023**\n\nhttps://www.tomshardware.com/reviews/intel-arc-a580-review-a-new-budget-contender/7\n\n>Oh, Sparkle... what have you done? If you're in a quiet environment, the Sparkle A580 is absolutely audible. What's worse, the fan speeds aren't constant. Even at idle, the fans would often turn on for a few seconds and then shut off again, repeating that every 15 seconds or so.\n\nSo, can more people that have picked up any of the Battlemage cards comment on the issue? It sounds like Intel has built this \"feature\"/problem into all their cards *and are actually proud of it.*",
    "comments": [
      "If true and reproducible it would be a huge bummer as it is a major annoyance (I own a sparkle A750 and hoped that they would have fixed it with the new B-Series 😐)",
      "I think it will depend on whatever temperature is set as the cutoff (which can be influenced by the intel software fan curve).\n\nSo if your system goes above 50\\*C, the fans switch on, then it drops to 48\\*C, the fans switch off... repeat indefinitely.\n\nIf your system is naturally cool, then it might be able to stay with the fans off indefinitely at idle. \n\nI think a software solution could add some \"hysteresis\" to the fan behaviour. So the fans could switch on at 60C, then it cools down to 40C, then it takes a while to get back up to 60C. Which would be better than the 10 second intervals!",
      "It makes the issue only slightly less annoying. I want the issue to be gone completely. I mean it has to be doable right? I am pretty sure that it has to do with the high idle power draw which keeps the GPU at around 50-52 degrees Celsius.",
      "Might have to do with higher power usage at idle or bad case airflow at idle? 1080p 60 fps single monitor systems might not get it? While a 4k 144 hz multi monitor system might get it? Idle power draw seems dependent on monitor resolution and refresh rate. Aspm might help if it works well?\n\nAspm for the pcie slot in the bios might reduce power consumption/heat/fan triggering. 7w vs 38w at idle??????\n\nIncreasing idle case fan speeds might help? Side fan or bottom fan at low rpm help to keep gpu temperatures cool?\n\nMsi afterburner has a fan curve you can adjust. Can set it low enough to cool the card but not make much noise? Likely a minimum voltage/speed/power required to start the fans spinning. Not sure about the intel software.",
      "i can confirm this fan off / on bug on my B580",
      "Can always do a custom fan curve.... it's available even with the current software",
      "maybe the plugin works? [https://www.reddit.com/r/IntelArc/comments/1bvrvo6/plugin\\_for\\_fan\\_control\\_that\\_provides\\_support\\_for/](https://www.reddit.com/r/IntelArc/comments/1bvrvo6/plugin_for_fan_control_that_provides_support_for/) haven't tried it myself yet, but seems to at least exist heh",
      "For single monitor 60hz I know that the PCI-E power saving settings can bring A-series down a lot, but that's a vanishingly rare setup for people buying GPUs for custom built computers... \n\nHopefully it can indeed be resolved fully heh.",
      "Already tried that. Unfortunately for me it has literally zero impact :(",
      "seems People here dont get the problem. It is NOT possible to fix this bug with a fan curve. At least with no software i know of. I set Fan to ZERO till 70c. Card was on 40C and ist still turning off / on with a electric noise. Its a real issue because your fans are always running while on desktop",
      "What model do you have? LE?",
      "Make sure you have ASPM enabled in the BIOS (which can be tricky and need multiple lines to be ticked) and that your card is idling at 5-10W. That will help a lot in getting it to stay cool. With ASPM disabled it'll be making 18-30W during idle and will inevitably heat up until it needs fans on.",
      "no",
      "yes",
      "the card is cool and takes only 19W. Its a Software bug by Intels Graphic Software unable to stop fan",
      "19W is a bit high, that's what I got before I enabled ASPM properly, now it idles well below 10W.\n\nWhile idling, my fan is still going between 0 and 100rpm which is very gentle and I can't hear it, but I agree that seems like a bug. My Arc A310 on a different machine can stay at 0rpm.",
      "yes it should be 0 because i set it to 0 on intels software",
      "owww, dang : /"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "a580"
    ],
    "title": "I think my arc a580 graphics card is doing its best.",
    "selftext": "Setting:\n\n\\- Lowest\n\n\\- 720p windows\n\n\\- XESS 1.3.1\n\nhttps://preview.redd.it/2gvsizqsk3me1.png?width=1275&format=png&auto=webp&s=e458123b3ca5d3445718f3cc93ec4f2f9f89242a\n\n",
    "comments": [
      "I just wanna see XeSS 2 asap. More games should have it.\n\nAnd if you take a look at the steam reviews, everybody is having performance issues with this game. Another unfinishes unoptimized game entered the market. So I wouldn’t count on this game for performance issues",
      "I dlas swapped xess 2...its the exact same. Its the xell and xefg that makes it xess2, which i hope they add",
      "man that sucks, b580 at least can run 45fps with high settings.",
      "well, Intel is not Asian, A not better than B :)))"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580"
    ],
    "title": "My AM4 holdout build",
    "selftext": "Since everyone is sharing their build I'd thought I'd share mine. I swear the LEDs aren't as intense as they are in the video lol my camera sucks. I was previously using an rx 580 4gb blower(LOUD ASF) i shucked from a Dell Optiplex before I got my b580 so needless to say it was a massive upgrade for me in both noise and performance. I'm loving the AV1 encode on the b580 a lot, I stream to my SteamDeck all the time and it looks great.\n\n\nThe Specs are as follows:\nCPU: 5700x3D\nRAM: 48gb of DDR4 3200mhz(I got 32gb free when I purchased the 5700x3D), 16gbs are G.Skill Trident Z and the other 32gb are Crucial Ballistix\nStorage: 1x Gen 3 NVME, 2x SATA SSDs, and 1x HDD for bulk storage.\nCase: Zalman P30\nGPU: Asrock B580 Steel Legend\nMB: Asrock B450m Steel Legend\nPSU: SeaSonic 850w Gold MII\nCooler: Deepcool Captain 240ex\nFans: Thermalright M-12S\n\nMost of these components I've had for years, the only new parts are the 32gb of RAM, 5700x3D, Case, Fans, and of course the B580.\n ",
    "comments": [
      "Nice looking build, gives off Star Wars Sith Vvbes.  Do you run any benchmarks?  If you do or can you run the 3DMark tests and show how the 5700x3d pulls in both Time Spy and Steel Nomad (Dx12 Standard test)?  Time Spy will give you a GPU and CPU number, that latter is of interest.  I have an old AM4 system I was thinking of upgrading, which is my interest in the 5700x3d results.  Thanks!",
      "Yeah I can do that... tomorrow, I'm in bed now. I did run Steel Nomad DX12 a couple of days ago and I believe my score was 3300-3400~ish(EDIT: I was wrong look for my other comment below). If you look on the results for the b580 on 3dmark it is in the expected range. But I'll make sure to run it again along with Time Spy.",
      "Works fine for me, very few issues.",
      "Basically identical build minus running a b450 and 650w. Id like the 4.0pcie slot but I aint spending for a new mobo, if I did it would be am5.",
      "I wanted to ask if the intel arc do well on the AM4 Mobo. I also have an RX 580 and looking to upgrade to an A580",
      "Would a pci3 board bench be of benefit to you? Same setup just older mobo.",
      "Alright sorry this took so long, it's been a looong day, but here it is: [My Time Spy Result](https://www.3dmark.com/3dm/124349867) and my [Steel Nomad Result](https://www.3dmark.com/3dm/124350030) i made a mistake in saying i got 3300-3400\\~ish, i seem to have misremembered my test score... those are from the heavily OC'd cards, mine is just a stock Steel Legend one. (I also did a Port Royal, and FireStrike test if you wanna see that: [Port Royal Result](https://www.3dmark.com/3dm/124350478), [FireStrike Result](https://www.3dmark.com/fs/32812462))"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "a580"
    ],
    "title": "Should get rtx 2060 or arc a580",
    "selftext": "My config is r 5600 and b450 ",
    "comments": [
      "2060, but I recommend saving for b580",
      "B580, it's still being optimized so it'll just keep getting better.",
      "i don't think the b580 would play nice with the ryzen 5 5600, regarding the overhead issue lately",
      "It's still worth it, it's greatly exaggerated on reddit, new benchmarks have come out.",
      "2060\n\nbased on your budget range, a 1080 or 5700 xt could also work",
      "The RTX 2060 6 GB card outperforms the Intel ARC Alchemist A580 by more than 10% with the only advantage for the Intel card as better video compression technology. The A580 along with all the other Alchemist series hardware has a manufacturing design flaw where the programming will randomly crash the system up to two hours upon initial loading of a program for full load usage of the memory capacity of the hardware. The developers determined that the scheduling section of the chip is broken and that the flaw is a design problem. This is why there is an upper limit to FPS. Video driver updates can reduce the problem but it will be by sacrificing FPS to a lower rate to reduce probability of crashing and lower 1% low system stuttering.\n\nAll Intel ARC discrete video cards were designed for use for DirectX 12 and newer application protocols, therefore, performance for usage of any programs that use DirectX 11 or older will have performance issues, because Intel will use emulation software inside of the GPU hardware to perform legacy programming. This technique always reduces overall maximum potential performance output.\n\nThe recommendation is to use an RTX 2060 Super 8 GB card rather than an RTX 2060 6 GB or ARC Alchemist A580 8 GB video card. In the used market, all of these cards are priced below 200 US dollars, while in the brand new video card market, the AMD RX 6600 8 GB card will outperform all the previous hardware mentioned with a performance level equal to the RTX 2060 Super 8 GB card.\n\nIf your budget is at or above 250 US dollars, then your options are much better than any of the hardware mentioned here. Since, that gives you possibilities such as the RTX 3060 12 GB, RX 6700XT 12 GB, and even the RX 6800 16 GB card at just over 300 dollars.",
      "cheap Nvidia cards are a scam, either get the cool one or get AMD"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "a580",
      "b580",
      "arc b580"
    ],
    "title": "Should I buy Intel Arc A580 for my current setup?",
    "selftext": "Hey everyone,\n\nI'm considering upgrading my system with the Intel Arc B580 and would love your advice. Here’s my current setup:\n\n* **Processor:** Ryzen 5 5600G\n* **RAM:** 16 GB\n* **Motherboard:** MSI B450 Tomahawk MAX II\n\nI'll primarily be using the GPU for Unreal Engine, Blender, and some gaming. Do you think the Arc B580 would be a good fit for this configuration?",
    "comments": [
      "Intel have fumbled the bag with the overhead issues since even if they fix it, it'll take a year for people to stop talking about it whenever someone mentions the b580. They had this issue with 13/14th gen cpus. Lots of misinformation about affected processors and then the same usual larping even after hotfixes. So, I would suggest. Don't let the overhead issue completely put you off as intel do seem to have a track record of fixing their drivers. They should at least close the gap on nvidia and amds overhead issues. \n\nHowever, Intel did recommend buyers that they should use a 10th gen or newer, or amd 3000 series or newer CPU to pair with the b580. You'll notice a lot of these benchmarks pointing out the issues are using 9th gen or 2k series cpus. So in a way, if you take their recommended advice, you won't have an issue. you'll be fine with the 5600g. As you can see someone has said they have the same CPU and have no issues. \n\nJust remember people on reddit take a snippet of information from a youtube video with no knowledge, and run with it, and will continue to run with it even if it was fixed years ago. So yeah, take my advice but do your own research to confirm what I'm saying. Can't trust anyone with tech on reddit.",
      "Titles says \"A580\" but seems like you meant B580?",
      "I have the same processor. No issues so far, but limited testing. I think you might want more RAM, however.",
      "My suggestions:  Upgrade your CPU to the 5700X3D.  It's the best performing CPU for AM4 for the money.  No overhead issues for the GPU then.  Upgrade your RAM to 32GB(or more) which will be an inexpensive and easy upgrade.  Make sure you have a decent tower cooler for the CPU as well, but you don't need to spend more then $30.00 on that. \n\nKnow that the B450 chipset is limited to PCI Express 3.0 speeds so you may not get the full performance out of your new GPU regardless of any other upgrades.  Also ensure REBAR is turned on and you update your BIOS ahead of any other upgrades.\n\nYou could also consider an AM5/Intel upgrade for board/cpu/RAM.  You'll spend a bit more but you'll be on a much more modern chipset and current/recent gen CPU's that will eliminate any driver performance issues.",
      "No. With a different CPU yes, but not the current one.",
      "Probably wouldn't recommend it with that processor and some people have had issues with the B400 series motherboards not to say yours will."
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580"
    ],
    "title": "Finals performance increase?",
    "selftext": "Is it just me or has anyone else noticed the finals performs a ton better on arc, specially alchemist.\nBefore I'd be lucky to get 60+ avg on all low with many dips but now I'm averaging over 80 fps and hitting a 100 with more stable frame times. This could be a sign devs are taking arc more seriously, probably because of the b580. Either way I'm happy.\nI'm using a a750 with a 5600g btw.",
    "comments": [
      "Before it was barely running 60, now I can play it decently. 60-80+fps. and I'm like using  r5 2600 with 3.8 ghz \\[oc tuner in bios\\] and 32gb ram. and i'm using a750 8gb  with settings  like 228w in the core power limit  29 performance boost in intel graphic software.  the graphical preset is custom with some settings medium but majority is high with xess ultra or ultra quality. \n\nwhich is potato cpu by today's standard lol. but yes i'm satisfied knowing that I have an outdated cpu.",
      "I'm just happy I ain't stuck at sub 60 anymore. Though I guess it wasn't a performance increase but instead a cpu overhead decrease.",
      "same here.  I tried it out to report here. but I already have a game that I main which is valorant. but I'm happy that even with my old cpu, I can play it decently.",
      "I don’t notice much difference before and after the driver release. I only play esport games like CS and valorant",
      "I have A580 with 10700k. Amazing 1080P medium to high setting with 70fps+",
      "Is it any better or is my setup just to trash and only ĺess guys like me saw the improvement."
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "a580",
      "b580"
    ],
    "title": "DX12 Support? 😮",
    "selftext": "I recently saw that OW2 added a beta of DX12 and included support for XeSS. I had to try it out on my ARC cards. On my A580, I’m now averaging 180-220 on High. Before I would get 120-150 on the same settings. Those small frame stutters are gone and the game runs much smoother. On occasion there’s one pause or dip when so much is happening on screen (which is listed on the patch notes) but occurs once in game and it’s not happening every second. Gotta try it on my B580 now. ",
    "comments": [
      "This plus a BIOS update literally saved the game for me. Previously it was unplayable with the stutters."
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "a580",
      "b580"
    ],
    "title": "i3 12100f + ARC A580",
    "selftext": "Im doing my first build and also im on a budget so im not sure if using the MSI PRO B760M and the i3 12100f would be good enough for a build having in mind i want to upgrade in the future in like 3-4 years, maybe a better cpu or a gpu like the B580",
    "comments": [
      "Ideally, want 6 core or more. \n\n\nAlso, lots of options depending on the budget whether Intel CPU platform, or newer AM5 platform. With AM5, will at least have a better upgrade path if looking ahead. Really depends on budget, and what looking for in a PC. Also, the region for pricing may be a factor.",
      "great idea i see you get a b760 so you want to upgrade your system but if ou ave money you could get a 12400f or 14400f(they are the same)or you could get 14700f",
      "Right now getting an i5 12400f it's more cheaper than the 5600x so I probably gonna go with that",
      "ali express"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "a580"
    ],
    "title": "asrock a320m/ac compatible with the intel arc a580",
    "selftext": "Hi I do not know much about computers but I want to update my graphics card from a 1050ti. My motherboard is an asrock a320m/ac, will the intel arc a580 be compatible. I’ve been trying to look it up but am struggling to understand pci and pcie and how to tell. ",
    "comments": [
      "By a quick look of it, it's unclear. \n\nhttps://www.asrock.com/mb/AMD/A320Mac/index.asp#BIOS\n\nhttps://www.reddit.com/r/ASRock/comments/pp9d86/i_cant_find_resizeable_bar_on_my_motherboard/\n\nhttps://www.reddit.com/r/AMDHelp/comments/15tr45x/is_there_a_way_i_can_enable_sam_in_this_system/\n\nYou could try updating the BIOS and seeing if you can enable resizable BAR (reBAR) which might be called SAM (smart access memory). A program like CPU-Z can tell you if it's successfully enabled.\n\nEven after that, with such an old CPU (what is your CPU?) you might run into bottlenecking.\n\nhttps://old.reddit.com/r/hardware/comments/1hsjqb2/intel_arc_b580_massive_overhead_issue/\n\nSo you might be encouraged to upgrade your mobo and CPU to run an intel card. If you must keep your old system then probably not an arc card.",
      "Assume you mean B (Battlemage) gen, as in B580, there's also A (Alchemist) gen and an A580 but the B580 is the one in the news atm- what follows works for both. See if you can do [this](https://www.youtube.com/watch?v=b9KTTLbn8Io&t=1s) in the BIOS. If yes you've got the ReBAR and 4G encoding to get full mileage out of a B580 (or A580) and are good to go. You've also got a PCIe 3.0 x16 slot on the mobo it'll need (it needs x8 IIRC but will fit in a x16, the A580 needs x16 so good for that too).\n\nThen you might want to post what CPU you've got here, so folks can comment on whether it's a good 'fit'.\n\nAlso I don't have a B580 (or A580) myself but check dimensions of your case to make sure it will fit.\n\nIf you really are talking A580 as opposed to B580 my advice is don't do it unless it's a really good deal and even then think x2 or x3 or still don't do it. Get the B580 or wait for next gen cards to appear this year before deciding. Or good 2nd hand deal on an A770 16GB (that;'s the ONLY A gen card I'd buy at this point)."
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580"
    ],
    "title": "Future B series cards? ",
    "selftext": "Sorry for the ignorance, i genuinely tried looking up to see if intel had announced other B series cards other than the B580 and B570. I really want to make the jump to Intel cards, but the 580 would be a side-grade at best. \nHow was the A series roll out? Did it take time for higher end cards to be introduced? \nIt just seems odd to me they only announced two cards. ",
    "comments": [
      "The A series started with the A380, with the A580 being one of the last cards. This time the B580 and A570 (no Alchemist equivalent) came out first. I think if there is going to be a more powerful gpu it will be released in March next year.",
      "During a recent interview the head of Intel Arc said Celestial (Arc gen 3) is ahead of schedule. I think i remember from like back in 2022 they wanted to do YEARLY gpu updates... maybe Q4 next year is C770?",
      "It’s strange to me, I can’t even find anyone online asking the same question. It seems like everyone is just hyped about the two they released, no one is asking, or even speculating about future battle mage cards.",
      "I really hope they come out with something in the spring. I don’t want to wait another year lol"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "a580",
      "b580"
    ],
    "title": "Not getting a signal out of my ASRock A770 Challenger and Sparkle A580 in my MSI B650. ",
    "selftext": "Switched to my main pc because my B450 died and now I can’t get 2 out my 5 Arc card working in my MSI B650 Tomahawk WiFi. The A770 makes a weird electrical noise (not like coil whine) and fans ramping when powered on and sometimes shows the bios but quickly goes black and giving no signal. Same for my Sparkle A580 but no video at all not even bios, fans also ramp up randomly but not weird electrical noises. Have updated the BIOS of my motherboard, CMOS rest but no luck. Bottom slot also doesn’t work. Windows doesn’t even recognizes the cards when I use my iGPU. Both cards work on other older motherboard. \n\nAnyone experienced similar issues? I’m really thinking about just buying a ASRock motherboard and hope this fixes it. Just finished recording 16 games for side with A770 but now I can’t get it working 😭. My A750 Challenger and A380 do work. B580 also. \n",
    "comments": [
      "I got the A770 working again by pulling out my 3rd m.2 and sound card. Guess the A770 can’t run X16 with those installed while the A750 can. My A580 now also shows video but as soon as windows loads it just goes black even after safe mode ddu.",
      "It's likely just that B650 motherboard. I had a similar case when my A750 stopped running on the x16 slot of my B460 Aorus Pro AC. At first I thought it was the GPU, but extensive testing with a 2080 confirmed that the mobo was busted. Basically with the A750 the mobo would boot on first boot and fail on subsequent boots unless I turn off the PSU switches and turn it back again, while with the 2080 it could boot just fine but turn into no signal after 20 mins of running.",
      "Any update on this. I'm getting that same motherboard and b580 and I'm curious is you have found a solution tot his issue."
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "a580"
    ],
    "title": "Arc A580 is among the minimun GPUs for running Final Fantasy 7 Rebirth ",
    "selftext": "Final Fantasy 7 Rebirth PC technical specs revealed\n\n16GB is almost mandatory for 2160p@60FPS\n\n[https://www.eurogamer.net/final-fantasy-7-rebirth-pc-technical-specs-revealed-ps5-console-exclusive-being-optimised-for-steam-deck](https://www.eurogamer.net/final-fantasy-7-rebirth-pc-technical-specs-revealed-ps5-console-exclusive-being-optimised-for-steam-deck)",
    "comments": [
      "B580 probably falls somewhere in the recommended area. Maybe it'll get 1080p high 60."
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580"
    ],
    "title": "My B580 finally arrived ",
    "selftext": "",
    "comments": [
      "Welcome to the club",
      "Congrats, nice to see more people getting them in the post, just one suggestion before installing unless you already have is to make sure your BIOS is the latest version and turn on Resize Bar unless you have a fairly new motherboard (last 6 months).",
      "They can pry my 770 from my cold dead.... Well actually another 770\n\nThat way I can ray trace my memes twice as fast. Baked-in DX would be nice too. lol.",
      "Aesthetically, Intel did a great job with these gpus. They look great!",
      "Mine arrived Saturday. I know it's a small detail, but the packaging on a cheap GPU has no right to be this fancy.",
      "Did turn on resizable bar, not sure if my bios is fully updated it, probably not, haven't updated it ever, but only bought it 2 months ago, although it's a 2 year old product.",
      "Holy shit it pairs well with your cpu cooler, looks super clean to me",
      "BIOS after 09/28/22 supported Resize Bar (if your version is after BIOS 7B86vAH1)\n\n[https://www.msi.com/Motherboard/B450-A-PRO/support](https://www.msi.com/Motherboard/B450-A-PRO/support)  \nThe \"Re-Size BAR Support\" option located under \"Advanced/PCIe/PCI Subsystem Settings in BIOS",
      "Silent protest against them, the NZXT logo on the case is covered up by my intel arc sticker too.",
      "Resize BAR technology unlocks CPU access to video memory, enabling full access to video memory and multitasking, thus improving overall data processing and gaming performance\n\nResize BAR support requires the motherboard BIOS, graphics card and driver to be supported simultaneously",
      "You're one of the lucky few. When I wanted to buy one, they were already taken up by scalpers who are selling them on eBay for 500+and leaving only the 350+ \"custom\" options from the other manufacturers, and I don't have the time or money to go to the nearest micro center to buy one in person",
      "I have to admit, the design looks great and I appreciate every competition against Nvidia!",
      "They look sexy too. I know once you stick it in, you don't see it as well, but definitely a nice overall touch.",
      "The power this thing has no right to be $250",
      "Does microcenter always have them in store?",
      "Make sure your bios is up to date and then look  thoroughly for Rebar",
      "I  only mention it as some people have had issues, best way I can explain is that the BIOS might have been written before this was even thought about. It may work just fine out of the box like most have. My board is about 2 years old and I updated the BIOS a couple of weeks ago and had no issues with the card being recognised. Some with older boards have run into issues (may not of been a BIOS issue) . Less so with the 580 LE over third parties.",
      "Really good looking card",
      "These cards are so smooth",
      "Hey, total noob to the intel Arc series graphics cards here, are they good for gaming? I don’t know anything about them but a lot of people seem to be picking them up, is there a stand out feature that AMD or NVIDIA cards don’t have?"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "a580"
    ],
    "title": "Any feedback on Intel arc cards regarding video encoding in VMIX?",
    "selftext": "I guys sorry if this was brought in the forum earlier, I searched and could found so I'm posting, for your help and feedback.  \n\n\nI'm on to buy a new GPU specifically for live streaming on vmix and some light media work on premiere or resolve, and read that the guys at vmix are making intel fully compatible, but cant find any good review on it. Also I am considering Intel over NVIDIA because of price and energy I'v built a micro atx pc for that, so want to keep system small and efficient to be portable. And also because Ive read that intel hardware encoders (HEVC, AV1, H264) are best quality and more efficient with more codecs.  \n\n\nSo to the point have you guys had some experience you care to share on vmix, premiere and resolve, with a380, a580, a750 cards?  \n\n\nThanks again  \n",
    "comments": [
      "Took a year for me to follow up, but just now settling down to testing a B580 and 265KF combo with vMIX 27.0.0.91. Very impressive, streaming to YouTube with an AV1 custom encode in vMIX, at 14000 Mbps. Very low CPU and GPU loads, and my card, the Gunnir B580 Phantom (3 fan), is at only 28 degrees C. Shockingly good. I tend to stay on the final run of an old build like 27 for 6 months, will switch to 28 next month. Likely even better."
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580",
      "arc b580"
    ],
    "title": "the graphics car has arrived (ignore Arc B580)",
    "selftext": "",
    "comments": [
      "how is purrrrrrrrrrrrrrrrformance ?",
      "+100 floofs purrrrrrrr second",
      "Nnnnnyyyyyyyooommmmm",
      "The Graphic \\*cat has arrived. Ignore the B580.",
      "The cat: 😏",
      "Cat tax is satisfactory :P",
      "The cat has pretty impressive graphics.",
      "This is such a great comment 🤣🤣",
      "It's a car",
      "That’s a lotta floofs!",
      "Wdym? I only see car",
      "good job!! - my B580 just arrived today as well!! :)",
      "Dad please stop posting online this is getting embarrassing now",
      "Cheers!! Here's hoping future drivers help our cards age even better",
      "No need to mention \"ignore Arc B580\", because couldn't see that in this pic.",
      "Nice car. What's the retail on one of those?",
      "Congratz on the upgrade. Nice car!. And nice dog!",
      "The cat is angry, because it know's that you will for the next 4 month's not play with her.",
      "Kittel Arc B580 Cattlemage Graphics Car",
      "Pleas go to r/roastmycat but i like the cat"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580"
    ],
    "title": "Time to start testing B580 on lower-end CPUs",
    "selftext": "Let me know what games you want to see!",
    "comments": [
      "General reminder. When changing CPU and GPU at the same time, don’t forget the following.\n\n1. Clear CMOS/BIOS\n2. Run DDU prior to changing to Intel GPU.\n3. Plug in the 8-pin PCIe power connector to the GPU.\n\nThis will aid your efforts greatly… as I just learned",
      "Baldurs gate 3!",
      "People like you are why we have to deal with so much shitty DRM.\n\nPirate what you can't buy, for whatever reason you may have. Don't just pirate because you don't think single player games are worth paying for.",
      "Reinstalling the chipset driver is also recommended, as it can influence power plan behavior for each specific CPU.",
      "I can help you find it... ;)",
      "Don’t own it, and it’s currently to $60 on Steam. Sorry.",
      "if anyone knows how tf2 cs2, and overwatch run with a b580 on an i5 11400 I would love to know, 4k and 1440p.",
      "I also have a 3700x and 2600.",
      "I'm sorry, lower end? The 5800x3d is not low end",
      "Good call! Doing that now.",
      "It's a 7 year old CPU dude. You can buy one for under $50. It's low end\n\nEdit:\nI'd like to mention that it wasn't even high end in its own product stack when it released, the 2700x was. And that's what I had bad it was DEFINITELY bottlenecking me in new games",
      "Starfield, Stalker 2, Hogwarts legacy, Warframe 😁",
      "Please test with the 3700x, that's my cpu and I'm curious on how it performs.",
      "ark survival ascended if possible!",
      "I refuse to assume that my 2600 is low end (it heats up hotter than the sun in 2.045 seconds when I launch Starfield)",
      "That game should be avoided asap. You don't even get 60fps a 4090 with forced framegen.",
      "Lol @this attitude in regards to baldurs gate. One of the best made triple A's in quite a long time. If any company deserves to be paid for their work, it's them.",
      "I buy games that you know had heart put into them others I don't play. A lot of those are 60$ games and I unlike you seem to understand that piracy is a useful tool only when needed.",
      "I had a 2600x up until a month ago and boy did it get hot",
      "It should get more than 120 fps on high settings at 1440p on those games, as in that resolution games uses GPU more than CPU, still check for a benchmark video"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580"
    ],
    "title": "Dual B580 go brrrrr!",
    "selftext": "",
    "comments": [
      "Not with a dedicated hardware bridge like SLI/Crossfire (which are dead as noone wants to implement a vendor-locked solution), but PCIe 4.0 x8 is plenty fast for multi-GPU data transfer, and cross-vendor compatible. My [FluidX3D software](https://github.com/ProjectPhysX/FluidX3D) can do that (with OpenCL!): [pool the VRAM of the GPUs together, even cross-vendor](https://youtu.be/PscbxGVs52o), here using 12+12+12 GB of 2x B580 + 1x Titan Xp, for one large fluid simulation in 36GB VRAM.",
      "Malcom in the middle there will indeed be going BRRRRRRRR\n\nat least the top of the GTX will always be clean.",
      "I need them for work, so my employer sent some over 🖖😉\n\n\n(I wrote big parts of the GPU kernels for XeSS Frame Generation and Super Resolution)",
      "So that’s where my B580 LE went.",
      "I'm actually curious what are the 3 gpus used for? Do the arc cards support an sli/crossfire like solution?",
      "I was wondering \"who the fuck needs two B580\"\n\nI guess \"person who wrote big parts of the GPU kernels for XeSS\" is the most valid answer I was ever going to get! :D",
      "This is actually super awesome, I had no idea you can pool different gpus together like this, I'll have to look into it more",
      "Yeah, I'm worried about that poor B580 in the middle haha. Poor guy starving for air",
      "Yes that's me!\nNot just that cow aerodynamics video, but I wrote the entire CFD simulation software for that myself ;)",
      "you're the one who made Aerodynamics of a cow, nice!",
      "\\>(I wrote big parts of the GPU kernels for XeSS Frame Generation and Super Resolution)\n\nDoing gods work here OP.\n\nDo you have a spare one left? 👉👈 /s",
      "Yes it's me Moritz, small world indeed! 🖖",
      "In OpenCL you can use any OpenCL compliant device, CPUs too in the pool......",
      "Oh Mortiz, is it you? Intel did hire you to write the XeSS kernel?! Oh man, what a little world we live in :|",
      "Yep, for games the return-on-investment unfortunately wasn't there, so Nvidia killed it after Ampere generation.\n\n\nThere is a good side though: in the meantime, PCIe 4.0 and 5.0 have become so fast that the SLI/NVLink bridge today is entirely obsolete. And while SLI only worked between identical Nvidia GPUs, PCIe works with literally every GPU out here. Developers only have to implement multi-GPU once with PCIe and it can work everywhere. For games this is still not done anymore because ROI is still not there, but for simulation / HPC software it very much makes sense. I've demonstrated this some time ago by [\"SLI\"-ing together an Intel A770 + Nvidia Titna Xp](https://youtu.be/PscbxGVs52o), pooling their VRAM over PCIe with OpenCL. PCIe is the future!",
      "Intel Core i7-13700K, in an Asus Z790 ProArt mainboard, which is really cool as it supports bifurcation of the CPU's PCIe 5.0 x16 lanes to the first two slots, as x8/x8, so both B580 cards are getting the max supported PCIe bandwidth. The third slot is a 3.0 x4 over the chipset, still good enough for the Titan Xp.",
      "Definitely gonna starve of air!\nI tried to pack 2x A770 LE together and that didn’t work so well!",
      "Yeah I saw your GitHub, It's super impressive and cool that you wrote it by yourself :)",
      "Bandwidth is very similar, but the 3060 Ti is only 8GB capacity. FluidX3D in that case can pair 8+8GB, or at some slowdown with several domains per GPU (4+4+4)+(4+4)GB. Not a perfect match but it will work.",
      "where did you find them???"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580",
      "arc b580"
    ],
    "title": "Intel finally notches a GPU win, confirms Arc B580 is selling out after stellar reviews",
    "selftext": "",
    "comments": [
      "intel's being successful is what we absolutely need in this duopoly market. It'll benefit everyone. Great news!",
      "Quote from the article:\n\n>“Demand for Arc B580 graphics cards is high and many retailers have sold through their initial inventory. We expect weekly inventory replenishments of the Intel Arc B580 Limited Edition graphics card and are working with partners to ensure a steady availability of choices in the market,” Intel spokesperson Mark Anthony Ramirez tells *The Verge.*",
      "The duopoly is dead, long live the triopoly!",
      "So much for a PaPeR lAuNcH",
      "MLID said it's fakepaper lauch of fakepaper card. 🤡",
      "MLID can retire now 🤣",
      "lmao, still better tho.",
      "That guy is still around??\n\nI watched him years ago, not for long though. He has always been salty, and he was always wrong with his predictions/leaks... \n\nCan't believe people still watch that troll",
      "By the desperation we see here in the sub 100x daily in the last couple months one would think people are buying these to run their heart and lung machines rather than use it for rendering graphics or do GPGPU things lol...",
      "Well AMD has consoles & handhelds going for them. So still a duopoly, IMO.",
      "I managed to get one on launch day, so it definitely wasn't a paper launch, it's just in high demand, higher than expected.",
      "barely even a duopoly",
      "Great news",
      "Hopefully this encourages them to move forward with at least 1 higher end Battlemage card like a B770.  I really wish they would release a high VRAM card too for home AI like a 32 or 64 GB B580 as it does not have to be the fastest to be truly useful and they could keep the price down.  A 32/64GB B770 would be great too.",
      "Pat saved Intel",
      "Grandma guy must be happy",
      "He is still saying that the B580 for $250 is overpriced by $50 but never replies to comments proving it is a completely different situation in the EU.",
      "Reference to Moore Law Is Dead. Guy have a hate boner for intel.",
      "Fantastic news. I bought an A750 LE back in the day as I was sure Intel were done with their 'GPU experiment' and I wanted to own a piece of history on my shelf. I can wholeheartedly say I'm glad I was wrong.\n\nI cracked it open a day after they announced the B580 and it's being used in my brother's PC.",
      "MLiD is just the type person who feels it mandatory to he is always right so he just makes up things that can’t be proven or disproven. Don’t get me wrong he has some good source but he doesn’t like to wrong and do anything to “prove” he isn’t."
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580"
    ],
    "title": "Got a B580. now what?",
    "selftext": "Got FOMO and was able to snag a b580 at msrp from Newegg. Didn’t do any research, just pulled the trigger once stock came in. My current setup is a b450f, ryzen 5 3600x and Radeon RX 5600xt.\n\nMy concern is the bottleneck issues I’ve seen with low end CPU’s. My motherboard also lacks pci 4.0 support. Anyone out there running this combo that can give any advice?\n\nWhat would you do? Do I take my chances and install the card? Order a new motherboard and cpu? Sell the card and get something different? The box is still sealed.\n",
    "comments": [
      "I've read somewhere that the pcie is no issues since this card is 4.0x8, and backward compatible. You need to see if your mobo supports reBar, also make sure you're on latest bios. It should be fine !\n\n Use DDU uninstaller to remove all previous vga driver ( do it in safe mode : windows key + r  , msconfig +enter I think it's the second tab there's a section at the bottom for safe mode, tik the square , it'll ask to reboot but don't chose the save options. \n\nRun ddu in safe mode and redo msconfig but untik safemode and let it reboot. Install whql drivers from intel. Once it's all done, download superposition bench, download ARC OC tools ( see vids on how to use, very easy) Start with 190w power, 0.90v.core and set the clock at it's max rate. Enjoy 👍\n\n Edit: When you set the settings in arc tools for C.clock and v.core , you have to input all the settings you change and then press save. If you do it one by one it won't work",
      "download dlss swapper so you can upgrade games that support XESS 1.3 to XESS 2.0. I'm not promoting, just think it's a handy tool/app since only few games have XESS 2.0 support",
      "Add cooking oil to a pan and set to a medium heat for, place the gpu in the pan and season with salt, pepper, and paprika, add an egg for extra flavour if you like. \n\nKeep on the stove for about ten minutes turning over the gpu ever couple of minutes to make sure both sides are evenly cooked.",
      "Maybe you should put in your computer.",
      "It was a great upgrade on my 9900kf, just keep in mind it is a PITA to get it running correctly, for my Motherboard not only did I need above 4g decoding + Rebar enabled, but I also needed CSM disabled to get it to work(required changing my windows install). But once working, it was a big step up over my old Radeon R9 290. Not as plug and play as a nvidia or radeon upgrade would have been but still worth it.",
      "you have to build it into your rig and install the driver...thank me later",
      "brother, its not that serious.",
      "This",
      "This guy is a downer. You'll probably see some small fps loss with a 3600x but people really blow the issue out of proportion and should be a big improvement over your 5600xt. If you want to remove the bottleneck get a cheap used 5600. If you want to stretch out your build as long as you can get a 5800x3d and you should be good for at least a few years before upgrading is worth it.",
      "Now u play some of the most advanced games just like me im talkin about pso, m&m 1 to 9, everquest p99 and many more",
      "That CPU is officially shit now.  I had it and it can’t even handle a 5700XT.  Your motherboard is fine I wouldn’t worry about the PCIe 4x the bottleneck there is small.  \n\nI’d upgrade to a 5700X3D or 5800X3D.",
      "Nope wouldn't reccomend, xess 2 the upscaling itself is the same. Wait until it gets added to optiscaler so that it'll have xell and xefg",
      "I'm running a 3600x with a 6700xt with no problems and great performances on Arch. And all metrics show a well balanced system (where GPU util is a fixed 99% on taxing games and CPU stays around 70/80).  \nCPU issues starts to show at 150+ fps, a target that I don't need to get over.  \nOnce I'll upgrade my main monitor to an oled that will be the right moment to upgrade the CPU, but calling it officially shit is absurd, is still a fantastic value for a 2k gaming rig (so much that I've build a living room machine with the same chip, for basically nothing).",
      "because yolo. its a cheap card at msrp.",
      "Just did a windows reinstall to be able to turn on rebar. It’s night and day once you turn it on. It’s like it’s a different card.\nEvery thing runsruns amazing so far",
      "The only thing im expecting, is better than I currently have. If the card doesnt work out its an easy sell.",
      "Try the new card out (after a driver uninstall).   If it's faster, then it's faster. Enjoy it and schedule a CPU/MB upgrade when you can afford it.    A faster CPU will almost certainly give you a bump.    If it's heavily bottlenecked, then return it and go for an AMD RX 7600.   Less VRAM but not sensitive to CPUs and readily available.",
      "You'll see **some** CPU overhead, but IMO that may not be a dealbreaker because it'll still outperform your Radeon RX 5600XT in your current config.\n\nIf you have the money, consider upgrading to a more powerful AM4 CPU a few months down the line; prices have been trending down on the 5600, 5600X, and 5700X3D as they're *finally* approaching the sunset of AM4.",
      "Its a good card, might struggle with your CPU in some CPU intensive games.\n\nI mean, you have it already, might as well try it out and play some games you know? Unlike most redditors who don't even play games lmao.",
      "you put it into your pc."
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580",
      "arc b580"
    ],
    "title": "Intel finally notches a GPU win, confirms Arc B580 is selling out after stellar reviews / Intel says it expects to restock the GPU “weekly”.",
    "selftext": "",
    "comments": [
      "A bit surprised they will be restocking so quickly. Bodes well for the future of their gpu platform.",
      "Even if their gpu have quite low margin it helps their overall business in several ways:\n1. Consumer mindshare and brand recognition \n2. More TSMC volume which means they can negotiate better pricing which helps the margins for other products where they use TSMC \n3. Increased gpu users will enable better software as they will need to support more users across more platforms. Better software is a plus going forward",
      "They have a few months before the new gen arrives. They need to sell as many as possible in that time where they are the absolute best value in the market.",
      "It's in AMDs best interest to quash any would be competition before it takes off the ground.\n\nIf Intel gets a hold in the low end AMD will be squeezed from both sides.",
      "The value at the low end hasn’t changed in years, why do people think that it will now all of a sudden.",
      "AMD has shown to be incapable of being a real competitor to Nvidia for over a decade, they deserve to get squeezed out of the market at this point. Ideally it would be nice to have 3 companies competing, but I'd prefer Intel over AMD at this point.",
      "Between a 4060 and 4060ti",
      "Absolute love to see the intel arc W. Much needed in the community. Here is hoping they do what AMD never could, and in doing so light a fire as hot as the 14900k running modded minecraft right below Lisa Suu's seat.",
      "Weekly? Amazon US didn’t even have stock in the first week. 😛",
      "Somebody tell this guy that there are other things that matter than immediate term profit",
      "The progress made by Intel in DGPU's is astonishing\n\nNot only did intel write an excellent driver stack that rivals the Nvidia/AMD, they also implemented AI Upscaling and AI framegen, with RT performance that rivals Ada Lovelace. even in heavily ray traced titles (where RDNA2 and RDNA3 completely fall apart)\n\nIf Intel can do all of this as a new player in the DGPU space, then why can't AMD do it?",
      "It sometimes outperforms the 4060ti, especially if overclocked and running at a higher resolution.",
      "I doubt Intel have any mindshare when it comes to GPU’s. Intel’s iGPU have always been regarded as crap.",
      "🤡 for you. No one expected it to be better than 4070 at 250 dollars 🤡🤡🤡",
      "Actually this is a lesson for their CPU division also, for the most, except those performing absolutely in the useless category, there's no bad product, only bad pricing",
      "How do you know that they're losing silicon on every unit sold?",
      "AMD is constantly busy with fumbling almost every GPU release in the last 10 years, I don't expect that to change anytime soon. Apart from Polaris and RDNA2 every other generation ranges from mediocre to trash. RDNA3 could have been a hit, even with its flaws, if the pricing was right, but they chose to slightly undercut an untouchable Nvidia and call it a day. Meanwhile Intel somehow managed to get the ball rolling in less than half a decade and with their super aggressive pricing they are slowly stealing market share from AMD. RDNA4 needs to be a huge success in the budget segment if they don't want to eventually go out of business. They can't compete in the high end anyway.",
      "what competition. AMD isn't even trying at low end. 7600 was a complete no show and uncompetitive. I don't see why they would care suddenly because Intel is selling a card. The equation hasn't changed for them. There is no profit in this sector so they just put the bare minimum out. AMD is maybe more interested in mid range like $500-600 cards when Intel is not even getting close to that price point.\n\nif B770 somehow magically can compete with $500 cards at $350 then yeah they should be worried but it won't.",
      "Intel GPUs are becoming popular. 4% market share, up from 1%.",
      "Very few reviewer actually talked about overclocking, unlike most modern GPU, Intel B580 actually overclocks pretty well and actually see performance uplift, doesn't even require lifting power limit in most case, just voltage. \n\nMy guess is Intel played safe and tuned GPU boost behavior more conservatively, which is fair."
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580",
      "arc b580"
    ],
    "title": "The Intel Arc B580 is Better Than You Think",
    "selftext": "**TL;DR:** Intel's Arc B580 at $250 is killing it... near RTX 4060 performance, slick as hell, and the driver horror stories are ancient history.\n\nI'm writing this for all you folks who've been stuck in GPU purgatory like I was - endlessly scrolling reviews, watching conflicting YouTube hot takes, creating and deleting Best Buy carts. I get it. The Intel fear is real. But after a month with the B580, I needed to share this with fellow overthinking budget gamers.\n\nIn a market where everyone acts like you need a 4090 just to play Minecraft, I took a chance on the underdog - and damn, I'm impressed.\n\n**The Hunt:** We're living in weird GPU times, right? Even grabbing a brand new 4060 is a headache. The 5060 TI reviews are lukewarm at best, the 5060 rumors are a snooze fest, and prices still make zero sense. I was hunting for a used card, but most were either highway robbery or smelled like they'd been mining crypto in a chain-smoker's basement for three years (*seriously, that smell haunts me*).\n\n**The Score:** So I set my eye on an **Intel Arc B580 for $250 at Best Buy** and thought: screw it, why not? Set up a Raspberry Pi to ping Best Buy every 30s, and one day my phone blew up - MSRP alert! Got lucky and snagged one before the bots.\n\n**My Setup:** Nothing fancy - **Ryzen 9700X, 32GB DDR5, Gigabyte AM5 board** \\--- just a typical Micro Center bundle I impulse-bought during a Chicago trip. I'm not some hardcore FPS sweat lord... more of a weekend pilot. *Microsoft Flight Simulator*, Forza Horizon, *Cities: Skylines II*, or whatever shooter my friends are into that month.\n\n**The Unboxing:** Holy crap, this unboxing experience blindsided me. Got the official Intel model, and it's gorgeous. The GPU was wrapped in what I can only describe as a sacred cloth like some tech priest blessing. As you open the box, the GPU rises up on a platform like some holy artifact --- Intel just out-Apple'd Apple on the unboxing experience. The B580 feels SOLID in your hands. No cheap plastic vibes. Holding it felt like Intel engineers actually gave a damn.\n\n**Installation & Drivers (The Part Everyone Warned Me About):** Very easy. Had it in and powered up within minutes. First boot? Yeah, straight back to Windows 95 era in pixelated 640x480 glory, which I expected --- quick trip to Intel's driver site, one install, reboot, and boom --- all fixed, rock solid.\n\nRemember all those horror stories about Arc drivers? Appear to be ancient history. The software now looks clean as hell. Modern, zero bloat --- and it just works. It even shows you whether Resizable BAR is enabled right up front. No digging through BIOS menus wondering if you screwed something up.\n\n**Performance:** Flight Simulator runs smooth as butter on my ultra-wide monitor, which honestly shocked me. Same for Forza Horizon - crisp, fluid gameplay that I wasn't expecting from a $250 card. I did notice a small glitch where some random driver in Forza appeared to be missing... pants! I know because I played quite a bit on Xbox and he definitely had PANTS there! But hey, I'll take one pantsless AI character over stuttering gameplay any day.\n\n**The Verdict:** If you're still on the fence, hop off. Arc is better than you think. And honestly, it's kinda refreshing to root for the underdog for once. \n\n**If Pat Gelsinger were around, I'd give him a hug. Let's just hope Lip-Bu Tan doesn't mess it all up. Lip-Bu Tan, keep the Arc team alive!**",
    "comments": [
      "Only issue is it’s getting harder and harder to find one at MSRP.. I’ve seen some go for $300 - $400 which isn’t worth it in my opinion. \n\nIt’s a great value card if you can snag one for $250 tho. I was lucky enough to get the Limited Edition at MSRP so I can’t complain. \n\nIt definitely has its issues tho, some games just straight up don’t work for me. Just this weekend I came across two games in my library that wouldn’t work. Detroit: Become Human gives you a warning that it doesn’t support the GPU and crashes on start up, and Batman Arkham Knight crashed on me every start up too, which is weird because every other Arkham game worked completely fine.",
      "Nothing fancy - 9700x 💀\n\nI mean yeah B580 is great if you have the CPU power to drive it and only use Windows.",
      "You're absolutely right about holding out for MSRP. At $250, the B580 is genuinely great value, but at $300-400 you're better off looking at other options.",
      "where are you finding it for $250?",
      "I'm doing well on Ubuntu 25.04, proton works fine and for one month I had no troubles.",
      "LOL you caught me there! You're absolutely right - calling a 9700X \"nothing fancy\" is like saying my Ferrari is \"just a car.\" 😂\n\nFair point about the CPU dependency too. The B580 definitely benefits from having solid CPU horsepower behind it, and yeah, Linux users appear to be out of luck for now, but did not try.\n\nStill stands though - for Windows gamers with decent systems, this card punches way above its $250 price tag. Just don't expect miracles if you're pairing it with a 10-year-old CPU or trying to run it on Linux!​​​​​​​​​​​​​​​​",
      "I'd say $300 is top-dollar for one.  If I needed a GPU I'd pay that for one but with gritted teeth.  I picked up my LE in a package deal but it ended up being a little lower than MSRP thanks to the package deal (basically GPU plus PSU at retail with $5 off so not a significant discount).  \n  \nWhen I got mine there was no news about VR compatibility so I picked one up just to play around with it.  VR is a no-go, sadly, but the overall gaming performance is incredible for what you pay and the people whining about CPU overhead need to quit riding the shaft of that Australian YouTuber--if you're pairing it with a CPU that predates resizable BAR (despite becoming an option in later BIOS revisions) you're going to have a bad time.  If you've upgraded in the last \\~5 years you're fine.",
      "Best Buy. Had to setup a price watcher to ping their website every 30 seconds.",
      "It works fine on linux as well.",
      "Google tells me that a $130 CPU is more than adequate for the B580. Certainly, my 5700X is absolutely fine, and it’s a three year old mid-tier part.",
      "I would agree with this if I could find a B580 at $250.",
      "I have a 5 7600 with the b580\n \nNo issues. From Denmark so had to pay more for it. \n\nWhat i got was the steel legends b580 and was the exact price or a 4060. \n\nMostly great experience.\nDont wanna say everything is a gpu issues but had very few issues here they are\n\nTarkov.\nTarkov on everything high expect texture quality on medium i have around 160 fps.\n\nWhen texture quality high i have about 20.  But if that is a driver issue game issue or something else i dont know.\n\nStar Citizen. (I know very unreliable game).\n\nHigh settings with some stuff set Medium.\nHad around 55 lows maybe 30 fps in new babbage.\nEverywhere else including lorrewille i had 60+ fps at all times \n\nIndiana Jones.\nStopped playing at the snake lvl cause im a pussy.\n\nHad great fps with settings high+ with xess.\nOnly problem i had was pop ins here and there.\n\nGood card i love it.",
      "Driver stories are not ancient history. Of course modern games would have Arc GPUs in mind. But still some legendary games like Forza Horizon 3 cannot even be played.",
      "There is a workaround for the Arkham Knight device detection issue, via .dll swap. This worked with my A770.\n\n\nhttps://www.intel.com/content/www/us/en/support/articles/000094465/graphics.html\n\n\nI don't own Detroit:BH, but the game has always been a compatibility mess on Arc. There were driver revisions that did finally allow it to run (albeit not well), but I'm pretty sure those branches also predate Battlemage.",
      "Arkham Knight has a built-in killswitch of the sort: \"if detecting Intel GPU, kill process\". It's the game's fault, not the driver.\n\nUse DXVK to bypass it.",
      "As long as you don't have an ancient CPU you'll be fine, 5\\*\\*\\* AMD does perfectly fine and 10th generation Intel will be enough with 12th generation giving a boost particularly in percentile lows.",
      "Thanks “person who contributes nothing of value”",
      "It’s absolutely fine with my 5700X, a three year old mid-tier CPU that costs US$130. I know money is tight for most people these days, but that’s half the cost of the GPU.",
      "Every Game Pass PC title I've tried runs flawlessly on the B580. Most mainstream games perform great.",
      "VR is something a lot of people have complained about around Arc, but I just recently set up PCVR for my younger sibling with their Quest 2 and A750. \n\nall it required was installing ALVR, making sure the OpenXR runtime was set correctly (it is by default iirc) and then SteamVR will run fine.\n\nfrom what I understand, ALVR is what bridges the connection between your GPU and headset streaming platform. It handles all of the encoding and transformation work to get SteamVR or whatever the Oculus software is to have your GPU not blackscreen. \n\nOnce ALVR is running, it'll launch SteamVR automatically, and you can open up a game to play. It just runs. ALVR also gives you full control over encoding settings and framerate and a bunch of other nice little things to get your video stream looking correct.\n\nI swear i haven't been sponsored lmao ALVR is open source anyways\n\njust try it, VR on arc is thriving 🙏🙏"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580"
    ],
    "title": "My GTX 1080 shorted itself after 8 years, so I upgraded to B580.",
    "selftext": "",
    "comments": [
      "Got the card for around $270. I was playing Dota 2 when my PC suddenly shut down. I initially thought it was the PSU or motherboard, but it turned out to be the GPU. The motherboard and PSU were preventing power draw as a safety measure. When I tested my GTX 1080 in another computer, it exploded—lol.\n\nI was considering getting a used 6700 XT (it was cheaper than the B580) and this card. But after some thought, I decided to go with the B580 because, even though my PSU is platinum-rated, its wattage is relatively low.\n\nAt first, I had an issue where my PC wouldn't boot and just showed three underscore symbols. I fixed it by converting my boot partition from MBR to GPT, disabling CSM in the BIOS, and enabling Resizable BAR.",
      "CPU: Ryzen 5700x  \nGPU: Asrock Challenger B580  \nMotherboard: Asrock b450 fatal1ty ITX  \nPSU: Corsair SFX SF450 Platinum  \nRAM: ADATA SPECTRIX D35G RGB DDR4 64GB (somehow very cheap)  \ncase: SGPC k49\n\nStorage: 1tb nvme ssd (Samsung 980 pro I think), 2tb SATA SSD.  \nMonitor: ViewSonic 1440p 170hz VX2780.",
      "Actually 47% faster (per TechPowerUp). Not to forget that GTX 1080 was a top tier card with $600 MSRP on release (probably around $1000 considering inflation).",
      "solid ass case, stickers included",
      "I’m gonna need you to tell me where you get those stickers",
      "Nice!! I like your build!! What are your specs and case??",
      "I'm glad it worked out for you. That looks like a nice small case that can actually fit a GPU.",
      "My 1080Ti also took a crap and is getting upgraded to a B580.",
      "I got mine for around $500 if I remember correctly. It was a Zotac 1080 Mini.",
      "I was rather thinking about AMD or NVIDIA cards from the 4000 series. I faced a similar choice and went with the RX 6700 XT, since I considered not only new games but also older ones.",
      "I have a 500w and it’s been fine for me. 3700X cpu",
      "Goated stickers my guy! Kessoku Bando ftw",
      "have you tried searching \"your game\" + b580 benchmark on youtube?",
      "You also get slightly more memory, plus RT and ML accelerated upscaling/frame generation.",
      "What is the name of that case",
      "Your case to gpu ratio is making me feel uneasy.",
      "Im new ITX lover cant wait to make my PC full intel with Arc celestial and nova lake S!",
      "SGPC K49",
      "Very nice, makes me want a sff pc",
      "No, you need an sfx psu. Couldn't find any <10L case that support an ATX PSU on the r/sffpc [master list](https://docs.google.com/spreadsheets/d/1AddRvGWJ_f4B6UC7_IftDiVudVc8CJ8sxLUqlxVsCz4/edit?gid=0#gid=0)."
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580"
    ],
    "title": "Returned 4060ti and got a B580 ",
    "selftext": "Recently built a new PC in December with a Ryzen 5 7500 on AM5 and 4060ti. Couldn’t find a decent deal used so I got a new 4060ti at Walmart around $455 after taxes. Way more than I wanted to spend. \n\nWas bummed B580 was sold out everywhere but set Newegg to notify on restock and snagged an ASRock B580 Steel Legend last week. Went with someone else’s “over clock” settings and performance is close enough to my 4060ti. Super happy with the $159 I saved after taxes - even if I might’ve left a little performance on the table. I’ll pass this card to my son if Nvidia or AMD come out with anything more compelling this year. \n\nStill working on cable management and the trinkets in the case will probably replaced. ",
    "comments": [
      "Get a GPU stand / bracket.\n\nIt's sagging already.\n\nMay cause further issues later",
      "From a 4060ti to a b580 its a bit of a sidegrade, but considering the amount of bucks saved there while getting more vram and the same performance, its absolutely worth it.",
      "Ordered lol. This GPU was much bigger than the 4060ti it replaced. I wasn’t expecting it.",
      "Also an option, you seem to enjoy figurines, find one the perfect size and use their head or shoulder!",
      "In 1080p*\n\nDont forget that monitor/display are least upgraded part, at leat least focused on upgrades so you might find people getting display in excess of current setup but ready for future. 1440p 4060 just dies",
      "you'll likely end up with better performance LOL",
      "Loving the aesthetic! :)",
      "I'm kinda surprised, the sparkle came with it's own sag stand didn't realize the steel legend and other 3 fans might not.",
      "That’s fine. I don’t really care to split hairs about the loss in performance when it’s mostly imperceptible without an FPS counter. I’d rather save the money and have more vram for the future. Money isn’t an issue - I made $150k last year as a hospital RN in California…but I just like a good deal. It’s the remnants of my childhood being broke getting second hand clothes all the time lol.",
      "I have a Sama case, Sama CPU cooler and the modular PSU from Newegg. No problems so far. PSU is almost inaudible even while gaming. I love the case I got with the wooden slats and it included  4 ARGB fans too.",
      "It's actually a downgrade unless you have a high end CPU to pair with it, b580 has less performance than 4060 if they both are paired with 7600",
      "congrats, ultimately dispute maybe raw performance what matter is longevity from a vram perspective and ability to use new gen features like ray tracing. In this case the rtx 4060 ti fails and the B 580 succeeds. (have both rtx 4060 ti is great for mining but terrible from a gaming stand point, the b580 has been nice in my system for daily use)",
      "I love the Lego McLaren F1 car. I have that exact set and might put it in my pc lol",
      "My man",
      "Great GPU choice, poor Team choice... Forza Ferrari.\n\nThis year is our year.",
      "Oooh nice, thats the same model that I’m waiting on",
      "Thank you 😌",
      "I have women trapped in my case 😆😆😝",
      "I had to buy two of them. When did you see a team with a single car 😅. Tiny enough to not be considered a man child, but big enough to satisfy that Lego itch",
      "The problem is you can't really know in a lot of cases. For me I identified it in retrospect, after unintentionally fixing it.\n\nHad a 3070ti for 2 years so far. Zotac Holo Amp. It's massive. It's been sagging from the start.\n\nI've been having more and more issues with sudden BSOD (like 2-3 times a fortnight) and screen randomly flashing black for a split second whilst i'm using it.\n\nI just happened to be buying a gpu bracket because i saw it on sale at a shopping website. Did not even realise these issues could be linked. I was actually thinking it was a PSU problem.\n\nSo when I installed it 2 months ago, the computer was left on. I mean, it's just pushing the gpu up in a non-conductive, non-moving area, what's the harm? But whoah, when I did that, the computer went into a BSOD. In other words, my just slightly touching / lifting the GPU actually triggered a BSOD.\n\nThat's one of the first clues leading to me realising that the sag had probably tugged the GPU out over time and pcie contact was getting iffy especially when the gpu fan was moving causing tiny vibrations.\n\nHaven't gotten a BSOD or black flash since the bracket was installed.\n\nThe massive weight and sideways strain probably wasn't doing the pcie socket any favours either.\n\nOne day when I muster enough energy I may reseat the GPU (since I never did that even after installing the GPU bracket), but that's the story of how I found out about GPU sag issues."
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580",
      "arc b580"
    ],
    "title": "PC World just gave ARC B580 «The Full Nerd award» for «Best GPU of 2024»",
    "selftext": "",
    "comments": [
      "I love that PC World shout out:\n\n>Worst trend: The enshittification of everything\n\nGoogle, Meta, Microsoft, Twitter/X - pretty much all of their software has become unusable garbage, where the only part that still works is the ads. As if they could pull off anything on their user's back. Their users will move to better alternatives, and soon they'll realize that they have no power at all.",
      "Although I don't fully agree with them, I do rejoice in the progress that the Battlemage has made.",
      "I said something similar to this to friends \n\n\"We haven’t seen a $250 graphics card this compelling this decade – you’d need to go all the way back to the halcyon days of the GTX 1060 and Radeon RX 480 to find a budget GPU with a value proposition this strong. In a weak year for graphics cards, Intel’s surprisingly great Arc B580 stands out.\"",
      "Battlemage is still not friendly to old systems and the majority of inexperienced PC users.\n\nTake a look at this subreddit. Since the B580 was launched two weeks ago, so many new adopters swarmed here asking for help mostly because they are using an older PC and take the B580 as an upgrade for their old GPU, but I really don't agree that Arc GPU is a good option as an upgrade for older systems.",
      "Just out of curiosity, what points do you disagree with them on? I'm getting my B580 on Saturday and I'm looking forward to it, I have seen a mix of opinions on here though.",
      "Man, this is how I feel about it. Like everything is just gargage. Windows 11, Android, Google search, Office 365, Youtube etc. They all had thier sweet spot before and pandemic but now two years after and it all feels a lot worse than fifteen years ago. How could they fall so hard?",
      "It is unfortunate because it’s probably scaring off a decent amount of people when they see others are having issues running it on older processors. Intel isn’t hiding anything though, it says the minimum CPU to run with it right on the box, it’s 10th gen Intel or newer, or ryzen 3rd gen or newer, with a MB that has rebar support.",
      "I consider the 4070 Super and the 7900 GRE both to be fairly incremental nothing special cards. They replace what came before them with at a slightly better performance and slightly higher price point. They might be decent cards, but there is absolutely nothing special about them. \n\nMeanwhile the B580 jumps into a budget category that has been unoccupied for years and makes a really good card. It is night and day better than its direct predecessor, especially comparing launch date to launch date. \n\nAnd it is made by the underdog 3rd party but still manages to threaten BOTH other brands. It threatens AMD's hold on the low-end, and it threatens Intel's hold on the not-just-gaming segment. \n\nIt isn't perfect in all ways, but it is absolutely the best in its weight class. It is absolutely the most improved. It is absolutely made the biggest mark on the industry as a whole. \n\nI don't think it is just the best card of 2024, I think it wins at least for the past 5 years, and might win card of the decade. (Although that last one has some tough contenders.) \n\nFor best card of the year? I think its Michael Phelps vs a high school swim team alternate. I don't see that there is any real competition, much less another card that takes the title.",
      "MLID can cry all he wants, but it doesn't change that this GPU is a great card and will see massive future gains when drivers are in a better state.",
      "Props to you for including office 365 in there.\n\nMicrosoft managing to turn a decade old software into a subscription based system without any major update to it, it’s baffling.",
      "Tom is busy organizing redacting plan atm, Can't really trust a guy who make living by profiting someone else breaking their NDA",
      "Which GPU do you think should have been given the award instead?",
      "Not really. It's just because there are still some disappointments with B580 for me as a 2-year Arc A770 user. I do agree that B580 has shown plausible improvement over the Alchemist (performance, efficiency, etc.,) but several issues that were much complained about the Alchemist remain unchanged on the B580.",
      "Now that clarified everything. Thanks for detailing. I also feel the same about point 3 and 4 of your list of disappointments.",
      "Awesome!",
      "i wouldn't, thats like a sidegrade, maybe wait out a little bit more and pray for b770, however ur money ur choice if u can find it well priced and you really want it then no one is gonna stop you",
      "Google has been steadily worse once they became a publicly traded company. It took a few years for it to start but that need for the line to keep going up ruined all their goodwill they developed over the years.",
      "yup, now they will gut the publisher. also used onedrive a lot but now it is more and more cluttered with nonsense.",
      ">I consider the 4070 Super and the 7900 GRE both to be fairly incremental nothing special cards. They replace what came before them with at a slightly better performance and slightly higher price point. They might be decent cards, but there is absolutely nothing special about them. \n\nAgreed, the 7900GRE had a little bit more headroom for overclocking than the rest of the 7000 series and the 4070S typically had a little bit of a better price to performance ratio than the rest of the 4000 series, but neither of those things are on the same level as what Intel did with the B580.",
      "YES! I Have one and am happy as can be. Definitely earned that title."
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580",
      "arc b580"
    ],
    "title": "SPARKLE Intel Arc B580 TITAN OC Unboxing",
    "selftext": "",
    "comments": [
      "BENCHMARKSSSSSSSS",
      "Congrats. You got featured on videocardz.  \n[https://videocardz.com/newz/first-intel-arc-b580-gpu-ships-to-gamers-before-launch](https://videocardz.com/newz/first-intel-arc-b580-gpu-ships-to-gamers-before-launch)",
      "I've always dreamed of a blue graphics card. Here you go, I picked up today.  At the same price, I could have bought an RTX 4060, but I decided to join the testers of new technologies. It's a 2.2 slot design so it will fit perfectly with the Lian Li DAN a3-Matx.",
      "They'd need to track down a copy of the .6239 or .6242 drivers, which isn't likely as the only folks who have those are bound by NDA. :c",
      "I ordered on Saturday, received today. I am also surprised.  I guess the drivers for the ARC B580 will be available from Friday, December 13?",
      "It's relatively cheap tho. And it has great cooling, so whatevs",
      "Guess they missed mine",
      "Can you benchmark it already? Dont get me wrong but we all want raw performance benchmarks",
      "why are they blueballing us?",
      "Drivers won't be released to the public until Friday",
      "How did you buy it before launch?",
      "Can you post benchmarks ?",
      "No he can’t because there is no driver available, also got mine from retail and can’t use it",
      "There's always Linux ... ;)",
      "What's idle power consumption?",
      "Yeah my sparkle A770 titan oc is a gorgeous card very well built",
      "Nothing wrong with overkill cooling. Just means the card will be cooler running, and longer lasting. Looks really cool too. Also, it makes me wonder about a hypothetical B770/B780. Perhaps AIBs are reusing coolers intended for those.",
      "Ok, hopefully someone else will.",
      "Could you send me the lottery numbers since you're already in the future?",
      "No drivers. It won't run well"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580",
      "arc b580"
    ],
    "title": "Intel announces Arc B580 at $249 and Arc B570 GPUs at $219 - VideoCardz.com",
    "selftext": "",
    "comments": [
      "It will probably also be around $500",
      "Intel is claiming that the Arc B580 is 10% faster than the GeForce RTX 4060 at 1440p\n\nhttps://cdn.mos.cms.futurecdn.net/9C9zEHTPExVJYkeAFNbvg-1200-80.jpg.webp\n\nhttps://cdn.mos.cms.futurecdn.net/42zbBqpa3CtyBcgp7fjw9n-1200-80.jpg.webp",
      "B580 for $250 sounds reasonable if the rumored spec is true, B570 feels like only exist to upsell B580.",
      "i think it is just there for the part defectly B580 chips.\n\nSo u can still sell those chips.",
      "And the scariest thing it will probably only have 8GB of VRAM",
      "I think that the pricepoint and VRAM amounts are definitely interesting.\n\nThe A580 looks really promising whereas the A570 may be a deal once it hits a sub 200 dollars street price.\n\nLet's see how the perform, Xe2 has been working great on Lunar Lake.",
      "4060 was also barely any faster than the 3060 and straight up tanks if vram limited (which can happen even at 1080p whereas a higher vram card can allow maxed out texture)",
      "did you write A when you wanted to write B?",
      "This looks like a pretty good deal and a real 3rd option to budget PC builders/buyers out there.\n\n12g of vram on a $250 card is pretty good.\n\nWaiting to see real benchmarks. I hope one day Arc GPUs can match 70-80 series nvidia performance, I might really consider going a full intel build for fun.",
      "It is also rumored to have 8GB of VRAM which will most definitely kill performance in modern titles",
      "This would have been impressive 2 years ago.",
      "No, Intel is claiming that it is 24% faster than the Arc 750 and 10% faster than the GeForce RTX 4060 at 1440p\n\nhttps://cdn.mos.cms.futurecdn.net/JasQiFARCQoPfWhn25V2Sm-1200-80.jpg.webp\n\nhttps://cdn.mos.cms.futurecdn.net/42zbBqpa3CtyBcgp7fjw9n-1200-80.jpg.webp",
      "Intel is claiming 10%, not 32%\n\nhttps://cdn.mos.cms.futurecdn.net/42zbBqpa3CtyBcgp7fjw9n-1200-80.jpg.webp",
      "Yes, the B570 might look weird at first blush, but it's there because of binning. And I suspect, like others, it'll drop in price to make the segmentation make more sense on the street.\n\nI only wish the power was down more.",
      "The 7900XT to B580's 7900XTX",
      "Its like 15% faster than the 7600 tho",
      "\"rumours\" this early are usually a bunch of nonsense and guesses. RTX 5060 won't launch until much later into 2025, the xx60 card has historically been released quite a few months after the top end xx90 and xx80 cards. So it's not gonna be relevant until maybe Q3 '25. Until then, B580 is exciting for the lower end market. It's cheaper, faster on average, more VRAM, better than AMD on the software side.",
      "Not by that much though, cheapest I found was 249 for the RX 7600 which puts it in a similar spot but with 12GB VRAM.\nHonestly it mostly depends on driver / software support because at the end of the day there is no reason to go for the Intel card currently unless it's significantly more raw performance / price",
      "I like your funny words, magic man",
      "It fills a gap in the market, it’s fine now"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580"
    ],
    "title": "A770 at 109fps, but this B580.... ",
    "selftext": "",
    "comments": [
      "I'll wait for a 770 to come out. I have the A770 and see no reason to jump yet",
      "I really hope we get to see the B580 being sold at 250 - 260 euros here in Europe. Last price tag I saw was around 332 euros. I don’t understand that pricing.",
      "That 40% increase almost makes it worth it to upgrade from my A770.\n\nThe charts I've seen make it look like they are neck and neck in some games, and the B580 stomps it in others.  I just want to see how it does in Helldivers 2.",
      "It's not going to be 250-260 due to tax being around ~20% in most EU countries. But it shouldn't be more than 290€ after currency conversion + tax so I don't know why they're so expensive currently.",
      "Looks good but should have linked the whole video I’m sure it’s drivers needing optimization but it’s a bit all over the place still. I want to know if there’s going to be a B770. Also AMD has new mid level cards coming too . Nvidia is in lala land with their pricing still but those leather jackets won’t pay for themselves 😂",
      "I am also a 1080p gamer. It's all my monitor does and I am happy. I am waiting because I have an A770. If I had something worse, I might rethink that",
      "Same. Wait until B770 comes out. It'll rock the GPU market.",
      "3060 12gb can perform better in certain titles because it has higher memory bus bandwidth than the 4060, and also 4 more GB of VRAM.",
      "Only for the one generation. If Intel goes down AMD and Nvidia will just return to where they are now. Intel needs the chance to establish themselves. I want to see Nvidia especially take this seriously too late to be able to truly crush them.",
      "AMD especially considering how badly they are struggling to sell their cards compare to Nvidia but still greedy enough to price their cards so high.",
      "Depends on what games you play, but most likely no.  On average, the B580 will perform better but not to such a degree I think it's worth it, better off waiting for a B770 if one comes or for AMD's 8000 series.",
      "How tf is 3060 faster than 4060 this benchmark looks wierd",
      "Whoa that's exciting! I didn't expect that big of an increase over Arc but I'm happy to see it. Can't wait to see more benchmarks!",
      "I'm here hoping that Intel soon announces the B770 or maybe B990. If they did, then this will be a lot better! This is how they did with the Alchemist. They usually announces the lower tiers Alchemist, then announces A750 and A770 some months later. So that means, we have to play the waiting game.",
      "male sure to post feedback. We never see HD2 stats and for me right HD2 is the only one that matter",
      "Ahhhhh. I am tempted to trade in towards a new camera lens or a NAS.",
      "If the B770 came out, nobody would buy anything else.",
      "> the b580 is a slap in the face to Nvidia and AMD to wake them up\n\nMy biggest concern is if they \"wake up\" they will likely crush Intel.",
      "There will be no B970 / B990. Lucky if there is a 32XE B770. \n\nWe may get 64XE Celestial which should be epic seeing how much improvement Battlemage is.",
      "Just wait. Even if you end up CPU bound it's always nice to have a little extra life in your GPU. 8 gb is killing mine earlier than I'd like."
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580"
    ],
    "title": "Finally uprading my RX580 to a B580.",
    "selftext": "Got mine today, though I might install it on Saturday because I want a clean install on Windows unless I find a good guide on how to remove AMD drivers properly 😅",
    "comments": [
      "Your upgrade path intrigues me. Please don’t upgrade again until the PTX580.",
      "just use DDU to remove AMD drivers",
      "GTX580 next?",
      "I like how this upgrade actually makes sense",
      "Based Yuru Camp enjoyer👌🏼",
      "This worked, installing drivers now. Thank you!",
      "If you have an amd cpu, please reinstall the chipset drivers also. DDU removes them in addition. I think there might be a setting to keep them but I wouldn’t trust it.",
      "Nah bro, I heard the ZTX580 is supposed to rival the highest end 5090.",
      "7500f",
      "Tremendous upgrade!",
      "The 580 man!! Congratz with your upgrade!",
      "Hell yeah budget bros unite",
      "I used DDU and everything is working fine as of the moment. I'll try and test it more when I get free time.",
      "why is this thread getting down voted XD",
      "ZTT580?",
      "Just to give you an example. My games used to get fps issues when I had my browser open (hw acceleration). Prime video used to glitch every 10 minutes causing noise frames etc. I thought my GPU is dying. DDU is a great tool but it only cleans the driver, not the software that got used to them. And then there is windows update that can mess everything up after the fact.",
      "isn't 7500f essentially a 7600 with no iGPU? 7600 works great for the b580",
      "Hell yeah",
      "Thats sounds like an awesome upgrade ! Have fun !",
      "If you run into any issues please still consider a clean install. I had so many weird driver issues in the past from not doing that!"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580"
    ],
    "title": "Picked up an open box B580 Sparkle for $239. Should I keep it?",
    "selftext": "So I’ve been going back and forth. Ordered and cancelled a few prebuilts. I hate overpaying. \n\nI hopped in to Microcenter this morning and they had this B580 marked at $239. Open box but complete with full return policy and warranty. I’ve done a little research but I’m not sure if it makes sense for me. \n\nI grabbed the 7600x bundle for $279. So I’m around $500 for everything in the picture. \n\nI only play iRacing. Currently have a 970xt with a i5-6600. Assuming this would be a nice upgrade from that ancient set up?",
    "comments": [
      "You should sell it to me :) \n\n\n\nJk that’s insane definitely keep it! Amazing find and enjoy it 🫡",
      "Not unless you want to pay 3x as much for an equivalent card.",
      "I'M ORDERING YOU TO KEEP THAT GPU.\n\nYou are one of the few unicorns that managed to get a Arc B580 around MSRP, and on top of it, you got a platform that is a big upgrade to what you're currently using!  Did you find a crop of four leaf clovers each one the size of a California redwood by any chance?",
      "Probably, is it worth spending that much more money?",
      "Keep it, nice buy",
      "It'll be a massive upgrade over your 970. It's on par with a 4060 I think",
      "That’s what I’m realizing. I love bang for the buck products. I feel like with the insanity going on right now a 5070 is the only other card available at MSRP. Would a 5070 get me almost double the FPS?",
      "10% more than a 4060 on average",
      "I would pick up a second stick of that ram. It’s 40 extra dollars and you will be leaving a bunch of performance o on the table if you are only running single channel.",
      "I would go with the 9070 XT and get the 5080 performance for cheaper. But the B580 is a good card.",
      "That’s fair but he already has the b580 and for less than msrsp",
      "What kind of question is that? 🫠 I’d keep it.",
      "Definitely keep it. I would at least I got my sparkle titan for msrp and after tax it was 300 soyou got the best deal I've seen yet. Forewarning, someone probably returned because of the coil whine. It's kind of a lottery with the coil whine. Put heavy load on it and coil whine will eventually go away. Or maybe they just opened the box and never installed it and you have a brand new gpu for a great deal either way you can always return it if something is wrong with it.",
      "💀💀I got mine for 328$ after tax",
      "An almost complete 1440p system for 500,-, should you keep it? Ofcourse! You did good 👍",
      "UNDER msrp in fact. I'd keep that thing for the next 10 years probably",
      "Thanks. I’ll pick one up",
      "Likely, but goodluck finding an msrp offer. What games are you planning on playing? B580 should be fine for almost all, not super great at 4k, but it’s strong point is 1440p",
      "I just got the same bundle and did the same. Especially considering am5 is getting at least 1 more generation it seems to be a good opportunity to build now and get a future upgrade.",
      "GTA v,fall guys,pal world, fortnite, cyberpunk, dark souls, terra tech and some other stuff. Honestly I love the card before I was rendering every game at like 720p or lower on the lowest settings because I was playing on integrated graphics with a 5700g."
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580"
    ],
    "title": "B580 everywhere?",
    "selftext": "Its just me or all I see is people buying b580 like crazy.",
    "comments": [
      "It's not just you, it's the top pick for budget builds for a reason~",
      "This and 9070xt",
      "This sounds like someone that hasn’t actually ever used a B580.",
      "I wish I actually see more of the intel limited variants, since the current line up of B580 mostly look so ass.",
      "Yeston ples make waifu edition card!",
      "If you can afford a 9070 I don't see why you wouldn't",
      "Right now, at least where i live, is now down to almost msrp and with enough stocks, that's why i'am quite tempted to cancel my preorder of the 9070xt even if it is at msrp",
      "Because i'am coming from a 9 years old gpu, anything would be a massive upgrade, and with the same money i could jump from am4 to am5 platform, or get a better monitor and so on",
      "Must be an application specific thing. Lots of professional programs have no support for it because nvidia and AMD still heavily dominate the professional side while intel is damn near making up a double digit percentage of the new gaming card market.",
      "Y'all are both being toxic tbh",
      "I just got my 2 days ago",
      "I sold it to get a RX 9070",
      "Yes, it even outperform A770 in most scenario",
      "I find this funny because on the AMD subreddit you see Nvidia people saying the same thing and also getting downvoted. I'm sure there are still plenty of little things because Intel is so new to the GPU game but is definitely mostly stable. The amount of user error being deemed \"bad products\" is hilarious.",
      "I had one didn't like it, wasn't compatible with some programs that I frequently use so I sent it back and got the 7600 XT instead 🤷‍♂️",
      "I got my 7600 XT for $229 on Amazon about a week ago. Had it on a tracker to notify me when it went on sale.",
      "Naah I definitely prefer it the way it is",
      "Boy I'm glad that I've sent the GPU back now. The Intel fan base sure seems toxic. I told you what my personal issues with the card were and that's that my guy. 😁",
      "Well, basically it is the decently priced alternative for mid-level performance and for secondary computers, just have had mine for couple months and for good time since my 3080 build decided to be not working properly (still troubleshooting) but the B580is basically up to anything I need after I paired it with 11900KF.",
      "Yeah Nvidia got consumer gpu situation so bad it's actually acting as an advertisement for competitors. Apparently it works too good that there is no stock of either of them 😭😂"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580"
    ],
    "title": "Is it time to leave Team blue? Seriously considering selling B580.",
    "selftext": "Now that the hype has died down and I've had about 2 months with this card, I am debating selling it.\n\nLast night, after Warzone crashed AGAIN, I'm at wits end with PC gaming. Every single time I play Warzone it crashes at least once mid game. When I look for solutions, the first thing everyone says is \"Sounds like it's your GPU since most games are optimized with Nvidia in mind\". I have frame rate issues with Delta Force. Other games seem to work well. \n\nI'm running 1440p High settings for both. I just don't like these random issues. I listed my entire PC for sale and might go back to Xbox but maybe as a last resort, I try to go team green and see if these issues go away.",
    "comments": [
      "They're not \"optimized for Nvidia\". They run just fine on AMD. But usually they're indeed, not especially optimized for ARC, unless Intel puts in the work",
      "pretty sure warzone has been bad for a while with arc cards. like, the worst performing game.",
      "How much ?",
      "https://github.com/IGCIT/Intel-GPU-Community-Issue-Tracker-IGCIT/issues/1074\n\nhttps://github.com/IGCIT/Intel-GPU-Community-Issue-Tracker-IGCIT/issues/890\n\nBoth seems to be known issues.\n\nGreen team has tons of issue since Blackwell launch btw.\n\nhttps://www.tomshardware.com/pc-components/gpu-drivers/game-developers-urge-nvidia-rtx-30-and-40-series-owners-rollback-to-december-2024-driver-after-recent-rtx-50-centric-release-issues",
      "I agree with the sentiment but it should be able to run codslop if they want to play codslop. Slop as it is it's still a huge game that should be a priority to have not crash at least.",
      "Well if u were able to upgrade to a 5080 then the b580 wasn’t meant for u anyway",
      "Only warzone Crash ?",
      "Everybody listen up. It's his Ram 100% guaranteed! That game uses way more RAM than it should of course only during spikes but that can cause crashes and is the most common cause for crashes in that game also there's a good chance something is wrong with his Ram also leading to the game crashing. Get a new set of ram go for 32 gig at least 3200 MHz if not more. Report back in a few days I'm sure that will fix your problem. I know it's not your card because I have several arc cards in my house that play the game with no problem including the b580 in my PC. The only issue that was had was on one computer that had the same problem as you and it ended up being the ram.",
      "Ahh sorry the gpu only.. but not in the US. Good luck and It's sad for the troubles.",
      "Your going to try to sell a b580 build for $1300 used?",
      "I just left team blue because they didnt have anything higher announced coming out any time soon. I upgraded to a 5080 and loving it. But reason im replying is because with nvidia warzone has crashed on me like 5 times so far and its only been warzone. It just freezes and saying it encountered a directx error. Just mentioning it because im thinking the issue is warzone and not intel.",
      "Seems like a hassle ngl",
      "I mean is only warzone Crash or the pc",
      "Drop the specs, yes, it seems like consoles are a better fit for you.",
      "Are you telling us or convincing yourself of this? Your hardware does not magically gain value because of your “labor” putting it together, that’s so ridiculous it’s funny. Computer parts LOSE value once you open and use them, not the other way around.",
      "This comment shouldn't even be upvoted...like at all. OP should be able to play whatever they want. Especially a game that's optimized to run on ancient potato hardware.",
      ">They run just fine on AMD\n\nActually I believe warzone is having issues on AMD 7000 cards atm",
      "Yes. Basically I only have issues with two games I have played thus far: Warzone and Delta Force. I have other games I want to try soon such as Assesto Corsa and Ready or Not.",
      "Hear me out. Before selling the card, if you buy another gpu you can run dual gpu's in your computer using lossless scaling\n\nThe Intel gpu can be the secondary gpu being used for frame generation up to 4k 180fps (assuming base fps of 60)\n\nThe primary gpu only needs to be able to run games at 60fps.\n\nLook up dual gpu lossless scaling on YouTube, it looks incredible, and the input lag is half of what DLSS 4 has.\n\nYou can always sell the card off after if you don't like the idea of it or need the money\nBut if you're planning on getting another gpu anyways, I would highly highly recommend trying this.",
      "I just priced most of those items on pcpartpicker canada. USD has a 1.4-ish exchange rate. I’m getting about $1600 for all the parts. New. In Canadian dollars. Of course there are taxes on top of that. But since you are American, that works out to be about $1142USD. For new parts. I assumed no name 2TB SSD, the cheapest 750W Corsair psu I can find, and the pro version of that case.\n\nSo yeah, I think $1300USD is a bit high."
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580"
    ],
    "title": "Why is my B580 showing 7.84 GB VRAM with new update?",
    "selftext": "",
    "comments": [
      "sorry needed to borrow 4gb",
      "Just updated BIOS, rolled back to a previous Intel Graphics Driver twice.\n\n  \nStill 7.84 GB VRAM.\n\nAny suggestions on how to fix this?",
      "Uninstalled all drivers. And started from scratch, when installing drives, the screen flickers and then goes permanently dark every time. So i must hard reset the PC.\n\nStarting to lean on a hardware failure here..",
      "Be sure to edit your post with a fix if you find one\nYou'll never know who will end up coming to reddit in 9 years looking for that one guy that had the exact same issue as you\n\nYou are the one guy 😂\nIf it ended up being hardware, update it anyways",
      "It's been 8 hours, give it back!",
      "B580 automatically became a580",
      "Someone out there has the 16gb version now😂",
      "wth",
      "Sounds like you have a bad RAM module or memory controller. The black screen after the driver recognizes the card is a dead giveaway. Have you tried using any other display ports when the card lost signal? Try disabling rebar and shutting down and then powering back on and re-enable it? Could be some weird Battlemage glitch but honestly sounds like a hardware failure. Also, the drivers are known to flash the bios on the cards too. I had a A380 die from a driver update. Worked totally fine until my kiddo updated the drivers and it just stopped working.",
      "Actually might be good advice. If linux shows 8gb - it is faulty. It uses different driver mesa. Id rma it if linux showed 8gb.",
      "No that would just kill the card since data is stored across all ram chips equally",
      "So, after a clean install of windows 11, formated the drives.\n\nDDU in Safe mode, installed previous graphic drivers that i know worked also in Safe mode.\n\nThe GPU-Z / Taskmanager / Delta Force still shows 8 gb of ram.\n\nAlso, now Intel Graphics Software does not work at all?\n\nThis HAS TO BE a hardware problem and not a software problem. After 10 hours of trying to fix this im done. Will contact support tomorrow.",
      "Device manager if applicable disable onboard /cpu gpu ?\n\nMake sure features in bios are enabled\n\nhttps://www.reddit.com/r/IntelArc/s/rNbmL8oGyY",
      "Ddu , uninstall any remaining intel software, download latest drivers, reboot, again run as admin and install drivers, reboot after.\n\nI had to reinstall/ddu a few times to get some stuff working unrelated to yours\n\nCheck gpuz in case intel is just wrong",
      "The other 4gb ram left to buy milk",
      "Yes, i noticed it while playing Delta Force, that the fps was lower and the VRAM said 8 gb which started my investigation.\n\n  \nGPU-Z also states the memory to be 8192 MB....",
      "On board graphics is disabled and Rebar is on per BIOS",
      "Does GPU give any warning that the gpu is fake or anything like that? Maybe someone somehow managed to flash a A580 or something to B580, swapped cooler and sold as B580. A bit far fetched but who knows maybe. Can you share GPU-Z screenshot so I can compare with my steel legend.",
      "Try in Linux with live CD image and some terminal command.",
      "Just to be sure, did you use DDU and in the safe mode?"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580",
      "arc b580"
    ],
    "title": "Intel Arc B580 \"Battlemage\" Graphics Cards Review Roundup",
    "selftext": "",
    "comments": [
      "Nice, Intel really is in the game right now, hopefully B770 will come pretty soon. \n\nIf not, I seriously hope they don't axe the desktop cards right now, Celestial should be pretty amazing.",
      "It's a good start, priced competitively, looking forward to what B770 has to offer, we really need competition in this unfair market.",
      "Finally, a budget card at launch price that finally moves the price/performance goalposts.  We haven't seen this since the Polaris days 7+ years ago",
      "Let's see if they will bother actually competing, because their low-end GPUs were pretty meh for the last 5 years.",
      "The Arc B580 looks good compared to the GeForce RTX 4060 and the Radeon RX 7600, but those GPUs are 1.5 years old now.\n\nIf you need a new GPU ***right now*** and you only want to spend ~$250 to $300, the Arc B580 is a good option.\n\nWith new GPUs from NVIDIA and AMD coming very soon; however, if you don't need a new GPU right now, it's best to wait.",
      "Finally some good news. Intel really needed this.",
      "There are already known manifests from TSMC for G31 orders. It's obviously taped out and has been for quite some time.",
      "They respond to market pressure.\n\nIf they feel that Intel is a threat, they will respond.\n\nThat's how a market economy works.",
      "You're missing two very important puzzle pieces here:\n\n1. nVIDIA makes their money with AI and high-end cards such as the 4090, they don't give a damn about low-end anymore -> besides, people buy them anyway, look at the Steam Hardware Survey.\n2. AMD has EPYC, Ryzen and now MI series as well, cards like RX 7600 are extremely inefficient value wise for them (in terms of $/chip die), there's no big incentive for them to compete in the low-end as well - otherwise, the 7600 would have cost 229$ instead of 269$ day one.",
      "Did MLID and RGT tell you that? 🫢",
      "Can’t believe the RX 580 still costs $250!",
      ">Finally, a budget card at launch price that finally moves the price/performance goalposts. We haven't seen this since the Polaris days 7+ years ago\n\nThe Arc B580 is 15% faster than the Radeon RX 7600 launched 1.5 years earlier.\n\nFor comparison, the Radeon RX 7600 is 27% faster than the Radeon RX 6600 launched 1.5 years earlier.\n\nSource: Techpowerup",
      "Yeah so don’t buy it from scalpers",
      "7600XT launched at $320",
      "Good work intel....may your gpus sell well",
      "Bro why does that even matter your eyes can't even see above 30fps! /s",
      "Intel isn't making money at this price point. Intel is buying market share in order to mature their stuff. The reason I say AMD might not have a viable play is that, unlike Intel, AMD doesn't get anything good from fighting a price war.\n\nI don't know how good Navi44 is. Maybe I'm worried for AMD for no reason -- it is plausible they have a part that solves the problem without effort.",
      "I've read a couple of those reviews but they don't seem to mention XeSS 2 (including its frame generation and low latency options) much. \n\n\n\nIt seems Intel has yet to roll out a driver update for this:\n\nhttps://overclock3d.net/news/software/f1-24-has-become-the-first-game-to-support-intel-xess-2/\n\n>Sadly, Intel has not released driver updates for their hardware that enable support for XeSS 2. \n(...) \n>XeSS 2 will soon be available in more games, with Intel confirming that Dying Light 2, Robocop, Marvel’s Rivals, and others will soon receive updates.\n\nCould be a real game changer as I've always found XeSS better (loses less detail) than AMD FSR.",
      "I might check it out. Looking competative",
      "It is not like Nvidia is going to launch their mid-range to low end GPUs first, they will take nearly a year after the enthusiast launch before launching the low and mid range."
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580",
      "arc b580"
    ],
    "title": "So how is the Intel Arc B580 4 months after release?",
    "selftext": "I would love to hear from your guys' experience. Do you feel like the card is it's money worth? Do you enjoy it? Any issues you've had? Do you regret going for Intel? Are you glad you went for Intel? ",
    "comments": [
      "Bought one for my new build around 1 month ago.B580 was roughly half of the price 4060ti and 4060 is like 1.5 times. Havent had the chance to try it on many titles but  so far b580 did its job flawlesly for witcher 1,2 and 3, cyberpunk 2077, PoE 2 and DoTA 2. Not once i felt bad about my gpu.",
      "Sometimes I just feel bad that most games don't support Intel GPU or XeSS. One game I play is Wuthering Waves, this card doesn't have 120fps or ray tracing option enabled, but 4060 has it just because it's a NVDIA card. And I'm sure b580 can handle it, I want the 120 fps at least. \n\nIn MH Wilds, can't really use the framegen option with Intel XeSS. Not that I want to use framegen, but having the option would be nice. \n\nOther than that, it's fine. I can play the game I played well enough. The most demanding game I played is MH Wilds, but it's not just B580 that suffers from the game's bad optimisation.",
      "I think everyone on here will say it’s good and I’ll get downvoted for saying that",
      "It's great. No issues so far playing older games from GOG and modern AAA on Steam at 1440p. The most demanding is probably Black Myth Wukong. I can lock it at a smooth 60 fps at 1440p on medium with RT enabled. I have it paired with an i7 12700kf that I picked up on sale. I'd absolutely buy it again. No way was I willing to pay $800.00+ for a GPU or $300.00+ for a GPU with less than 10GB of vram in 2025.",
      "My 13 year Olds first build and I haven't had to do anything to help him. So flawless from a dad point of view.",
      "Nothing special ryzen 5 7600 non x no overhead so far. Also i play at 1080p so i imagine the performance would be even better for 1440p",
      "You can enable the 120fps and RT in WUWA in seconds with the DB editor. PS don't enable RT GI as performance sucks ass. With only RT reflections on high the gpu usage hovers around 60-75% in open grassy areas but in mostly reflective areas starts stuttering like crazy. Enable GI and Reflections tanks the performance like crazy and we dont have DLSS to help with the fps.\n\nIn general the B580 gets higher fps than the desktop 4060 but the 1 and 0.1% lows are worse than a laptop 4060. With custom engine.ini the stuttering is reduced but still not beating the 4060.",
      "The B580's performance in MH Wilds is honestly kind of impressive considering how hard it is to run on any card.  Wouldn't be surprised if my more powerful GPUs performed similarly in that game in particular.",
      "The overhead “disaster” is far from a disaster. The last few driver updates have mitigated the issue and it was overblown in the first place with a cherrypicked sample of extremely CPU-heavy AAA games like Spider-Man Remastered. Most games there is either no overhead or you wouldn’t notice the difference without a 4060 to compare numbers with side-by-side.\n\nYour 5600 will be fine.",
      "Hi there!\n\nI bought mine a month ago and I've been really enjoying it.\n\nI'm a casual gamer so the B580 is more than enough for 1080p gaming on highest settings.\n\nSome games have been having issues, especially the DX11 ones, but as long as they have DX12 or Vulkan variants then it's easily solvable.\n\nIt's not perfect, but I would say that it was a good purchase! Definitely recommend it for people who want to build a modern system on a budget, definitely worth the money!",
      "Nice",
      "NOBODY got banned for editing the ini and db files since launch. I have friends who uses custom skins and custom settings since the launch and not a single ban was given to them. Its only the ones that try injecting cheats bringing down the reputation if this.",
      "Fantastic card, very powerful, killer deal for $250",
      "What cpu you running? My plan was to put it in our old rig with a 5600 but after the overhead disaster idk now.",
      "I’ll be comparing my fps with my 5600x against nothing and jumping to conclusions and there’s nothing you can do to stop me!\n\nI’m getting my B580 today and can’t wait to wallow in misery due to 4fps missing that I have no way to verify.",
      "Got Sparkle Titan with 7600x3d, playing steam backlogs, very satisfied.",
      "I have found it's \"mostly\" fine, put one in my son's pc and not had major issues. There is texture flickering in hogwarts legacy and to a lesser extent in Spiderman. Other than that it's been good and for the price the performance is excellent.\nGames played: Spiderman, Spiderman MM, spiderman 2, hogwarts legacy, another crabs treasure, brotato, all orcs must die 3, Indiana Jones \nCPU:i5-12400f \nRam: DDR4 3200MHZ",
      "Not very satisfied with my B580. Obviously less progress on driver improvement than the Alchemist. Some major issues have not been resolved for quite some time already. Intel is struggling right now.",
      "I know it's possible to force 120fps and ray tracing, but I'm not gonna risk editing the game file or using third party app and getting my account banned, even if it's \"safe\"",
      "My brother bought one and paired it with the 5600 but his mobo didn't support rebar (it was a cheaper board) so he had very lackluster performance at first. Once he got a better mobo with rebar, it was night and day. He started getting great frametimes, especially for the price. He also says the encoder is great, and his clips/recordings were way better than his older RX 6600."
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580",
      "arc b580"
    ],
    "title": "Intel Arc B580 Overhead Issue! Upgraders Beware",
    "selftext": "",
    "comments": [
      "Already commented on the other more clickbait post. But just focusing on the actual 'issue' here with older CPUs, I agree that it should be maybe more clear but at the same time this test is with an R5 2600 when intel clearly says that the lowest platform supported is AMD Ryzen  5000 series or most AMD Ryzen 3000 series?\n\nWhy would I expect the B580 to work to the best of its abilities with a CPU that is lower that their minimum requirments?",
      "The reviewer confuses \"budget\" with \"old\". If you build a budget PC you won't build a 2000 or 3000 Ryzen System.\n\nWould've been much more interesting to pair with with a 14100F and such. The lowest end modern CPUs for a new build.",
      "I guess this is good info but I have to wonder who is expecting a ryzen 5 2600 to run 4K textures and ultra settings well? Like wouldn’t testing at medium - high range be better data?",
      "I see people downvoting posts bringing up this issue and I wonder why. You don't want to see Arc get better? Covering these issue is how Arc will improve, being defensive does not help.",
      "Because what is the actual issue here? I would say that if we see similar bad results with a CPU that intel say they support, i.e. > Ryzen 5000 series (and some 3000) or > 10th Gen Intel then sure, that would be a lot more interesting to take a look. \n\nBut a 2600? Not really - [https://www.intel.com/content/www/us/en/support/articles/000091128/graphics/intel-arc-dedicated-graphics-family.html](https://www.intel.com/content/www/us/en/support/articles/000091128/graphics/intel-arc-dedicated-graphics-family.html)",
      "Yea I was on a 1060 and on a 1600 ryzen, guess what? I upgraded my cpu as well, because I didn't expect an incredibly old cpu to be able to not create problems with a much newer gpu",
      "Especially when the 5500 or 5600 are roughly 100 euro and are a good upgrade to a 2600 anyway.",
      "There is no line, the moment you become CPU limited you will lose performance, if the game is super CPU heavy you may even lose performance with CPUs like 7800x3d.",
      "It’s a zen refresh aka zen 1 cpu. Almost 6.5 years old. \n\nMore importantly neither the 9600K or any sub 3000 series Zen CPUs meet min requirements. And likely for this exact reason. \n\nSo the lesson is don’t pair the GPU with anything less than the stated mins.",
      "The texture quality and most of effects are mainly done by GPU so ur example is a little silly. Why do you think higher resolutions hits GPU more",
      "No, it's clearly a driver overhead for the CPU. The problem has been known for a while. It's just that B580 is the first mainstream Intel GPU and now more eyes are on the Arc series. I got my A770 in April last year and already people here were saying: \"Get at least a Ryzen 5600\" which prompted me to switch my old 2700X with a 5700X. 2000 Ryzen CPUs are just too weak at this point, even with ReBAR enabled.",
      ">Why would I expect the B580 to work to the best of its abilities with a CPU that is lower that their minimum requirments?\n\nStop with the BS please. The B580 cannot work to the best of its abilities even with the 5700X3D and is giving me the 70% GPU utilization in many games at 1080p in the instance, where 6700XT/NV counterpart can go well over 95% utilization and way higher framerate.",
      "Because they also targeted those who are using older midrange GPUs like the GTX 1060, rx 480, GTX 1660, etc in their material.",
      "Speaking about real issues and allowing people to make a more informed purchase is \"trash the product\"? There was a bunch of posts daily on this sub from people claiming they have like 20-30fps in certain titles and no one knew what was the problem, while it was probably this. People bought a GPU that simply doesn't work for them and you call voicing any criticism towards that \"trashing the product\".",
      "Moreover, this is not just about 4--6 years old CPUs. This CPU overhead issue is seen with newer CPUs too, to some more or less degrees. The testing is ongoing, and I guess we will see multiple channels posting their findings soon. I don't see an issue if Intel is getting this feedback and fixes the issues.",
      "Hardware Canucks and Hardware Unboxed need to retest and compare a 9th gen Intel with a 10th gen and a 2000 with a 3000 Ryzen, 10th gen Intel/Ryzen 3000 are the minimum cpu's Intel and AMD both say officially support rebar/SAM. Without doing this one could say this is only proving these rebar workarounds (or backports if supplied by a bios update) for older systems are faulty somehow. Maybe Steve from Gamers Nexus could do this, he's much more thorough than these two.",
      "So.. where do you draw the line of budget cpu, that is capable of mitigating Arc's driver overhead issue.. Intel 10th gen? Am4 3000 series? Im interested to see a video like this if someone can publish it, how different gen CPUs scale with the b580\n\nImo anyone with anything older than the 2000 series or 9th gen should've just upgrade the whole platform or cpu already....if you are not gonna change, the b580 should not be considered \n\nSome games suffer, some games do not suffer that much. But it's a problem nonetheless, not blown out of proportion by reviewers \n\nI feel like this is a very hard blow to Arc as a whole, stellar reviews at launch, but then only to realize there are performance issues with old CPUs which this GPU is meant to target the budget sector. B570, is releasing in a few days or weeks which is even more damning for anyone looking for just a drop in upgrade\n\nI hope it's just another software flaw rather than another architecture one... Not looking forward to the b770 if it ever comes out",
      "It seems to also have problems with the 5000 series and 11th gen",
      "After all the rave reviews I see we’ve entered the trash the product phase.",
      "Very few \"reviewers\" took the time to thouroughly evaluate the cards. There was a wave of who can be the first to get clicks at any cost. Very few games and systems were actually benchmarked before everyone declared it \"big success\"."
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580"
    ],
    "title": "B580 disappointing performance",
    "selftext": "Update: Alright, so thanks to those who actually tried to help. BIOS update and redoing DDU in safe mode worked. At least for the most part. Most games now give me more FPS and stutters are completely gone from any game I play. Even R6 which many said has problems now easily runs 144 fps constantly.\n\nIT'S FIXED NOW. PLEASE LEARN HOW TO READ.\n\n\nMy god the amount of stupid comments I got now although it's fixed as I said. Deleted the rest here so people only see the update now smh. That I even have to do this.\n\nNever posting here again. \n\nAgain thanks though to the few actual nice people who were helpful. I appreciate you.",
    "comments": [
      "Turns out VRAM is not everything. Who knew.",
      "Is your bios latest?\n\nBecause with older AM4 board rebar is sometimes broken older bios.\n\nhttps://gitlab.freedesktop.org/mesa/mesa/-/issues/8032#note_2462421\n\nMake sure you have latest. Like in the issue i linked OP's issue was fixed with bios update.",
      "both of these rules are made up and arbitrary lol",
      "At my resolution it's important. Can't play some games with 8GB without getting abysmal graphics settings.",
      "[Going from a recent daniel owen video, it definitely shouldn't be running worse.](https://imgur.com/tpxuYWc) It should be running significantly better.\n\nI'm not sure what's the cause of your issues. Did you wipe drivers in safe mode and choose the \"wipe drivers and shut off\" option? Or did you do it in normal windows? Or, did you only wipe drivers AFTER installing the new card?\n\n[Regarding the driver overhead, this is only an issue if you run your games at 1080p or very low settings, as HW unboxed puts it](https://imgur.com/lvgpnwV).\n\nAt 4K basically any setting should be SEVERELY GPU bound, you shouldn't see any overhead. And, you mention having a 4K monitor.\n\n[In this benchmark, the b580 outperforms the 4060 in R6 even at 1080p](https://imgur.com/tJvncfG), your results are quite puzzling.",
      "BIOS update definitely improved it, thanks a lot.",
      "B580 is absolutely reliant on rebar, it’s a requirement. Update bios, (remember til enable ram XMP again) and enable rebar. It’s super important.",
      "I'm pretty sure the difference between 7600 and B580 has to do with your CPU and the driver. Either wait for driver updates or return it. The B580 is known to have a greater CPU overhead than AMD and Nvidia.",
      "Oh absolutely not. 2022 BIOS. Guess I'll try that first. Never did it because BIOS update is scary.",
      "It's done. Now is heavily improved. Stutters are gone and I get more FPS in most games. Thanks a lot",
      "To be fair, never change a running system I think is a stupid rule. Even systems with a GT710 would still run, just not the newest titles. Besides I never had any problems upgrading, no matter what it was. Only now with the B580.\n\n\nAnd with the 30% or more upgrade. The B580 would give me even more as I am able to play games that just require more than 8GB VRAM which are quite a few at 3440:1440. So it made sense still.",
      "I also never changed my car oil at the recommended per KM, never broken down, but it is common that engine fails if not followed the instructions.",
      "Not everything but extremely important in video editing",
      "My only recommendation would be:\n\nDownload the install files for the ARC drivers.\n\nTurn your wifi off.\n\nRestart in safe mode without wifi.\n\nUse DDU to wipe ALL graphics drivers.\n\nRestart to normal windows, make sure your wifi is still off.\n\nInstall the arc drivers with the files you had previously downloaded.\n\n  \nUpdate me again when that's done.",
      "Glad your problem got fixed. Ignore the ppl in the replies being immature and it really should be on Intel to have a manual of some sorts that has a list of \"things to do/check\" when setting up the card on the first place.",
      "It's just one graphics setting. Texture make it lower. it's not like B580 is super fast to run high texture and get huge performance uplift.",
      "> I'm playing on a 3440:1440 monitor",
      "I’ll buy the b580 of ya if interested.",
      "Maybe it’s your monitor. Seems that keeps being the denominating factor in every comment.",
      "glad it worked for you!"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580"
    ],
    "title": "2 Months of owning a B580 here is my experience and opinion.",
    "selftext": "Well to be honest i’m thoroughly disappointed. The card itself can handle gaming well but as soon as i try to stream any games beyond rocket league or valorant on twitch my encoders get overloaded and my game/stream will crash. i have spent days optimizing settings all for it to keep happening. \nCertain games such as fortnite it is a gamble whether the game will actually run or not.\n\nThe driver updates are horrendous every time i update all i get is problems such as my pc completely not recognizing my GPU causing me to have to uninstall drivers and reinstall.\n\nThat being said for purely gaming it can handle high fps on high settings. but for someone like me who enjoys streaming it has been nothing but a nightmare. will be switching to a 3060ti for NVENC encoding\n\nthis is just my opinion",
    "comments": [
      "From my experience, OBS has always worked horrendously with my A750. Apparently it just keeps using the iGPU and not the Arc's encoder. Maybe try a different software instead.",
      "I mainly use Mirillis Action. It's a paid software but I prefer it over OBS for better performance and ease of use. Unlike OBS, it lets you pick the encoder if you have both Intel iGPU and Arc. It has a Steam version which is much cheaper and is on sale often so you can give it a try. For stability I'd recommend the standalone version which is a lot more stable and won't force you to update. The Steam version has a tendency to break every few updates, and can make certain games freeze/crash, which I don't get on the standalone version at all.",
      "Interesting, \n\nOBS has worked fine with my B580 and B570 since I got them around a month ago\n\nMy thing was making sure OBS ran in Admin mode in Windows\n\nMy driver updates have been smooth, no real reason to use DDU or tweak driver settings",
      "oh? i’ve never heard of this is there a software u recommend?",
      "I thought I was the only one with encoding issues. Can't get a single decent clip from OBS replay buffer. This used to run fine on my A770. I think I'll be switching back!",
      "Yeah, I always record at max quality and I haven't noticed any overload with my A750. That is with a few terabytes of footage captured over a year. With OBS I get horrendous fps no matter the config I try.",
      "you might be having a unique issue, since my a770 runs my gpu's acceleration for av1 just fine. one driver a couple weeks back forced the igpu to be used, but it was patched after two days",
      "7600X3D\n\nIve tried using AV1, H.265 and H.264 in games like Last of Us Part 1 and the Callisto Protocol\n\nRebar on",
      "what software and CPU do you use? windows and programs automatically assign streaming to the iGPU so that your gameplay isn't affected by streaming. eithe tinker with the recording software or get  CPU with a decent iGPU. (meteor lake of later)",
      "okay cool i’ll def look into it. everytime i try stream a demanding game my obs says encoder overload and everything crashes so hopefully this can be a fix otherwise ill have to buy a new gpu",
      "i’ll give it a go for sure… so you find no encoder overloads on it??",
      "it’s frustrating because for purely gaming it works rlly well… everything else is extremely questionable though",
      "Looks more like a software issue than the B580, unless you have the same issue on multiple recording apps. OBS is not a good example when I've seen it breaking on both Nvidia and Intel every once in a while.",
      "I think it's most recorders in general that Arc has a hard time handling, like Medal or Discord. I can't even stream my desktop (with no games running) with Discord at 1440p without my system tanking - hard stutters, audio bugs out. I'll stream Valorant at 1080p, which works alright, but my system takes a decent hit on performance.\n\nI'm disappointed too because my 1660 ti performed better than this when it came to streaming, even at 1440p.",
      "So i have had the 580 for about same timeframe and had the same issue here are my tips. Run obs or slabs or even prism as administrator. Inside the game as well as the intel software set vsync and frame lock at 60. I can post my settings inside obs later when im off work. But setting the frame lock helped tremendously i play on a 34 ultra wide and can play and stream just fine. Took me alot of playing around to get to this point.",
      "Discord probably has the worst performance on Intel in general from what I've seen. Even attempting to play a Youtube video on Discord causes tremendous frame skipping on Arc, and also bad on Intel HD/UHD, which doesn't happen when I view the same video on a browser. I wouldn't trust it for streaming either since I've had black screen issue on it before on my 1060.\n\nI just gave the newest version of OBS a test. Looks like they improved the performance a bit but frame pacing in the footage is still all over the place. I made sure the Arc media encoder was in use (as reported by Intel Graphic Software). OBS made the encoder work harder (higher usage) yet the final footage looks jerky, while the in-game FPS tanks quite a bit. Meanwhile Action uses less percentage of the encoder and results in smoother footage. This is on the A750 so I wouldn't be surprised if OBS handles the B570 or 580 worse, as those cards are much newer.",
      "i5 14400",
      "B580 has 12GB VRAM.",
      "the B580 is so close in price that I'd go for that for raw performance. I personally prefer 16gb of VRAM vs the 12gb, though. the choice is yours :3",
      "Based on my experience, it's most likely because these recording/streaming software don't prioritize them high enough. I have a need to record, and I also experience encoding overload for some GPU demanding games. Turning off HAGS works a little bit for me, but it still occurs sometimes. That's why I'm now recording with a capture card or windows game recorder."
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580",
      "arc b580"
    ],
    "title": "Intel Arc B580 Review, The Best Value GPU! 1080P & 1440p Gaming Benchmarks",
    "selftext": "",
    "comments": [
      "\"Nvidia would need to price the 4060 at $200, and AMD would need to price the 7600 at $180 to be competitive\" is really high praise for this GPU.  They're not going to do that -- I think Intel has a GREAT card here!",
      "Price/performance wise they definitely hit their goals. Some of the contenders (the 6700xt/3060 ti) you can no longer find them on the market (maybe used) so it's not even a competition. All things being the same this is basically the go to for budget builds. The next step in performance is paying another 100$ (for abysmal uplift) and the big jump happens after 7700xt but that is like 200$ more expensive so we are not in budget territory anymore.\n\nNow it is all up to the market. If the customers at that price point stop blindly going Nvidia then Intel can acquire a certain market and work from there. AMD is mostly targeting the mid-tier for rasterization while Nvidia is going hard on the high end with strong RT performance being the main attraction.",
      "I might actually consider sidegrading from my 3060ti just out of interest of trying new things. My wife needs a gpu update anyways. \n\nI paid 500€ for the 3060ti and that was a great deal at the time. That was a really awful time.",
      "Well nvidia values it by almost doubling the price, just look at 4060 ti with 16gb, insane price jump from $300 to $500. That is really what the b580 12gb is competing with most healthily. Enough vram for 1440p, and usually enough performance, especially with XESS 2, which is the only serious competitor to DLSS.",
      "I bought a B580 LE even though I don't need one just to support them. Really hoping they do a B770 with 16gb.",
      "Thanks Pat and GPU team!\n\nFire the board & Bring back Pat!",
      "I don't think new GPUs are around the corner for this price point. RTX 4060 launched 7 months after RTX 4090. RTX 5090 should launch in January, so that would put 5060 around August. AMD usually likes to launch their card slightly after Nvidia does.",
      "It's just Steve valuing additional 4gb of vram at $50",
      "Same here, no one in my family plays PC games, I bought it just to support the underdog , also I may use its compute power in some of my projects later on.",
      "Could make sense if you are interested in computers and have it as a hobby",
      "The 16gb version price compared to the 8gb one is fine. The problem is that the base has way too little vram and the 16gb has too narrow bus. There should have been just one version with 12/192 but then they wouldn't be able to upsale the 4070.",
      "I felt I should thank you. Thing is, in India, Intel GPU sells at actual dollar converted rate, but nvidia and AMD don't. \n\nFor example, take 4060 which is the most liked card because people can somehow afford it and it can give some reasonable level of gaming performance. \nBut it costs about 560-600 USD. \n\nI am hoping Intel sells more in India and thus earns enough profit to keep their GPU division going longer and we will get competition to Nvidia 70-70ti series cards soon from Intel at good cost.\n\nThis will actually, practically move the technology further than just in theory while keeping price competitive.",
      "Fortnite ain't that big anymore.",
      "Rumours say that the 5060 comes with 8GB of VRAM. Its going to struggle a lot going above 1080p with higher settings, unless NVIDIA comes up with some tricks to decrease VRAM usage.",
      "I've got to say, I had my hopes up for the B580 and it came out better than I thought! Now they just have to focus on the drivers to keep them competitive. Recording software similar to Shadowplay would also be an amazing addition to the overall ecosystem.\n\nIt is the new value king, unless we start going for the second-hand market.",
      "Here in Europe pricing is a mess. For example where i live 7700XT is only 100€ more than B580 and 4060 is 40€ cheaper.",
      "Intel Arc B580 is overwhelming good with those pricing, not only it beat RTX 4060 and RX 7600 but at some game even B580 able to match RTX 4060 Ti, not to mention Intel still have bigger room to improve with drivers.\n\nImagine if they dropped B770 with an RTX 4070/Super performance but only for $350, Amd and Nvidia will be in a big trouble !!",
      "Also thanks to Tom Peterson, he is always GOATED since his days at Nvidia.",
      "True but lot of people building around Christmas for sure",
      "nVidia gets to set their MSRP based on their costs and desired profit margin. \n\nIf people don't buy them, they can either drop their price or stop making them.\n\nThe market can't make nVidia sell cards cheaper if they don't want to."
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580"
    ],
    "title": "The first available intel B580 models in the EU cost €320...",
    "selftext": "",
    "comments": [
      "The reference card in the UK is £248 \n\nYou never see proper currency conversion though",
      "I payed 280 euro and got Assasins Creed Shadows(70 euro) for free.",
      "It is not about currency conversion, it's about EU import taxes",
      "I'd pay 70 euro just to not see this game",
      "every country besides US getting screwed over",
      "The £248 in the UK includes 20% VAT and any other tax \n\nIn the PC hardware market proper currency conversion including tax is never done. Usually they just change the currency sign\n\n€320 is roughly the same price for the same cards in the UK £259/260 etc\n\nThe third party cards always carry a higher RRP",
      "This is expected, just wait until the market saturates a bit.",
      "Europe once again getting screwed over for no reason at all",
      "Worst game of the decade edition?",
      "That the card in EU is not that awesome for FPS per money, as compared to the US market. It's great card for the money. I will get one, once it's available in my country. The A770 in my country is around 400 Euro.",
      "260 usd is $409 aud, so plus GST makes the LE price of 439 pretty much spot on RRP. Can't be mad about that\n\nThe rtx4060 is usually around $450-500, or 400 for a really good special",
      "Bought in Poland for 1289PLN = 309 euro\n\nyeah EU sucks",
      "Payed means you've put tar on a wooden ship's hull, paid is what you are looking for.",
      "Yes... In Europe for the current price it doesn't seem to be the better option apparently",
      "OCUK have stock \n\nThey were taking pre-orders and will be shipping 200 by the end of today \n\nThey have another 200 coming into stock next week",
      "So in Serbia i expect it to be at least 400. Long live 3rd world",
      "Tell this to CIS countries...",
      "he is talking about the so called \"trade war\" the EU is fighting against U.S. companies. (But gpu prices shouldn't be included in this discourse), for example Apple's new IOS update has AI features, but not in europe due to recent problems the EU has caused to Apple. So I agree with him, the EU isn't a good place for U.S. companies to sell products, however the intel Arc prices in europe are Intel's fault, NOT the EU.",
      "What?,😂 i bet you didn't do your homework",
      "The Steel Legend is $270 here in the US. I was able to get an LE for $260 but it looks like everything here is sold out already."
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580"
    ],
    "title": "The reality is that the B580 is still going to sell out…",
    "selftext": "\nNo matter what fud there is right now with the overhead issues the B580 will still sell out and you wont be able to get your hands on it. The card may be worse than the 4060 by 10-20% in 1080p but it is also 30% cheaper and destroys the 4060 in 1440p gaming.\n\nPlus I dont really see an issue as 1440p gaming is better than 1080p so Im glad thats still fine. Maybe its time to upgrade yalls builds. The reason its good as a budget gpu is because you dont have to spend the cost of the rest of the build on a single gpu. 1000$ budget on a gaming pc now will get you a nice build with the B580. ",
    "comments": [
      "Unless Nvidia drop 4060 TI to B580 price category, it still sell\n\nThe issue with 8GB card is they obsolete faster, you can upgrade the CPU but you will never able to upgrade the VRAM\n\nHistorically in last 5 year, there is a competition within CPU market, Intel and AMD keep try to lead against each other, upgrading CPU into faster one is easy\n\nMeanwhile on GPU space it’s the opposite, a very stagnant on subs 300$ price, 4060 and 7600 barely beat last generation, and in many case 3060 perform better because They not run out of VRAM",
      "I bought A770 on the release day. Not because it was a great GPU (but it wasn't bad), but I wanted to help develop better drivers (I talk about Linux, of course).  \n\nI daily drive 7800XT as the main GPU, but I am, once again, hyped to buy B770 when it's out. And still for the same reasons:  \n\n1. Why not?\n2. I can\n3. I want to",
      "things are not exactly like that...\n\nB580 is a decent card, but it comes 2 years after the competition to which it is compared and often vs the 4060 which is one of the cards with the worst price-performance ratio ever.\n\nAnyway it is a card that depends a lot on the CPU and there are tests where it is worse and not a little worse than the 4060 for the CPU, it works well only if combined with high-end CPUs.\n\nworks better (in some cases) in 1440p for the vram, but it is not the only card in this range to have 12gb, example:\n\nRadeon RX 7700 XT 12\n\nGeForce RTX 2080 Ti 11\n\nRadeon RX 6750 XT 12\n\nRadeon RX 6700 XT 12\n\nRadeon RX 7600 XT 16\n\nGeForce GTX 1080 Ti 11\n\nTITAN X Pascal 12\n\nRadeon VII 16\n\nGeForce RTX 3060 12\n\n\n\nIf one wants to make an informed purchase, one should at least wait for the release of the 5060/5050 and equivalent AMD.",
      "Pack it up people, acknowledging there's an issue with a product is considered fud now",
      "Nvidia have this with 8GB VRAm, doubt they change anytime soon",
      "Not true at all. Nvidia in the 90's couldn't keep up with 3Dfx or S3 at the time, but they stayed relevant and eventually came out ahead. Also, there are markets and sub-markets aside from gaming, which care more about best bang for the buck and not overhead issues and FPS in gaming. There's also consumers and businesses. If an engineering company needs to buy a budget PC for 3D CAD design and their I.T. department opts for gaming cards instead of workstation GPU's, plus go with as much VRAM as possible with the budget they have to work with, buying 2,000 PC's could save tens of thousands going Intel. \n\nAll these comments completely focus on gamers and while gaming cards are marketed to gamers, businesses and other non-gaming PC users and I.T. professionals use gaming cards as well and the non-PC gaming market is bigger than you could ever imagine.",
      "I see the \"maybe you just need to upgrade\" cope is in full effect. Who are you, Jensen Huang?",
      "Just upgrade you CPU for this budget GPU! The more you buy the more you save!",
      "Yeah no they won't. They don't care and every single leak shows 5060 having 8gb of vram and 5070 getting the 12gb along with major price increases across the board so likely the 5050 with 8gb will be the new $300 card",
      "If that rumored 24gb card comes out imma order that thing with a quickness",
      ">B580 is a decent card, but it comes 2 years after the competition to which it is compared and often vs the 4060 which is one of the cards with the worst price-performance ratio ever.\n\nEver ... so far. The only reason I could see 5060 maybe offering better price-performance than 4060 is that Nvidia got scared of B580. I doubt that will happen, though. I expect 5060 to again be pretty disappointing stuff.\n\n>Anyway it is a card that depends a lot on the CPU and there are tests where it is worse and not a little worse than the 4060 for the CPU, it works well only if combined with high-end CPUs.\n\nI think it's fine to wait for that story to become clear. So far, we have quite little data to go on and Intel hasn't made any comment.\n\n>If one wants to make an informed purchase, one should at least wait for the release of the 5060/5050 and equivalent AMD.\n\n5060 is not soon. The 40 series launched November 2022. The 4060 launched June 2023.",
      "It would probably be aimed at professional workloads, sure, but the initial rumor said it was a B580 with 24gb, and a newer one said it may be a B770 or B780.\n\nI don't really see a reason it wouldn't be able to use the same drivers",
      "Right?\n\nThe whole point of budget cards is being able to upgrade your gpu with typically lower end hardware. People with a ryzen 3600 or 5500/5600 pc. Those type of people who want to spend $300 max. To have to spend that + another $ on a cpu and maybe mobo too. Come on. That’s wack.\n\nIntel dropped the ball here on the drivers side. Cant recommend a b580 at all anymore. Too much hassle for lower end pc users",
      "I think you're just worried about your portfolio.",
      "\"It's cheaper than the competition\" does not justify the product having major issues. \"The 4060 is bad value\" and \"The B580 has a problem\" are statements that can co-exist.\n\nIf you really cared about Arc succeeding, you'd be hoping Intel fixes this, not closing your eyes, covering your ears and pretending nothing's wrong.",
      "Most budget gamers with 12th gen Intel or AMD equivalent will still buy this because it's cheap and works well.",
      "Isn't that card aimed at professional workloads?\nOr would it be decent for gaming too? \n(I don't know alot about architectures)",
      ">burning AMD stuff\n\nYup that's acknowledged because it was mobo vendors who decide to over volt them, or user error of installing the cpu\n\nAll those things was taken care of quickly",
      "Acknowledging isn't, but there is an ongoing FUD right now, as Arc's user base is not loyal yet.\n\nTake a look at burning AMD stuff. Everyone acknowledged it, literally 0 FUD happened, and people kept buying AMD, because they have a loyal user base.",
      "Why would I want to buy one now when it's allegedly \"time to upgrade my system\"?"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580"
    ],
    "title": "B580 suffers from enormous driver overhead at 1080p",
    "selftext": "In recent days, I acquired a B580 LE to test on my second rig, which features a 5700X3D (CO -15), 32GB of DDR4 3600 MT/s RAM with tight timings, and a 1080p 144Hz display. My previous card, a 6700XT, offered similar raster performance with the same VRAM and bandwidth. While the B580 is a noticeable step up in some areas—mainly ray tracing (RT) performance and upscaling, where XeSS allows me to use the Ultra Quality/Quality preset even on a 1080p monitor without significant shimmering—I've also observed substantial CPU overhead in the Arc drivers, even with a relatively powerful CPU like the 5700X3D.\n\nIn some games, this bottleneck wasn't present, and GPU usage was maximized (e.g., Metro Exodus with all RT features, including fully ray-traced reflections). However, when I switched to more CPU-intensive games like Battlefield 2042, I immediately noticed frequent dips below 100 FPS, during which GPU usage dropped below 90%, indicating a CPU bottleneck caused by driver overhead. With my 6700XT, I played the same game for hundreds of hours at a locked 120 FPS.\n\nAnother, more easily replicated instance was Gotham Knights with maxed-out settings and RT enabled at 1080p. The game is known to be CPU-heavy, but I was still surprised that XeSS upscaling at 1080p had a net negative impact on performance. GPU usage dropped dramatically when I enabled upscaling, even at the Ultra Quality preset. I remained in a spot where I observed relatively low GPU usage and a reduced frame rate even at native 1080p. The results are as follows:\n\n* [1080p TAA native](https://ibb.co/Jrdym5j), highest settings with RT enabled: 79 FPS, 80% GPU usage \n* [1080p XeSS Ultra Quality](https://ibb.co/0ydqP5h), highest settings with RT enabled: 71 FPS, 68% GPU usage\n* [1080p XeSS Quality](https://ibb.co/GdJdVfd), highest settings with RT enabled: 73 FPS, 60% GPU usage (This was a momentary fluctuation and would likely have decreased further after a few seconds.)\n\nSubsequent reductions in XeSS rendering resolution further decreased GPU usage, falling below 60%. All of this occurs despite using essentially the best gaming CPU available on the AM4 platform. I suspect this GPU is intended for budget gamers using even less powerful CPUs than the 5700X3D. In their case, with 1080p monitors, the driver overhead issue may be even more pronounced. For the record, my B580 LE is running with a stable overclock profile (+55 mV voltage offset, +20% power limit, and +80 MHz clock offset), resulting in an effective boost clock of 3200 MHz while gaming.",
    "comments": [
      "Try to report the finding in Intel Arc Graphic Community Forum?   \n[https://community.intel.com/t5/Intel-ARC-Graphics/bd-p/arc-graphics](https://community.intel.com/t5/Intel-ARC-Graphics/bd-p/arc-graphics)",
      "All drivers will have a CPU overhead, Nvidia's driver overhead is ludicrous and has been for years but used to be worse under DX11 due to their software scheduler. Intel might be taking a similar approach? \n\nAMD has always used a hardware scheduler which has had pros and cons over the years \n\nI am running my B580 on a 8600G APU and not seen many CPU overhead issues tbh",
      "Yeah uh getting an answer there is about as probable as the world exploding tomorrow. I posted there about a week ago and only got a response by some bot account that said some random words.",
      "Nvidia's driver overhead is way less severe than the one of Arc. NV driver overhead is noticeable when you have really old CPU - in this case I would expect my problems with something like Ryzen 2600 or 3600 even, but not with 5700X3D.\n\nI expected that at 1080p there is a certain threshold where you will end up CPU limited, but the intensity of this overhead is very big - 5700X3D is still a pretty capable CPU and having a budget GPU like B580 with capped usage to 60% at 1080p maxed settings in some games, is just mind blowing.\n\nIt depends on which type of game you play. If you play game that are light on CPU, you're not gonna notice it the way I do. But already mentioned BF2042 is one of the most CPU heavy games, especially in multiplayer with 128 players.",
      "Yeah thats why they marketed it as 1440p gpu",
      "Try to update XeSS DLLs to 1.3.2. That solved for me the CPU overutilisation - spiky frametimes graph - in Talos Principle 2 (UE5).....with an Arc A380.",
      "Report it to intel customer support and on this issue tracker\n\nhttps://github.com/IGCIT/Intel-GPU-Community-Issue-Tracker-IGCIT\n\nWendel from Level1techs briefly spoke about the CPU overhead problems in his review. It needs to be investigated by other YouTubers",
      "It happens the same to me as it does to you with Battlefield 2042. In my case, I have a 7600X with an undervolt of -20 on all cores and 32 GB of RAM overclocked to 6000 MHz with reduced latencies, and the main one at only CL28. In this same game (1080P), with the previous GPU I sold, the RX 6700 10 GB from XFX, I had stable and constant performance above 100 FPS. However, with the Intel Arc B580 Steel Legend OC from Asrock, there are specific areas in the maps where suddenly the performance drops to 60-70 FPS, and at certain points, there are even some drops lower than that. This did not happen with the AMD GPU.\n\nDo you think the performance of this game will improve in the future with driver maturity? Honestly, I doubt my RAM or CPU would bottleneck this GPU in any way.\n\nI also tried testing Control with ray tracing yesterday, and it doesn't let me select the option in the game menu. I think it's because the game runs on DX11, and it doesn't give the option to start it in DX12... not sure if anyone knows a solution to that.",
      "Used 3080 will get you no warranty at all, just a 10GB vram buffer that is already very limiting at 1440p and130W higher power consumption. But yes, drivers are much less of a hassle I agree.",
      "Tried it, same result.",
      "And you're still ignoring Intel's problem here which is far worse and the point of discussion",
      "That's nonsense and you know it.\n\nMost reviewers are using 7950X3D or 9800X3D and despite the monster of a CPU it's still showing a bottleneck.\n\nNeeding a $500 CPU and still not performing to it's potential is a serious problem that needs to be addressed soon, especially because if you pair it with a more balanced pair like a $250 CPU then performance will drop even further.\n\nYour bias against Nvidia is blinding you.",
      ">Just because graphics card load drops doesn't mean you have CPU bottlenecking.\n\nProblem is clearly not my CPU, but the immature drivers in some instances. \n\n>  \nDoes the entire Arc series have significant driver overhead? Yes. Does that mean you have a BoTtLeNeCk GuYzZ? No, it doesn't.\n\nWhat is \"BoTtLeNeCk GuYzZ\" supposed to even mean? My whole point was that ARC has much more severe driver overhead problem at lower resolutions than any of the competitors. That is all. \n\n>  \nWho cares about the performance on 2008 resolution anyway?\n\nLiterally 63% of PC playerbase do play on resolution equal to 1080p or lower, according to Steam HW Survey from November 2024. So hey, vast majority of PC gamers as of now still do care about this resolution. \n\nSorry that I don't want to play games at 60 fps anymore. B580 is lower mid-end GPU at best, that may serve you well for 1440p in lighter titles, but without XeSS 2 FG, it's nowhere close to being enjoyable higher refresh rate 1440p card.",
      "And for years in the consumer GPU market you haven't sadly until now",
      "coz its not a critical experience issue? Can share the forum post link?",
      "1080p still performs way better than 1440p for B580, it's just that relative to competition it does way worse when paired with a much slower CPU than reviewers use.\n\nTPU, Hardware Unboxed, GamersNexus are all using X3D CPUs, and lot of them Zen 5 at that. And it still shows a bottleneck.\n\nThe issue is entirely Intel's period.",
      "Man don't even try to suggest that in this subreddit when 99% of people *still* think 1080p is less intensive on CPU's than 1440\n\n(I did it all year ago with the A series cards, I realized that this place has worse brain drain than related tech subreddits)",
      "That is the point of driver overhead problem. If 5700x3d is not enough to feed this fairly low end gpu, I don't know what you expect people would pair this $250 GPU with. A 14900K or 9800X3D?",
      "I know you are bit a of a degenerate, but this is purely a software issue lol. Hardware is perfectly fine.",
      "Nvidia driver overhead doesn't even compare to ARC. Intel needs to fix this before they get B770, and future parts like Celestial because it'll get worse... and worse."
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580"
    ],
    "title": "ONIX LUMI B580 came in :)",
    "selftext": "",
    "comments": [
      "I have to ask why the low profile cooler if there is enough room for a bigger one?",
      "Honestly, it's just for aesthetics lol. The cooler + 15mm fan is the exact height as my ram and I personally think it looks sleek.\n\nHere's a pic of the alignment: [https://imgur.com/a/sCo0sSz](https://imgur.com/a/sCo0sSz)",
      "CPU overhead differs from game to game. I mostly play e-sport-type games with a few graphic-intensive games being GTA V and modded Assetto Corsa so it doesn't apply to me or affect me that much.",
      "Ryzen 5 5600 undervolted (PBO -30)\n\nHere's my part list: [https://www.reddit.com/r/IntelArc/comments/1i767ne/comment/m8ib0pg/](https://www.reddit.com/r/IntelArc/comments/1i767ne/comment/m8ib0pg/)\n\nI just ran GTA V 1440p with everything on Ultra except Grass and Motion Blur, and FXAA On only. I averaged 70C via HWiNFO. Also, I'm in a tropical climate so I'm surprised it stayed around this temp lol.",
      "RYZEN 5 5600\n\nThermalright AXP90 X36 Black Low Profile CPU Cooler (I attached a Thermalright TL-9015)\n\nONIX LUMI B580 12GB\n\nASRock B550M-ITX/ac\n\nAsgard Valkyrie Series 2x16GB 3200MHz\n\nSamsung 970 EVO Plus 2TB M.2-2280 NVME\n\nKXRORS M02 SFX Mini-ITX Case\n\nCooler Master V750 SFX GOLD 750W 80+ Gold Fully Modular SFX\n\nThermalright TL-C12015 RGB (top exhaust)\n\nThermalright TL-9015 (rear intake)",
      "Love the look. Clean build.",
      "The total cost is \\~$1095.\n\nHowever, this price can be greatly reduced if you choose a smaller NVME or a cheaper SFX PSU. I went with parts that were a little more expensive due to the size or rarity at the time. Also, white and itx tax.",
      "thats actually pretty neat",
      "nice",
      "This is beautiful",
      "i have low profile cooler too in my full atx build. im not into overclocking so its enough for me.and of course for asthetics",
      "I chose 750W since it was the only available white SFX PSU at the time. A white SFX PSU was scarce (and still is I believe). The only other options were from Lian Li and Corsair, but Lian Li had weird fan curves that would make it loud and Corsair didn’t offer a white color way. Thankfully, Cooler Master had one so I picked it up :)\n\nI recommend a 600W PSU from a reliable manufacturer. You could run a 500-550W, but 600W is the sweet spot and you’ll have space for upgrades. You can search up “PSU Tier List” from cultists network and compare models you see within your budget and pick the higher-tiered one.",
      "What CPU, and how are the temps?",
      "Would a Ryzen 7 5700X3D work well with the B580?\nJust curious, thinking of getting a pc soon",
      "The ONIX ODYSSEY (Black) retailed for $259.99, clocked at 2670 MHz.\n\nThe ONIX LUMI (White) retailed for $269.99 but it is factory overclocked at 2740 MHz. So it's not just the color this time lol.",
      "Damn okay, Thats pretty solid. I can definitely see the aesthetic you were going for with that cooler. And lucky for you the temps work out. Solid build!",
      "Yes. If you plan/choose to stay on the AM4 platform due to budget or whatever reason, the 5700X3D is the best in price to performance. I plan on upgrading to the 5700X3D in the future as well.",
      "Nice. Most were having a hard time getting them at that price",
      "Nicely done",
      "looks clean. whats the total cost?"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580",
      "arc b580"
    ],
    "title": "We Were Accused of Forgery: \"Fake\" Arc B580 Benchmarks",
    "selftext": "",
    "comments": [
      "Biggest forgery here is Steve not actually being bald",
      "I'm hoping to be running my B580 with a Zhaoxin KX-7000 tonight. I'll test some of my benchmarks against theirs, lol.",
      "I suspect he'll be tearing his hair out over the upcoming 5060 / 5060 Ti review, so perhaps it's just a preview.",
      "Hahahah… almost everyone downvoted me when I said “I rather trust hardware unboxed than some russian channel“ a few weeks earlier. Now I’m glad this proves %100 my point.",
      "The issue has been happening with Nvidia for 2+ years now, but people somehow forgotten about it. \n\nAMD will always be the superior choice for pairing with a low-end CPU.",
      "“How dare he make another video I don’t like!!!😡😡”",
      "Yes it’s not as bad as most people think on CPU’s like 5600 and up but anything lower those issues can really start to cause problems and I think people should know about it. Also this coverage might push Intel into trying to improve or fix it for future generations. If this would have gone under the radar again like Alchemist, maybe Intel would have not bothered trying to fix it.",
      "Their results were literally posted here in this subreddit.\n\nI hardly ever go here specifically and even I got to see it because it popped up to me on my feed.\n\nWere you complaining when HU did three videos where they were saying that XeSS was better than FSR2/3? Or are you complaining now because there's something that makes Arc look not as great?",
      "He really did lose his hair over this...",
      "The problem wasn't them posting wrong/fake benchmark results, it was them calling out hardwareunboxed for erroneous testing (or not testing at all) that was the problem.",
      "OK... If I start publicly telling people that you are bad at your job, and not just 'some guy on the internet', but you, your real name, your actual job/employers, and will give some 'evidence' that might seem right, but is actual wrong, will you just give up and accept it because fighting it would mean 'there's an agenda'?\n\nSome people see 'agenda' where ever there's something they don't like.",
      "Thats why i recommending for 1440p (best option for price) but not for 1080p",
      "I can see  this guy will be milking this issue for years. \n\nIn 4-5 years Intel Arc will be in the 5th generation, without having this issue since Celestial, and he will come back to test the B580 again for the 10th time, just to check if this problem has been fixed on that card. It seems that reporting this issue has become a matter of honor for him.",
      "AliExpress",
      "Yeah, it's price to performance is already terrible.  I would not pay $450 + tarrif for it now.",
      "Don't bother responding to the cultists here in the future. This small community used to be cool until some people starting believing they are doing something noble that would save humanity by going with Arc and they let it get to their heads.",
      "How did you managed to get a Chinese CPU?",
      "Thanks for the informative videos steve you are a hardworking one(specially ur motherboard testings). And for b580 i am giving recommendation for 1440p use for 4 build all was happy.",
      ">people would notice any performance hit with the naked eye\n\nIn some cases performance dropped by 50% (see the war thunder and spiderman benchmarks in the video), if you can't see that with the naked eye you should have your eyes checked.",
      "I just installed a contact frame on the lga 1700. It gave me a bit of a heart attack because it was running memory training when it booted up, and it took a bit longer than I was expecting."
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580",
      "arc b580"
    ],
    "title": "Intel ARC B580 might be the highest performing GPU to ever exist...",
    "selftext": "",
    "comments": [
      "Why didn't we think about going above 100% before?!?! Fools!",
      "nvidia never saw this one coming",
      "And this is to go even further beyond.",
      "I think B580 secretly has the MUG (Multi Utilization Generation). It can just magically perform 178315331633152.0% better.\n\nUnfortunately this feature only works in HWMonitor.",
      "158%?!",
      "Yea, something is weird about the reporting 🤣",
      "If it wasn't for that blasted driver overhead!",
      "go above and beyond ! PLUS ULTRA",
      "It uses more GPU per GPU\n\nCave Johnson would be proud",
      "They got lots of dedotated wam as well",
      "HWMonitor can be pretty weird and glitchy sometimes.\n\nI recommend switching to HWinfo64 when you get a chance",
      "You mean Magic Utilisation Generation?",
      "Yea, but Hwinfo64 isn't going to make me feel better about my purchase, though.",
      "I overclocked to 3100mhz and I gained like a 8 fps increase",
      "intel says \n\n you get 1 gpu and it will perform like one plus  and  a half  gpu\n\nnvidia says \nthe more you buy the more you save",
      "Use Hwinfo64 \n\nHwmonitor is notorious for weird readings.",
      "the real overclocking",
      "Instead of battlemage they should of called it Intel ARC Saitama",
      "Just proof that those giving 110% are just slackers.   Always give 178315331633152% like this hard-working GPU.",
      "Can someone explain me this image to me? What do those numbers mean?"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580",
      "arc b580"
    ],
    "title": "Can everyone post their CPU/MOBO with the ARC B580?",
    "selftext": "For those who have the ARC B580, can you post your CPU/MOBO combo and your experiences?  \n\nI am still on the fence whether to go Intel or AMD.\n\nThanks.\n\nUPDATE 1:  Awesome feedback.  Thanks to everyone who shared their posts and to those who continue to post.  Based on the feedback so far, it looks like the majority of you are on AMD with a higher spec'ed out combo.  I'll do a bit more research on some of the boards you provided and hopefully make a final decision soon.\n\nUPDATE 2:  I decided on the i5-14400F for the CPU.  This was based on trying to maintain a budget/mid-range build.  My usage will be 30% gaming and 70% general usage (email, web-surfing, general office tasks).  Now I am looking for the MOBO.",
    "comments": [
      "CPU - i5-14400F (got a steal deal on one brand-new)\n\nMOBO - AsRock B760M-C\n\nThis setup is demolishing everything at 1080p, 240hz",
      "Mobo - Gigabyte B650M Gaming X AX\nRAM - 32GB GSkill DDR5\nCPU - AMD Ryzen5 7600X\nAmazing combo and zero overhead issues or stuttering. Ultrawide 4K monitor getting 50FPS in RDR2 on ultra settings at full rez which I think is pretty tough. Has been a major upgrade for me compared to my old PC.",
      "Asrock a620i \nIntel i514400f\nB580 le\nT-create 32gb ddr5 cl 30 6000hmz\n850w sfx coolermaster hold rated psu\n2tb gen 4 kingspec m.2 nvme ssd\nDeepcool ch160 case\nWith 3x noctua 120mm fan + deepcool assasin iv cooler \n\n0 issues.\nGame at 1440p\n\nCheers and gl",
      "- Core Ultra 9 285K \n- Asus ROG Maximus Z890 Extreme\n- ASRock Arc B580 Steel Legend 12GB OC\n- ASRock Arc B580 Challenger 12GB OC\n\n———\n\nNo issues, working great. No games installed though. I use it for 3D graphics programming, data processing, and deep learning.",
      "Ryzen 5500 and Asus Prime B450M-A II. I can definitely notice a bottleneck but it's not super detrimental to my gaming. Not perfect but for sure playable in all my games (at least to my standards)",
      "CPU ryzen 7 5800x3d, \n\nmobo MSI B550-A pro, \n\n32gb ddr4 3600 (cheapest i could get, can't remember make)\n\nNo problems with anything so far @ 1080.  Desperately saving for a decent 1440p monitor....",
      "Building a pc with 7945hx and the Minisforum mobo it comes with. Still waiting on ram and ssd to come in.",
      "11400 and ASRock z590, run 4k/60 on my tv",
      "I5-12400 with 32gbs of ddr5 6000 cl30 ram. \nThere's a slight cpu bottleneck in more cpu demanding games, but otherwise, it's good for budget 1440p. The only game that I've had issues with is Spider-Man remastered. It randomly blue screens, and I'm still not sure why.",
      "CPU- i7 14700K\n\nMOBO- ASRock Z790 Steel Legend WiFi\n\nGPU- ASRock B580 Steel Legend",
      "My B580 currently lives in a 14400F/Z790 setup with 6400mt/s RAM. It's solid. \n\nIt was also rather cheap; $130 Z790 board, $130 processor, $?? RAM (came out of another system that got an upgrade), $250 GPU. Currently waiting on a new M.2 because my only leftover has a failure warning, and then I'm debating recasing it as it's currently in a \\~9 year old cheapo case with minor cosmetic issues. \n\nZ790 is 0% worthwhile for a cheap processor, and I kinda got screwed on the WiFi/BT module used on the particular board for Linux use, but the B580 likes being under Windows anyway. Also, while I'd mostly recommend upgrading beyond the stock cooler on the 14400F for noise reasons, it makes *absolutely no change* with regard to performance.",
      "9600x w/ Asus B650m-E \n\n32GB t-force 30cl RAM \n\nPlaying at 1080p 180hz \n\nWorks like a charm",
      "CPU : Ryzen 5 7600\n\nMOBO : MSI Pro B650M P\n\n\n\n**Cyberpunk 2077 1080p + XeSS Ultra Quality**\n\n* Ultra : 100 - 120 fps\n* Ray Tracing (High) : 55 - 80 fps\n* Path Tracing : 25 - 35 fps\n\n  \nSo far, i haven't encountered any buggy/unplayable games, and i played a lot of old JRPG",
      "12400f + asrock Sonic B760m. Really cheap combo, plays all my fave games really well. Planning to get a 14600kf soon",
      "Asus x870 am5 gigabyte 8500f\n\nWish i knew 8500 was apu, once up and running disable the onboard gpu in device manager which fixes several games.\n\n\nLinks\n\nhttps://www.reddit.com/r/IntelArc/s/rP9dkmvu2b\n\nIts decent after tweaks",
      "I had an A770, which is similar in performance minus 5-10%. i9 12900KS, MSI Z690.\n\n(Can't complain! at 1440p it was excellent and gave me >60 fps.)",
      "Amd 7600X3D, 32bg 6000 ddr5, intel b580. I play at standard 1080p and triple wide 1080p. Overall I've been very happy with the B580. Only had issues on an older title (the evil within 2). Runs triple 1080s great for iRacing and Assetto corsa as well as runs cyberpunk in 1080 at ultra settings with ray-tracing with zero problems.",
      "Ryzen 5 9600x + ARock B850M Pro RS Wifi",
      "5800X3D x570 Aorus Pro WiFi. Worked like a charm. No real issues. If you’re on Zen 3 or 11th gen Intel, you should be fine. Ymmv on something like a 3700x. It’s usable but you might run into the cpu overhead issues at that point.",
      "12700k + MSI Pro Z790-S"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580"
    ],
    "title": "The current GPU landscape",
    "selftext": "For a GPU that's reasonably priced and often restocked, B580 isn't a bad choice. Might as well not pay the inflated mid tier GPU prices and put it to a faster CPU. ",
    "comments": [
      "It’s not like the b580 needs any more advertising it goes out of stock incredibly fast if near msrp\n\nEven at 300 it sells out at microcenter",
      "Yeah I don't agree with this. It's doing great. Keeps selling out, keeps getting updates. Love mine.",
      "I paid $300 for my Sparkle B580 at Microcenter. Considering the large improvement in thermals over the Intel edition of the B580 I can avoid feeling too bad about paying over MSRP by a little bit.",
      "To be fair when it launched they were all losing their minds over the B580; it’s not as if they completely ignored it. NVIDIA and AMD launched shiny new toys so naturally they have to move on to covering those.",
      "B580 will be relevant when rtx 5060 and rx 9060 drops in market, for now the reviewers are focused on upper mid range GPUs so no talk on b580",
      "b580 is the only one at msrp",
      "I bought an A770 on the release day.   \nI helped to develop and debug Linux drivers for over a year, then I switched to 7800XT.   \nI would be more than happy to help once again but there is no B770 (or similar) on the market.   \nI don't really care for B580.",
      "Maybe that's true where you live, but where I live the b580 is around the same price of the 4060 even now, i waited for this gpu like many of us here, i really wanted it, but not at that price.",
      "Yeah now that they sold out of the sparkle they are selling a 2 fan Acer b580 for 330",
      "Definitely not the US",
      "What the fuck is this? Post made by Userbenchmark?",
      "Why are people who like the 9070 xt shilltubers? If you can find it at MSRP it is the best dollar per frame GPU on the market, it's obviously a completely different price bracket than the b580 but that doesn't mean they both can't be good options? And I do see the b580 recommended a lot for budget builds.",
      "According to the Tech power up video the 3 fan Sparkle and 3 fan Asrock cards run about 10 degrees cooler than the le. I really like the look of the le though.",
      "Because OP is bad at making memes, or is an Intel shill, or both. Those same \"shill tubers\" have been calling out AMD for how hard it's been to get the 9070/XT cards. Just because their reviews were good doesn't mean they haven't been vocal about the crappy supply and MSRP vs street price situation",
      "Probably, gotta make sure they offset AMD's advanced marketing strategies",
      "I bought mine for 300 in Europe. Idk where you live and what your prices are there, but 300 euros is a fair price in Europe because of the VAT tax.",
      "They do, that's my point.  \nAnd I want top of the line model from Intel if I am going to bother myself with it.  \n\nDon't get me wrong, I like Arc cards, I think it's a good thing that Intel released them.  \n\nBut B580 is not interesting for me.",
      "It went out of stock everywhere in Australia for a while",
      "I can't help but think that it goes out of stock so fast because Intel made 4 of them.",
      "Lmao what victim complex is this. No one thinks about gaming 4k everyday\n\nPeople aren't giving it a chance because of the underlying driver issues that happened in the 1st gen which put them off, the cpu overhead issues, lack of wide support for xess2 and the new features, the lack of availability worldwide and it's pricing"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580"
    ],
    "title": "Oh boy does this card ever look good (B580 12GB Steel Legend)",
    "selftext": "",
    "comments": [
      "are you using a m.2 ssd for a gpu holder lol",
      "Nope it’s a 1TB 970 Evo that still has some important data and my old windows 10 install on it (recently switched to win 11). I just did it for picture only. Don’t have it lean on it all day.",
      "Your whole rig looks fantastic. The cards is just the cherry in the middle.",
      "Temporary solution until I 3D print something better",
      "🫡",
      "Considering my future white build, I am really looking forward to when the steel legend becomes available in my country.\n\nThat or the Gunnir white.",
      "i hope the drive is dead at least lol",
      "Thanks also just got the ASRock X870 Pro RS because my MSI B650 Tomahawk refused to boot with Sparkle A580 at all and the ASRock A770 with all SSD slots occupied for whatever reason, tried million things to get the A580 working but was so done with that board and just ordered a new one. With new board all cards booting perfectly and now I can continue my benchmarks without having to trouble shoot when switching cards.",
      "B580 Steel Legend sounds like a motherboard name ngl.",
      "Fair enough but I have one so it’s a few cents of plastic",
      "average linux chud the picosecond someone mentions a window(doesn’t even have to be the software)",
      "Holy shit I didn't even notice that but that is hilarious.\n\nOP did it not come with a gpu brace?",
      "I have a 5700X3D. Does it support AM4 and how good would it cool it?",
      "3D printing is cheaper",
      "They had us till the last two sentences",
      "Looks rad!",
      "Man that is one clean build. I love every part of it",
      "That CPU cooler is DOPE",
      "that looks sexy, my days, amazing build dude",
      "Battlemage makes it hard deciding gpu lol"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580"
    ],
    "title": "Got a sparkle b580 for my birthday!",
    "selftext": "Upgraded from a 1060 3gb",
    "comments": [
      "Happy birthday",
      "happy birthday\n\nthat’s gonna be a day night difference lol",
      "I just booted up rdr2 and it is amazing. It’s like 80 fps on ultra settings this is so cool",
      "Thank you :)",
      "I like it, very huge upgrade from my old gpu, I’m able to properly run games at higher settings and actually able to play rdr2 now",
      "Nice, I've got the same card 👍",
      "Good for you bro, My birthday was three days ago and I ordered a B570 Sparkle as my first GPU. Can't wait to have it with me:)))",
      "Very nice 🙂. What are thoughts so far?",
      "Must be a huge upgrade for you. Congrats man , happy birthday , God bless you!",
      "That's a beautiful card, triple fan too\\~",
      "Happy birthday mate",
      "Ain't no way, I'm also upgrading from 1060 to B580 which I got for my birthday on the 25th what a crazy coincidence",
      "Soon... Soon my flair will change and I will be free...\n\nEdit:whoops thought I was on pcmasterrace I'm running a EVGA Nvidia 1050 ssc right now",
      "Idk, it’s probably the one on Newegg tho",
      "It feels so powerful. Thank you!",
      "Happy birthday.",
      "Happy birthday! You got yourself a nice gift",
      "Congrats!",
      "happy bday :) very nice upgrade, the sparkle card looks so nice too",
      "Happy birthday man 🎉"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580"
    ],
    "title": "RX580 -> B580",
    "selftext": "The RX580 has aged well, I hope it will be the same with the B580.\n\n#team580 😅",
    "comments": [
      "Uninstall AMD adrenaline software, DDU in safe move to remove AMD GPU, update the board bios, enable resize bar and disable CSM in bios. Reinstall the chipset drivers from board support page. Congrats on the upgrade!",
      "The 580 lives on",
      "Good upgrade",
      "B580 needs a fast CPU and a motherboard that supports resizeable bar. I hope you have both.",
      "Similar update in my experience. I went from rx 570 4GB to B580 month ago. Good Lord!",
      "did not no there was a 2 fan model",
      "you’ll still experience the overhead issue but you probably won’t tell in gameplay",
      "3600x should be fine.  Bottleneck should be minimal. I'm sure it's not existent on 1440p.",
      "I have a 5 3600x, so there may be overhead problems, but I am replacing it with a 5700x. I think it should be enough.",
      "Goat card to goat card",
      "omg same",
      "It will be a non-issue unless you're playing on the very edge of your system's capabilities",
      "I downgraded from 4080 to b580 bcoz i want to try intel's gpu, mostly play at 1440p.No regret so far",
      "A B580 is kinda the perfect GPU for a 3600x.",
      "same here bro, i upgrade from rx 560 4gb to b580 :) going to get mine soon",
      "Same here! Got the 3 fan sparkle from micro center and waited a good long while for it. What an improvement!",
      "I hope you had the GTX580 before that",
      "580 Went from prescription to over the counter 😅",
      "Was the card you had before the rx580 the gtx 580?",
      "That’s a great upgrade man congrats. What did it cost?"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580"
    ],
    "title": "The B580 scalpers were too quick..",
    "selftext": "Tried to buy one of the 3rd party B580 cards on Newegg and all of them sold out instantly..the closest I got was adding an acer nitro version to my cart and it literally sold out before I could checkout. So unbelievably frustrating. Already seeing them being listed on eBay for absolutely ridiculous prices of $380-400+ which at that price makes them a very poor deal and I’d be surprised if anyone actually buys them for that price when you could get something like a 6900XT for less than $100 more. I have a feeling once they restock more they will be sold out within minutes from all the bots and people who missed out on the first round. Still going to try though! ",
    "comments": [
      "One of the reasons I remain skeptical of the \"just wait for the next generation, the supply will be so much better and cheaper\" crowd. GPU's remain in high demand, and between people legit wanting cards for themselves, production limitations, and scalpers, it's not particularly easy to get new cards at launch. Especially when it's something as solid as the B580. \n\nGood luck getting your card. The restock alerts on Newegg do actually work pretty well, fwiw, but you need to be keeping an eye on your email and be ready to pull the trigger.",
      "Plenty of cards available for $300 like 4060 and stuff. No scalper is taking all this pain and tax and shipping for 20 bucks. \n\nIf they price it too high like 350 no one will buy, at 300 they cant make any money. Makes no sense to scalp these cards. I think most are genuine buyers.",
      "Been hearing that since 2020",
      "Cards on eBay make 0 sense anyways. They often have cards that are being sold as used that sell higher than New at Microcenter. People will buy them. If you were only trying from Newegg, that is why you didn't get one. They were available on Newegg as a preorder most of the day on the 11th. B&H had them for preorder for at least the first 4-5 hours this morning. It isn't some instant scoop up like you're making it out to be.\n\nIt is a brand new card. They are always 'low stock' in the beginning.",
      "Won't be any stock until the hype dies down or scalpers move on to the following generation.",
      "Take a winter vacation in Sweden, 40-50+ stock in most web-stores here...",
      "We need the GPUs to make it through.",
      "This and so much this. It is a for the people card. If some idiot is going to spend 4060 money on an Intel card, let them. This card should almost be scalper proof. Scalpers will buy everything fresh because the worst case, they break even. They won't make crap and 1 scam back on them, they lose hard. GPUs from eBay are riddled with people switching hardware and returning.",
      "\\> In Stockholm, the sun rises at 8:47 AM and sets at 2:55 PM in January\n\nMy brother in Christ. How do people even function. Is there a baby bump every sept/oct there?",
      "Well tariffs are coming",
      "Honestly the real scalpers here are Gunner. Dude their prices are so crazy high. Lol\n\nYou know it's bad when board partner pricing makes the eBay scalpers look like a good deal. 🤣",
      "NEVER buy anything from any scalper. If no one ever buys from them, they will eventually have to refund those cards and then those cards go back to the original retailers.",
      "I mean in a few weeks itll be fine, and then cheap on the used market when scalpers realised they bought too many",
      "Eh, you would think it's scalper proof, but there's an LE on ebay for $700 and a few AIBs for $400+. Scalpers aren't the brightest bunch.",
      "They can charge whatever they want, but click on Sold. They aren't selling for that. Only listed.",
      "It doesn't have to be selling to be scalped. But you're right, they'll find out quickly why you don't try to scalp cards like this.",
      "I'm in America. This is true for the USA. Outside the USA may have a different experience, but since I'm not there, I'm not going to make up crap to be inclusive.",
      "Their mark up is absolutely insane. Highly doubt they are any better than the LE model either. At least the sparkle, while being $20 more, was benched as quieter and more thermally efficient overall than the LE at least based on Steve’s testing.",
      "Brought one in micro center :)",
      "Keep checking Newegg.  After a swing/miss on the ASRock Challenger on Newegg (into cart, didn't place order in time), and maybe 100 or so \"refreshes\" over 12 hours, I scored an Intel Reference card this morning (and got the email auto notification).\n\nMicrocenter appears to be the main brick/mortar retailer that has them right now, they had both ASRock models available for pickup at my local one this morning.\n\nWas pretty pumped to get actual MSRP, all the 3rd party boards are marked up..."
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580",
      "arc b580"
    ],
    "title": " Arc B580 Overhead Issue, Ryzen 5 3600, 5600, R7 5700X3D & R5 7600: CPU-Limited Testing",
    "selftext": "",
    "comments": [
      "Thanks for posting this. People really need to wake up and stop using the ReBAR argument. This overhead issue is much worse than anyone could have imagined. I really hope Intel can fix this.",
      "A budget GPU that doesn't work well on budget systems. Incredible.\n\nIntel better hope this can be fixed in drivers because otherwise the B580 becomes pretty much unrecomendable.",
      "Most people will be pairing the Arc B580 with the Ryzen 7 9800X3D.\n\nClearly, this is a non-issue.\n\n...nothing to see here; move along\n\n/s",
      "Yep people were also trying to say its due to the processor being \"old\" as if that has any bearing on it with no instruction set requirements or features missing like rebar.\n\nI hope they can fix this as well as I dislike what intel did in the CPU market for a decade or so but their GPU division seems to be making a genuine effort (and the modern CPU attempts are getitng better), I want a great GPU competitor to the big two so we can hopefully drive the cosnumer cost down again.\n\nb580 is pretty good and looked really promising, hoping this can be fixed in the near future to make it onpar again with the competition regardless of a high end CPU, cant wait for them to enter the high end GPU market as well.",
      "I found out from some guys in PCMR that the B580 has fewer drawcalls than even the almost 8 year old RX 580\n\nHere's a comparison between them and a 7900 XTX and 4070 Ti Super for reference: https://imgur.com/gallery/arc-b580-api-overhead-comparison-u3UHMyZ\n\nI'm no software or hardware engineer. My knowledge on this type of thing is extremely limited, but from a short time Googling it looks like the more drawcalls are sent the more strain is put on the CPU. The Nvidia overhead talk from a few years ago makes a bit more sense to me now. But what doesn't make sense is Intel having *more* CPU overhead with a lot *fewer* drawcalls. There's something fundamentally wrong with their drivers still, or maybe the hardware. \n\nI hope they publicly acknowledge this soon and release a fix or at least some improvements. Because a budget card not playing nice with budget CPUs is a big problem for it's value proposition",
      "The issue happens on modern CPUs just one generation old like the 7600 and even the very well regarded 5700x3d. Which are both within Intel's supported cpu sheet. The \"old CPU so doesn't work\" argument makes no sense when the issue is this bad.",
      "but some guys on Intel sub told me I shouldn't pair \"ancient\" tech like R5 3600 and 5600 with B580 so this shouldn't be an issue. \n\nI'm sure most people will totally not use it with their 12100f's or 5500/3600's for their budget builds right? If they can't spend more than $250 on a cpu to run the $250 GPU in full power that's cleary user's problem. /s",
      "100% agree. We need Intel ARC to suceed, but they must adress their serious driver issues before that can happen.\n\nI think a lot of the backlash is due to[ this video](https://www.youtube.com/watch?v=Dl81n3ib53Y&t=696s) \\+ the vague official support spec info. The clip is one of the most egregious examples of misleading marketing I've seen, based on what we now know. **TL;DR:** F*or everyone with a 1060 and 1660 you can now safely upgrade to B580*, no asterisk about CPU or anything :C",
      "sarcasm bro",
      "The 3600 ***is*** supported. And there are tons of them out in the wild. It's not ***that*** old or ***that*** slow, and it's getting eaten alive here. Even the 5600 is leaving performance on the table. Looks like even the 7600 is too and that CPU is barely 2 years old...\n\nAlso, Coffee Lake is architecturally identical to Intel's 10th Gen. The only reason why the 10th Gen recommendation exists, ostensibly, is because it's the first generation where Intel is able to guarantee ReBar support. There's basically zero difference between a 9900k and a 10700k. If a Coffee Lake board has ReBar enabled via a bios update, it's going to perform the exact same as a 10th Gen equivalent.\n\nNo matter how you slice it, this isn't good.",
      "Well Intel is currently using a die that costs the same to make as a 4070TI to beat a 4060 when paired with a high end 9800x3d and it loses when paired with a 7600.... Something tells me the B770 won't compete with a 4080. I've just got this hunch.",
      "Watch the video, current midrange CPUs are losing performance.",
      "correct deduction, considering high price of 9800X3D, not much money will left for graphics card",
      "Look i can understand your optimism but man what kind of fantasy have you built yourself here lmao.\n\nThe B580 doesn't consistently beat a 4060/Ti which really means that there's no infinite scalling your thinking off.\n\n2nd of all the B770 won't be $349. And if it is it won't be anywhere near the 4080.\n\nWhat i also find fascinating is that in a post about real issue about a GPU you've somehow managed to spin it into something positive.",
      "Intel shooting themselves in the foot again.",
      ">Intel better hope this can be fixed in drivers because otherwise the B580 becomes pretty much unrecomendable.  \n  \nNeeds to #1 issue on their priority list now. There were probably loads of Ryzen 5600 or Intel 10400 (or any other similar tier CPUs) owners who might have been in the market to upgrade their GPUs to breathe a bit more life into their systems. The B580 was looking like the perfect candidate for those owners. But now that's definitely not the case. I would tell those people go scour your used market and try to pick up a 2080 Ti on the cheap.",
      "The Core i3-12100F and the Core i5-12400F are even slower than the Ryzen 5 7600",
      "Drawcalls aren't a physical thing, they're code submissions.\n\n\nWith that said, intel hasn't figured out how to submit a lot of them with cheap cpu usage.  Unfortunately it means you either need a newish processor,  or you have to run the b580 at it's maximum gpu limit (1440p) to make the video card the slowest component.",
      "[https://i.imgur.com/T8hu1M2.png](https://i.imgur.com/T8hu1M2.png)",
      "It's 4 years old and still being manufactured. The 5600 and 5700X3D are very popular among budget focused buyers. Intel is primarily marketing the B580 on its value.\n\nThe value GPU that doesn't work with value systems. Even the 7600, a modern CPU, shows some small issues.\n\nKeep in mind games are only going to continue getting more taxing on the CPU so Intel better hope to hell they can mostly address this with driver updates.\n\nAs it stands, this is a flop. It'll be fine for new builds if you pick the right CPU.\n\nBut it's a dud for the people just doing a GPU upgrade, which is a good chunk of what people spending $250-300 intend to do.\n\nIt also means a lot of the most popular bang for your buck CPUs that are currently in stores can't be considered. No 5600 or 5700X3D for you."
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580",
      "arc b580"
    ],
    "title": "Experience feedback for people who want to buy Arc B580 (a bit long post sorry for that)",
    "selftext": "Recently bought Gunnir Arc B580 to upgrade from GT1030. Everything was fine. Was able to uninstall the old driver using DDU on safe mode. Install arc driver. Play some games on it.\n\nOn next bootup, screen was stuck on glitched motherboard logo. I can't even access BIOS settings. Contacted the dealer to apply for replacement. The seller wouldn't accept replacement because my PSU is not Tier C and + on Cultist tierlist. \n\nBrought my GPU to a technician to check if my system is not compatible, or defective GPU. He had MSI MAG A-BN 650W PSU. Technician had same experience. He was able to boot for few times. Then experienced freeze on motherboard logo during boot.\n\nFor comparison, I changed the GPU back to GT 1030 and PC booted up normally.\n\nMy options are to send this to a technician for board level diagnosis and/or repair (more expenses ofc), or just give up on hopes of upgrading GPU.\n\nMy PC specs:\ni5 10400\nMotherboard: Asus h510m-v3\n2x 8GB Ram\nPSU: Cooler master Elite series 600W\nTP-LINK wifi card installed\n\nAny thoughts?",
    "comments": [
      "I’ve had my B580 LE in my old Z97 Xeon rig (no rebar) and it still worked pretty well.  Moved it to my new AM5 rig and also had zero issues.\n\nSounds like your GPU may be a lemon.",
      "Hey and tbh that's not representative of all experience with the B580. For example, mine has been running fine for about a month and I never had any difficulties besides the recent driver mistake by Intel.",
      "Either battlemage or you've got a defective GPU. Although my arc likes to throw weird stuff in bios sometimes too, but it boots at least",
      "I know where OP bought it because I bought mine from there too. His dogshit PSU pretty much killed his B580 lol",
      "Guess I'm cooked",
      "Sounds like the seller is sketchy. If contact your bank and try to get them to do a charge back. Never heard of a pain requirement for a GPU issue",
      "It sounds like the issue here is the vendor where you bought the card.\n\n* Is it a reputable vendor?\n* What is your location?\n* How did you pay for the card?\n* When did you buy it?\n* What did your technician say?\n\nSure it is possible that a bad PSU have broken the card, and it would probably not show the issue if you used the 1030, since it does not get PCIe power. I once had a xfx branded PSU that died and took my 780 MSI Lightning along with it. Shit does happen.\n\nThere are always a few bad products that make it past q&a. If you want 100% tested and validated products, you are going to have to pay for it, and you don't pay for it when buying anything budget oriented, that's just the reality of things.\n- and as you said yourself, it worked fine for a while after plugging it in. So that can be something that slips past q&a.\n\nIntel does have a professional lineup, where products are tested and verified. This is not one of them.\n\nIf you can't pressure your vendor into honoring your warranty, you should try to contact the brand or your local consumer service.\n\nIf the fault truly isn't your doing, the brand is going to want the card back for fault analysis, so they can avoid this issue on other cards.\n\nI would say that a cooler master PSU is proper for a budget build, as long as it is not 10 years old and otherwise working properly, I wouldn't be expecting that to be the cause. It sounds like you sadly got a lemon, and your vendor is trying not to honor their end of the contract.\n\nAnd do also keep in mind, you are having a beef with a Gunnir branded graphics card and a vendor that does not want to honor their warranty, not specifically with intel Arc. Gunnir will be the brand that has to honor the warranty, not Intel. - if the vendor honored their warranty, and gave you a replacement, would you have had any problems then? The problem here is not Intel Arc.",
      "You should’ve never went with Gunnir. Acer, ASRock, Onyx, or even Intel brand are the reliable choices.",
      "Fact op went with gunnir might mean the other brands aren’t available in his country but I digress",
      "Not all 600w psu's are equal... not by a long shot.",
      "Can't return it to where you bought it?",
      "No the drives didn't mess up the card itself but rather the software you use to control it",
      "The one that destroyed the graphics software",
      "I've got the gunnir b580. Works fine since December. Roommates bought it for me on the release date.\n\nSeems to pair nicely with 5950x. No driver overhead issues.",
      "This is a user error, not on the side of the seller. This user used an F tier PSU to run the B580 which posed so much risk in killing the gpu (which may have happened). The seller had clearly stated what can void the warranty in the product description (I know where they bought it). And out of virtue, they're requesting for the item to test on their own as well even though they don't need to given the warranty being voided.\n\nI would honestly want them to expound more as it seems like they left quite few details too.",
      "Rebar does help a lot for alchemist gpus (not sure about battlemage), but not having rebar support shouldn't prevent you from booting or accessing bios.",
      "Arc has their own problems, no one here is afraid to admit that. This just isn't an Arc problem. This community is really helpful to iron out those errors and finding workarounds until Intel makes official fixes for them.\n\nIt is the equivalent of complaining about a Mercedes car, because their after-market tire got a flat. - it does not mean that Mercedes is bad.\n\nThe vendor that refuses to honor the warranty is the culprit here, and if anything it is Gunnir that produced a lemon, not Intel.",
      "So contact Amazon via their chat or request a call and let them know that what the seller is saying. If you are in the return window then return it.",
      "Recommend specs for gpu say 600W... guess not.",
      "do you have ReBar enabled? Tell them you have a Tier C PSU and are what they say then, this seems like a DoA situation sadly"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580"
    ],
    "title": "Broke down and finally bought a B580...",
    "selftext": "I'm upgrading from a1660ti which has served me well but it's definitely showing its age this generation.       I ended up finding a sale on newegg for one thats just 50-60 over msrp and it seemed like a decent price for a brand new card.\n\nWhat am I in for?  Some people claim the card is an experimental purchase, some claim its the Holy Grail of the budget gamer.  I did a fair amount of research but I want to hear from you guys about your experience.\n\nThanks for your time in advance.",
    "comments": [
      "I have one it’s a pretty good one. It can run almost all popular games at high setting. I have friends who work at computer store telling me that it has the best performance for the price range. It is sort of similar to a RTX4060 (If not in some cases better). The downside is that the Intel software is not as well updated like Nvidia and AMD, but this is a problem that can be solved in the future.",
      "Helped a buddy build a new PC recently and we used a Sparkle Titan B580 and he’s been super happy with it.  Like not even a single complaint since. \n\n12gb of vram is also very nice compared to Nvidias paltry 8gb options.",
      "I just bought Intel B580 LE, they're available here in Canada. It's all good so far! Did a clean windows install. I have a feeling it has serious Fine Wine potential",
      "Probably cleans your registry hives of all the unused crap and legacy display drivers. To say nothing of services u “acquired” from bloat ware and forgotten software packages.",
      "I was already on an AM4 and didn't think it was financially responsible to get AM5, new RAM, and an expensive CPU in this economy. If it can run games and help me do work then it's all I need.",
      "You will find out soon. I bet it will be satisfactory experience.",
      "I've been doing research for weeks, I just want to hear some first hand accounts from a thread that isn't a month old.  Not my first Rodeo little bro lmao.",
      "**It still is an experimental purchase**, especially if you've bought it closer to launch. But the experiment is paying off very quickly since the card has been performing better as drivers roll out. I used to have a fair amount of frustrations with the card. Namely texture corruptions, artifacts, low 1%, stuttering, weird Windows errors, blue screens, etc. So many to name. As March came, I realized that **I no longer had weird problems with the card** anymore.\n\nI remember the drivers that came with Pirate Majima, it was doing all sorts of weird glitches to the ocean and the shadows were oddly pixelated. Infinite Wealth would freak out and dip to 3fps or crash every time a shark would appear. But since 5 drivers ago, none of those issues exist anymore. I've been playing Fortnite, Marvel Rivals, Zenless Zone Zero, Skyrim, Oblivion Remaster, The Forest, Minecraft (Shaders+Iris), Ghostrunner, and EVEN OLD GAMES like Sid Meier's Pirates. Red Dead Redemption used to run like ass, maybe around 50-70fps. Today, it runs 120+ no problem even at 4K. Emulators also run perfectly, if you're into that.\n\nAs long as your CPU is at least 5600 or 12400, you'll be fine. I used to rock a 5600G (which is just a 5500) so I had to upgrade into a 5700X3D to make the most out of my purchase (I was due for an upgrade anyway).",
      "I game exclusively on 1080p (aside from steam deck), and going from an almost decade old gaming laptop (6700HQ, 970M) to a desktop with a B580 is a night and day difference for me. Maxing out all settings and getting comfortable 100+ fps framerates is so good. So far, the only game i don't max out is expedition 33, but it still looks gorgeous with medium-high settings and 50-60 fps at 1080p. Overall, it's different for a lot of gamers tbh depending on our perspectives, game selections, personal preferences, and (sadly) regional pricing. For me? I am extremely satisfied.",
      "Not sure exactly why it works just know that it does.",
      "What are you in for?\n\nA good time mostly. Yes the drivers have a few screws loose but they have been pumping updates out at double speed recently fixing bugs. I really hate the deprecated the screen capture feature but everyone uses OBS anyway. Considering Intel's XESS is pretty dammed good and VSR actually works, it's already a leg up on AMD. It's RT rivals NVidia but at this performance levels, that should not be a consideration.",
      "My A580 WAS a holy grail since the only cards at that price range were 1660 or a crappy 3050. Then again my previous card was a friggin 740GT, so at that point anything from 2020 onwards is great",
      "Hearing this is really comforting because I've decided to buy a b580 as my next video card. I've been trolling this sub to see if there are serious breaking problems with drivers but it seems there are just a few small things. Hope I get it soon.",
      "The clean windows install makes such a difference.",
      "Picked up a Ryzen 9 5900x on the cheap cheap awhile back.  It actually massively improved my gaming experience in itself just not graphically per se.\n\nI know its not technically a \"gaming\" CPU but I don't only play games on my pc lol, it's an all around work horse.",
      "Nice one bro, yeah the issues are mostly with the drivers but given time I think it will be just fine",
      "I'm running a Ryzen 5 5500 and it seems to be perfectly capable. I'm probably leaving a little bit of performance on the table as it's PCIe 3, but from reports online it's not as much of a bottleneck as you may think.",
      "That CPU has sufficient firepower to handle your B580. Go forth and game.",
      "Agh, that's rough buddy. Although I think AM4 is practically viable for another 5 years lol so it's not too bad. You might get a decent sell value out of it in the future too. As it stands, games these days are far more GPU intensive anyway",
      "It's okay card the for the price it doesn't like older cpu gens because of the card architectures it design to run with newer intel cpus and chipsets."
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580",
      "arc b580"
    ],
    "title": "Intel Arc B580 Review, The Best Value GPU! 1080P & 1440p Gaming Benchmarks (HUB)",
    "selftext": "",
    "comments": [
      "Bravo intel arc, bravo",
      ">completely destroys the 4060 in most games\n\n\n>Almost reaches the 4070 in a few\n\n\nDamn and I thought the b770 was supposed to be the 4060 killer lmao.\n\n\n\nIn reality this will actually kill Radeon it seems.",
      "Is this the first time that a video card has exceeded the hype?",
      "this kills any card below the 7700XT in the market",
      "Holy shit",
      "Tldw;\n\n12 Game Average:\n\n***FPS @1080p:***\n\nRTX 4060Ti 8G - 92\n\nRTX 3060Ti - 83\n\n**Arc B580 - 77**\n\nRX 6700XT - 75 \n\nRX 7600XT - 73 \n\nRTX 4060 - 72 \n\n***FPS @ 1440p:***\n\nRTX 4060Ti 8G - 63\n\nRTX 3060Ti - 58\n\n**Arc B580 - 57**\n\nRX 6700XT - 54\n\nRX 7600XT - 51\n\nRTX 4060 - 50\n\n***RT FPS @ 1080p:***\n\nRTX 4060Ti 8G - 75\n\nRTX 4060 - 56\n\nRX 7700XT - 47 \n\n**Arc B580 - 45**\n\nRX 7600XT - 29\n\nRX 6700XT - 26\n\n***RT FPS @ 1440p:***\n\nRTX 4060Ti 8G - 52\n\nRTX 4060 - 37\n\n**Arc B580 - 32**\n\nRX 7700XT - 32\n\nRX 7600XT - 19\n\nRX 6700XT - 17",
      "Are we kidding? The video kept saying it isn’t mind blowing performance but to me it 100% is.\n\n$250 to get, at a minimum 50 fps, but mostly 60 fps in games running ultra settings at 1080p.\n\nI think we need to remember this card was never going to be top of the line and set expectations straight. This is incredible performance, in my opinion; way better than I was expecting to be honest. \n\nI was waiting to see if a b770 would be release but man, this is very tempting.",
      "The B580 even beats the A770 across the board; if you're interested in a generational uplift and don't want to wait for the halo card later in 2025, grab the B580. :)",
      "Legit not expecting it to beat the 4060 but Dam it did",
      "Looks like they have ok power draw at idle. Apparently it was an issue with the A770.",
      "Matched 4060ti in various instances.",
      "Kudos to Intel ... I was kind of thinking they were going to get roasted or at least luke warm praise for the new cards but nope they have done a solid job.  Curious as to why Steve u/Seriously GN failed to test Blender, DAVinResl etc with the B580 as I was looking to see if those results had also improved.  \n  \nSeriously well done by Intel and Arc card is definitely on my list for my new PC.",
      "There's even the odd benchmark showing it competing with the 4070, though of course in the general case it does not.",
      "Sure, but paying $50 more to get an extra 4gb of VRAM is totally worth it, in my opinion. These results are very very good in my opinion.\n\nI’m excited to see what else comes out",
      "Gamernexus shows 34-36 idle pd. Its not resolved yet.",
      "The B580 looks very promising! :)",
      "Is there really any reason to go with any other GPU in this performance bracket now? Obviously will have to see what Nvidia has in store for the 5000 series but highly doubtful a 5060 will be comparable, especially not at $250.",
      "that's super helpful. seems like this is going to be my next GPU.",
      "MLID on the ledge!",
      "if i'm running this in a b450 motherboard with pci-e 3.0 am i going to take a performance hit?"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580"
    ],
    "title": "Sparkle B580 in stock Newegg",
    "selftext": "They’re in stock now, GO!",
    "comments": [
      "Aaaaand it's sold out after 3 minutes...\n\nEdit: Holy shit, I kept hitting refresh and I got to the checkout screen and paid for it.",
      "every time you don't post the link, god kills a kitten\n\n[https://www.newegg.com/sparkle-intel-arc-b580-titan-oc-12gb-gddr6/p/N82E16814993013?Item=N82E16814993013](https://www.newegg.com/sparkle-intel-arc-b580-titan-oc-12gb-gddr6/p/N82E16814993013?Item=N82E16814993013)",
      "Lucky, I was ready to pay for it but was so surprised that I forgot my cards ccv and was wondering why it wouldnt let me pay for it. When I realized it was too late.",
      "Lmao completely overlooked in the rush to ring the bells 😂",
      "No, ppl have success. Just cus you didn’t or didn’t have the patience to attempt again in case someone who had it in cart failed to purchase it doesn’t mean someone else won’t be able to",
      "Damn, OOS in minutes.  I didn’t even get the email from NewEgg.",
      "womp womp",
      "Despite me having a bookmark folder with all the b580 links and checking them every 10 minutes, I did find op’s post helpful. I can understand your frustration not being able to get it even with posts like op’s (I couldn’t), I did get to see the “add to cart” button that I had never been able to otherwise.",
      "I just got one. Kept spamming buy till it let me, don’t stop refreshing and trying",
      "Yea they go quick, some ppl here get upset when someone posts these stock posts but ppl across Reddit have managed to grab one cus ppl alert others",
      "I’ve been watching this sub and my email all day.  Walked away for a couple mins and missed it.\n\nI just wanna upgrade my old Vega56 at a decent price.  That seems like a huge ask right now.  Lol.",
      "Right there with you had the cc code in there and confirmed and bam out of stock",
      "HotStock app on iOS. Don’t pay for their bs upgrades. I still barely got one after a few close calls. Newegg seems better than amazon.",
      "Bro, you're supposed to be ready at the drop of a hat.  I got my ccv memorized just for this. With the rest of the card information saved on Newegg. \n\nYesterday Newegg notified me when it came in stock but I was super busy at work and missed it. I didn't get a notification for this restock, even though I renewed the notification thinking that having had the notification canceled it out.",
      "Nah, bigger cooler and an extra fan mean better temps which means lower power draw and better/longer boost clocks",
      "I don't even need a new GPU, but those SPARKLE cards look awesome. Love the color and heatsink!",
      "Out of stock already",
      "They have some of the asrock Challenger at memory express in Calgary, Canada. I picked one up yesterday",
      "thats what happened to me yesterday I am super happy",
      "out of stock"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580"
    ],
    "title": "How are most people buying a B580?",
    "selftext": "I am so curious how most of you are getting it.\n\n1\n\nA) paying more than MSRP\nB) paying MSRP\n\n2\n\nA) in a brick and mortar \nB) online\n\n3\n\nA) had a notification \nB) just got lucky \n\nI am just a casual looker as I really like my A750. But if I saw one at Newegg, I might hurry up and buy it.",
    "comments": [
      "Getting really lucky lol.  Walked into Microcenter 4 days ago and they had 3 B580 LEs sitting on the shelf for $249.99 MSRP.  Grabbed one quick then walked past people standing in line waiting to pick up their 9070s, 9070xts, and 5070tis lol.",
      "This sub got me mine. Happened to refresh the subreddit right as someone posted about Onix dropping their first set of cards on Newegg. Paid MSRP (plus state tax). \n\nSo to answer the direct question, I paid MSRP online after getting lucky",
      "Used a tracking website and grabbed one through BH.",
      "Randomly searching new egg at 9am at work one day. Got a b580+650w 80 gold power supply for $340",
      "God I wish Arizona had a microcenter",
      "I use hotstock app and nowinstock discord.  Got one on newegg at msrp.",
      "Ok, I'm curious - isn't having Microcenter in your state just one condition to be able to shop there. Arizona is big - even if you had the chain in the state, isn't there a big chance the nearest store would be far-far away. I know the American definition of a short drive is completely different from the European but still - would you drive 5 hours to a store?",
      "Depends. If they were going to open one here it would likely be in Phoenix since that's really the heart of the state, Phoenix is a big city but it wouldn't be an *insane* distance to travel compared to driving out of state. But yeah preferably one in my local area would be the best lol.",
      "I'm in the UK I paid £269.99 for a sparkle b580 through overclockers.",
      "Got mine at Micro Center. Total RNG",
      "Microcenters between Wed-Sat\n\nI have seen them get 15+ that sell out same day or within 2-3😭",
      "Best Buy in Canada, got mine for C$369.99",
      "1. I woke up at 6am\n2. I logged into new egg\n3. Dunked my head into spring water\n4. Added card to my cart\n5. Checked out\n6. Went back to sleep",
      "I went to the Tustin microcenter this weekend, they had tons of 5070s and b570s, plus a few 9070s. No 5070ti-5090s, 9070xts, or b580s though. Also the 9070s sold out when I revisited a few days later.",
      "Hi there!\n\nI live in a country where not a lot of people are interested in Arc (and even Radeon) GPUs, because of that they're pretty much always in stock here so I suppose it does fall under the \"I got lucky\" category~",
      "Microcenter and getting them with PSU bundles seems to be a common thread",
      "I got one of those that exact same way as well. Haven't even used it yet because I had just purchased another one through B&H Photo like 2 days before",
      "Yeah 😅 crazy how is bad it is over the pond",
      "My current drive to a microcenter is 7 hours including 2 10 minute stops. If I had one 5 hours away it would be worth a day trip imo but I would *just* get a b580",
      "1B 2B 3A"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580"
    ],
    "title": "Price check! How much does the B580 cost in your country?",
    "selftext": "In europe the price is all over the place, I wonder if the prices will drop to a stable number anytime soon, so I wonder how much if cost in your country right now?\n\nThank you for sharing.\n\nHere is some of the comments summarized (AVG):\n\n||\n||\n|Country|Price|Additional info|\n|Canada|357 (CAD)|AVG|\n|Denmark|320 (USD)|\\~|\n|Germany|309, 330 (EUR)||\n|Netherlands|304 (EUR)||\n|Turkey |320(USD)|\\~ AVG|\n|Ukraine|320(USD)|\\~|\n| Malaysia|310(USD)|\\~|\n|Australia|285(USD)|\\~ AVG, all models|\n|Japan|335(USD)|\\~AVG|\n|Poland|330(EUR)||\n|Costa Rica|340(USD)||\n|Romania|310(EUR)|\\~AVG|\n|Hungary|339(EUR)|\\~AVG, all Models|\n|Norway|372(USD)|\\~|\n|Sweden|308(EUR)|\\~AVG, All models. Some data not included|\n|Singapore |430(SGD)||\n|Bangladesh|300(USD)|\\~|\n|UAE|377(USD)|\\~|\n\nSorry if I missed some of the countries.",
    "comments": [
      "Ontario, Canada: $360 CAD at Canada Computers.",
      "In Denmark it is 2599 danish kroner which is around 346,5 euros.",
      "Assuming you mean American. When people don’t say where they are from you can guess American because they tend to forget the rest of the world exists.",
      "Msrp Canada at Memory Express. $349.99 CAD. Limited stock but stock keeps popping up.",
      "N/A Malaysia",
      "Germany 309€ in Stock",
      "Right now only pre-order, for about $333 sparkle and asroc, there are resellers for $380 asroc and $476 le\n\nI got for $259 le in bph, shipping $17 and tax hopefully $35 total comes out to about $315",
      "Got the LE for 260usd plus taxes (8.25%)",
      "Bulgaria - cheapest is 327 euro (639 BGN)",
      "Ukraine: SPARKLE Guardian (2-fan variant) = **$320** (tax, import tax, shipping included)",
      "Between 8000 and 10000 local currency which converted to USD is around 386.69 - 483.36 Dollars on Amazon and Aliexpress and only the overpriced Gunnir version, no local computer stores sell it.\n\nHilariously enough, scalpers on Ebay sell it cheaper here for around 6600 which is around 300 Dollars lmao.\n\nFor comparison a 4060 on Amazon is 8000, and on Aliexpress is 6000.",
      "390 and 480USD trough Amazon mexico, shipped from the USA",
      "It's sad to say that we don't have them \nLike not even scammers or non official retailers have them, and that's for one specific reason. \n\nIf I'd try to buy it from Amazon, let's say for 299 USD, it'd cost me at least 385 dollars. \n\nJust for information in our country, we have only opened boxed A770 for 205 dollars, not used, and its some guy selling it, not a shop.",
      "Australia: \n\nAUD$449 for AsRock Challenger B580 (US$279)\n\nAUD$459 for Intel B580 LE (US$285)\n\nAUD$469 for MAXSUN Milestone B580 (US$292)\n\nAUD$479 for AsRock Steel Legend B580 (US$298)\n\nAUD$489 for MAXSUN iCraft B580 (US$305)\n\nAs a reference the Intel B580 LE is pretty close to MSRP plus local 10% GST at current exchange rate.",
      "Japan, here:\n\nSparkle Titan/As rock challenger - ¥50,000 (330 USD)\nAsrock Steel Legend - ¥53,000 (341 USD)\n\nFor comparison:\n\n4060 8gb - ¥46,000 - ¥50,000 (296 - 330 USD) \n4060ti 16gb - ¥75,000 (483 USD)\nRadeon 7600XT - ¥55,000 (354 USD)\n\nSo not too bad in comparison to other cards. I built up a system with The Asrock Steel Legend last week and its running nice.",
      "Yup. Went in and asked to be put on the list. As soon as it came in, he gave me a call. $350 + GST. No HST here.",
      "It costs $349 at memory express",
      "Dang, teach us your secrets",
      "I bought the LE in Canada for $359 CAD, from Canada Computers.  Haven’t installed it yet as I’m finishing collecting the rest of my PC parts I’m building around it. Almost there.",
      "340$ - Costa Rica"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580",
      "arc b580"
    ],
    "title": "Intel Arc B580 Overhead Issue! Upgraders Beware",
    "selftext": "",
    "comments": [
      "For people who didn't watch the video, there's something weird going on. With a modern processor the B580 often beats the 4060. With the 2600, the 4060 is pulling ahead.\n\nLike in Spiderman Remastered the B580 is 20% ahead when both are using the 9800x3d. But with the 2600 it's 40% SLOWER than the 4060 also using the 2600.\n\nCPU scaling doesn't do that. \n\nWith a slower cpu bottlenecking the computer, we should see framerates be extremely similar. But they aren't.",
      ">\"Just reviewers not doing research before they open their mouth. Nothing new.\"\n\nI mean did you do any researce before posting that? This has nothing to do with Rebar. The Canucks video and this clearly say that.\n\nThis overhead also appears to be an issue with more modern CPU's like the 3600 and even 5600 even if not to the same degree. \n\nHell there was a post on the ARC subreddit pointing out this problem with a 5700X3D and i've even seen a 7800X3D example.",
      "It's an issue for anyone upgrading from an older GPU since apparently even the [5600 has issues](https://www.reddit.com/r/hardware/comments/1hsjqb2/intel_arc_b580_massive_overhead_issue/m5651pi/), especially in [\"CPU limited scenarios\"](https://www.reddit.com/r/hardware/comments/1hsjqb2/intel_arc_b580_massive_overhead_issue/m58kt64/).\n\nI'm sure we'll probably see more benchmarks with more CPUs since this seems to have only just been found out, so we'll probably know more soon.",
      "No. With a modern processor the B580 was ahead of the 4060. With an older processor, the 4060 pulled ahead of the B580.\n\nThere's something going on.",
      "Even the Ryzen 5 7600 [isn't good enough for the Arc B580](https://x.com/HardwareUnboxed/status/1875378992871809367).\n\nYou have to open your wallet for a Ryzen 7 9800X3D.\n\nNaturally, the Ryzen 7 9800X3D is the processor that most people would pair with the Arc B580. \n\nOh, wait!",
      "even with something as modern as [ryzen 5 7600 b580 can lose up to -33% performance](https://x.com/HardwareUnboxed/status/1875378992871809367/photo/1). I want you to look at me with a straight and serious face and tell me that ryzen 7000, on the ddr5 platform, is an old outdated and slow cpu platform and does not meet the system requirements to run a $250 gpu. Please go ahead, and thank you.",
      "In the same page you link Intel lists \"most 5000 series Ryzen CPUs\" and on the [test HWUnboxed did](https://youtu.be/00GmwHIJuJY?si=oHCcQWXaKlhZwJzC) it shows the 5600 suffering from this overhead issue. Your own source proves you wrong lol",
      "The card specifically made for budget PCs suffers from diminishing returns using budget CPUs. Even a relatively modern and decent 5700x3d suffers from the problem, yikes",
      "going from 9800x3d to ryzen 5600 (or 12400f/12100f which perform about the same), rtx 4060 loses about 13% performance. b580 loses [***50%***](https://x.com/HardwareUnboxed/status/1875378992871809367). Also with a ***ryzen 5 7600*** b580 goes from defeating 4060 with a healthy +20% lead, to losing by -10%. \n\nNow, please reply to me \"the 2 year old ryzen 7000 cpus on the am5 platform, with fast ddr5 6000+ ram, is old hardware and isn't fully compatible with new hardware that supports modern features\". \n\nplease go ahead.",
      "This is just blatanly wrong. Rebar or atlest the underlying tech behind it is a PCIE2.0 standard lmao.",
      "And they used the 2600 with Rebar. Rebar works on the 2600.\n\nIt always has with the correct bios and thats facts. Steve knows that and you should know that.\n\nRebar has nothing to do with whats the issue being pointed out in the video. Your latching onto the system requirements page when the same issue can be observed on Ryzen 3000, 5000 and even 700 cpu's.",
      "https://youtu.be/00GmwHIJuJY?t=111\n\nAlthough i would recomend the whole video as it goes more into the underlying issue of the B580.",
      "No one bothers to read Intels page on system requirements that pretty much said you must have 10th gen Intel or AMD Ryzen 3000 or later as those systems are when Rebar is introduced.\n\nJust reviewers not doing research before they open their mouth. Nothing new.",
      "Ryzen 5 7600 apparently [isn't \"modern\" enough](https://x.com/HardwareUnboxed/status/1875378992871809367)\n\n...gotta open your wallet for a Ryzen 7 9800X3D",
      "I'm just gonna keep saying this: Given a Ryzen 7600 isn't enough, this is a straight up defective product. Everyone who has one should return it, anyone thinking about buying it should avoid until the issue is either sorted or the price drops pathetically low.",
      "I'd say a large portion of gamers who buy budget GPUs have older systems and don't have enough to also do a whole motherboard, ram and CPU update",
      "Nononono.\n\nWe know that a 4070 will perform worse on a 2600 than a 5600. That's not what the video is about at all.\n\nWait, what do you think the video is talking about?",
      "You could very well be upgrading from 3600/RX 580 or GTX 1660 to this GPU. If your budget was 600€ back then and you want to upgrade for 300€, the reviews would have you believe that B580 is a good upgrade path over 6600XT, but you would be mistaken.",
      "Are you saying that a 4070 would suddenly start performing **worse** than a 4060 with a potato CPU? If so, I'd love to see the benchmarks.",
      "5700X3D also experiences these issues."
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580"
    ],
    "title": "I've tapped into black magic. RDR2 running 4K60 ultra without upscaling on B580. Thanks to Lossless Scaling Adaptive FG and the old friend GTX 1080 Ti in dual GPU setup",
    "selftext": "",
    "comments": [
      "Funny how dual GPU setups are starting to become more popular nowadays.",
      "On reddit. Nobody I know of outside of reddit and specific tech youtubers give a damn.",
      "So you're actually playing at 30fps. I guess it's borderline fine for controller games\n\nI'd much rather use upscaling than FG to achieve a target fps though, if possible",
      "May I know why dual gpu is becoming popular?",
      "Dual GPU is primarily becoming popular again for the \"budget\" gamer.   \nbecasue getting a $300 gpu is much easier to acquire than a 700+ gpu. 'specially when you keep the old one.\n\nvia a steam app \"lossless scaling\" its an open source software based Upscaler/Frame Gen for \\~7$ (Opposed to Hardware bassed DLSS/FSR) in the app it has an option to do the process of the Frame generation and upscaling on the second GPU if you so desire. \n\ndue to being software based it works with (practically) any game regardless of developer support.\n\nthe popularity is its crazy quality for its crazy price.",
      "What was old becomes new again",
      "Lossless scaling allows you to offload its processing to a secondary GPU. Run the game on one, and run LS on the other.",
      "The real trick is to use upscaling to hit 60, then use FG to hit 120 or higher.",
      "Acquire Lossless scaling via steam  \ndig through settings to find a Dual GPU mode,   \ntell it to game on main GPU, tell it to process on secondary GPU  \nset you upscale as desired, set your target FPS as desired.  \nplug display cable into the secondary GPU to see results. \n\n(this is the best way I can explain without having any direct interaction, this is just what I have learned from the casual doom scrolling I get.)",
      "Can you describe a little more about how this is set up?  I currently have both an arc a750 and a 5070 ti.  I've been trying to sell the second card and system it's in, but Facebook folks only want free shit or to trade used, poopy towels for stuff, so I'm considering keeping it and doing a dual GPU thing, but I'm really struggling to come up with any way in which it works and is actually useful.",
      "I don't care what anyone says, this is cool af regardless of whether it's worth it or not.",
      "Yes, I second this.",
      "So same as SLI",
      "They used Lossless Scaling as a way to generate more frames which i assume the program uses the GTX 1080 TI for the frame gen to not strain the b580",
      "allegedly yes. May very by game.\n\nit's generally recommended to not because the Generated frames create the feeling of input latency. where you click on a fake frame, you do still have to wait for a real frame for it processes. (same concept as DLSS/FSR)\n\nif you just upscale, this aspect should't be an issue. (again similar to DLSS/FSR)",
      "Lossless Scaling's input latency is too painful for me.",
      "Yep. Pretty much.",
      "I bet it fades just as fast if not faster too.",
      "My brother in Christ mentioned the 1080 ti and frame generation like its no big deal",
      "> I second this\n\nWhat are you, an extra GPU in a setup?"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580"
    ],
    "title": "Update! B580",
    "selftext": "B580?  \n\nIt looks like it could be a 580 for Christmas.  What do you think?  \n\nUpdate*. Looks like a joined the Intel club.  Hopefully will be quite the upgrade from my 1050 ti.  ",
    "comments": [
      "Congrats and Merry Christmas!",
      "Congrats!",
      "Congratulations! I'm still working out the kinks for mine (I want it to be an eGPU setup), but I love the card, and it was very capable when I put it into a native PCIe slot. Enjoy your new card!",
      "1050 ti? You're going to love it",
      "Enjoy dude I'm tempted to get once to replace my RX580, it all really depends if AMD releases anything competitively priced or not",
      "I was so hoping for socks.",
      "in splinter cell pandora tommorow indonesia embassy mission \n\nwhen lambert says merry christmas fisher he means now it can kill many people he want",
      "The Thunderbolt setup that I had was an extreme bottleneck. It had to run through my iGPU, and it seemed like the Thunderbolt port was rate limited (possibly because it was going into L1). I'm getting an M.2 eGPU setup though, and that should work a lot better.",
      "I have an Asus Vivobook 14 OLED. It has a pretty capable CPU (i5 12500H), but the Intel Iris Xe iGPU really kills it. The screen is phenomenal (1440p)",
      "i too have rx580 and am just reading all the driver support issues people may be having with older titles but also recent titles like valorant have fps stuttering. just out of curiosity what game stop support for your 580 to make you wanna upgrade?",
      "Looking really great you lucky bastard! Enjoy 🎅",
      "Make sure you can enable resizable bar",
      "Congratulations! Interesting times where people now prefer Intel GPUs and AMD CPUs.",
      "Nice, does that work with your existing card too?",
      "Let me know how that goes I also want to egpu a b580 eventually currently using a 1080 ti",
      "No game stop support really, I've had it since new so bought it back in April 2017, so really I'm just due an upgrade, I'm going to wait though because I know new releases are coming from AMD and Nvidia, so I will pick the brand I want and the best price Vs performance graphics card I want.",
      "curious as to what laptop(?) you're running yours on. external gpu setups are always neat lol",
      "Probably will be a substantial upgrade to the 1050ti",
      "Great upgrade!",
      "Lucky… I’m still waiting for mine"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580"
    ],
    "title": "UPS stole my B580 and I leave the country for a year on the 6th :( Newegg will only refund and told me I will have to order again when available :( Wanted to rant :c",
    "selftext": "",
    "comments": [
      "Cali, limited release item, day 1 never updated again, UPS confirmed it was \"lost\".",
      "It’s the holiday season, so it could be “lost” in a back log of packages. Trust me I work in a package sorting facility and they get swamped this time of year",
      "Can I ask how you know it’s stolen?",
      "Hello u/[ducktoucher0](https://www.reddit.com/user/ducktoucher0/) ! Can you please send me your order number on that Acer B580 GPU via Reddit chat? I want to look into that tracking with you and will assist the best I can. Thank you!",
      "Newegg W",
      "11 years old acc with 11k comment karma. If its a fraud its a really good one lol\n\nEdit: Also, some companies do that. When my GPU arrived and I had issues with it (I got a dead PSU as it turns out), so intel customer support reached out  to me in DMs. Good to see good customer support",
      "I will hope for the best then. My time frame definitely hurts my chances lol 💙",
      "Highly recommend using Newegg support through Reddit in the future. If anybody needs help, they were extremely helpful and seem to know what they're doing, no offense to regular Newegg support. They launched an investigation with UPS and whether it was from my phone call or theirs, my shipping status was updated. So everything worked out great, I hope lol",
      "Ya but would cost $70 plus import tax is 10% of retail. So $100. Id be better off getting another gpu. \nI have a steel legend on backorder with b&h, they are rumored to be shipping on the 2nd, with 2 day shipping. I only hope it will fit in my PC xD Also, it doesn't fit the color scheme, but who cares, it's a beautiful card lol",
      "Yes that is for sure. Do you have someone at your current address that could forward it to you if it does arrive?",
      "Just give it some time. It didn't update for me for a while either. Look for the date of next expected update, and if it hasn't updated by then you should be concerned.",
      "Unfortunately, UPS confirmed it was lost already. Also, it being in Cali basically doubles down on that lol.",
      "Wait Newegg is being nice now wtf??",
      "At least you got your money back",
      "Damn people be stealing Budget GPUs too?! Bruh",
      "W support. Rare to see",
      "people have been stealing candy from kid, so these villian will do their thing as always\n\nI hope they have stomatch issue for a year",
      "Idk if it's to be trusted tho. Like how do you know they're genuine?",
      "Oh, that's unfortunate. I was just speaking from experience. Mine was like that for almost a week before it updated. Still, give it some time. Maybe it'll turn up in their system.",
      "UPS is hiring thieves this year. They stole my brother's $2K Alienware laptop on Black Friday and it's been stuck in Maumee, OH for the last 3 weeks. Dell is still looking for it."
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580",
      "arc b580"
    ],
    "title": "Intel Arc B570 \"Battlemage\" GPU Tested In Geekbench: Roughly 12% Slower Than Arc B580 For 12% Lower Price",
    "selftext": "",
    "comments": [
      "%1 down the price for every 1% performance lower performance? That's absoutely incredible\n\nNvidia and AMD could never",
      "Let's make sure it doesn't need a 7800X3D to meet it's potential.",
      "Hopefully it is tested with midrange CPUs as well",
      "No, they aren't. These already showing its age. Intels drivers are Ok. It will be getting better over time. But soapy FSR scaling can't be fixed on current AMD gen cards, neither RT performance. Additional 4 GB of VRAM can't be added either. \n\n\nRegarding value. As per article, B580 still holds its value even when paired with 5600. Now look at it this way - you get nice performance bonus sometimes, pairing it with faster CPU. Which is additional value (I mean - upgrade path).",
      "Well, driver overhead is when cpu hits the wall and can't feed GPU fast enough. So GPU has to wait. Therefore common sense says that for lower performance GPUs driver overhead should be less noticeable. Of cause, RTX 4090 have the biggest driver overhead problem - just look at performance difference between 9800x3D and ryzen 5600 when using RTX 4090. :))",
      "great now hope we can actually get them for near msrp.",
      "Me still with a GTX 1080…",
      "Kind of missing the point there. AMD cards suck at RT and upscaling because they're cheaping out on hardware RT and AI accelerations, as well as encoding and others. Their die size is tiny mainly because they're skimping out on those features, their GPU design department was divided into RDNA and CDNA so they could skimp on features they think a consumer grade GPU doesn't need. They're (barely) cheaper than nvidia cards because they really are cheap to produce. Look at RX 7600's tiny die fabricated on last gen node. It should be much cheaper than it is now. Why is their driver better? Because they had acquired ATI long ago, one of the oldest high performance GPU producers, far older than Nvidia. They had been toe-to-toe with Nvidia since the late 90s.\n\nI don't think their last and current gen GPUs are necessarily good value. 6700 XT at $330 is definitely not good value, it's barely even more powerful than RX 7600 in rasterization in some games, and the gap is even smaller when you compare it to B580, with B580 dominating at some titles. It used to be $269 or something at some point last year but I guess the stock has run out. RX 6600 has been $190-200 since forever. 6600 XT and 6650 XT also haven't been cheap for a long time. In the used market, the supply of cheap RDNA 2 GPUs has also dried out. \n\nAnd unlike Radeon GPUs, B580 isn't only good for gaming. It has good hardware acceleration for video editing, streaming, 3D modelling, and such. Alchemist was also like that. Unlike intel with their CPUs, they don't nerf their GPUs or make arbitrary distinction between consumer grade and workstation grade products. $250 is well worth the money for a well rounded multi purpose GPU, the second best you could have with that money is an old RTX 3060 8GB.",
      "That's amazing.\n\nBut also: for the love of God, give us the B770 already 😅",
      "Intel 13400f is like $130 and fast enough to feed a B580.  You don't need a midrange CPU, even a low end one that's not 5 years old works.\n\nYou can get an i9 12900k for $290, and it'll be way more than sufficient for any game for a long time.  As is the i7 12700k for $200, honestly.\n\nYes, the 9800X3D is great.  But not strictly necessary.",
      "Because AMD constantly do this thing where they have a specific GPU they want to sell, they then price a significantly slower GPU B right next to GPU A in price to entice people to pay more for GPU A, for example the 7900XT and 7700XT were intentionally priced poorly to try and get people to buy the XTX and 7800XT",
      "Won't the B580 perform well still with a 7600x/9600x? I think (not sure) the performance drop happens with much older CPUs and the 7600x should be fine, you may lose some performance in some games but for the majority it would still outperform the 4060 right?",
      "Tbh had my RX6600 for 2 years and was always plagued with this annoying stuttering issue. Tried multiple fixes over that time nothing fixed it. Drove me nuts. Replaced it with a B580 recently and there is the slightest performance dip but thank the good lord the stupid stuttering is gone. All of this paired with the legendary 5600. Imo the overhead things blown up for no reason I’m honestly getting the performance I expected and hopefully it improves with driver updates or else I’ll just have to save up and pay the nvidia tax down the line somewhere lol.",
      "Looks like the price to performance is acceptable, then.",
      "Using this standard, Nvidia is giving you great value if you consider something like the 4070ti is 60% less expensive than the 4090 while giving you 40% less performance!\n\n  \nI don't think it's a great benchmark for value. Using something more standardized would be better ($/fps etc)",
      "rx 6900xt for under 250$? ill believe it when i see it buddy,full working with proof",
      "If they actually release it in volume, it's pretty darn good value for 10GB. 8GB isn't acceptable at all anymore, but 10GB is ~okay for the lowest of the low end cards, which this is.\n\nBut yes, I get your point; It may be 12% for 12%,, but it's practically more than 12% because it's losing ~17% of its VRAM. Upvoted. I think people missed your point.",
      "Wait for drivers maybe? Idk.",
      "this. I feel kinda weirded out by all the fuss tbh. Yes, the overhead issues aren't pretty. But all you need is a reasonable setup. Pairing a $150 CPU with the B580 (a $250 GPU) will result in fine performance. If you use your >10 year old CPU and jam the newest gen GPU into your system...what did they expect?\n\nAs far I know Intel even has 11th gen or newer as system requirement.",
      "Iirc 7600 already sees drop but its mostly caused by using old apis in games. Modern apis handle it much better and closer to top cpu performance"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580",
      "arc b580"
    ],
    "title": "Intel Arc B580 rumored to get custom dual-GPU version with 48GB memory",
    "selftext": "",
    "comments": [
      "It will be sold out for eternity.",
      "Its not for gaming but for AI. If this is released then the 4090 will lose value on the used market.",
      "...***what***?\n\ntwo cores one pcb? we doin an asus mars 760 moment? intel is bring sli back?",
      "They sell it for $700 5090 GPUs become scrap metal overnight.",
      "It really wont, cause its significantly stronger single chip and 32GB of vram (compared to 2x24GB when memory pooling probably wont be a thing) will be still superior for many tasks.\n\nThat said, this is interesting and exciting news, no doubt.",
      "but it would still just be 24GB per GPU I guess and not shared?",
      "I imagine it will just be 2 cards on one board each using their own x8 pcie lanes",
      "AI workloads can use memory across multiple GPUs. Probably it won't work for gaming though unless they have a new approach.",
      "hopefully it should make the standard version more available though",
      "It depends on the workload. AI and workstation stuff can pool the memory, hence people building workstations with 2-4x 3090s since it's a cheap way to get lots of VRAM and decently fast GPUs. AI clusters have even more GPUs working together through special networking.",
      "9800GX2 was fun with literally two sandwiched 9800GTX PCBs and chips on it.  The PCBs faced inwards.",
      "If this comes out I might pick. I'm curious on the performance of it. I'm getting annoyed with Nvidia ATM and want to switch. Already said I'm getting Celestial when it comes out.",
      "24 vram is great for inference (load deepseek) but bus speed is not great for training",
      "They used to do this a lot on 90 series cards. Take 2 80s and make something 50% stronger.",
      "Not so sure. If this special version is from one of these China-only brands, I doubt it will affect the availability of the base B580 in other markets.",
      "Probably requires bifurcation support from motherboard to make the most out of the two GPUs",
      "5090: FORTY EIGHT?????? Nah I'm Cooked.",
      "Most games don't support multiple GPUs, most programs can pool 24gb of memory from two separate GPUs.\n\nIt can sell well for a very small portion of people, for most it will be useless due to the price to performance ratio.",
      "a bit overkill, no?",
      "In regards for ai??? Let’s say cost to produce is $500 for 2 2.4 GHz 20 core at 48GB DDR6? That’s the max throughput of a GEN 4 256 Bit bus, and they sell for $700 ? Even a $1000 , this would be a amazing enterprise cards considering a 5090 sells 3000 consumer and probably double enterprise , minimum, make it make sense"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580"
    ],
    "title": "New B580 build",
    "selftext": "My B580 arrived today. \nI've paired it with an i5-12400 and 32 gbs of ddr5 ram",
    "comments": [
      "So much left over space. I love it.",
      "12400 is locked so there's no point getting an AIO. It will do fine with single tower air coolers like Thermal Assassin X, Arctic 36, SE 214 XT, Hyper 212, AK400, Frozn A410 etc",
      "Please upgrade from the stock cooler. Otherwise, awesome",
      "“Did you overclock your gpu” 🤓☝️",
      "Don't bother with AIOs, they have problems with long-term reliability. The 12400 isn't exactly a hot-rod anyway, so a more powerful air cooler should suffice.\n\nI recommend the Peerless Assassin 120 / 120SE. They're budget two-tower coolers that have actually good performance in cooling CPUs. For further information, Gamers Nexus has a review of the Peerless Assassin 120 on their Youtube channel.",
      "Lol, thanks",
      "Fractal Design Focus 2",
      "I don’t have a lot of experiences with AIO’s but a 12400 should be fine on air unless you really want a AIO. I would recommend the Thermalright aqua elite 360 since I have actually tried it and it worked great\n\nOn air the ID-cooling SE 214-XT ARGB",
      "Arctic 36.",
      "I got the peerless assassin 120 to replace my stock cooler on my 12400f and it felt like over kill.  Moved it over to my new build and it's a beast. CPU temp sits at between 56 and 60c normally. \n\nI only have about 2cm clearance between it and the case though. Absolutely no need for AIO",
      "Hey man I have GTX 1050ti I want to upgrade it to Intel arc b580. I have a ryzen 5 3600. Pls tell me should I upgrade it . In future I will upgrade my processor",
      "Please try to train a lora and let us know how it does",
      "damn thats a really nice clean looking system, love it.",
      "Lol, I'll have to look at the arc settings",
      "In gpu intensive games, there isn't a gpu bottleneck. But in cpu intensive games, there's a tiny bottleneck. However, it's like 1-5%, and my monitor maxed out at 1440p maxed 165 fps way before either was used.\n\nFor tripple a games, I think it'll be perfect. Maybe a little too powerful. I think I could've gotten away with a more powerful cpu. But if you get maybe a 5600x, then it's practically the same as an i5-12400. \n\nI haven't tested that many games yet, so please research more before pulling the trigger. But I think the b580 will be great.",
      "I actually went from a 1050ti to my b580 and it doubled my frames in most games just be careful of cpu bottleneck with that older cpu",
      "wonderful, amazing",
      "What case is this?",
      "can the rgb be changed?? i was initially wishing i got the b580 challenger instead of the le but saw it doesn’t have controllable rgb which is a massive deal breaker even as someone who will always use some form of rgb",
      "Looks absolutely fantastic!"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580"
    ],
    "title": "First build, with B580",
    "selftext": "B580 because gpu market is broken, \n\nCPU 9900X",
    "comments": [
      "Bro got no likes in 30 minutes 😭\n\nLooks nice bro happy gaming!",
      "Chose the best B580 out there (totally not biased)",
      "It has double the power connections so it must have double the power. \n\nThat’s just math.",
      "Therefore double the performance, for just 50 more bucks, that's a deal of a lifetime right there!",
      "That's some fancy looking build, holy sht....I was confused at what I was looking at XD",
      "Love build but also the range of sockets this cooler supports. Own exactly same one and cant be more happy even under gaming with light OC is not exceeding 47 and 5600x is relatively warm on especially on OEM cooler",
      "Oc to 3150 MHz I love it",
      "thermaltake cte c750 air",
      "I guess I'm still confused, too...so all the I/O is on the top? It almost looks like the case in the pictures is sitting on its front. Is the PSU *under* the GPU?\n\nClean build - definitely an eye-catching look!\n\nEdit: [I found the case](https://www.thermaltake.com/cte-c750-air-full-tower-chassis.html) - and I understand what I'm looking at now!",
      "what case is that?",
      "Stylish. but gah the lights.",
      "Sexy steel legend wish I had one :(",
      "Looks beautiful! Happy gaming!",
      "i have the same thermalright mounted the same way",
      "I love it . Beautiful.",
      "Nice build now make it pay for it's self 👌",
      "this is just manasingly great. it looks sacredly awesome. nice build there",
      "What riser did you use for the GPU?",
      "It looks wrong, but cool pc",
      "Beautiful!"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580"
    ],
    "title": "B580 successful? Just wondering if Intel is seeing the b580 as a success at this point? How many have been sold? Are they making money? What’s the figure of arc?",
    "selftext": "As an owner of a b580 I can say I think it’s a good product that offers a great value. Just kind of wondering if Intel will continue the current trajectory and give Nvidia and amd a run for their money? Pic is of my sparkle b580 right before I installed it. It’s running great!\n",
    "comments": [
      "Considering how much they are selling and how constantly out of stock they are, they probably consider battlemage a success, now whether they actually made a significant profit out of this is a whole another story.\n\n\nNobody knows really, only the execs at Intel know or something.",
      "I'd rather see something in the $350-400 range that competes with a 4070 or 4070 Ti, getting into the higher price range they'd have to compete with CUDA, DLSS, and the better RT performance of NV.  XeSS seems to be really good, QuickSync is great, and overall they're pretty solid but Intel lacks the mindshare to really break in to that price bracket at this point.",
      "It’s gaining market share. I just hope they see for to release a B770. Something about $500-$600 that competes with the 5080 would really stick it to Nvidia and force them to be more competitive.",
      "Anecdotal, but B580's are consistently sold out on newegg.  I don't know if they are making money at the low price but the goal was less about that and more capturing market share.",
      "well considering it sells out within 30 minutes or less after a restock. i think its doing alright its been a month and some. I’ve only seen the b580 stay in stock for two days at most and i think thats because the onix card doesn’t pop up on newegg sometimes and its not on pc part picker at all.",
      "The B580 made intel relevant overnight... AMD and Nvidia is yoo busy trying to milk their consumers...",
      "Surely you have to understand that Intel are selling their GPU's cheaply to gain market share as fast as possible right?\n\nIntel will use this time to gain experience with their GPU, gather consumer feedback and cover their losses with the sale of other products to further their market share going forward.\n\nSelling your product at cost or at a loss is a very very common strategy to gain market share.\n\n\nI think you lack common sense, as well as business sense.",
      "From the way the pre-launch publicity interviews went, I think they were a little surprised at how well received the cards were.  I seem to recall one of their reps saying \"please buy the cards.  Buy as many as you can.\" at the end of an interview.\n\nHonestly, I bet the engineers understood the value the B580 brought to the table, but it is harder to make the executives that bankroll the production and logistics understand.  They likely approved the initial manufacturing runs based on the Alchemist series sales numbers.",
      ">CUDA, DLSS, and the better RT performance of NV\n\nRT performance in Blender is almost on par already and even beats them by a little in some game titles because Nvidia tends to use smaller dies for the same class/price range. OneAPI is still not widely used like CUDA is, but any apps that do use OneAPI run just as fast. Similarly XeSS now has framegen. That's the main advantage i see intel having over AMD - they aimed for feature parity with NV from the start, not just raw raster performance.",
      "Intel said from the gitgo that they weren't making a whole lot of money off of the card itself and this was a \"gift to gamers\" so idk if well ever know but atleast they're stirring up excitement.",
      "I'd buy another!",
      "None of these companies tell you how many they produce, and sell, or what the profit is.",
      "5070 (ti) would be huge already. No way they match 5080. Nvidia has too much of a lead for that. But they don't really need to. To focus on 1440p is fine.",
      "If they have a B770 I’m sure they will have a B750. That would probably be what you’re interested in. Multi frame Generation DLSS looks horrible. Too many artifacts. Intel already does surprisingly well with ray tracing even with the A770. If the B580 is doing it better with less then a A770 then a larger B770 with more faster cores is going to have a large uplift. If it scales and the cores also go faster yet it might be 30 or 40% faster than a B580. XESS already has a good image. I’m not sure what that would work out to be performance wise compared to nvidia but I’m sure it would be a major win for Intel.",
      "I doubt Intel GPU division is profitable but this is a long game - they need quality, competitive products in the world just to put the flag in the ground. Then they can start building the base.",
      "I just installed the LE tonight. I love the design of that card.",
      "Making money no. Selling lots yes. They are forcing themselves into the market by selling at a a loss",
      "A $399 B770 will still be more profitable because fixed costs stay the same. GPU die cost increase isn't that much.",
      "The fact that the A770 competed with the 4060 makes me think the B770 competing with the 5080 is a truly delusional thought. Maybe the 5070 ti.",
      "Considering the 5080 is just a 4080s overclocked in performance then the 5070ti wont be that massive of a jump from a 4070s."
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580"
    ],
    "title": "1k worth it B580 prebuilt?",
    "selftext": "I have a budget of $1200 and I’m REALLY afraid of building a PC on my own. My local microcenter ONLY has the b580 in a prebuilt but I can build the same spec PC w b570 for $880. Is it worth the extra $200 for the 580 or is the B570 good enough??",
    "comments": [
      "Since the B580 needs a great cpu, I think 1k will work",
      "This prebuilt is an insane deal. People are sleeping on it because it's a Micro Center pickup-only exclusive :)",
      "Arguably, 32GB is the new 16GB. You don't *need* it, but it would be nice to have and not need it than need it and not have it IMO. It has DDR5 6000mhz CL30 G.Skill RAM so they're not bad sticks either.",
      "That CPU is.. ehh. Let's run the parts, rounded up from Amazon/NewEgg:\n\n* i5-14400f | $130\n* AS Rock B760M | $150\n* 32GB DDR5-6000 RAM (Corsair/G.Skill/T-Create) | $105\n* 1 TB SSD PCIe 4.0 (Samsung) | $105\n* That PC Case | $75 (An actual good-looking one would only run you up to $20 more)\n* 650w Bronze PSU | $65\n* ARGB Cooler Master CPU Coller | $25\n* Two ARGB Fans | $15, and that's pushing it\n* Keyboard & Mouse | $20\n* GPU Anti-Sag thingamajig: $5\n* Arc B580 | $270\n\nTotal: $975\n\nPart for part it's close to what you'd pay if you built it yourself, but I **really** don't like that CPU, and you could get a much better looking case than that.\n\nI was nervous as hell when I built my first PC and watched 3 different YouTubers walk me through it during the process, but it turned out good after a few hours. Watch some videos of people building them first just to really make sure you don't wanna attempt it. Here's a semi-depth one: https://youtu.be/wbujLJ25oUU?si=OmyJVvQYL10lC0hL\n\nIf you're still set on going with a pre-built after that, then this one is okay, but I would recommend getting a 12th - 14th Gen *600K/KF if you start running into performance issues during gaming.",
      "32GB is far away from overkill.",
      "Wait so there's a $200 difference between just the b580 and the b570 prebuilts? Wtf?!?",
      "Forgive Me. I rushed in.  Yes this is an excellent price",
      "I’m pretty sure, it doesn’t anymore. It changed with some driver updates.",
      "I run 32 gigs because I play a lot of games that are super ram heavy like space engineers but nothing wrong with having extra ram even if you don't need it .",
      "Oh my bad, I misread. I have the b580 and I'm loving it so far. TechPowerUp shows the B580 has 119% the performace of the B570; that 19% may make a difference. With newer games beginning to have trouble with 8gb VRAM GPUs, how long until 10gb has trouble? \n\nI guess considering you're terrified of building a pc (totally fair) plus the better VRAM of the b580, getting the prebuilt b580 may be worth it. Plus you're still a bit under budget.",
      "If you have a bit of patience, you can likely get a B580 from B&H or Newegg for under $300 and then get the rest of the parts from Microcenter/Amazon. I did this recently and managed to build a very nice workstation and get a 34” ultra wide for under your budget. \n\nB580 - $270\nCore ultra 7 or 9700x combo - $450/$500\n(Or save money with a tier lower cpu combo)\nThermalright cpu cooler and case fans - $50\n1TB SSD - $60\nPower Supply - $70\nCase - $70\nWin 11 pro from sftkey - $30\n\nIt’s a pretty impressive build for about $1k",
      "It doesn't need a great CPU just not the absolute worst.",
      "Thank you for the in-depth response and advice!  I’ve been looking at the reviews on the CPU and everyone seems to be correct about it being a possible issue down the road. I’ll look into those options you’ve mentioned and might just swap the CPUs. An extra $250 won’t hurt my budget",
      "If I get the parts and build my own the with b570 it’s $200 cheaper. The b580 isn’t sold solo at any MCs in my state",
      "The b580 is worth like $350 by itself , would be nice if this had a better CPU tho",
      "Something that nobody seems to have said but it is VERY important: \"check the exact model of the psu\"  i've seen too many times E or even worst F tier psu (F tier does not just mean bad, it means is dangerous) in these prebuild",
      "Yeah I mean its microcenter they tend to be one of the only decent prebuild sources.   \nBeats the shit out of a dell.",
      "Not a bad deal at all considering how bad prebuild prices can be",
      "I would recommend that you build what you can yourself. I can guarantee they cut a corner with at least one of the parts. It’ll be some unreliable brand you never heard of.",
      "Definitely worth it"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580",
      "arc b580"
    ],
    "title": "Intel Arc B580 “Battlemage” Limited Edition card listed at $259",
    "selftext": "",
    "comments": [
      "I was scrolling through a few comments on the linked site, and they were quite bleak. I would like to  be cautiously optimistic and will wait and see what it's like after reviewers get their hands on it. Also would love to see a B770 or higher. I like what Intel is doing and don't want them to stop.",
      "I wouldn't pay much attention to the comments on tech forums, they are filled with some of the most toxic people you could ever come across. That being said, I wasn't expecting that price tag. It feels like a big jump for the 5xx tier cards but who knows maybe the performance will justify the price. And the B750 could end up being a budget beast.",
      "I was hoping for $199, but this is probably the highest they could price it while being reasonable. Hopefully it can reach AD106 perf",
      "If it gets good 1080p performance in games I could see myself getting it since I want to build a new PC at the end of this year",
      "if its gonna be 259 its probably faster than 4060 probs close to 4060ti",
      "Unlikely to exist. There's not much point when A310, A380, Core 200V, and Core 200S exist. Those GPUs will cover off the vast majority of people who want a GPU lower in performance than the B580.",
      "Um, yes, this is exactly how this works. Intel is unlikely to release additional SKUs for a segment that it is already the only option on the market. It would only be competing with itself, and the difference between two generations would not be enough for it to drive people who already have an A3xx to upgrade. It would be a poor business decision to put resources into making SKUs for that segment when those resources could go to segments that will make a bigger difference to Intel's competitiveness against AMD and NVIDIA. \n\nI would not expect a bottom-tier GPU from until until perhaps Druid, at which point people who purchased an A380 might be looking for upgrades, even at their lower budget.",
      "Hoping that at least the high idle power consumption issue is addressed with this model. Drivers will be a whole other thing.",
      "The a750 is already fairly good at 1080p. What kind of numbers are you looking for?",
      "$250 for 12GB sounds pretty good to me and also wait for new year sale might lower its price!",
      "I am not intel so its not my problem 🙃",
      "Well well well, that's a very promising price",
      "Seems like I got my predictions about right.\n\nI'm looking forward to real benchmarks now. If it's as good as the 4060Ti I'm probably snagging one on release, otherwise I might wait for the B770 or go Nvidia.\n\nEdit: The name B580 brings back memories of the legendary RX 580. I hope it delivers the same kind of value that the RX 580 did.",
      "When b310 or b380",
      "A true DLSS alternative.\n\nTechnically FSR 4 should be a true DLSS alternative, but as long as FSR 4 isn't available; AMD is pretty much a non-starter. At least unless the price is just out of this world good.\n\nXeSS is good and Intel seems to be at least somewhat in touch with what the market wants, just limited by the pretty damn terrible Alchemist architecture. If Battlemage is good and the drivers work this time I'd be very happy to get an intel card.\n\nLike I really, really don't want an Nvidia card with how insanely stingy they are with VRAM, but as long as Nvidia has DLSS and AMD doesn't, I'm taking Nvidia.",
      "See you in 6 months! \n\nEDIT: Assuming you've not deleted your account by then.",
      "I guess nobody read the article in the link, it says it's down to 246$ now.",
      "For $260 I feel it needs to match 3070 and 6700XT, as you can get those around that price used nowadays. If this card is slower than the 3070/6700XT, I don't see it doing well.",
      "The leaks are estimating (guessing) it will be at least A770 perf, but seriously, no one knows.\n\n  \nAlso that “step-up” estimation is an awful way to estimate performance, and is exactly how marketing tries to trick buyers into thinking they’re getting more for their money. While it may have held true for some past generations, it’s definitely not a good rule of thumb now. To use your example, the 4060 is slightly weaker than even a 3060ti, so it is not even nearly equivalent to a 3070.",
      "4060 is decently slower than 3070"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580",
      "arc b580"
    ],
    "title": "Upgrade from rx 6600 to Intel Arc B580",
    "selftext": "I'm going to see how it goes with a ryzen 5 8600g ",
    "comments": [
      "Perhaps a hot take, but I did the same and really liked it.",
      "Nice, those ASRock cards look good!",
      "I'm talking with Intel about insufficient power delivery on the B580\n\nIf they see this as an issue, you should be getting a performance boost once they fix it.\n\nFor example.\nThe finals uses 154w to run\n\nMonster hunter wilds uses 138w to run\n\nAtomfall uses 158w to run.\n\nTDP is 190W\n\nNot a single game has gone that high.\n\nAnd to prove this, if you go turn your voltage up to 60%, your power draw will be higher and your framerate will increase.\n\nMonster hunter wilds goes from 45fps to 56fps in the same scene.\n138w to 165w.\n\nAnd before anyone says \"oh well blah blah it's not the cards fault\"\n\nThe RX 9070 can use up to 320w of power when it is rated for 230w, just by flashing the XT bios to unlock more power draw.\n\nThe B580 can't even get to its TDP with an overclock",
      "Combine them and use both for a dual GPU LSFG setup!",
      "Yeah went from 7600 to b580 and the extra vram is nice but still a few issues with some games, for the most part does fine and",
      "yeah power limit setting doesn't even work for me",
      "Went from an rx 6600 to an A770 16 gb last year in conjunction with switching from 1080p to 1440p, I recommend doing the same here.",
      "6600XT to B580 here 😊",
      "a cpu upgrade would be better but congrats",
      "I mean, I can play AC Shadows at 1080P high settings above 60FPS with my ARC B570, I imagine B580 can do even better so.",
      "With outstanding 1080p and now with 4 playing Fortnite between 75-90 fps all in epic, playing from bed connected to a 50-inch Smart TV",
      "Isn't the 6600/7600 about the same as the b580 performance wise?",
      "Ive been considering an upgrade from my 1660 gtx to the b580. But I'm uncertain about the driver issues and of course how new the company is to gpu manufacturing",
      "The funny thing is I always find it photographs terribly. Like even the official promotional photos I find it looks super cheap but in person it looks great.\n\nI do with the RGB could be adjusted though. You get Blue / Green or off; nothing else and no way to tune it.",
      "You would be fine as the minimum is a 5600X.",
      "How did that upgrade feel?",
      "i have regular 5600 would it do fine or no go?",
      "Yeah because the card doesn't even draw 190w at stock. Move the power limit down until you see a change and you'll see how much performance you could have if the card actually asked for it.\n\nIf you turn the voltage limit up to 60% you'll gain 300 mhz. Just out of nowhere.",
      "I mean. I haven't seen a graphics card yet that doesn't always run at its TDP during max load.\nIts a max power guideline, and if a card isn't at max power, then some of my card is locked away behind a wall. \n\n(9070 has a tdp of 230 but you can flash an XT bios to go all the way up to 320w, which improves performance to be as good as a stock XT with less cores)",
      "Yes."
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580"
    ],
    "title": "3 different GPUs, 1 CFD simulation - FluidX3D \"SLI\"-ing (Intel A770 + Intel B580 + Nvidia Titan Xp) for 678 Million grid cells in 36GB combined VRAM",
    "selftext": "",
    "comments": [
      "My FluidX3D CFD software can \"SLI\" any GPUs together, regardless of microarchitecture or vendor, as long as VRAM capacity and bandwidth are similar. Here I'm running FluidX3D on 3 different GPUs:\n\n* Intel Arc A770 16GB (Alchemist)\n* Intel Arc B580 12GB (Battlemage)\n* Nvidia Titan Xp 12GB (Pascal)\n\n12GB + 12GB + 12GB VRAM are pooled together via domain decomposition, allowing for one large CFD simulation using 36GB combined VRAM, to fit 678 Million grid cells. This is made possible the most powerful GPU programming language, OpenCL.\n\nFluidX3D is available on GitHub, for free: [https://github.com/ProjectPhysX/FluidX3D](https://github.com/ProjectPhysX/FluidX3D)\n\nThe model in this simulation is Santa's sleigh - with some X-wing modifications. Merry Christmas! :)\nThe CAD model is from Zannyth / Kevin Piper: [https://www.thingiverse.com/thing:2632246/files](https://www.thingiverse.com/thing:2632246/files)\n\nPS: My second B580 is currently in my other PC for testing, and for gaming... hence only one B580 here, and an A770 to fill the top PCIe slot instead :D",
      "Is that Santa's X-Wing?",
      "the aerodynamics of a sleigh fitted with x-wing parts, obviously",
      "FluidX3D splits the simulation box into equally sized domains, here 3x 12GB - this simplifies the implementation a lot. The extra 4GB of the A770 are not used.\n\nIt would also be possible to split into 4+3+3 domains, each at 4GB size, and deploy multiple domains on each GPU. But the communication overhead then would slow it down a lot.\n\nHaha yes I need an AMD card for the ultimate team Red-Green-Blue SLI abomination build :D",
      "Yes! Merry Christmas! :)\n\nThe CAD model is from Zannyth / Kevin Piper: [https://www.thingiverse.com/thing:2632246/files](https://www.thingiverse.com/thing:2632246/files)",
      "You mentioned that the pool is 3x12GB. Is the other 4GB of the A770 unable to be used, or is this actually a 40GB pool?\n\nThis makes me want to try a cursed 3-brand, 3-size setup of my own. 3080ti + A770 + 7900XTX should be interesting.",
      "What is it simulating?",
      "Best thing I've ever seen in this sub, amazing",
      "What motheboard are you using does the arc cards require bifurcation?\n\nThanks in advance for any info.  There is quite a few of us noobs that wants to try dual arc gpus.",
      "It would be the ultimate RGB Build",
      "I love seeing these kinds of posts. People actually using their computing horsepower for simulations etc.",
      "Asus ProArt Z790-Creator WiFi. That board supports PCIe 5.0 x8/x8 bifurcation on the first 2 slots, and the third slot is PCIe 4.0 x4. Here they are running at 4.0 x8/x8 and 3.0 x4. Bifurcation is definitely beneficial but not a must; would work with slower PCIe connections too but at a performance hit.",
      "What matters here is VRAM bandwidth. CFD performance is directly proportional to bandwidth. A770 is 560GB/s, B580 is 456GB/s, Titan Xp is 547GB/s. All pretty close to each other which makes them a suitable match.",
      "7700XT + 4070 + B580 would get every fully utilized, or A770 + 4080 + 7800XT.",
      "How did the ARC performance compare to Nvidia? I've been a Cuda user/dev since Cuda 1.0 but the ARC line is speaking to me",
      "When you say that only bandwidth matters but the architecture doesn't. Does that mean it'll work with any combination of gddr6 cards?\n\nAnd/or gddr5 pairings or gddr7 etc.",
      "You are a mad scientist!",
      "Would love to see AMD - INTEL - NVIDIA trio combinations, all in ref/LE for greater good!\n\nTried adjusting that triangle symbol to form RGB - AMD/NVIDIA/INTEL [https://imgur.com/rDiZZA8](https://imgur.com/rDiZZA8)",
      "you need an rx 6700 xt so you have a gpu from every current brand",
      "is 3x A770 is the cheapest way to go for this workload?"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580",
      "arc b580"
    ],
    "title": "Intel Arc B580 rumored to get custom dual-GPU version with 48GB memory",
    "selftext": "",
    "comments": [
      "Perfect for video rendering, 3d rendering and ai.",
      "Ok now this one is stupid. I believe the 24GB ARC PRO B60 but this makes no sense.",
      "It will be for GPU Flex data center cards.",
      "It makes sense density-wise, i guess they would just need an onboard PCI-E multiplexer and then done, everything is managed though software anyway now. No need to like connect the memory like SLI or crossfire",
      "Yeah, there's the thing. They don't care about gaming when they design something like that lol \n\nAlthough I think technically UE5 can do multi-GPU",
      "This reminds me of the GTX 295 and that AMD/ATI Radeon 6990 (if memory serves).",
      "They don't even need a PCIe switch. Just give each card X8 lanes and that's it. The target workstation and server systems where such cards would be installed support bifurcation anyways.",
      "Not for gamers, but for markets like VDI it makes perfect sense. Heck, depending on price it would sell like hot cakes to AI enthusiasts. I'd buy one if it was under 1k",
      "A card like this wouldn't be being made for gaming. \nIf it becomes a reality, and isn't priced in to doom, I'll be replacing my A770 with it.\nReally hope it's real!",
      "Epyc has even more lanes, but that's missing the point. The reason to make such a card is never the number of PCIe lanes or VRAM. Nvidia makes similar cards for the same purposes (ex: Tesla A16).\n\nThe number of physical cards that can be fit on single 4U server is the main driving factor. Each GPU will get forwarded to a VM for accelerated VDI support. The A16 has four GA107 GPUs, each with 16GB VRAM.",
      "Yup. I'm excited for the simulation possibilities as well.",
      "Maybe not sold on the open market, but there's no shortage of weird hardware comissioned by companies for their own use cases. I've seen weirder stuff on ebay.",
      "Have a look at Intel Data Center GPU Flex Series and Max Series. They've done Arc on a dual GPU setup before specifically for that use. VDI, media encode, etc. is perfect for such a SKU. You're missing the forest for the trees.",
      "More details of affirmation?",
      "This probably doesn't exist. It's much more likely that we'll get the 24GB model.",
      "Please be real, and release at a good price!",
      "It was pretty common at the time for them to band even two lower end cards together essentially pre-packaged in SLI or Crossfire. It just wasn't that successful because it would negate the cost savings of buying two midrange cards and getting top tier performance",
      "What's a 1080p card ? Nobody cares about that in compute workloads.",
      "> a really unreliable website \n\nIt's not.",
      "Maybe the Melville Sound idea isn't dead. GPU Flex B250 or whatever could be a fun one. \n\nBut also makes me wonder why DeepLink HyperEncode and HyperCompute got discontinued. Maybe it's just getting replaced by a different technology?"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580",
      "arc b580"
    ],
    "title": "I really want to believe Intel's claim that the drivers for its new Arc B580 1440p graphics card suffer from 'no known issues of any kind'",
    "selftext": "",
    "comments": [
      "Guaranteed there’s an internal bug database at Intel that shows there are in fact issues of some kind. Because all software has issues.",
      "> That is the reviewers' job.\n\nYou should know that a comment like this is bait for someone like me ;) I try to find at least one thing that reviewers miss with every cycle, the last being [loading time regressions with Arrow Lake](https://old.reddit.com/r/intel/comments/1gdib7e/a_regression_that_most_reviewers_missed_loading/)",
      "I'm tempted to buy a B580 just to put this claim to the test",
      "Of course there are issues, but I think they meant there are no big show-stopping issues, which I'm inclined to agree with.",
      "Save your money.\n\nThat is the reviewers' job.",
      "Where did Intel claim this?",
      "On the PC World podcast a couple days ago, Tom Peterson just said they aren't aware of any known issues. Literally just a CYA statement in response to a question that he was asked, then PC Gamer turned it into an article for some reason.\n\n36:08 in the video linked in the \"article\".",
      "oh that stuff was you? that explains the 'fuck it we ball' vibe",
      "I’m getting one. I’ve had enough of nvidia.",
      "he's had enough of refinancing his mortgage every time a 90 series card comes out.",
      "Even Nvidia and AMD have issues with some games from time to time, don't know why people expect perfection when it's Intel.",
      "Intel never said there were no known issues. The article's author just lied.",
      "No, I’ve had enough of the prices, lack of vram, overhyped raytracing (basically it’s useless on lower end cards), dlss is also overhyped and looks mucky. I’m currently using a 3070. The 4060 and 4070 were a joke, barely even an upgrade. Overhyped, overpriced and underwhelming.",
      "I’m so impressed by my intel arc a770, never had a problem with drivers of course just play Diablo 2 resurrected and path of exile weeee",
      "I hope Battlemage does well, XESS seems like it's a true competitor to DLSS, and the price to performance is competitive.\n\n\nThe extra VRAM & Bandwidth thanks to the wider bus widths will also age incredibly well.",
      "Bugs are fine, they can get fixed. \n\nI'm actually kinda intrigued and will most likely buy a B580 and then later upgrade to B770 once it comes out",
      "It's silly of them to make that claim. Arc software was garbage on release and  look at the Arrow Lake fiasco as a recent example.  Drivers and software have never been Intel's strong point and GFX drivers are one of those things that are constantly updated to address bugs and improve performance.",
      "XeSS intel actually did something very right there versus fsr which is still a blurry mess (but even that has value as it may enable gamers with big budget constraints to enjoy otherwise unplayable titles)",
      "All software has bug, but if they solve the major issue it's good enough",
      "\"known\" whoever said that isn't lying if they never checked"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580",
      "arc b580"
    ],
    "title": "Intel Arc B580 Battlemage GPU specs leaked in accidental retailer listing — Arc B580 features PCIe 5.0 x8 interface, 12GB GDDR6, and 192-bit memory interface",
    "selftext": "",
    "comments": [
      "Hopefully we can get a A770 16gb successor\n: (",
      "They meant for the A770 to match the 3070, and the devil in the details is it *can* under synthetic loads and certain gaming loads where its particular design can shine.\n\nHopefully the B7xx model matches or exceeds the RTX 4070. I've been holding off on getting a used 40 series GPU because I want to see if Battlemage can deliver.",
      "A PCI-E card is always going to fit in a 16x slot by design.",
      "I'm not really a fan of this trend of making GPUs run at x8 electrically while fitting in a x16 slot.",
      "if they don't solve the idle power consupmtion without ASPM then my next gpu will be nvidia, even while watching youtube videos the card eats 40W while nvidia cards eats less than 10W",
      "My power outlet is capable of 1500W for the likes of toasters, hairdryers and electric kettles, I think it's a cop out that my night light fits into the same slot and barely draws any power. The port is capable of so much more, why don't we use it?",
      "maybe so on pcie 5.0 supported systems the gpu hogs less lanes? that's the only optimization i could think of. \n\nthose two extra 4x lane ssds are surely worth it or something lmao",
      "Baseless claim",
      "Fingers crossed. I’m hoping they will make the G31 die. With 0% market share and not much money they have no incentive to do so.",
      "i mean, it's supposed to be the point from how i understand, the A770 was no 3090 competitor but rather an actually reasonable 3060ti-ish card. same goes for the rest of the lineup (a380, a580, a750) all affordable cards given their each respective roles for what they offer.",
      "Now one thing that is weird is the fact that one model comes with one 8-pin, while the other comes with 2 8-pins. That's just straight up strange. If one came with 8+6 pins and other one with 8+8 I would get it, maybe the other model is a bit more overclocked and thus needed a bit more power, but jumping from 8pin to 8+8pin in one same GPU? \n\n(Could be also just an error and the 8+8pin is in fact a higher model but still strange)",
      "this =(\n\nbut i want 16gb and 256bit",
      "Not everyone has integrated graphics.",
      "Really? They gonna cut down 16gb to 12gb? I don't believe it",
      "I hope that it will work well on Linux. I also wonder if there will be a worthy upgrade over my 6700XT in the lineup.",
      "Meanwhile my 3090 idles at ~100W",
      "Yeeees, but I still think it's a cheap cop-out.",
      "for me it would work out since my CPU (8600g) i believe has a limit of x8 for the GPU. Obviously i want to upgrade the CPU at some point, but a nice upgrade to the B580 would be a nice interim GPU. Hopefully its price reasonably, is more performant than the A750 and actually releases in the UK.",
      "Get a halogen night light",
      "Why needs pcie 5.0 x 8 with gddr6 and 192 bit. What benefit of it?"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "580",
    "matched_keywords": [
      "b580"
    ],
    "title": "B580 Crushes 1440p",
    "selftext": "I'm running on an $800 build and this thing just crushes 1440p.  I know 4k is like, the coolest, but I honestly don't know what 4k gives in terms of gaming experience that 1440p doesn't.",
    "comments": [
      "Play what you like\n\nI like 1440p ultrawide",
      "The B580 does seem to be a 1440p/60 card. That is the sweet spot.",
      "Unbelievably inexpensive to build out a DDR4 system, 32GB of Corsair 3200 was like $54 to go with this silly cheap mobo.  And $250 for a 12GB GPU...  It's like a golden age for PC gaming right now, in my wildest dreams I didn't think I could go on-brand, high-quality for every single part in my build for under $800 and play games like Metro Exodus on 1440p ultra with silk for FPS.",
      "Refresh rate is better than 4k.",
      "What CPU and motherboard are you running?",
      "I5-14400 on a gigabyte DDR4 B760m",
      "I run a 9800x3d/4090 on UW1440p OLED and its the most beautiful thing in every gane I try. Love me some ultrawide.",
      "Bruh when did he say that",
      "To me the 4k thing all comes down to display size.\nThe crispness of a 27 or 28 inch 4K is glorious.  I had a cheap 28 inch 4K@60 monitor for a couple years and \"upgraded\" to a nice Acer 27 inch 170 HZ 1440p and I wish I'd kept the 4k. I play a lot of strategy type games, so while the higher refresh is nice it doesn't make much difference in most of my gaming so I'd rather have the pixel count. \nDifferent people have different use cases, so while you may not see a benefit, some do. \nUltimately, though, you either play on what you want or you play on what you have. Don't let anyone else's opinions spoil your enjoyment.",
      "What if I told you I hooked up an Xbox controller to the PC too 🙃",
      "I play at 5120x1440 on my A770LE currently, but moving to B580 once it comes in next week.",
      "Yeah after the last driver update I'm getting 60+ FPS on cyberpunk with raytracing on. Shit rips.",
      "I mean 4K honestly is for TV gaming, otherwise not many reasons to need 4K gaming",
      "I agree completely.",
      "I'm running that same motherboard with a 14900ks",
      "The A750 does it well too.",
      "Pixel density is the metric you're thinking of I believe. 1440p on a 24 inch screen is very different than on a 28 inch",
      "I have a 4k OLED and a 4090. I bought a b580 and tested it out. Worked well at 1440p, 4K was a challenge outside of cs2. Performance from 1080p to 1440p scales better than you expect. Currently in my daughter’s build and working very well with her 5800x3d.",
      "I just got myself a 27\" 1440p monitor and so far I've only played RDR2 but I'm getting 100+ fps with almost everything on high.",
      "Honestly it can handle 1440p at 80-100 gps in most games"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "570",
    "matched_keywords": [
      "b570"
    ],
    "title": "Future B series cards? ",
    "selftext": "Sorry for the ignorance, i genuinely tried looking up to see if intel had announced other B series cards other than the B580 and B570. I really want to make the jump to Intel cards, but the 580 would be a side-grade at best. \nHow was the A series roll out? Did it take time for higher end cards to be introduced? \nIt just seems odd to me they only announced two cards. ",
    "comments": [
      "The A series started with the A380, with the A580 being one of the last cards. This time the B580 and A570 (no Alchemist equivalent) came out first. I think if there is going to be a more powerful gpu it will be released in March next year.",
      "During a recent interview the head of Intel Arc said Celestial (Arc gen 3) is ahead of schedule. I think i remember from like back in 2022 they wanted to do YEARLY gpu updates... maybe Q4 next year is C770?",
      "It’s strange to me, I can’t even find anyone online asking the same question. It seems like everyone is just hyped about the two they released, no one is asking, or even speculating about future battle mage cards.",
      "I really hope they come out with something in the spring. I don’t want to wait another year lol"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "570",
    "matched_keywords": [
      "arc b570",
      "b570"
    ],
    "title": "ASRock Intel ARC B570 Out",
    "selftext": "At your local Micro Center",
    "comments": [
      "I'm upgrading my son's PC from a GTX 1080 and the other one is for me. I work I don't need to scalp it",
      "> I work I don’t need to scalp it\n\nGood man",
      "Unfortunately, I think even with this post you're going to get some anti-scalping downvotes from people that don't bother reading lol thanks for the update though and I'm sure you'll both love the upgrades!",
      "Wait they are actually released!? What the heck, I thought they weren't going to be released so soon. I seen the listing on my local Microcenter store, but it got taken down right away. It's not even understand sold out!",
      "That's because the people jump to conclusions without reading",
      "One for his PC, one for his son's.",
      "Damn, i did ask too early, getting nuked in downvotes haha",
      "You have to get creative to get drivers working on those. The installer likely won't work, but at least one other B570 user was able to manually point Windows to the unpacked driver directory via the card's Device manager properties.\n\n\nIt reports as a B580, but should get you going until Intel updates with launch drivers.\n\n\nIf you can share some benchmarks, that would be great!",
      "Another braindead exaggeration smh",
      "I wouldn't be able to tell you I can only compare it to what I have which is the GTX 1080 never had an RTX would that said is definitely better than what I have 👍😎🔥",
      "1080 non-ti is somewhere around an 8 gb 3060 in performance.  B570 should be a slight upgrade, a bit faster than a 3060 and with 10Gb if on a modern CPU with ReBAR enabled.  Hardware XeSS would be a nice plus.\n\nThe big question is: does a PC from the 1080 era have ReBAR enabled, or can it?  My Gigabyte DS3H latest BIOS enables it on a 3600 or newer CPU, but none of the older BIOSes have the option.\n\n1080 non-Ti is what I'm running, and the minimum I'd bother as an upgrade is the $25 more expensive B580.  Holding out for the B770 or 9070XT for a true upgrade though.",
      "Okay, so I had to wrestle with the latest Arc drivers on Windows 11 Pro and manually install them. I used 7 zip to extract the drivers to a folder. From there I opened up the device manager right click on the Microsoft display adapter driver and updated the driver. From there I had to tell it to let me install my drivers and went through the extracted folder.  My workstation/server/gaming rig now sees the Arc b570 as a b80.  Crazy, right?  Playing Last of Us on Ultra, I only got 15-20fps, but after tweaking settings, I hit 50-60fps on High. That's with my W-2155. Haven't tried it on my other server (the W-2175 with 14 cores) yet.\n\nHP Z4 G4 workstation\n32 gigs of RAM ddr4\n2TB Western digital nvme\nIntel Xeon w-2155 processor\nIntel ARC B570",
      "What about this looks bad?",
      "Bro asked too early 😔",
      "rebar uefi mod.",
      "Hardware Unboxed did not find that it needs a 9800X3D to run fine and you know that.",
      "It's sold out everywhere and I'm not going to pay $150 over the mfp price which what I see on Craigslist eBay and even Facebook Marketplace",
      "Sold out everywhere. I am not paying $150 over manufacturer price",
      "Was getting ready to pull the trigger on one but:\n\n\nA) I found out these things suck at VR, which unfortunately is 50% of why I'm upgrading from an RTX2060\n\n\nB) found an RX6900XT for 80% of its price which well, destroyed the bang for the buck-ness of the Intel one...",
      "Don't be so sure.  The 1080, even non-TI, is on the same level as the 6Gb version of the 3060.  Not far off, at any case.\n\nThe B570 should be somewhere between a 3060 6Gb and 4060 8Gb in performance.  So don't expect a night and day difference, especially if you don't have a latest gen CPU."
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "570",
    "matched_keywords": [
      "arc b570",
      "b570"
    ],
    "title": "Intel announces Arc B580 at $249 and Arc B570 GPUs at $219 - VideoCardz.com",
    "selftext": "",
    "comments": [
      "It will probably also be around $500",
      "Intel is claiming that the Arc B580 is 10% faster than the GeForce RTX 4060 at 1440p\n\nhttps://cdn.mos.cms.futurecdn.net/9C9zEHTPExVJYkeAFNbvg-1200-80.jpg.webp\n\nhttps://cdn.mos.cms.futurecdn.net/42zbBqpa3CtyBcgp7fjw9n-1200-80.jpg.webp",
      "B580 for $250 sounds reasonable if the rumored spec is true, B570 feels like only exist to upsell B580.",
      "i think it is just there for the part defectly B580 chips.\n\nSo u can still sell those chips.",
      "And the scariest thing it will probably only have 8GB of VRAM",
      "I think that the pricepoint and VRAM amounts are definitely interesting.\n\nThe A580 looks really promising whereas the A570 may be a deal once it hits a sub 200 dollars street price.\n\nLet's see how the perform, Xe2 has been working great on Lunar Lake.",
      "4060 was also barely any faster than the 3060 and straight up tanks if vram limited (which can happen even at 1080p whereas a higher vram card can allow maxed out texture)",
      "did you write A when you wanted to write B?",
      "This looks like a pretty good deal and a real 3rd option to budget PC builders/buyers out there.\n\n12g of vram on a $250 card is pretty good.\n\nWaiting to see real benchmarks. I hope one day Arc GPUs can match 70-80 series nvidia performance, I might really consider going a full intel build for fun.",
      "It is also rumored to have 8GB of VRAM which will most definitely kill performance in modern titles",
      "No, Intel is claiming that it is 24% faster than the Arc 750 and 10% faster than the GeForce RTX 4060 at 1440p\n\nhttps://cdn.mos.cms.futurecdn.net/JasQiFARCQoPfWhn25V2Sm-1200-80.jpg.webp\n\nhttps://cdn.mos.cms.futurecdn.net/42zbBqpa3CtyBcgp7fjw9n-1200-80.jpg.webp",
      "Intel is claiming 10%, not 32%\n\nhttps://cdn.mos.cms.futurecdn.net/42zbBqpa3CtyBcgp7fjw9n-1200-80.jpg.webp",
      "This would have been impressive 2 years ago.",
      "Yes, the B570 might look weird at first blush, but it's there because of binning. And I suspect, like others, it'll drop in price to make the segmentation make more sense on the street.\n\nI only wish the power was down more.",
      "Its like 15% faster than the 7600 tho",
      "The 7900XT to B580's 7900XTX",
      "\"rumours\" this early are usually a bunch of nonsense and guesses. RTX 5060 won't launch until much later into 2025, the xx60 card has historically been released quite a few months after the top end xx90 and xx80 cards. So it's not gonna be relevant until maybe Q3 '25. Until then, B580 is exciting for the lower end market. It's cheaper, faster on average, more VRAM, better than AMD on the software side.",
      "I like your funny words, magic man",
      "You seem to not understand the problem. Problem being regression between generations in VRAM department. And the thing is that insufficient VRAM means:\n\na) faster aging of your card\n\nb) quality issues (look at RTX 4060Ti 16GB vs 8GB reviews, turns out that performance is one thing but 8GB version in multiple games outright loaded lower quality textures/had more visible pop-in effect)\n\nc) at some point you just can't pick higher quality textures just cuz you don't have enough memory\n\nd) customers should not have to decide whether they want a 3060 or a 4060, it should be a one sided upgrade (higher generation, same tier). \n\n>unless all you play is the handful of games that are VRAM hogs\n\nWhat is a \"VRAM hog\" today will be a normal amount needed for medium/high settings 2 years from now.",
      "272mm2 die is poor PPA Vs AD106 & 107 and likely Navi 44 & 48. Despite that Battlemage iGPU on LNL is much better in that regard. Great price points but I kinda dread if they're likely losing money on these things. I suppose that's why G31/B7x0 is canned as rumoured because it won't be cost competitive Vs N48."
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "570",
    "matched_keywords": [
      "arc b570",
      "b570"
    ],
    "title": "Intel Arc B570 \"Battlemage\" GPU Tested In Geekbench: Roughly 12% Slower Than Arc B580 For 12% Lower Price",
    "selftext": "",
    "comments": [
      "%1 down the price for every 1% performance lower performance? That's absoutely incredible\n\nNvidia and AMD could never",
      "Let's make sure it doesn't need a 7800X3D to meet it's potential.",
      "Hopefully it is tested with midrange CPUs as well",
      "No, they aren't. These already showing its age. Intels drivers are Ok. It will be getting better over time. But soapy FSR scaling can't be fixed on current AMD gen cards, neither RT performance. Additional 4 GB of VRAM can't be added either. \n\n\nRegarding value. As per article, B580 still holds its value even when paired with 5600. Now look at it this way - you get nice performance bonus sometimes, pairing it with faster CPU. Which is additional value (I mean - upgrade path).",
      "Well, driver overhead is when cpu hits the wall and can't feed GPU fast enough. So GPU has to wait. Therefore common sense says that for lower performance GPUs driver overhead should be less noticeable. Of cause, RTX 4090 have the biggest driver overhead problem - just look at performance difference between 9800x3D and ryzen 5600 when using RTX 4090. :))",
      "great now hope we can actually get them for near msrp.",
      "Me still with a GTX 1080…",
      "Intel 13400f is like $130 and fast enough to feed a B580.  You don't need a midrange CPU, even a low end one that's not 5 years old works.\n\nYou can get an i9 12900k for $290, and it'll be way more than sufficient for any game for a long time.  As is the i7 12700k for $200, honestly.\n\nYes, the 9800X3D is great.  But not strictly necessary.",
      "Won't the B580 perform well still with a 7600x/9600x? I think (not sure) the performance drop happens with much older CPUs and the 7600x should be fine, you may lose some performance in some games but for the majority it would still outperform the 4060 right?",
      "Kind of missing the point there. AMD cards suck at RT and upscaling because they're cheaping out on hardware RT and AI accelerations, as well as encoding and others. Their die size is tiny mainly because they're skimping out on those features, their GPU design department was divided into RDNA and CDNA so they could skimp on features they think a consumer grade GPU doesn't need. They're (barely) cheaper than nvidia cards because they really are cheap to produce. Look at RX 7600's tiny die fabricated on last gen node. It should be much cheaper than it is now. Why is their driver better? Because they had acquired ATI long ago, one of the oldest high performance GPU producers, far older than Nvidia. They had been toe-to-toe with Nvidia since the late 90s.\n\nI don't think their last and current gen GPUs are necessarily good value. 6700 XT at $330 is definitely not good value, it's barely even more powerful than RX 7600 in rasterization in some games, and the gap is even smaller when you compare it to B580, with B580 dominating at some titles. It used to be $269 or something at some point last year but I guess the stock has run out. RX 6600 has been $190-200 since forever. 6600 XT and 6650 XT also haven't been cheap for a long time. In the used market, the supply of cheap RDNA 2 GPUs has also dried out. \n\nAnd unlike Radeon GPUs, B580 isn't only good for gaming. It has good hardware acceleration for video editing, streaming, 3D modelling, and such. Alchemist was also like that. Unlike intel with their CPUs, they don't nerf their GPUs or make arbitrary distinction between consumer grade and workstation grade products. $250 is well worth the money for a well rounded multi purpose GPU, the second best you could have with that money is an old RTX 3060 8GB.",
      "That's amazing.\n\nBut also: for the love of God, give us the B770 already 😅",
      "Tbh had my RX6600 for 2 years and was always plagued with this annoying stuttering issue. Tried multiple fixes over that time nothing fixed it. Drove me nuts. Replaced it with a B580 recently and there is the slightest performance dip but thank the good lord the stupid stuttering is gone. All of this paired with the legendary 5600. Imo the overhead things blown up for no reason I’m honestly getting the performance I expected and hopefully it improves with driver updates or else I’ll just have to save up and pay the nvidia tax down the line somewhere lol.",
      "Because AMD constantly do this thing where they have a specific GPU they want to sell, they then price a significantly slower GPU B right next to GPU A in price to entice people to pay more for GPU A, for example the 7900XT and 7700XT were intentionally priced poorly to try and get people to buy the XTX and 7800XT",
      "rx 6900xt for under 250$? ill believe it when i see it buddy,full working with proof",
      "Looks like the price to performance is acceptable, then.",
      "Using this standard, Nvidia is giving you great value if you consider something like the 4070ti is 60% less expensive than the 4090 while giving you 40% less performance!\n\n  \nI don't think it's a great benchmark for value. Using something more standardized would be better ($/fps etc)",
      "If they actually release it in volume, it's pretty darn good value for 10GB. 8GB isn't acceptable at all anymore, but 10GB is ~okay for the lowest of the low end cards, which this is.\n\nBut yes, I get your point; It may be 12% for 12%,, but it's practically more than 12% because it's losing ~17% of its VRAM. Upvoted. I think people missed your point.",
      "Wait for drivers maybe? Idk.",
      "this. I feel kinda weirded out by all the fuss tbh. Yes, the overhead issues aren't pretty. But all you need is a reasonable setup. Pairing a $150 CPU with the B580 (a $250 GPU) will result in fine performance. If you use your >10 year old CPU and jam the newest gen GPU into your system...what did they expect?\n\nAs far I know Intel even has 11th gen or newer as system requirement.",
      "Iirc 7600 already sees drop but its mostly caused by using old apis in games. Modern apis handle it much better and closer to top cpu performance"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "570",
    "matched_keywords": [
      "arc b570",
      "b570"
    ],
    "title": "Intel Arc B570 specs leaked: 18 Xe2-Cores, 10GB memory and PCIe 4.0x8",
    "selftext": "",
    "comments": [
      "an actual true budget card with 10gb vrm and under 200$",
      "\\>10gb\n\n\\>1x8 pin\n\nLow power draw budget GPU with better specs than a 4060?",
      "If it’s actually under $200 that would be awesome. My hope for battlemage has been it can help Intel grow into a true 3rd gpu competitor to keep the other companies in check. The market has gotten ridiculous due to a lack of competition.",
      "Honestly funny how for months on Videocardz, Intel was getting dragged through the mud, with everyone saying their dGPUs would never come out. But now that a few specs have leaked, there's been a total turnaround, and suddenly everyone is celebrating because the budget segment is back.\n\nI only hope Battlemage performs consistently well in most games and we get some serious competition for the sub 300$ GPUs.",
      "Now just take a look at this: the lowest end Battlemage for now has 10GBs of memory. Not 8, not 6, not 4. 10 gigs for lowest end card. Remember the time Nvidia or AMD did something similar?",
      "Really solid card. Hope we get some news on the B770 (or whatever it ends up being) with the press release next week, too. If that segment gets a similar glow-up could be a really attractive card. Looking to upgrade my A770 in the new few months, and would be happy to stick with Intel.",
      "Yeah, wishing for the same thing. Even alchemist gpus already have a better professional workflow than amd ones which is a travesty, how amd can dominant the cpu space and then be such a joke for everything but GAMING ONLY or apus in the gpu space is mind boggling to me. Really hoping Intel pushes it up so Nvidia has to actually innovate and amd wakes up.",
      "B580 is targeting 4060ti performance with 20 xe2 cores. 18 might as well edge out the 4060",
      "Not really, Intel still has massive market share in enterprise and business. Most pre-builts are also Intel systems. People saying they're on the verge of bankruptcy are simply incorrect, Intel is nowhere near bankruptcy.",
      "Well at this point it's B580 or I'm waiting for RDNA5.",
      "If the B580 is 250, then the B570 is probably going to be like $200 to $220. I don't think it'll be much lower than that. They need to turn a profit from this generation, and there's still Alchemist stock to sell off.",
      "Really waiting for the B770!",
      "It sounds cool but the thing that scares me away is Intel Arc's hit and miss performance in older games",
      "True with AMD, but they also didn't bring a lot new with 7600, still 8GB, still 128bit memory bus. Nvidia though is on a next level, giving the 4060 and 4060Ti such reductions in VRAM parametres. I hope Battlemage gets so successfull it'll force Nvidia and AMD abandon that petty 8GB 128bit memory config for lower end cards.",
      "To be honest, I'm more interested in the B770. Wonder when we'll see info regarding that.",
      "I hope people are actually buying and not wishing for a third competitor so they can buy cheaper NVIDIA.",
      "There was a similar feature developed under the ExtraSS name a while back, but they haven't really referenced it recently. ExtraSS is also a bit unique in that it's not an interpolative technique, but rather an extrapolative one. Pretty neat white paper, hopefully it ends up in a product at some point, either via future software rollout or otherwise.\n\n[https://wccftech.com/intel-frame-generation-technology-xess-coming-soon-extrass-frame-extrapolation-boost-game-fps/](https://wccftech.com/intel-frame-generation-technology-xess-coming-soon-extrass-frame-extrapolation-boost-game-fps/)\n\n[https://dl.acm.org/doi/pdf/10.1145/3610548.3618224](https://dl.acm.org/doi/pdf/10.1145/3610548.3618224) (direct PDF)",
      "Probably not. I think the B570 is probably going to end up somewhere around the A770 to 4060 range.",
      "Wait for reviews, nobody knows how it will perform in games. The synthetic benchmark (for the B580) is looking solid and even winning against the 4060 Ti based on some leaks.\n\n[Intel Arc B580/B570 and AMD Navi 44 may reportedly win with RTX 4060 Ti in synthetic tests](https://videocardz.com/newz/intel-arc-b580-b570-and-amd-navi-44-may-reportedly-win-with-rtx-4060-ti-in-synthetic-tests)",
      "B580 was listed at 260 usd"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "570",
    "matched_keywords": [
      "arc b570",
      "b570"
    ],
    "title": "Exclusive: Intel Arc Battlemage to launch December 12th",
    "selftext": "Intel will unveil its Battlemage GPUs next week, launching mid-December. The Arc B580 and Arc B570 will be the first models from the Xe2-HPG-based series, with reviews expected to go live on December 12th.",
    "comments": [
      "Long awaited. Can't wait to see the benchmarks for these. I'm really hoping that these end up lighting a fire under AMD and Nvidia to improve their offerings for the lower end and mid-range.",
      "As someone only interested in the enthusiast sector these probably won't have anything that interests me but more competition in the market is always good. \n\nI'm just holding out for the 5080 hoping that the performance and price are decent.",
      "As someone who likes compare apples to apples... This card is supposed to list for $250.\n\nAs someone who hasn't been hiding under a rock for the last few years, the 5080 price will not be \"decent.\" It might actually be obscene.",
      "Forever the bane of GPU market competition: consumers. \n\nEveryone wants more competition, and then only ever buys Nvidia thus creating the self-fulfilling prophecy of Nvidia forever dominating sales and thus R&D and thus sales and thus…",
      "https://www.reddit.com/r/intel/comments/1h184nv/intel_arc_b580_battlemage_limited_edition_card/\n\nThey are claiming these b580 will list for $250, and come with 12gb RAM. 4060 ti level of performance in another leak, but gotta wait on 3rd party benchmarks, and then see how the newest Intel drivers are in various games. \n\nSince NVIDIA, and AMD doesn't have a decent $250 card, this could end up being a solid option depending on the games someone plays.",
      "Smart move by Intel to launch B570 and B580 first, obviously mid end GPU is what most people going to buy. B580 arround $200 but with RTX 4060/Ti performance is going to sell like hot cakes.",
      "The type of uninformed take that will forever keep the Nvidia hegemony alive.\n\nNvidia absolutely owns the top end, but everywhere from a 4080 and below has direct competition from AMD at every price point. DLSS and some more software like NVENC are definitely differentiators.\n\nBut to say that AMD only has a place in budget builds is factually and practically false.",
      "With the 8-core Xe2 LPG hitting 4.1K TimeSpy, I'm hoping a 20-core Xe2 at 40% higher clocks can at least hit 12K TimeSpy.",
      "I just want a B310 or B380 equivalent of the A310 and A380",
      "Its going to be 4060ti performance seeing the a770 is already on par with the 4060 now with the new drivers. Youre also forgetting the crucial vram amount.\n\n1440p monitors have really plummeted in price and this would be the perfect pair for value. Even the 7600 xt and the 4060ti 16gb suffer there because their memory bandwidth is too low but the b580 has 55% more than them.",
      "Hey alcoholicplankton69, your comment has been removed because we dont want to give *that site* any additional SEO. If you must refer to it, please refer to it as *LoserBenchmark*\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/intel) if you have any questions or concerns.*",
      "A Thanksgiving Battlemage Surprise - Let’s Do This",
      "Intel have the XMX XeSS which is DLSS level of clarity and have intel quicksync which is better NVENC. The only advantage nvenc had was super fast encoding, but now arc have the same speed.",
      "1440p gaming with the B580 and Frame Generation",
      "> I'm just holding out for the 5080 hoping that the performance and price are decent.\n\nif you are going 80 series then you will have the performance but will have to pay for it... I would think a 70 series ti would be better bang for the buck. plus with all the AI its not like these babies are going to go down in price I would bet expect the 50 series to be even more expensive. Especially if there is a new import tax.",
      "They probably won't release a card like that unless they have a lot of defects.",
      "Theres a big hardware change that made compatibility way easier. You can already see how much it helped with lunar lake thats also battlemage",
      "> their cards work on like 40% of games\n\nYou should be honest rather than just making stuff up.\n\nhttps://youtu.be/Y09iNxx5nFE?t=1207\n\nOut of 250, 218 worked flawlessly, 11 of the ones that required a \"fix\" (Disabling the iGPU) was because of the game, not intel, as the game has it set to launch on an AMD GPU if it detects an intel GPU.",
      "They don’t want genuine competition, they want another entrant to make their Nvidia purchase cheaper. Real shit.",
      "Excited to see if Battlemage will be the new go-to budget range"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "570",
    "matched_keywords": [
      "b570"
    ],
    "title": "The Intel Arc B580's (estimated) Production Cost + Profitability Analysis",
    "selftext": "Hey everyone!\n\nA lot of discussion in this forum has centered around wondering if Intel makes profit on the Arc B580.  I will attempt to provide a best and worst case scenario for cost of production.\n\n**Important Disclaimer**: I am not a semiconductor industry professional. These are just very rough estimates based on a combination of publicly available and inferred information (and I'll indicate which values are estimated).\n\nLet's begin! A GPU consists of a few main components namely the die (the silicon itself), the memory (VRAM) and the board (PCB).\n\n# 1. BMG-G21 Die Cost\n\nAccording to [TechPowerUp](https://www.techpowerup.com/gpu-specs/arc-b580.c4244), the B580 uses Intel's BMG-G21 die.\n\nBMG-G21 has 2560 shader cores, 160 TMUs and 80 ROPs. If you're interested in reading more about the core microarchitecture at work here, Chips and Cheese has a fantastic breakdown [here](https://chipsandcheese.com/p/intels-battlemage-architecture). These numbers aren't too important as they can change between architectures and aren't directly comparable, even between the same vendor. The B580 uses a fully enabled version of the die, while the B570 uses the same die but with around 10% of the cores disabled.\n\nThe main things on that page that we care about are the \"process size\" and the \"die size\" boxes.\n\nLet's start with the die size. Underneath the heatsink, the B580 looks something like this:\n\n[Isn't it beautiful?](https://preview.redd.it/0lhwbz3aulle1.png?width=977&format=png&auto=webp&s=0fc9cba3ba2d4d3dfd2604c8c9c5d509c8fafc95)\n\nWe know from TPU and other sites (and a little pixel math) that the die measures \\~10.8mm tall and \\~25mm across. 10.8\\*25 = \\~272 mm\\^2. This is a rather large die for the performance class. For example, the RTX 4070 uses a \\~294 mm\\^2 [AD104](https://www.techpowerup.com/gpu-specs/nvidia-ad104.g1013) die, and the RTX 4060 uses a 159 mm\\^2 [AD107](https://www.techpowerup.com/gpu-specs/nvidia-ad107.g1015) die.\n\nTherefore, the B580 is \\~71% larger than a RTX 4060 and \\~8% smaller than a RTX 4070.\n\nThe second thing we need to consider is the node, which in essence is the \"type\" (very generalized) of silicon that the GPU is made out of. A node has a certain number of production steps required to achieve a certain level of density/power/performance etc.\n\nA good video for those who want to learn more about semiconductor production is Gamers Nexus' tour of Intel's Arizona fabs [here](https://www.youtube.com/watch?v=IUIh0fOUcrQ).\n\nThe node determines characteristics like **density** (how many transistors can be put onto a chip), **performance** (how fast can you make the transistors switch), **power** (how much power it takes to switch a transistor, how much power the transistors leak when they're not switching, how much power is lost to heat/resistance, etc.), **cost** (how much it takes to produce) and ***yield*** (how chips on a wafer are defective on average). A chip designer like Intel usually wants as high density as possible (more GPU cores = more performance), as high performance as possible (faster switching = higher frequencies = more performance), as low power as possible (low power = less heat, cheaper coolers, cheaper power delivery) and as low wafer costs as possible.\n\nIntel notably does not use its in-house fabs to produce the Battlemage cards - instead the GPU team decided to use TSMC's N5 node, first seen in Apple's A14 Bionic mobile CPUs in late 2019. Importantly, the Intel Ark site specifically notes [TSMC N5](https://www.intel.com/content/www/us/en/products/sku/241598/intel-arc-b580-graphics/specifications.html), rather than Nvidia's similar but more expensive 4N process.\n\nSince semiconductor cost is a function of wafer cost, die size and yield, we can use [SemiAnalysis' Die Yield Calculator](https://semianalysis.com/die-yield-calculator/) to estimate the cost of production.\n\nThis is where the variability begin. Unlike the die size, which can be measured physically, we can only guess at yield and wafer cost. We'll start with the wafer cost, which according to Tom's Hardware (citing sources) ranges from [$12730](https://www.tomshardware.com/news/tsmc-expected-to-charge-25000usd-per-2nm-wafer) in a 2023 article to $[18000](https://www.tomshardware.com/tech-industry/tsmc-may-increase-wafer-pricing-by-10-for-2025-report) in a 2024 article (apparently N5 has gotten more expensive recently).\n\nNext is yield, which is measured in something called a d0 rate, the number of defects per cm\\^2. This is much harder to verify, as the foundries guard this information carefully, but TSMC announced that for N5 the [d0 rate was 0.10](https://www.anandtech.com/show/16028/better-yield-on-5nm-than-7nm-tsmc-update-on-defect-rates-for-n5) in 2020. Defect rate usually goes down over time as the fab gets better at production; Ian Cutress (former editor at Anandtech) who has a bunch of industry sources pegged the N5 d0 rate at [0.07 in 2023](https://morethanmoore.substack.com/p/tsmc-oip-forum-fabs-n3n2bspn).\n\n[TSMC N5 Yield \\(2023\\) ](https://preview.redd.it/yv0ywh961mle1.jpg?width=1179&format=pjpg&auto=webp&s=455a11c7f3fd402981f6a4efb266b5fc3ca8fe14)\n\nKnowing this, let's set a d0 of 0.05 as our best case and 0.10 as our worst case for production cost.\n\nPunching these values into the die yield calculator gets us something like this\n\n[for a 0.10 d0 rate](https://preview.redd.it/b53hwp3r1mle1.png?width=1886&format=png&auto=webp&s=755a73ba536f908779fb758b1df81b43d12817dd)\n\nand\n\n[for a 0.05 d0 rate](https://preview.redd.it/5ufjsdxo1mle1.png?width=1886&format=png&auto=webp&s=99919520a3ea97e5e7640990d2015f607031d8eb)\n\nTherefore, best case scenario Intel gets 178 good dies per wafer and 156 good dies in the worst case scenario.\n\nFor the best case, $12,000 per wafer / 178 = $67.41 per die before packaging.\n\nFor the worst case, $18,000 per wafer / 156 = $115.28 per die before packaging.\n\nNext, the die must be put into a package that can connect to a PCB through a BGA interface. Additionally, it must be electrically tested for functionality. These two steps are usually done by what are called OSAT companies (Outsourced Semiconductor Assembly and Test) in Malaysia or Vietnam.\n\nThis is where there's very little public information (if any semiconductor professionals could chime in, it would be great). [SemiAnalysis' article on advanced packaging](https://semianalysis.com/2021/12/15/advanced-packaging-part-1-pad-limited/) puts the cost packaging a large, 628mm\\^2 Ice Lake Xeon as $4.50; since the B580 uses conventional packaging (no interposers or hybrid bonding a la RDNA3), Let's assume that the cost of packaging and testing is $5.00\n\nThus, **estimated** total cost of the die ranges from $71.41 to $120.28\n\n# 2. Memory Cost - 19 GBps GDDR6\n\nThis is the other major part of the equation.\n\nThe B580 uses a 12 GB VRAM pool, consisting of GDDR6 as shown by [TechPowerUp](https://www.techpowerup.com/review/intel-arc-b580/4.html).\n\nSpecifically, 6 modules of Samsung's K4ZAF325BC-SC20 memory are used. They run with an effective data rate of 19 Gbps. Interestingly this seems to be downclocked intentionally as this module is actually rated for 20 Gbps.\n\nWe don't really know how much Intel is paying for the memory, but a good estimate ([DRAMexchange](https://dramexchange.com/)) shows a weekly average of $2.30 per 8 Gb, or 1 GB with a downward trend (note: 8 Gb = 1 GB). Assuming Intel's memory contract was signed a few months ago, let's assume $2.40 per GB x 12 GB = $28.80\n\n# 3. The Board (PCB, Power Delivery and Coolers)\n\nThis is where I'm really out of my depth as the board cost is entirely dependent on the AIB and the design. For now, I'll only look at the reference card, which according to TechPowerUp has dimensions of 272mm by 115mm by 45mm.\n\n[Front of B580 Limited Edition PCB \\(TechPowerUp\\)](https://preview.redd.it/gszrkimxgmle1.png?width=1055&format=png&auto=webp&s=bc10b691443aa0aa85f6aa2f157bfc187e035c30)\n\nJust based on the image of the PCB and the length of the PCIE slot at the bottom, I'd estimate that the PCB covers roughly half of the overall footprint of the board - let's say 135mm by 110mm.\n\nAssuming that this is a 8 layer PCB since the trace density doesn't seem to be too crazy, we can have some **extremely rough** estimates of raw PCB cost. According to [MacroFab's](https://factory.macrofab.com/) online PCB cost estimator, an 8 layer PCB that size costs around $9 per board for a batch of 100,000. I think this is a fair assumption, but it's worth noting that MacroLab is based in the US (which greatly increases costs).\n\nHowever, that's just considering the board itself. TPU notes that the VRM is a 6 phase design with a Alpha & Omega AOZ71137QI controller. Additionally there are six Alpha & Omega AOZ5517QI DrMOS chips, one per stage. I don't have a full list of components, so we'll have to operate based on assumptions. [DigiKey has the DrMOS for \\~$1.00 per ](https://www.digikey.com/en/products/detail/alpha-omega-semiconductor-inc/AOZ5517QI-02/13922542)stage at 5000 unit volume. The controller [chip costs $2.40 per lot of 1000](https://www.edn.com/multiphase-controller-meets-intel-imvp-9-2/#:~:text=The%20AOZ71137QI%20also%20works%20with,in%20lots%20of%201000%20units)\n\nLooking up the cost of every single chip on the PCB is definitely more effort than it's worth, so let's just say the PCB cost + power delivery is like $25 considering HDMI licensing costs, assembly, testing etc?\n\nAgain, I have no idea of the true cost and am not a PCB designer. If any are reading this post right now, please feel free to chime in.\n\nThe cooling solution is an area that I have zero experience in, apparently Nvidia's RTX 3090 cooler costs $150 but I really doubt the LE heatsink/fan costs that much to produce, so let's conservatively estimate $30?\n\nThe total **estimated** cost of production for an Intel Arc B580 Limited Edition is **$160.21** on the low end and **$204.08** on the high end, if I did my math correctly.\n\n# Important Caveats\n\n1. No tapeout cost\n\nIt costs a substantial money to begin production of a chip at a fab (\"tapeout\"), details are murky but number is quite substantial, usually in the tens of millions of dollars for a near-cutting edge node like N5. This will have to be paid back over time through GPU sales.\n\n2. No R&D cost\n\nIntel's R&D costs are most likely quite high for Battlemage, [this article from IBS](https://semiengineering.com/big-trouble-at-3nm/) from 2018 estimates a $540 million dollar development cost for a 5nm class chip.\n\n3. No Tariff cost\n\nThe above analysis excludes any cost impact from tariffs. Intel's LE cards are manufactured in Vietnam but different AIBs will have different countries of origin.\n\n4. No shipping cost\n\nI also did not consider the cost of shipping the cards from factories in Asia to markets in the US or Europe.\n\n5. No AIB profit\n\nAIBs have a certain profit margin they take in exchange for investing in R&D and tooling for Arc production.\n\n6. No retailer profit\n\nRetailers like Amazon and Microcenter take a cut of each sale, ranging from 10% to 50%.\n\n7. No binning\n\nNot all defective dies are lost, with some being sold as B570s at a lower price. This will decrease Intel's effective cost per die. No binning process is perfect and samples with more than 2 Xe cores disabled or with leakage that's too high or switching performance that's too low will have to be discarded. **Sadly, only Intel knows the true binning rate of their production process**, so it doesn't give me any solid numbers to work with. Hence, I had to leave it out of the analysis.\n\nThanks for reading all of this! I would really love to know what everyone else thinks as **I am not a semiconductor engineer and these are only rough estimates.**\n\nIt seems to me that Intel is probably making some profit on these cards. Whether it's enough to repay their R&D and fixed costs remains to be seen.",
    "comments": [
      "I calculated this a long time ago—**the A770, A750, and A580 were all loss-making products**. Intel's profit and loss situation in the GPU market has largely been influenced by these models.\n\nThe **A770** had a production cost of **$250-$300**, while its actual selling price averaged **$220-$320**. It has now been **discontinued**, and its price will no longer drop.\n\nIn contrast, the **B580 and B570 are expected to have a production cost of $150-$180**, which is **a significant improvement** over the A-series. This is largely due to the **clock speed increase** from **1600-2300MHz in the previous generation to 2700-3000MHz** in the new series, effectively boosting performance **by 35% at the same cost**.\n\nThis means that a product of the same cost level can now be sold at **a 30% higher price**. Based on its specifications, the **B580 should be priced similarly to the A580**, but due to its **much-improved efficiency**, it can be sold at **30% higher pricing than the A580** while still being well-received in the market.\n\nBecause of this, **Intel has officially established itself in the discrete GPU market**. Of course, there is still **a considerable efficiency gap compared to NVIDIA**. Compared to AMD, **Intel is slightly weaker in rasterization performance**, but **significantly stronger in media engines and ray tracing**.\n\nI believe the **B770 will put immense pressure on AMD’s higher-end GPUs**, though it **won't directly threaten NVIDIA**.",
      "Just an FYI N5 is the cheapest of the lot in N5/ family and N4P is the most expensive an N4P wafer cost around 15.5K while N5 is approximately 12-13K and N5 is a really mature process it has D0<0.1 deflects/cm2.",
      "God! So well done job buddy.   \nThanks for your effort on this one.",
      "Please see caveat #7:\n\n>Not all defective dies are lost, with some being sold as B570s at a lower price. This will decrease Intel's effective cost per die.\n\nI do understand how binning works LOL. Defects aren't all created equally, Nvidia's Blackwell launch has shown us that. All chip designers build a certain level of redundancy into their chips, but there's no way to harvest all of the bad dies. If there's a defect on a specific chip, you better hope that it's not something critical.\n\nFor example, if leakage on a sample is simply too high, there's no way to sell that as a B570 and it goes in the garbage bin. \n\n**But since we have no way of knowing Intel's recovery rate for bad dies, there's no numbers for me to work with, so I left that out of the main body of analysis.**\n\nAlso please actually engage with the post on its merits LOL instead of on pedantic bullshit, a frame buffer is just a colloquial term for VRAM. I can edit the post if that makes you happier.",
      "This is very interesting. I knew that with the A-Series they couldnt possibly make money, but the B-Series seems a lot more economical for Intel. But still considering how exlensive R&D is I think at best they dont make a loss on it, but they very likely dont make profit. I would guess that only Celestial and later are going to make a profit, because while R&D might be expensive driver development is a serious cost factor and I think it was very expensive to get the drivers to work properly. And that is a cost that is currently split up between A and B series.",
      "Agree with all the points here, Intel was taking some really vicious losses on the A series.\n\nA 400 mm\\^2 N6 die and 16 GB of VRAM (especially back in 2022 when VRAM exploded in price) was doomed to fail if it was competing against a mature, dirt cheap GA106 on a shitty Samsung 8nm process that nobody used. \n\nIntel has made extremely meaningful strides this generation in PPA.\n\nI wouldn't necessarily agree with the analysis with frequency because you really don't know how they tweaked the architecture, Xe1 and Xe2 are not really comparable on a clock by clock basis and your realistic clock speeds are influenced by stuff like the node, the power budget etc.\n\nTo be honest on a PPA basis Intel is still behind AMD and Nvidia on raster and RT, but the gap has closed to maybe 1 generation behind rather than 2.\n\nIf a hypothetical BMG-G31 \"B770\" existed it could probably at least match the RTX 4070 and Intel could probably hit a $375-$400 price point without taking a huge loss.  But that's their decision to make. I would really love to see it though!",
      "Regarding the development cost, remember that the Xe2 architecture is used also in IGPUs and server GPUs (eventually in the case since falcon shores was cancelled, so this only applies to Xe3 or Xe4). Not sure how to account for that in total cost, but there is some discount to the battlemage program cost if you look at it that way. And this would further get amortized if there ends up being a B770, which we're not sure even exists.",
      "Digikey costs are very high in terms of volume manufacturing. US manufacturer prices are generally super high. They can be 2-4x in cost.\n\nThe actual power stage costs can be cut by at least half or more. The board costs similarly may  be cut in half.\n\nFor memory you said they aren't using the full 20Gbps speed right? So they could be getting the cheapest versions of them. We don't know exactly what internal deals they are getting either. Same with the TSMC manufactured die. Often the relationships with the manufacturer is important because you can get deals beyond what is there for public.\n\nHeatsink costs are likely high too, even at $30. I, as an individual can find significant discounts by talking to various vendors online. Now imagine that with a much higher volume. It's probably again about half.\n\nSo they are probably making some money at the BoM level. From a bigger picture point of view, it won't be significant, even if we theorize their R&D is mostly paid off by needing iGPUs. Because even $1 billion is chump change for Intel. 5% of the 10% of the 100 million shipments per year is 500K. So they are earning maybe $30 million per year assuming $50 profit per card, which is nothing.",
      "I don't think Alchemist or Battlemage were ever meant to make a profit. Both of them seem to be \"Public Beta\" products which are sold to customers at a low price for the performance you get in order to get the product out into consumer hands, learn/refine based on feedback, and gain marketshare/mindshare so that when Celestial is released, they will be poised for success and able to build upon it. More than likely, Celestial will be the first architecture with the intention of being profitable.",
      "Don't worry, I already covered this, I did a best and worst case in the post, I assumed 12k wafer cost and 0.05 D0 for the best case, 0.05 is a REALLY low defect rate.\n\nI also already said that N5 was cheaper than N4\n\n>Importantly, the Intel Ark site specifically notes [TSMC N5](https://www.intel.com/content/www/us/en/products/sku/241598/intel-arc-b580-graphics/specifications.html), rather than Nvidia's similar but more expensive 4N process.\n\nBut any further refinement of the analysis is difficult since we don't really know for sure the exact wafer costs/defect rates. I think the range I provided represents a good estimate.",
      "I mean the original Alchemist due was supposed to be a 3070/3070 Ti competitor.\n\nI bopenif B770 does happen it's 4070 Ti performance. I mean TBVH B580 should be 3070/3070 Ti performance.",
      "Thank you for digging in and breaking this down.  I actually got a bit of the warm fuzzies that my wild guesses were even remotely close to someone who clearly understands this stuff better than I do. \n\nMy estimates were pretty far off your much more detailed analysis.  \n  \nThe only reference I could find for TSMC 5nm wafers was $17k, I guessed somewhere between 0.04 and 0.08 on D0. I WAAAY overestimated the cost per GDDR6 module (I guessed $8 at best, $20 at worst), with a PCB cost somewhere between $5 and $25, not including the SMD components, which I estimated to be around $20-$100 (I had absolutely no clue how much some of those components cost)\n\nI think I came up with $175.66 to $373.79 per unit, with a more likely cost being somewhere around $180-$220.\n\nAgain, thank you lots for doing this analysis.",
      "Nice post, it was a good read I expected way higher costs. i didnt know it was made on N5. I hope 18A is gonna be good.",
      "The real problem for Intel is not the Bill of Material(raw cost), nor the cost of R&D, nor the cost for support.\n\nIt's that the absolute dollar amount is pitiful.\n\nSo a 250 million per year PC market consists of roughly 40% desktop, which makes it 100 million per year.\n\nThe dGPU's portion is 20-30%. Let's say it's 25%.\n\nIntel's share of the dGPU market is \\~5% during the peak. Now let's assume they are earning $100 per card.\n\n100 million x 0.25 x 0.05 x 100 = $125 million dollars\n\nEven if you assume they are earning $100 per card, it's a pathetic $125 million dollars. In reality it's probably less than half, so $50 might be reality. In that case it's just $50 million net profit on $150 million revenue. It's nothing for Intel.\n\nIn the case of Nvidia, they have 85% or so marketshare. And they might be earning $250 per card in average, maybe even up to $350. In that case they are earning $5 to $7 billion a year net profit. Much better.",
      "Man a celestial reboot on 18A in 2026 could be crazy good but Intel HAS to fix their driver overhead if they aim for higher performance tiers.",
      "Agree people keep asking for b770 but with this driver overhead i dont think it makes sense.",
      "Rn focus should just be to supply as many B580s as possible and keep rewriting and fixing the driving stack.",
      "You are counting retail prices and not manufacturer prices. \n\nAlso, a smaller price is made when there is \" economies of scale \" meaning instead of making 500 unit you place an order with suppliers for 50000 or more, then the price gets even smaller. \n\nI assume that what you calculated has to be divided by x2.",
      "Thanks :) glad you enjoyed it.",
      "Agree with all points here. Hopefully 18a yields are good enough for them to make Celestial on it, Intel's wafer cost is much lower for their own nodes. If you watch interviews that's what the engineers in charge of Arc say, it'll take them a few generation to work their way up.\n\nAnd yes, the expensive driver development cost is a big caveat I haven't considered yet."
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "570",
    "matched_keywords": [
      "arc b570",
      "b570"
    ],
    "title": "Intel Arc B570 Review, The New $220 GPU! 1440p Gaming Benchmarks",
    "selftext": "",
    "comments": [
      "Ya I agree with the review that B570 does not really seem worth getting when $30 more dollars get you B580. \n\n\nOf course with the B580 constantly being out of stock it might be a tempting alternative. Though it still suffers from cpu overhead issue that the B580 also suffers from.",
      "Does it really suffer from \"overhead issue\" that match?  \nLook at theirs 13 games average.  Going from 9800x3d to 5600 fps drops:  \nB570 - 4%, B580 - 8%, 4060 - 0%, 7600 - 3%. \n\nWith upscaling enabled, fps drops:  \nB570 - 9%, B580 - 14%, 4060 - 6%, 7600 - 6%.\n\nRemove \"Spiderman\" from their 13 games average, and you will be getting even less difference.",
      "Anecdotally as someone who owns a b580 with a 5600x\n\nI don’t notice the overhead issue. I’m sure it’s there but when you turn off the fps counter and just enjoy your game you don’t notice. I’m getting playable (60fps+) frame rates at everything I throw at it.",
      "I see his complaints about B580 being out of stock, but where are his complaints about the 9800X3D also still being out of stock? Why does he not attach that to his numerous CPU reviews.\n\nLooking at his data, the RX 7600 also has some overhead issue as well.\n\nNow were attaching upscaling to reviews and using FPS produced as a metric? OK fine, why not use the far better image quality XESS produces over FSR as a metric as well? Is he going to measure that?\n\nAll these new hurdles and metrics were now using because Intel has a very competitive GPU.\n\nWait, he actually came to the conclusion you should buy an 8gb $250 RX 7600? LOL!!! I do agree that B570 should cost no more than $200 though. Otherwise I'd just buy a B580. The 8gb 7600 and RTX 4060 are joke products in 2025.",
      "B580 is the price value king at its price range.\n\n\n\nB570 is the price value king at its price range.\n\n\n\nIntel gpu division is killing it!",
      "Honestly the extra ram itself is worth that $30. Nowadays you'll want as much ram as you can get if you're gaming above 1080p just in case a new feature that eats ram gets introduced",
      "No it doesn't. Look at Tech Yes City that looked at other GPUs with a Intel lower end CPU. And both AMD and Intel GPUs suffered. This is not an Intel exclusive issue and even this HWU video shows this.",
      "The \"overhead issues\" is a campaign by anti-Intel PR teams.",
      "Australia is small and prices are artificially inflated there. Europe pricing would be objectively more interesting or maybe select asian countries",
      "Spider-Man might be a bad case but even with 5600x still perfectly enjoyable. Tested this back on my A750 and 5600 and great experience. Now it also has FSR Frame gen so you’ll be able to get a very smooth experience.",
      "I’m just pointing out the real world effect vs reviews. People are stressing about this overhead issue, it’s minor and you won’t notice it in your day to day life. \n\nJust enjoy what you have",
      "Thing with 8GB Vram is that you can turn down the settings and increase the fps. With Arc you literally can not increase the fps unless you upgrade your CPU. But will people who buy $250 GPU really upgrade their CPU? Its better to spend that money on a faster GPU. Something like rx6700xt will always perform better than Arc and you dont have to upgrade CPU.",
      "This just seems really negative. Even in his worst case scenarios its still better value (even if just 5%), and this is a 'disaster'? \n\nAm I missing something?",
      "The only B580s I have seen in stock since they dropped are priced at or above 380.\n\nI just ordered an ASrock B580 for 220.",
      "Yeah I think it's a placebo effect when u have the fps counter on, u feel like the game is stuttering if u watch the fps fluctuate but if u dont see the fps at all u don't feel the placebo stutters haha",
      "Very well thought out response, very mature indeed",
      "It's definitely a healthier way to look at things, but at that stage is there actually much difference between different GPUs for you?",
      "RT is a joke argument, both of those cards cant run RT at acceptable performance. But you do you.",
      "Upscaling at 1080p to get a playable result that favours B580 is a result for sure.\n\nB580 has 2.4 million more transistors on more advanced node. The fact it can loose in anything against RX 6700 XT is kinda impressive, not in a good way.",
      "Point of testing with upscaler is not to compare quality of upscaler, its to compare max fps that you will see if you drop down resolution or quality settings. Problem with CPU overhead with ARC is that it makes games CPU limited and no amount of turning down settings can increase fps numbers. Nvidia and AMD have no such issues, and they are better buy if you have lower end CPU. In the future Arc will continue to perform worse and worse compared to competition as games become more CPU intensive."
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "570",
    "matched_keywords": [
      "b570"
    ],
    "title": "Intel starts to make its mark in new GPU sales stats with the Arc B580, just ahead of B570 release",
    "selftext": "",
    "comments": [
      "Where do you buy one?",
      "Plenty of people are still buying those processors.\n\nRyzen 5 5600X is the 3rd best-selling processor on Amazon.\n\nRyzen 5 5500 is the 5th best-selling.\n\nThat and Intel markets the Arc B580 as the GPU to upgrade to from the GeForce GTX 1060 and GeForce GTX 1660 Super.\n\nWhat are the chances that PCs with those GPUs would have relatively new processors?",
      "I picked up the B580 LE, fantastic GPU for $249.",
      "Probably terrific encoding and transcoding support too.",
      "Now, you are changing your argument.\n\nYou implied that the issue is unsupported processors, which is not the case.\n\nCompeting GPU products from Intel's competitors don't suffer nearly as much performance drop.",
      "There will be plenty.\n\nThe overhead issue is warding off a lot of potential buyers.\n\nHopefully the scalpers lose money over this.",
      "How has scalping still not been solved?",
      "Were you replying to someone else?",
      "Nearly every game I saw tested at 1080p had performance nosedive when using anything older than 7000 series cpus...\n\n\nEven a 7600x saw it falling behind a 4060 in most games, so let's not pretend this isn't a serious issue, especially since the market for this gpu will be the ones more likely to not have a brand new high end cpu\n\n\n\"Only specific games with old low end cpus\" is as blatant as misinformation gets, gross this is being upvoted",
      "12400 and 5600x are dirt cheap probably $200 or less  for cpu, mobo and ram combo. The problem with used gpu are reliability and lack of tech like xess2 so people prefer newer gpus. Thats a total system cost of $600.",
      "Why shouldn't we be hyper critical? We're spending our money. We have a right to be informed and Intel must have known about this.",
      ">Overhead issue is blown out of proportion. There is a reason they state system requirements for the GPU.\n\nHmm...",
      "Right now you cannot get one for less then $400-$450 thanks to scalpers.",
      "Overhead issue is blown out of proportion. There is a reason they state system requirements for the GPU.",
      "The overhead issue happens in all CPU bound scenarios while the GPU is in use, which is probably quite a common scenario for a lot of people. Some of the most popular games in the world are generally CPU bound.",
      "Ebay is flooded with tray 5600x for $80 price range. Same with 12400. Cpus are super reliable. If I would be building a low end system I would buy 12400 with 580 thats an excellent budget system. Total cost of system will probably be less than 600",
      "I’m interested to see the A770 vs B580 production numbers. The A770 was barely scalped.",
      "Currently the best on the market I imagine? Does a 4090 still beat it in raw performance or does the newer tech give it the edge?",
      "That's a good question.",
      "My guess is that there is not enough volume to overwhelm their budget nor an incentive to favor selling to legitimate users."
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "570",
    "matched_keywords": [
      "arc b570",
      "b570"
    ],
    "title": "First Intel Arc B570 performance leak: 12% slower and 12% cheaper than B580",
    "selftext": "",
    "comments": [
      "In germany 12% slower then a B580, but 15% cheaper. It seems to be a really good budget GPU, we will see in many builds.",
      "It isn't suprising being 2GB less Vram, the chip used is exactly the same on both. The B570 uses the chips that had minor imperfections and instead of throwing them away Intel decided to use them and name the card the B570 with less ram. It is win win for Intel if people buy them. Tom Petersen talked about this in a video but haven't got the link to hand.",
      "Intel: 12% slower for 12% cheaper\n\nNvidia: 12% slower but you'll buy it anyways so let's double the price and half the vram LOL",
      "A perfect balance xd",
      "Driver overhead \"issue\" is too much fuss about nothing. Depends on specific game optimization all cards may suffer from it to some extent. You can cherry pick set of games were AMD 7600 will be doing much worse than B580/B570...  \n[https://www.youtube.com/watch?v=mVC2eP9xmXQ&t=1s](https://www.youtube.com/watch?v=mVC2eP9xmXQ&t=1s)",
      "So why do we even need this card? Neither the performance or price is significant enough to make a difference in this price range.\n\nSounds like cards that did not make the cut in performance tests during production to become B580.",
      "Options are options.  I could see this more for budget prebuild machines.",
      "[https://www.proshop.de/Grafikkarte/Sparkle-Arc-B570-Guardian-OC-10GB-GDDR6-RAM-Grafikkarte/3324297](https://www.proshop.de/Grafikkarte/Sparkle-Arc-B570-Guardian-OC-10GB-GDDR6-RAM-Grafikkarte/3324297)\n\n  \nHave fun with it. :)",
      "People did this with A750s as well and were able to match the A770 in most games that didn't need the higher VRAM, so I look forward to seeing how the B570 overclocking goes :)",
      "Another price value king for its segment",
      "Anyone got news on that b780? Amds lineup is looking like this generations winner if the price and performance are close to the recent leaks. I would love to see Intel compete in the 450 dollar range.",
      "Notebooksbilleger has B580s for €309 so that would be 12% cheaper. For some reason Dutch NBB the ASRock model is €300.",
      "which website my brother?",
      "You can overclok b580 too",
      "Probably waiting on clearer release dates on the NVidia and AMD.",
      "I think both are the best bang for your buck we‘ve seen in the last few years. My overclocked B580 runs like a beast for 300€",
      "Yea i have a 1660super and a 5 3600 .........i was gona get this gpu but now i think i gona buy a 4060 or a amd gpu .....they can still fixt it tho i hope",
      "Based",
      "So, the price to performance is scaled correctly. Amazing.",
      "Yes you are right, most companies do this kind of thing, thats why we suddenly got the 5700x3d for example. All the chips meant for 5800x3d that didn't make the cut. It is a good thing, reduces waste, and provides cheaper products for people."
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "570",
    "matched_keywords": [
      "arc b570",
      "b570"
    ],
    "title": "Arc B570 at Micro Center",
    "selftext": "",
    "comments": [
      "It's ***not*** a problem with rebar.\n\nRebar is enabled.",
      "12600K would be fine",
      "If they can fix the driver overhead, it will be. I’m rooting for Intel here, but they have a lot of work ahead of them.",
      "My guy, R5 5600 supports resizable bar, and is enabled by default.\n\nhttps://youtu.be/CYOj-r_-3mA?si=xbJ5XBcwGjDGeQvP",
      "Is there even a driver that works out for the 570 yet? 580 runs fantastic paired with my 5600x. Super performant, cool and quiet (power draw is way lower compared to any other card I’ve had).",
      "Why do they store them in the freezer?",
      "This isn't the issue and it's been posted ad naseum. Hardware ubboxed just posted a video of him testing with an R5 5600 and he dropped up to 57% performance from a 9800X3D in certain games. The RTX 4060 and RX7600 he was testing against did not see such issues in the same games or their drop was much less.",
      "The driver overhead really is a problem, budget cards should *not* drop 15% of their performance on $150 CPUs.",
      "Been that way since Covid and the mining craze for all GPUs.",
      "So down votes for speaking the facts, love Reddit",
      "You obviously seem to care.",
      "Incorrect. Rebar works on most amd motherboards including b450, b550, x570, x470 and even some x370 with at least ryzen 2000 or newer cpus.  Older boards do need a bios update. Not all boards got one but most have them available.  I’ve used it on ryzen 3950x with x570 and another system with a b450 asus tuf motherboard with a ryzen 5700x. \n\nRebar is a pcie feature and goes back to like pcie 2.0 spec. Even some older intel motherboards support it like skylake / kabylake era stuff.\n\nThe arc problem is not related to rebar that they are talking about but you effectively must have rebar on for them to give decent performance.",
      "Ryzen 5 5600X is from 4 years ago and still the 3rd best-selling processor on Amazon\n\nCore i5-12400F is from 3 years and is also around the same performance and still a good option for budget oriented.",
      "Yeah, this sub is just like that sometimes.",
      "In the video he states he is working on a 12400F review with the B580 to test as well on Intel side.",
      "Yes.\n\nSomeone got the driver working by extracting the driver and pointing Windows to it.",
      "lol get a load of this guy",
      "Just pair high end CPU with a budget B570 roflmao.",
      "Because it’s immensely significant and not everybody knows about it - without it, the B580 is just the clear best value GPU, but with it, the B580 becomes very difficult to recommend.\n\nAnd honestly, I’m not sure how much or how quickly Intel *can* fix it - it’s not a hardware flaw, but it will probably require them to make fundamental changes to the software, all for cards that have a pretty short window to be competitive before Blackwell and RDNA 4 eat their lunch.",
      "B580 at MSRP is a goated card for 1440p gaming.\n\nnvidia 50 series starts at more than 2x the cost right now, and doesn't get you 2x the fps (not counting fake frames)."
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "570",
    "matched_keywords": [
      "b570"
    ],
    "title": "Intel Arc Battlemage B580 & B570 GPU Specs, Price, & Release Date",
    "selftext": "",
    "comments": [
      "You have to wonder if we're getting shafted by Nvidia all the same.",
      "Assuming that Intel’s claims are true (which is a big assumption), that would put the Arc B580 at around the same performance as the Radeon RX 7600 XT.\n\nI am guessing that AMD, in response, discontinues the Radeon RX 7600 and slashes the price of the Radeon RX 7600 XT",
      "His presentation seems reasonably optimistic for what is claimed to be only a small uplift above the 4060. He also talks about overclocking optimistically in a way that indicates he has already taken a shot at it and that he wants to do an OC stream.\n\nWe know review samples are already out due to the guy that showed one off on stream.\n\nI suspect the drivers are at least solid this time given his general attitude, and overclocking appears promising as well, hell, even official intel slides showed it running at well over 3.2 GHz. I just hope there is a chance to squeeze more performance out of it with the drivers. 272mm\\^2 is a big boy die for such a weak card.",
      "Yeah, and when Nvidia gives you a lesser card for 350 it is totally fine,",
      "You have to wonder if Intel is actually making any money on this.\n\nBMG-G21 (used in Arc B570 & B580) is 272mm2 on TSMC N5.\n\nNAVI 33 (used in Radeon RX 7600 & 7600 XT) is 204mm2 on TSMC N6",
      "Dat price per frame\n\n<3",
      "7600xt is stupidly overpriced anyway. 350€ for it or 520€ for 7800XT. Easy choice",
      "never mind AMD lol, AD107 is 160mm\\^2 on the same node.\n\nthe die itself is well under 100$, closer to 50$ i'd say, going by the latest publicly available pricing information, so they're probably doing *okay.*",
      "looks like overclocked 3060 12gb",
      "$249 for ~3060 Ti performance minus a few percent is… *good*, but nothing to write home about, and given that this is still ~RDNA 2 efficiency, they’ll have to be more than 20 bucks cheaper than a 4060 to justify the product’s existence.",
      "It's very difficult to break into an established market.\n\nThe Intel of yesteryears with an unlimited war chest and the world's best foundry might be able to do it.\n\nThe Intel today? I just don't see it.",
      "I don’t think the current goal is to suddenly blow out the competition but to be comprable. Intel still needs about 3 years to become competitive in that space.",
      "Intel's own numbers don't put this anywhere near a GeForce RTX 4060 Ti",
      "According to **Intel's own numbers**, the B580 is about 10% slower than the RX 6700 XT and about **15-20% slower than the RX 6750 XT**.\n\nThe RX 6700/6750 XT have been selling for $270-300 for over a year now.\n\nThis did not move the price to performance needle at all. We had this level of price to performance for a long time now.",
      "RT performance is weirdly the selling point in price to performance vs AMD.  \n  \nBut what is your use case for RT at this performance level? Playing 5 year old RT games like Control and Metro Exodus?",
      "Very possible he already tried it, they have to pretend to not know anything but sometimes it shows a bit.",
      "We definitely are, you only have to read their annual accounts and shareholder statements to know that",
      "[About 22% in 1080p and 29% in 1440p](https://youtu.be/7ae7XrIbmao?si=NlxgHhUn3KHAdUqj).",
      "Speculating that it will be 10% faster than a manually OC 3060, but only at 1440p and the B580 can be overclocked too.",
      "Intel needs a high performance GPU architecture. It’s important for APUs, it’s important for machine learning, and it’s important for gaming. It’s a growing field that their competitors (Nvidia, AMD, Qualcomm) all have offerings in. Intel can’t just be a CPU company with minimalist iGPUs going forward."
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "570",
    "matched_keywords": [
      "arc b570",
      "b570"
    ],
    "title": "Intel Arc B570 ships one month ahead of launch: gamer mods drivers to make it work",
    "selftext": "",
    "comments": [
      "I honestly still don't get why the B570 even exists, when the B580 is substantially better for just an extra 30$. I'll build my first PC with Arc, I'm not a hater. I just don't don't get the point of the B570.",
      "If I am not wrong b570 doesn't need external power and can just stay in the motherboard a low profile GPU these are used for people who don't play games but still need a GPU for tasks",
      "this way Intel can sell the partly-defect chips. thats it.\n\nless waste and more profit.",
      "My only guess it's for people that buy a really good motherboard and CPU but are going to upgrade the gpu later down the line maybe. But I don't know why they are making it honestly",
      "For those defective chips that can’t be used for B580 maybe?",
      "didn't consider this, completely forgot about low profile GPUs tbh",
      "Yeah it's literally 2x what the PCIe slot spec covers (75W).",
      "For some markets, cheaper options may be more suitable. Even if have just B570 and B580 in the scope.",
      "Market segmentation and die binning. $220 is probably a little high at first, but availability may be better, and the price will probably drop a little later on. It's the same thing as the A770 vs A750.",
      "Are you sure? the official slide deck shows it has an 8 PIN connector it is also way higher wattage than what PCIE slot is rated for. \n\n  \n[https://www.notebookcheck.net/Intel-Arc-B580-and-Arc-B570-New-desktop-graphics-cards-announced-with-affordable-price-tags.927306.0.html](https://www.notebookcheck.net/Intel-Arc-B580-and-Arc-B570-New-desktop-graphics-cards-announced-with-affordable-price-tags.927306.0.html)",
      "B570 will come in low profile version from various vendors soon. My small homelab server currently uses A380, B570 will be a major upgrade.",
      "The b570 exists to compete with 8gb VRAM GPUs at a similar price point. Basically to eat market share from NVIDIA/AMD low end GPUs",
      "Server build is looking more power efficient little by little 🫡",
      "On one hand Intel GPU division is doing amazing things  on the other hand desktop CPU  division keeps screwing things up",
      "You dont know how fabs and wafers work, it seems.",
      "That’s all I can think too",
      "lol their distributors keep messing up, not that I'm complaining",
      "Would be interested if it has better idle power consumption than the B580 cuz honestly I would get it since I don't care about 1440p and the requirement of ASPM to have the lower idle power draw continues to be a disappointment.",
      "I agree this card should probably be $180-200. But it is for 1080p gaming and not 1440p",
      "I wonder if OEMs will get steeper discounts on the B570?"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "570",
    "matched_keywords": [
      "arc b570",
      "b570"
    ],
    "title": "is the arc b570/80 compatible with a asrock b650m-hdv/m.2",
    "selftext": "",
    "comments": [
      "Yes, it's compatible",
      "Everybody gotta start somewhere",
      "Yes but next time you should just use pcpartpicker, it shows what's compatible and what not.\n\nUsing that next time when you have a question you won't need to wait for people to respond",
      "Really?",
      "Yes, 7500F is enough for a B580.  Make sure you have a mobo that supports rebar.",
      "ryzen 5 7600",
      "thanks!!\nIm building my first pc and I couldnt find this information anywhere, so I dicided to just ask here",
      "is 7500f enough for b580?",
      "Yes, they are compatible.  But more importantly, what is the CPU?",
      "thanks!!",
      "that's really weird, is there a chance it could be because of the size for the case or something?",
      "You should have no issues with that combo.",
      "thanks!!",
      "Im actually trying to avoid any lightning, so it doesnt really matter for me.\n\nAlso, Ive seen one person saying that for newcomers this isnt a very good pick for a gpu",
      "Well vram is Like engine displacement the more the better but 10g should be enough for 1080p. Just search for some Reviews that compare both vram sizes. i don't know your Budget for me the 30-40*Insert your local currency* difference wouldnt Stop me.\n\nbut you probably wanna go for the Intel original Card.",
      "that's my mobo and I use the b570. yer fine",
      "Upgrade the BIOS and you should be good.",
      "I used the site but sometimes it was showing the 570 was compatible and other times it wasn’t. So I got confused lol",
      "Well… if im asking lol",
      "Hmmm I was afraid thats the case…\nSadly is the one of the few 12gb vram I can afford"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "570",
    "matched_keywords": [
      "arc b570",
      "b570"
    ],
    "title": "Arc 570 performing far worse than expected",
    "selftext": "Hey everyone, just got my first Intel GPU, the Acer Arc B570 Nitro. Now I'd expect it to be a significant improvement over my VERY old GTX1060 but...it isn't. CoD Black Ops Cold War doesn't run at all, massive graphics bugs, though it seems that might just be how it is.\n\nHowever I also noticed the GPU going to 100% load on something like Stellaris. It slows down the whole system, can't even watch YT on my 2nd screen, it lags. I am not sure what is causing this, has anyone experienced something similar? I am happy to provide any additional info you need.\n\nSystem:  \n\\- Win11 (up2date)  \n\\- AMD Ryzen 5 5600X  \n\\- Gigabyte B450 GamingX v2 (BIOS updated and REBAR enabled)  \n\\- 32GB RAM  \n\\- Triple-screen setup, all connected via displayport, main screen 1920x1200, 16:10, at 60Hz\n\nTroubleshooting so far:  \n\\- driver up2date (GPU was mounted today)  \n\\- tried CoDBO:CW with the various workarounds like deleting compiled shaders mentioned on the web, none successful so far, extreme screen tearing on the left side slowly stretching to the right, currently loading the HD graphics pack to check if it gets worse  \n\\- Stellaris initially ran the GPU to 100%, after restarting the game it is alright but \"feels\" off, also crashes after about 5-10m which didn't happen with the old GPU\n\nEdit:  \n\\- erased drivers with DDU (both potential Nvidia leftovers and Intel), reinstalled new driver  \n\\- updated MB chipset  \n\\- found out the CoDBOCW issue is known, no fix: [https://www.intel.com/content/www/us/en/support/articles/000095319/graphics.html](https://www.intel.com/content/www/us/en/support/articles/000095319/graphics.html)",
    "comments": [
      "Did you try things like enabling rebar and downloading the latest drivers?",
      "Have you tried reinstalling the games? Both games work perfectly for me.\n\nSometimes you need to reinstall the games for it to work properly.",
      "Is this also Windows 10 or 11?",
      "I was having issues with my b580. I fully reinstalled windows (left the game drive alone) and things work much better now.",
      "Ddu all nivida drivers also double check the gpu is fully slotted in. All else fails clean windows install and see if that works if not possible bad gpu. It happens mass producted parts have bad eggs from time to time",
      "Reinstall windows, update chipset drivers.",
      "My CPU doesn't have any.",
      "Huh, you are saying something there. I actually struggled to seat the card properly. The Asus B450 Gaming X has a plastic covered part right next to the GPU slot, maybe that's the issue...will check tomorrow, thank you!",
      "Sorry if that was the impression, added more info.",
      "Enabled rebar (shows as enabled also in IGS), driver is clean fresh install. Installed in Win11.",
      "Thank you, and no, this one very specifically does not.",
      "Not yet. My internet speed is horrible, CoDBO:CW would take half a week. -.- But could do it if that's the only fix.",
      "Second this. I moved from Nvidia, and every game I reinstalled works perfectly.",
      "AMD Ryzen 5 5600X, added more info on the original post.",
      "Agreed. This was the case for me as well. Even though I used DDU to uninstall the Nvidia drivers, some games (like Cyberpunk) didn't work properly until I reinstalled them.",
      "Run DDU and wipe all display drivers properly, then install latest intel driver.",
      "I was planning on getting this card and pair it with my 5600. 😭💦 I have a A580 and a B580, so far I had no issues.",
      "Go to bios and enable above 4g decoding",
      "That doesn't seem normal at all. You will probably have more luck contacting intel support about this",
      "Cod just doesn't run as well on intel/nvidia as AMD bro. Rven a rx 6600 probablt performs better then a b570. One of rhe few games where it's the case but yeah"
    ]
  },
  {
    "brand": "intel",
    "generation": "battlemage",
    "tier": "570",
    "matched_keywords": [
      "arc b570",
      "b570"
    ],
    "title": "Intel Arc B580 or Intel Arc B570 for budget gaming",
    "selftext": "So the headline say it all, want to give a shot to Intel Arc gpu since new \"budget\" gpu's from amd and nvidia seems a total disaster.\n\nUsually every post i see about Intel Arc B570 is that ppl say just to buy B580 since its \"only 30$ more\", well there a catch in my country difference is more like 100-70 usd between this 2, so that said is it worth to pay 70$ ish or more for B580 or B570 is just about fine for the price? For reference B570 priced starting at 284$ min price in my country while B580 cheapest is starting at 351$\n\nIm sitting on gtx 950 2gb so yeah either one will be like travel in damn space for me that's for sure, but still want to know if it worth to pay 70$ more in this case\n\n(cpu ryzen 5 5600) and also as a note the fact that i still on gtx 950 kinda gives that away already but yeah if i buy either one of this or any new gpu i will more likely hold too it for a long while it's not a situation to buy it and replace in a year kind of situation for me.\n\nAlso ppl often recommend used market but is very bad here so not a real alternative either, used gpu being very overpriced and just a little off a new one like maybe 30% lower its crazy tbh",
    "comments": [
      "Not really bro. With a b570 the bottleneck is waaaay less thrn with a b580. I have ab570 with a 5600x and get similar performance to videos i see",
      "Get a used amd or nvidia used gpu. Your cpu will bottleneck you, but I guess unless you will buy used you don’t have a better choice",
      "And like guys i appreciate recommendation of alternatives i rly do, but i searched and weighted a lot of gpus that i can afford with prices in my region and B570 or B580 seems most optimal and cheapest in a long run for price to performance and future proofness (more vram) the main question i have is it worth to pay 70$+ or more for B580 or not, is the performance jump between B570 and +2gb of vram worth that kind of money or not that all i want to know.\n\n\n\n#",
      "B570 and B580 share same BGM-G21-GPU, so they're literally the same - but with some differences.\n\nHad the B570 (was 280), returned it for a B580 (was 330) - it just felt bad somehow, to have 2 GB less memory with lesser bandwidth, to have 2 cores less and remaining cores with less MHz, topped by 200somewhat less shaders. B570 is a failed B580, relabeled to still make some money from it. \n\nBenchmark-wise there's just 12% difference. So I thought, 150W vs 190W, that's 25% more power for just 12% more performance. Wanting a frugal system, I took the B570. But playing on 1440p it felt like I do miss those 2 GB of RAM. \n\nIt's a slight difference, in performance, in specs, in price. Only you can say, if it's worth it to you. For me it was.\n\nAround 20-25.5  I'd expect the AMD RX 9060 to be announced, perhaps you want to delay your decision unitl then.",
      "From lowest to highest price it seems your options are B570/$284, RX7600/$320, 4060/$341 and B580/$351. I think you have to look at the games you play. Newer games - choose B570/580 depending on how much you want to spend. Older games - RX7600 or 4060.\n\nIn terms of B570 vs B580 - how much is that $65 worth to you? It's probably 10% more performance a bit more future proofing, but I suspect your CPU might run out of gas before 10 vs 12gb VRAM becoms an issue.",
      "The B570/B580 still experience a noticeable drop in performance in certain games paired with a 5600. At 1080p, the B570/580 doesn't perform as well as it does at 1440p. \n\nFor competitive gaming, AMD and Nvidia are better choices. For story games, it varies. You should research the games you play and consider potential future performance gains with updates if you choose Intel.",
      "well i still using gtx 950 2gb so not very haha",
      "Then B580 is your best bet, the 12GB vram and higher specs meaning it'll remain viable for much longer. It can even go up to 1440p if you ever feel like getting a new monitor.\n\nI'm currently using a GT 1030 and looking to upgrade to a B580 in the future.",
      "Whats your CPU?\n\nWhat are the prices of the rtx 4060 and rx 7600?",
      "would be nice if you told us your country, that way people of the same country could give you more accurate recommendations",
      "sorun varsa buradan veya dmden sorabilirsin, 4 aydır kullanıyorum ve çok memnunum 1080p oynayacaksan 570 iyi bir tuning ile base 580 ile eşit ve ayarlardan bunu yapmak çocuk oyuncağı, 1080p için özellikle istiyorsan 2. el 3060ti 16gb vs işini daha çok görür ama ne kadar ileri dönük olur bilmiyorum, bu kart tamamen pc oyunu oynamak için bunu bilmen lazım ne vr ne render alırsın",
      "Used market is very bad here not worth at all like for example i was looking in rx 6600 and a used one is 195-180$ ish while new one is like 220-240$. WIth a newer models like used 4060/7600 situation even worse with only like maybe 20% off a new one with zero warranty so a cba that gamble can't afford to get burned like that",
      "5600",
      "Cheapest rtx 4060 is **341$ and rx 7600 is 320$**",
      "Thanks, i will w8 because why not it's close enough but tbh leaking info about price of RX 9060 is very disturbing",
      "Hmm tough choice.\n\nWhat resolution do you play on?",
      "1080p",
      "how often do you replace GPUs?",
      "If he has money for a monitor, he should buy a better GPU instead, which seems more logical to me.\n\nThe B580 still isn't the top budget GPU for all purposes, but its VRAM is beneficial for certain games. \n\nI recommend waiting 1 to 2 weeks for the announcements of the new RTX 5060 and RX 9060 (XT) to see if prices drop."
    ]
  }
]