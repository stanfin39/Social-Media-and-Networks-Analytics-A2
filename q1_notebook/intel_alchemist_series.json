[
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "My ARC A380 recently arrived",
    "selftext": "",
    "comments": [
      "Please let us know what difference does it make if you max out the power limit.",
      "Cool ! How much did you pay for it? (If you had to pay for it)",
      "It was $139.99 USD.",
      "i agree people saying their is a significant increase in performance when increase power limit",
      "nice! please do some AV1 Encoding! Hopefully Nvidia and AMD will follow suit",
      "I hope those higher power limit gaming benchmarks were legit. I was expecting from the A380 something to beat the GTX 1060, not struggle against a RX 6400.",
      "GL bro, please let me know how it runs RE2 Remake, Albion Online and LOL in case you own them. I wanna get the a7\\*\\* since Nvidia its giga overpriced here in Mexico.",
      "newegg sells them, so I assume through them",
      "Correct, I purchased it on the launch day in Newegg at around 2a.m. CST.  That was the right choice as it went out of stock a couple hours later.",
      "The performance is quite good for its price, if I had a chance to buy it in Turkey right now, I would like to buy one at this price without thinking. I have a GT 1030 right now.",
      "I'm curious to see long term improvements, since a driver update granted a 100% increase in ray tracing performance (even if it's not exactly amazing to begin with) \n\n$140 is a decent price, but I'm really hoping they can get performance to *above* mid-high end Pascal cards, seeing as they're 5 years old at this point. And if XeSS will be as beneficial as DLSS is for RTX cards",
      "I'm interested in this too. With its price it could be great for AV1 encoding, running more displays and just to play around with.",
      "Youtube supports it for Video uploads they're even upgrading all of the server hardware for AV1 Encoding same with Netflix.\n\nNow for Twitch they been working on AV1 support for some time and i think by the end of the year you will see more platforms supporting AV1.",
      "It might be the case of genuine interest and those buying it to encourage Intel to keep at it.",
      "Newegg on Launch",
      "They are currently the only GPUs to support AV1 encoding, making them well worth it for that reason alone. They have double the video output than any other budget card. And they are good for certain productivity applications and linux.",
      "How did you purchase one?",
      "I would at least get a 2 fan model if I wanted to tinker for performance.",
      "This one is their first Dedicated GPU if I'm not mistaken. They have higher end models coming out sometime this year allegedly.",
      "Have fun🤞"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "a380"
    ],
    "title": "Intel Arc 2024 Revisit & Benchmarks (A750, A770, A580, A380 Updated GPU Tests)",
    "selftext": "",
    "comments": [
      "Arc is starting to look seriously impressive. If the A750 beats the 6600 XT in 90% of games and the latter is still $240, the former is starting to look like the best value card for *everyone* in this price range, not just enthusiasts.",
      "This is really impressive, I thought the a770 was closer to the 3060/6600. Gj intel",
      "you are not missing anything with starfield. To say that this game is mediocre will be praising it. Focus on good games not 5/10 games.",
      "Why do you have such allegiance to a company that doesn't care about you? \n\nAMD, Intel, Nvidia just buy whatever seems better for you",
      "Based on AMDs slow progress to increase performance over the years.",
      "Can we not with the fanyboyism? More competition is good for everyone.",
      "A good video, and a fair comparison.  \n\nIt’s a bummer Stairfield still appears to be having issues on ARC.  \n\nHonestly probably better if Battlemage isn’t launched until another 3-6 months of driver development time has passed overall.",
      "Really pulling for Battlemage. We need a third party option.",
      "Looks like ARC is racing to the top fast. BattleImage will leapfrog AMD and get very close to Nvidia.",
      "What do promises have to do with anything? You can see current performance then realize they are making faster chips that will have more cores as well.",
      "The problem with Amd they also still have massive driver issue after years. If Intel keep progressing their driver, it's very possible for them beat Amd in gpu market especially with Battlemage.",
      "I agree - but it is one game that a lot of well known sites like to benchmark, and it certainly left an impression on many that Intel wasn't ready at launch.   Games hyped that much should be given prioritization for drivers.. \n\nI think we need to see Intel stay ahead on more AAA launches in the future. \n\nI'm optimistic..",
      "Amd drivers are pretty mature and their driver issues (Intel's too) are very overblown. Their drivers today are about as stable as Nvidia's and are still more reliable than Intel's. I really hope that battlemage will moreso give Nvidia a reality check at the high-end and upper mid-range.",
      "But but but, BIG NAVI",
      "Nvidia getting out of the GPU space would be more likely than that",
      "Remember \"RIP Volta\"? What happened after that? AMD shooting themself in the foot with their meme marketing LOL",
      "I will happily if you can point to anywhere I did that.",
      "It's the most current version of the Creation Engine, which TES6 will be based on. So it's worthwhile to keep it there for that.",
      "Driver is only matured if didn't cause serious issue, like Nvidia driver they are worth to say matured, but for Amd even many people on Amd sub reporting they still see massive problem on radeon like game crash, bluescreen, or problem on gpu acceleration, now with antilag+ which is the worst so far. The problem with Amd they always promise but rarely deliver, unlike Nvidia who are fast in fixing issues.",
      "I think the driver issues are still common enough that the rx 6600 would be the go-to entry level GPU."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "ACER to launch Arc B580 NITRO in white and Arc A380 low profile GPUs",
    "selftext": "",
    "comments": [
      "Still releasing *Alchemist*?? That's an architecture that really doesn't need more money thrown at it.",
      "Intel probably still has some TSMC N6 allocation left over",
      "Those A380 Low profile cards make awesome AV1 encode PCs and HTPCs. So by all means, if the prices reflect the age of the architecture and GPUs are priced right, there is still a market for them.",
      "Im very interested in this specific a380 to add in my unraid since it is low profile.\n\nI just hope they don't have the same fan issues the current a310 and a380 available today.  Also the pricing of a380 now is $150 in microcenter.  Acer should price it aleast $120 or $99."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "[VideoCardz] - Intel ARC A380 desktop GPU is outperformed vy Radeon RX 6400 in first independent gaming tests",
    "selftext": "",
    "comments": [
      "I think it mostly due to the drivers, we'll see how long it takes them to get to a stable driver... I guess more time.",
      "I think you mean *encoding*, the RX 6400 also does decode (excluding AV1 decoding).\n\n[Also, the drivers Xe are broken for encoding too currently](https://media.discordapp.net/attachments/682674504878522386/984701519603564544/unknown.png) lmfao\n\nEdited for sake of clarity",
      "Frankly speaking, I don't think there's a clear cut answer to that question. We don't really know how much performance is left in the tank in the first place.",
      "ELI5, but considering we're all hoping the drivers will stabilize and optimize performance to a degree where we can claim this will be competitive, what are the performance uplifts that we can expect to see once the drivers mature? 20%? 30%? 50%?",
      "I don't like how Intel is putting same price on their product as the competition eventhough it's slower, less stable and Intel having really bad reputation for their graphics. \n\nNo one except OEM and a few people who don't even know what they're looking at will buy it for this price.\n\nThis is sellable only around 100$ as dedicated gpu. Otherwise zero reason to buy over competition if it isn't even cheaper.",
      "Will intel finewine be better than amd finewine?",
      "Intel couldn't roll out a stable set of drivers to save their life. This is starting to look like a spectacular flop from where I'm standing.",
      "yes it was using \"only\" 65w while being significantly outperformed by one using 50w",
      "Dg1 is just an igpu in a separate package. So cannot even call it a dgpu. So essentially this is intels first dGPU.",
      "> Intel couldn't roll out a stable set of drivers to save their life.\n\nThe same used to be true for Radeon",
      "It's probably not really about stability. Driver optimizations are made for gaming in general and even on per game basis and that's a lot of work that needs to be done over time. Nvidia and AMD have years of work done already. And game engines also make optimizations for different graphics architectures which obviously hasn't happened for intel yet since there have not been intel gaming GPUs in the market. \n\nConsidering the card can do a lot more in the 3dmark tests than the competition when everyone has application optimized drivers I wouldn't be too worried about the actual performance of the hardware. But it is probably a good idea to wait a year or two before expecting a solid performance in all games.",
      "Source: just trust me bro.",
      "People have used \"the drivers aren't ready\" excuse for Intel for the past 15 years of gpu issues. They started hiring to improve drivers (for like the 8th time) in 2017 when they decided to push into dgpu. Why anyone thinks the drivers will turn everything around at some point still I don't know.\n\nThe big problem isn't that it's behind, it's that it's almost 50% more transistors and and what, a 53W card vs a 92W card, and it's slower. If it was 20% smaller and used less power and performed 20% less, that's great. This thing should be drastically faster than a 6400.",
      "Well this first generation is ruined, it should have come out with good drivers at the beginning of the year to be really competitive, but they could not likely for the drivers.\n\nHopefully with battlemage it will be another pair of shoes! We need a new competitive gpu manufacturer to stop the Nvidia and AMD duopoly.",
      "Intel's? Their iGPU drivers are still very stable for everything *besides* gaming. But that's also kind of an issue when building dGPUs aimed at the gaming market, you ideally want rhem to be able to... game.",
      "If that were the case the 6400 would be way worse off.",
      "It's hilarious that all the \"It's Raja's fault!\" people think Raja does everything; he does the hardware, he does the driver, he mops the office, he takes out the garbage, anything that goes wrong is Raja's fault!",
      "Wow then the A380 really sucks at gaming if it can't even beat a GPU that's not for gaming.",
      "Where does Intel say the 6400 is 10x worse?",
      "Now you're outright lying. Intel claimed it was about about 25% better performance/yuan, but it's MSRP is also about 20% lower. In terms of actual performance that gives you a lead of about 5%."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "I chose Intel Arc a380 over rx 580 and GTX 1660 super. Is it worth it??",
    "selftext": "I'm a beginner gamer. I chose Intel arc 380 over rx 580 and GTX 1660 super, for my ryzen 5 5600g processor and 24gb ddr4 memory. I did so for the Ray tracing and av1 encoder in the new Intel card, but I heard Intel GPUs are not so good with all the games, but support is upgrading day by day. Did I make a right choice or should have gone for one of the old cards without active support and new features? And is this graphic card too slow for my processor according to bottleneck??",
    "comments": [
      "The Arc cards are fine budget cards with a modern feature set. I'd choose one over either the alternatives you mentioned, but I'm otherwise and think Intel needs to succeed and would give them my money. *We* deserve that they succeed.\n\nIf you have an intel CPU with iGPU you should be able to have it share compute workloads via DeepLink, which is a very nice feature - I hope AMD is paying attention.",
      "probably the 1660 super would have been better overall vs the others HOWEVER given you went with Intel Arc, which is fine, I think the 5-series card would have been better than the 3.\n\nBut you made a choice, its fine, and you can hold onto that till battlemage comes out and perhaps an affordable upgrade that is NICE will be yours.  \n\nIn battlemage we hope!",
      "a580 is not available in our country. Moreover with 750 in existence, 580 is a mere alternative at same price. I guess since I already chose, I'll hold on to it till battlemage releases",
      "Driver support isn't on par with Nvidia and AMD, but is slowly improving.",
      "Esports is what I play mainly though. A580 is not available in our country and rx 6600 and arc a750 are way too costly here. Converting to usd, I got 380 for less than 100$ and rx6600 and a750 are 300$",
      ">techpowerup\n\nThat review was more than 1 year ago.\n\nThe drivers have improved substantially.\n\nI can play Cyberpunk 2077 with Ray Tracing on my A380 with XeSS set to Quality.",
      "Also XeSS upscaling is very good on games that support it.",
      "It's better than AMD's FSR.\n\nEven the guys at /r/AMD admit that.",
      "Yes. I'm hoping it should be enough for esports titles gaming and streaming.",
      "$/frame and your use case, you did good.\n\nJust know it'll never do raytracing to a good level and is struggling already on AAA stuff",
      "Yeah. a380 max power is 75w and it doesn't even have a dedicated power pin, so I guess it's ok with the power",
      "And make sure you have ReBar/SAM enabled.",
      "Can't wait for battlemage and celestial",
      "RX6600's are 150 used. The A580 isn't much more than that.\n\nA380 is only worth it as a dedicated AV1 rig, but not for gaming.\n\nIt'll do esports stuff fine though.",
      "RX 580 is 35% more powerful and 1660 super is like 68% more powerful than A380, according to techpowerup, so not the smartest choice imo.",
      "Well so far the letters B C and D were rumored, B for Battlemage, C for Celestial, and D for Druid\n\nSpec wise I have 0 clue, only thing I've heard is that Battlemage is gonna be in competition with the level of a 4080",
      "The techpowerup chart data is based on launch drivers. We all know how much performance has increased since then",
      "Intel GPU is a new variety and they are providing constant update which increases performance and their GPU will get update for a log period of time while GTX 1650 will stop getting GPU update after 2yrs",
      "> If you have an intel CPU with iGPU you should be able to have it share compute workloads via DeepLink, which is a very nice feature - I hope AMD is paying attention\n\nYou mean the thing AMD did 9 years ago with APUs and their lower end GPUs? Yeah, I think they know.",
      "are there leaks about the new intel cards already?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "Views on Intel Arc A50, does anyone have one?",
    "selftext": "Hi again r/graphicscard\n\nAnother question, this one a little more niche.\n\nI have a 3U rack mount server which I use for NAS/Media and offload media encoding, I essentially feed it my physical media and let the iGPU on my CPU turn it into media I can play on all my devices at leisure.\n\nIt's a Intel i7 11700T, UHD 750.\n(35W efficiency CPU, runs very cool and still packs a decent punch)\n\nI want to stick to Intel as the encoding results (at least quality on playback to my eyes) have been far superior on that iGPU compared to NV and AMD.\n\nBeing 3U, it rules out nearly all full height cards as I can't fit anything in that goes more than a few mm over the PCI bracket height and none PCIe bus powered cards have power connectors at the top.\n\nThis is where the A50 looks good, low profile, bus powered and appears to be an overclocked professional Arc A380 with a beefy blower cooler that'll throw the hot air out of the case.\n\nQuestion, does anyone have one? Do you encode media with it? Does it floor iGPU results?\n\nI know it's expensive for what it is, but it'll be on 24/7 and suit my needs if performance is worth it.",
    "comments": []
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "How do I stop my Intel arc a380 from stuttering?",
    "selftext": "",
    "comments": [
      "Yes A520 motherboards do support ReBAR according to MSI. [https://www.msi.com/blog/improve-performance-with-resizable-bar-now-available-for-msi-geforce-rtx-30-series-graphics-cards](https://www.msi.com/blog/improve-performance-with-resizable-bar-now-available-for-msi-geforce-rtx-30-series-graphics-cards)\n\nIntel GPUs absolutely need ReBAR to be turned on in order to perform as intended.",
      "Do you have both ReBAR and Adaptive Sync turned on?",
      "Does your motherboard support ReBAR?",
      "If you bought arc expecting a problem free experience out of the box, you made a mistake.\n\nIntel is new to making dedicated GPU's, and their platform is still being built up. Their drivers are not caught up with nvidia's and amd's and it'll take quite a bit of time before they do. Games aren't optimized for arc since the userbase is comparatively tiny. Then there's the question of whether intel will continue spending money on the arc division while the company is in rough waters financially and is trying to cut back expenses. Optane was cut a while back too, there's a reasonable chance Arc will be cut too or at least will be put on the back burner. \n\nAdd to that that the a380 is simply a weak card and you basically got a card that shouldn't really be used as a daily driver for gaming. I'm all for a 3rd player in the market, but Arc just doesn't compete at this point in time.\n\nMaking GPU's is hard and requires time to compete with the established brands, so no critique of the state of the arc lineup. But marketing managed to trick people into thinking it's more than it really is. \n\nMaybe there's a fix, and I hope for you there is, but otherwise you might be better off switching to nvidia or amd for a daily driver.",
      "I have adaptive sync on but not ReBar on.",
      "Yes it should. From what I know you will get more performance on the A380  by increasing the power limit rather than the GPU clock speed [https://www.youtube.com/watch?v=mBepsi1JDrA&t=296s](https://www.youtube.com/watch?v=mBepsi1JDrA&t=296s)",
      "But you're wrong to say that.  The poster did not ask about your opinion about Intel products.  He was looking for help.\n\nEssentially telling him he should buy something else or that he wasted his money is not help",
      "Would oc it also be good? since I get a maximum of 45 power draw not even close to the max 75 watts.",
      "I couldn't really do anything since in my country it's the only gpu near it's msrp price.",
      "I have a msi b520 pro vh? So I think it does.",
      "Oh sorry it's a520m pro.",
      "Yeah like others said, you need resizable bar to get the optimal experience with intel cards.",
      "I don't think it's returnable now since in my country retailers are real picky.",
      "Without re-bar on it won't help much",
      "An Rx 6600 is 100 dollars more than how much I bought the arc a380 so I don't think that's a great idea. But thanks for the suggestion.",
      "Is it B Two-Fifty or A Five-Twenty because b520 isn't a real thing.",
      "Ok cool. So like Pat said, you can flash the bios from the msi website for your board if need be and enable rebar.",
      "Did you manually download the latest drivers and enable ReBAR on your mobo? I had noticed when checking drivers that it didn’t automatically bring me up-to-date. As for ReBAR, if you have at least a Ryzen 3000 or 10th gen Intel processor your board is likely to have the option (may need a BIOS update depending on the version). I got an A380 just the other day and have been able to play most games at medium-high settings at 1080p. Hope you get your card working and enjoy! Definitely going to be a card that will need some time for drivers to catch up though and that’ll help overall performance\n\nEdit: saw in another comment that your board should support ReBAR - I would make that your main goal as that should give you a decent boost. Been seeing people say up to 30%ish",
      "Turn rebar on. Install latest drivers. (Be sure to run DDU old drivers if your system used to use either AMD or Nvidia previously). Beyond that you're kinda just out of luck. Intel is a poor choice if you're not very tech savvy and actually want to play games without issues. \n\nI like my A770. While the most recent drivers have helped a bit, it still has issues on lots of games. Especially older games. My nvidia and AMD cards game MUCH better and get much more consistant framerates. \n\nI'm sorry to say, but outside of waiting for quite a while for drivers keep rolling out, (assuming rebar is already turned on) there likely isn't a solution for your stuttering issues. But in the event rebar isnt turned on on your bios you absolutely need to turn it on. Stuttering is much much worse without rebar turned on.",
      "I turned it on before I turned it on it was averaging about 200fps when I turned it on it was about 280 fps on valorant"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "Intel Arc A380 Gaming Graphics Card Review & Benchmarks (Hardware Unboxed)",
    "selftext": "",
    "comments": [
      "AMD Unboxed back at it once again.",
      "Look at comment: WTF\n\nLook at username: what in the fu",
      "Lol so salty. :P",
      "Don't you dare ratio me!",
      "This particular model is hopeless but the other two waiting in the pipeline don't inspire much confidence either. The A750 was running Death Stranding at 1440p with the Default preset at 80-90fps, which is also what you get from the RTX 2060. Except the RTX 2060 is three years old, doesn't require resizable BAR, and won't fall apart if you show it a DX10 or DX9 title.",
      "Solid review. \n\nOther outlets have mentioned that the new Intel GPUs have trouble with DX11 and older games, and only work relatively reasonably with DX12. \n\nAs you own the GPU, have you considered trying something like [DXVK](https://github.com/doitsujin/dxvk) (it says it's for Linux, but also works on Windows) or maybe even d912pxy for older games, to see whether replacing a few DLLs in the game's folder is enough for the GPU to start performing better? As far as I'm aware, nobody has tried this yet.",
      ">officially \n\nI dont think its officially supported... but it works just fine. As expected - it is just a platform feature.",
      "To be fair, Intel has spent the better part of the last decade or so developing very rudimentary drivers for their iGPUs (since their only real purpose has been simply a display output for systems without a discrete GPU, save for some encode/decode scenarios) \n\nI'm not trying to make excuses for them, or cut them any slack--if they wanna compete in the big leagues, they need to work on compatibility and performance across the board. Rather than rely on just having cheaper pricing overall vs AMD and Nvidia. Hopefully XeSS will gain widespread adoption, and they'll continue to work on how the cards handle DX9 & DX11. They really shouldn't have even tried to claim that any of the Arc series were \"comparable to a RTX 3070\"; they still have a ways to go before they can even beat out high end Pascal / midrange Turing GPUs. \n\nFinal note, I found it kinda funny that their ray-tracing was more or less disabled initially and all of a sudden gained a 100% improvement from just updating the drivers. So far it's been a rough start, but time will tell.",
      "Yeah, this just sucks. At least the $129 price point, 6GB of VRAM, encoding features and decent efficiency show that Intel’s heart was in the right place here. This effectively kills the RX 6400 for anyone who doesn’t absolutely need a low-profile card, but that doesn’t make it any less horrible. This thing needs to match or beat a 1650 Super before it’s worth buying.",
      "It is disappointing no doubt on that, but we actually didn't expect Intel to come in guns blazing to beat both AMD and Nvidia for their first time to bring consumer based Graphics Card to the market.\n\nI really really want to see Intel become extremely competitive against Nvidia and AMD.\n\nJust imagine if they committed to this in the long run and was able to bring out a product that competes with the \"RTX 5090\" or \"RX 8900 XT\".",
      "Did you notice performance difference using Intel CPU? Just Curious.",
      "Rebar has been a part of the PCIE specification since 2008. If a BIOS exposes the option to turn it on, then it should work with cards that also support it. Just like plugging in a Dell branded USB device into a Lenovo computer. Nothing special here.",
      "Lmao. It's not.\nI would recommend a real technical channel. But 99 of people on here are gamer bros. Not worth time or effort.",
      "I don't know for sure if it works \"just fine\". It works way better than it being \"off\". \n\nGamers Nexus had the A380 faster than a 6500xt in F1 2021, while here it's like 20% slower than I expected. And they used an Intel system I believe.\n\nI'd be curious to see if other games are effected. We need like a Ryzen 5600 vs 12400f comparison using this GPU, or some up coming A700 series.",
      "Jeez. It's really gotta be priced right (e.g., no more than $120 MSRP?) to feel like a worthy buy for gamers (if and when it actually *arrives*).\n\nIt doesn't seem catastrophic (e.g., $130 A380 + reBar is about the $150 RX6400 on PCIe 4.0).  So I agree Intel should keep iterating instead of throwing in the towel (the GPU market *is* pretty large), but, yes: I'd rather *not* pray for driver updates when my return period is 30 days.\n\nHopefully, it won't turn into an Optane: great idea, good potential, painful price, few niches, and weak software support.",
      "Thing is - this is just a PCIE feature. If Intel or AMD willed it, even older FX systems, Ryzen 1000, Intel Haswell etc. could do it just fine.",
      "Probably because before Comet Lake, the only boards that have a BIOS that exposes the function were workstation and server class motherboards, including HEDT.",
      "I would argue GN's testing has lots of issues too but you will probably disagree if you are so deep into his fan club.\n\nAnd before you respond - I am not in HU's fanclub. The man makes so many mistakes and says such stupid stuff and is so unprofessional over social media - I dont like him, the person.",
      "Boy, thumbnail says it all on this one.",
      "I think there is a term for people who love brands so much they will make absurd financial decisions and buy whatever product they put out when other products are dramatically better in every metric.\n\nA card with horrific drivers, lacking performance all over the place, higher power usage and (from what I've seen and certainly from all Intel graphics products for the past 15 years) horrific IQ issues even in the games that perform okay."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "Arc A380 Reportedly Gets Extra 150MHz Clock Boost With Latest Intel GPU Driver Update",
    "selftext": "",
    "comments": [
      "Intel literally already has better ray tracing than AMD and they use pretty much all open source for ray tracing, upscaling, and frame generation, unlike the other 2. I can’t wait for battlemage gpus",
      "im glad they are doing these kind of updates. i personally wouldnt go intel gpu until 2-3 years down but i hope they are sucessful. we need another competitor. i wonder how the yare going to do ray tracing and add features that rival nvidia. right now nvidia has too many features to turn down.",
      "It's an excellent name, much better than weird acronyms like RDNA.",
      "It's probably the best GPU name of all time.",
      "It's one of the dumbest names of all time.",
      "It's not that deep. Professionals don't care what the development codename is, and it might come as a surprise, but people making computers are massive nerds.",
      "It is actually Radeon DNA. CDNA stands for Compute DNA.",
      "Confidently wrong.  \n  \nC is for Compute, and it obviously derives from RDNA. Everybody knows that, it's not confusing or ambiguous.",
      "I'll have to give this a quick check! I have an A380 machine I haven't fired up in a while so I can do a before and after.",
      "Strangest thing honestly. Don’t get why Intel’s board partners have some fetish for putting extra mandatory power extensions for a GPU that barely touches 75 watts.",
      "Can't find the LP edition anywhere",
      "For a second, I thought this said Airbus A380 and I was awfully confused :p",
      "Very interested to see this",
      "Asrock low profile a380 seems to be finally available in Europe. I have no idea about North American  market.",
      "I get it, room for OC potential. But it's not like the LPs aren't being made...",
      "Yea. I can see them in Chinese and EU market. And I can find the listing in their catalog, but it's immaterial in the US market. And I'm not willing to pay the extortion to import from EU.",
      "It became available in EU like a week ago. So I guess there is a chance they have a global launch plan.",
      "Good to know! I guess that explains the exorbitant price point.  Thanks for the heads up.",
      "RDNA is a superb acronym. Radeon + DNA.",
      "It doesn't sound professional,kinda nerdy, if the sole purpose is to sell it to gamers I guess it is right."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "GUNNIR launches Arc A380 Photon in white, over 2 years after A380 introduction",
    "selftext": "",
    "comments": [
      "We need A480. Come on Intel.",
      "💗💗😱😱",
      "Waiting for this💗"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "Intel Arc A380, A750 and A770 8GB GPUs price slashed, A380 now listed for $120 - VideoCardz.com",
    "selftext": "",
    "comments": [
      "Man, I realy wish to have your prices in Europe",
      "The A380 still isn’t even remotely cheap enough, considering that it loses to a GTX 1650. That class of product should really be, like, $70 by now.",
      "Never buy a promise. The A380 *might* be better than that by now, but there’s no 2023 review data for it, so we really don’t know. At any rate, it has a long-ass way to go before it hits GTX 1650 Super performance, which is really the minimum you should get for this price in 2023.",
      "Wish there was a arc a380 low profile",
      "It’s nonsensical to blame increasing tech prices on inflation when tech markets have found ways to exponentially improve price/performance regardless of inflation all throughout their history. The A380 is at best *identical* value to a GTX 1650 Super from four and a half years ago. That’s abysmal.",
      "Cheapest gtx 1650 on newegg is $170 and amazon for $160. A380 at $120 is not bad at all, especially with more RAM, AI cores, and a way better encoder with AV1 support.",
      "So far this is true, but nobody knows what tommorow will bring.",
      "Uk prices have been comparable with the cheapest European markets (ie Germany & Netherlands) for years  and they still are. Nowt to do with Brexit. The relative strength of the dollar due to their interest rate policies are why prices are higher in europe, including the UK.",
      "meanwhile in brexit britain A770 16gb acer bifrost is gbp 427 = 484 euro = 518 usd .....  or could just get an asus strix oc RX6750 instead.",
      "Brilliant!",
      "will only buy it under $50",
      "Let’s just remember that the GTX 1650 Super launched for $160 *four years ago*, and outperformed the A380 by ~35%. That’s the only benchmark worth comparing new budget cards to, because everything else that’s available in that segment right now is trash.\n\nI’m in full agreement that the A380 might well be the relatively best option in the hellhole that is the post-2020 sub-$200 market, but when compared to the market we should have, it’s a detestable waste of sand. Sub-$200 price/performance hasn’t improved in *four and a half years*.",
      "And if they dont? It's also perfectly possible performance could go down with more stable drivers.",
      "yes indeed. even worse when you compare it to the rx 480 which launced for around €200 7 years ago, and still should be around 25% faster on average.  \nthe rx 400 and rx 500 series gpu's also where insanely good at raw performance/compute tasks compared to other gpus. since while in gaming the difference is only around 25% back then a single rx 480 could easily beat and sometimes even double the performance of a gtx 1080 in cad software and similar compute heavy things that wheren't optimized for a speciffic set of hardware and instead relied on raw performance.  \n\n\nso actually the last 7 years there hasn't really been much advancement in gpu's in some cases the ai or raytracing can be usefull however. but ofcource we have to see how well it works on low end cards, since if it works bad then the raw performance of the old 480 might still manage to beat it in such things.",
      "I mean, Alchemist seems similarly compute-heavy, but point taken.",
      "perhaps it is indeed, I didn't yet see as many benchmarks from it outside of gaming and don't own one right now.  \nif that fully is the case, there might actually be a lot of improvement in price per performance next gen, or alchemist gpu's mught be capable of much more performance(probably won't really see that for most people, but some might experience it).  \nit makes sense since typically raytracing cores can be used quite much like cuda on nvidia, so they are typically capable of quite some raw performance.  \n\n\nso sad about performance per price not going up, but intell arc indeed seems like quite much a good trend in the gpu market, since they push the prices less insanely high. perhaps next gen or such might be a lot cheaper per performance since after all this was the first gen, and so it likely had by far more reasearch and producion cost into it, due to much more reasearc being needed for a completely new line of products, also early on optimizations for reducing cost are also limited. so I by far am more angry at amd and nvidia now.\n\nopenly I hope that Intell actually uses this, since amd and nvidia increased prices so insanely much that even their first gen was a quite good or the best competor on the market despite the pricing being as high as  7 year old gpu which performed the same. this could reduce the amount of money loss on the first gen or possibly even generate a lot of proffit and name loyality so also driver support, which might make a next generation much better in price for performance. I hope.",
      "Yeah, I think Arc has a really high ceiling in terms of raw compute performance, but I doubt it’ll ever get particularly close to that ceiling in games. It feels like a Polaris-type architecture, with more features.",
      "Your arguments make no sense, even not taking features into account. And zero goalposts were moved except by yourself.",
      "You manifestly could get something equal to or better than an A380 in… hm, probably early 2020 for $100. Also, GTX 1650 Super, for $160, 35% faster than the A380, available everywhere. We still haven’t beat that, you know, and cheap cards are supposed to be *better* value than expensive ones.",
      "And if the drivers improve?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "Should I switch from AMD rx 6400 to Asrock Intel Arc A380?",
    "selftext": "The specs of the a380 look way better (6gb compared to 4gb) but im worried about compatibility with games. I just want to know if the a380 is better for gaming / streaming. Thank you!",
    "comments": [
      "I'd recommend choosing the A380 over the RX 6400 especially if you can either recuperate some of the cost of the 6400 by selling it or if you have another use case for it. The A380 is a much better GPU overall due to it being faster and supporting features like AV1 encode and decode and the Arc drivers are pretty solid at this point.",
      "https://www.techpowerup.com/gpu-specs/arc-a380.c3913\n\nLooks like the 2 gpus are pretty much equal and I doubt the extra 2GB memory will help much with such low end gpus. Don't waste your money.\n\nIf your Dell can fit any normal psu, get a new psu and a better gpu. But it probably can't and that's why prebuilts suck.",
      "Go for RX7600",
      "i am mainly wanting it for the encode which would be very useful. just to be clear, there arent really any issues gaming with it?",
      "i can't connect to power supply, i need a low profile gpu",
      "On what system? If it doesn't have pcie 4.0 or resizable bar support I wouldn't even bother with Intel",
      "honestly feels like a side grade.",
      "Both are so close that it would be hard to justify the change. I mean sure, you get an extra 2GB of VRAM but rasterization performance (in most titles) is very similar. If anything, you will probably need a different PSU to get anything better. If you are using a pre-built of some kind that has proprietary connectors, then there's not much you can do outside of using SATA to PCIe adapters to run a single 6 or 8 pin into something faster like a 1650 Super (or even better, RX 6600). It's just not something I'd really advise, since the reliability of such a configuration is very questionable...at best.",
      "Thank you",
      "if you want to encode the definitely replace the 6400. 6400 is missing hardware encode\n\nfor gaming, neither card is high performance",
      "The system you would be putting the GPU into. The motherboard, the CPU, etc",
      "oh, just a dell inspiron 3880 i hooked up with 16gb ram and rx 6400",
      "Yes get the Intel instead.",
      "no,  get a 750 minimum",
      "That would be a decent improvement for sure.",
      "got it",
      "cant connect external power",
      "What wattage is your PSU? Do you have any molex adapters?\nYou could get an A750 with two molex to 6+2pin adapters, then use predator bifrost utility to limit GPU wattage to around 150W if you have at least a 400W unit. Also depends on your CPU and rest of your setup",
      "PCIe 3.0 vs 4.0 is basically no difference. ReBAR is more important. There's many people who got Arc cards to work without issues in Z370/390 systems",
      "200 or 260"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "Intel Arc A380 desktop graphics benchmark leaks, almost as fast as RTX 3050 Ti Laptop GPU",
    "selftext": "",
    "comments": [
      "I expected more from Airbus, but it's still decent, indeed.\n\nThe \"3\" is from the \"i3\"-like naming scheme, which makes sense. :D",
      "OK A380 appears to be the ‘full’ small die version of Arc - 128 EUs.   It looks like an even lower end A350 is coming with 96 EUs, and the top end model should have 512 EUs.\n\nThis appears to be a good result for the small die.  I was worried that A380 was the brand for the top end model…",
      "[Leaked image of the inside of A380](https://www.airbus.com/sites/g/files/jlcbta136/files/styles/airbus_608x608/public/2021-08/A380_welcome.jpg?itok=iLKytcqN)",
      "3050Ti performance with more than 4GB of VRAM would actually be quite good.",
      "Intel 380 + 2,4ghz = 5 Tflops\n\nRTX 3050ti = 5tflops\n\nIntel 512EU + 2.4 ghz = 19 tflops\n\nRTX 3070 = 20 Tflops",
      "300$? Wish it was $150 lmao",
      "~~If 128EU is almost as fast 2560 Cuda Cores (RTX 3050ti) then 512EUs should be in the somewhere between the RTX 3080 (8960 Cure Cores) and 3080ti (10240 Cuda Cores), of course that obviously doesn't mean it'll translate to gaming performance, but at least it's some hope of a high-end competing GPU for those that are interested in the high-end.~~\n\nForgot about the 3050ti being a laptop only GPU (much lower clock speed).",
      "Honestly, with the current state of the market it will be a success as long as it outputs video through HDMI and/or DP.",
      "what's the expected launch time for top model guys?",
      "Yeah I see that now.  The ‘8’ was throwing me for a loop at first.  “80” series nvidia marketing in my brain.",
      "100% this. Intel doesn't need to set the world on fire with this release.   \n\n\nHaving it available, affordable and perform good enough to run 1080p, it will sell like hotcakes.",
      "March (Rumored)",
      "More than 4GB would mean it would be a very viable mining card, driving up the prices.",
      "Yeah same. I was thinking how bad is A370 and A360 going to be. Makes sense now!",
      "Performance is somewhere around 1660 TI desktop, then. Hopefully Intel does an MRSP under $300 and has enough supply to blow up the market, but I'm not counting on it.",
      "Hasn't it been compared to the 3050ti laptop card? Which would be around half the speed of normal desktop 3050ti",
      "Goddammit",
      "Seems kinda small for a modern GPU?",
      "scalpers are probably going to get them anyways, but I hope Intel does mass production on their GPU's as I'm very curious about their open source drivers, and I would like to get my hands on one but with the current market I doubt I will.",
      "Isn't Intel going to launch their first models in march?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "a380"
    ],
    "title": "Wait for new Intel ARC or Get Arc 380 just for AV1",
    "selftext": "Got a 3080 for gaming, I have my streaming PC with GTX1070. I have eyes on this AV1 tested the quality using CPU x264 streaming looks amazing. Now I want to get myself a cheap A380 intel arc to replace my GTX 1070. just for Av1. is this a good decision? or should I wait for upcoming CHEAP AV1 intel second gen GPU?\n\nPlease suggest any other ideas as well",
    "comments": [
      "The average performance for the Intel ARC A380 is well below the GTX 1070 8 GB, RTX 2060 6 GB, or RX 6600 8 GB card. Video cards from AMD or Nvidia that originally sold at the 200 dollar and up level will outperform the Intel GPU. I don't think you know or have a benchmark rating system that tells you if the quality of AVI or any processing work by Intel, AMD, or Nvidia that can assist you in getting the best value, but the point is AVI processing is only one aspect of GPU hardware that may not fulfill the work for a specific task if the video output compression/ decompression has compatibility issues. And the only advantage if it exists within the A380 GPU is incremental speed and not so much quality of the visual presentation.\n\nYou want a streaming GPU that is better than a GTX 1070? Any video card that performs around the RTX 3070 would be a marked bigger performance boost than anything that performs at the RTX 2060 or lower.\n\nAs long as you realize you are experimenting with the A380, then your disappointment if things are less than what you want will not be as great. The Intel ARC A380 is a bleeding edge technology product. That means you are more likely to lose money and not benefit in any way, because something will go wrong.",
      "What other options do i have? \nTo enable AV1 in my stream? Cant buy 40 serious or any other expensive one. \nI felt like this thing intel arc 380 is cheap.",
      "How about 2 gpus? \n1070 and A380?\nMy streaming PC is ATX and how about 3080 + A380m\n\n\nMy main goal is to get AV1 in a cheapest possible way.",
      "You should be going to help forums that specialize with streaming video for live feeds in combination with gaming or other business uses. Most people use two hardware setups where one is exclusively for streaming and the other for gaming or performing assignments the user wants to do. Some people purchase and use hardware that is exclusively for streaming that does not require you buy a discrete video card.\n\nWhat is strange is that you want to save money, but at the same time you want to have two video cards with more than one hardware system setup. That's an oxymoron situation, because most people go with just one system and or they go with two setups but they spend whatever they need and they are not sensitive about the cost.\n\nI don't advise this direction of use or thinking, because there's a good chance you are wasting time and money and you will not get the best results. I don't think you use AV1 for resolution improvement, but rather you want to increase the bit rate quality that will require you have a better CPU both on the server side and client side to see improvements.\n\nWhen should you use the AV1 encoding for the A380? when you are streaming and you have much less allowance in broadband connection. At higher internet connection speeds, you will not see a difference.\n\nIf you plan on encoding at lower bit rate speeds, then AV1 with the A380 GPU is the ideal situation. At higher speeds and with a good internet connection, you will not get any significant improvements.\n\nThe Intel A380 is currently on sale for around 100 dollars, but Woot, Newegg, Amazon, and BestBuy occasional sell them for under 90 dollars with the lowest I've seen them at around 60 bucks about a month ago. Please note - the customer review ratings for the product are not good, so, there's a high risk of not being happy.",
      "Makes sense.... I stream from my 1070 but with avery high bitrate so that viewers can get good quality as i stream high motion content which causing my streams to buffer almost for everyone i can spare some money 90 ish if i sell my 1070 for some and add like 50$ more i can get this 380, apart from encoding to YouTube here is no use for it. Thats the actual plan. I can return it right?",
      "I do not stream enough nor do I do enough encoding to give you a useful answer for anyone that does it regularly. The return policy of BestBuy is good and Micro Center also is rated well, but all other retail sources especially those coming from outside your region of the world will not be reliable or consistent."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "a380"
    ],
    "title": "What happened to the Arc A310?",
    "selftext": "All I've heard is that Gigabyte and Matrox partnered with Intel to produce these, but so far I haven't seen a single available unit neither here in Europe nor the US.\n\nWhen are the A310 cards supposed to release?  \nWhat about pricing? Apparently there are some offers from Russian(?) sites which target a price similar to the A380, which would make it DOA.",
    "comments": [
      "Arc a580 is also non-existent at this point. Don't think Intel has explained either.",
      "while we're on the topic of missing arc cards, i'd like to point out that the Arc Pro A40 [allegedly launched in august 2022](https://www.techpowerup.com/gpu-specs/arc-pro-a40.c3925) but [vanished beyond its single review](https://aecmag.com/workstations/exclusive-review-intel-arc-pro-a40-a50-gpus-graphics-cad-bim/)",
      "Dust in the wind. The Matrox cards apparently start at $500, so they're a nonstarter for non-professional use (custom digital signage software, long term support, etc).\n\nhttps://www.guru3d.com/news-story/matrox-and-intel-arc-introduce-luma-series-high-resolution-1-slot-graphics-cards-for-multi-display-environments.html",
      "I really hoped Intel would be the one to disrupt the 1650 as somehow *STILL* the best you can do in low profile bus powered cards, excluding maybe a few Pro priced niche cards. But alas, even they don't seem interested in making low end GPUs right now.",
      "Didn't see the price when they were originally announced.\n\nThat's crazy.",
      "It's not crazy for industrial applications. But it's shame for consumers that the cards probably wont be available for reasonable prices. There has been small but clear demand for low profile passive card.",
      "It doesn't even need to be low profile for me, I just want to have a sub $100 office and transcoding card.",
      "The A380 can often be found at about $120, Newegg had one up right now for sale. I have had one for a while now running reducing my large Linux distros.",
      "But do they offer what matrox offers? Remember that for businesses the hardware cost of these devices is usually trivial. Everything else costs more.",
      "They didn't have enough juice to be competitive.",
      "can double it\ncosts about 130$",
      "Can confirm that they are sold in russia",
      "I thought I read it was cancelled.  Maybe they never officially announced that.  I keep hearing about Battlemage which is slated for next year, but there is supposed to be alchemist+ cards between now and then.",
      "A listing for the Arc Pro A40 is currently on EBay at the moment…for $899",
      "A380 is supposed to be a bus powered card, and in fact does use under 75 watts, but all we’re getting right now are “OC” cards that have unnecessary power pins.",
      "oh nice, thats probably a good sign",
      "Update: a second listing has popped up on EBay, and the original listing was cut to…$425\nAccording to the EBay metrics, one has been sold, 2 are left.",
      "AMD is much cheaper, and their cards are still not cheap.",
      "The problem is AsRock STILL won't actually sell their low profile A380, so there's basically nothing you can do.\n\nIt's not a matter of cost. *It might as well not even exist*.\n\n...and yeah, I landed on this page via web search, trying to find any information, whatsoever, on when AsRock plans to actually launch their low profile Arc A380.",
      "thanks for the update"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "Adlink announces Intel Arc A380 GPU in MXM form factor - VideoCardz.com",
    "selftext": "",
    "comments": [
      "So, will I actual be able to buy one for an old laptop or will they be yet another oem only option?",
      "Well, the article mentions MXM so I thought you meant HHHL for laptops, but after reading it again I realise you just wanted more compact Arc GPUs, my bad.",
      "Why not HHHL though?",
      "Mxm gives longevity to custom gaming laptops. Soldered cpu/gpu for thin profile laptops are only good for disposable machines.",
      "Umm.\n\nWhere did you get any of that?\n\nHalf Height Half Length is for PCI-E add in cards like GPUs, NICs, HBAs, etc.",
      "What you are proposing exists, its called egpu and its not ideal. Mxm is the best way to handle this.",
      "absolutely OEM",
      "Boo",
      "Less demand for HHHL in the industrial applications that Adlink typically targets. They'll also bring SKUs to their COM Express and PCIe/104 lines before the things consumers are familiar with.",
      "Well, shit.",
      "There's laptops with oculink? I thought that was pretty much exclusive to servers/some hedt",
      "Nope, there's not. But there easily could be if anyone cared.\n\nLike I said you can diy it with m.2 oculink adapters, obviously limited to 4 lanes but the potential is there.",
      "Isn't HHHL for storage? And even then m.2 has pretty much replaced it. Plus there's not really any other standards for GPUs in a laptop. Although, even then a majority are now embedded instead. Edit - didn't read properly-ignore",
      "It's not ideal because no one is trying seriously for reasons, be it too small of a market or whathaveya.  \nYou can diy it better than thunderbolt implementations with something like oculink. The tech already exists.",
      "Ahem, if you read my entire post I'm proposing we integrate GPUs into power bricks with some sort of standard like Thunderbolt 5 giving you a viable upgrade path. You could simply upgrade your graphics card/power brick combo instead of needing a whole new laptop when it comes time to get a new GPU. GPU upgrading tends to be slightly more frequently needed than processor upgrades, so this works out nicely to get you 1-2 extra cycles out of the same laptop. \n\nThe last time CPUs weren't soldered on laptops regularly was a decade ago, that idea is long since dead. Yes, there are Clevo and Alienware 10th/11th gen laptops which used desktop processors, but the problem with those is that they only lasted two processor generations. That'll never come back. \n\nMXM has a small potential of coming back, but when thunderbolt 4 can deliver power and graphics I really think the solution is to just build graphics cards into power bricks with the standards for thunderbolt 5 being increased.",
      "Hot take time: Laptops shouldn't even have a discrete GPU, let alone a bulky MXM style gpu. \n\nRealistically, you can't really use a discrete GPU on battery for very long and you pretty much have to plug in your machine if you're gonna do a workload actually utilizing GPU performance.\n\nIf I were Intel, what I'd be working on is integrating dGPUs into a power brick as the basis for thunderbolt 5. New standards have shrunk the size of power bricks that can supply 200+ watts, I'd leverage that technology to build a graphics card within a power brick that would be roughly the size of power bricks of a few years ago.\n\nJust have a big power brick with a GPU built in that also acts as a charger using a single cable. With how efficient laptop processors are, you could realistically have an Ultrabook sized device and just hook up your graphics card/power brick/charger to it for graphics performance. When you don't need/want the graphics performance, you could just carry a regular power cord on a business trip. It could be what External GPUs aspired to be, but actually done right this time. Intel could limit it strictly to their thunderbolt connectors to give them a market edge over AMD. It would be like that Asus 3080 mobile portable GPU, but done right this time by being seamlessly integrated and supported across multiple devices/manufacturers utilizing Intel processors and only needing one cable."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "Intel's First Arc Custom Graphics Card Pictured, Meet The GUNNIR Arc A380 Photon 6 GB With Full ACM-G11 GPU, Starting at $150 US",
    "selftext": "",
    "comments": [
      "According to TechSpot/HardwareUnboxed the 6500xt is 32% faster than the 6400. This thing works out to be 21% faster than a 6400 if you take the averages of their likely very cherrypicked benchmarks. But to be honest, with 3-6 more months of driver improvements Intel likely has a lot more to squeeze out of their cards than AMD, so it might be true. And I'm hoping it'll match the 6500xt eventually.\n\nIntel might have even better FineWine ^(TM) than AMD, because right now Intel GPU drivers are stomped grapes.",
      "Is low end with decent memory capacity finally back?",
      "i don't think you realise how expensive memory is.",
      "Faster VRAM than mobile. Note that it's 157 mm^2 vs RX 6400 with 107 mm^2 (cut down?), with 2GB more VRAM, sold for less.\n\nWhether these are a good value has less to do with drivers and silicon design than people think.\n\nAlthough if my math is anywhere near reality, and I'm not certain it is, the high end cards will be insane value for compute and relatively bad at gaming.\n\nFor comparison to prehistoric cards like mine: it seems to be slightly worse than an ~~RX 580 8GB~~ RX 570, although at half the power usage.\n\nedit: fixed bad math, I misread the relative performance comparisons.\n\nedit2: the Gunnir card is 92W and 2450Mhz vs Intel's benchmarks of a reference design at 75W and base 2000 Mhz, so actual cards may be significantly faster than their numbers and would put it back at RX580 level.",
      "I love that last sentence",
      "On top of performance you also get encode and decode, which the 6400 and 6500 don't support. And native AV1 encoding which by itself will be worth it to some people. \n\nAnd 6gb of vram while AMD's offerings are only 4gb.\n\nIf the drivers keep improving this is definitely better than AMD and Nvidia's low end offerings.",
      "3080 only has 10GB and it's fine for 4k/1440p currently, 6GB is perfectly fine for any card with less power than a RTX 3050 IMO.",
      "Coming from RX 480, I'll take cards that draw less power but perform close enough. RX 5500 XT used to fit the case but it's no longer available and RX 6500 XT is a disgrace to RX lineup. I skipped RDNA1 due to numerous end user reports on random downclocking which eerily sounds similar to how my 2500U likes to randomly downclock",
      "Also 8 lanes of PCIe on ARC — so it’ll scale better on PCIe 3.0 systems than 6400/6500XT",
      "Seems to be priced decently, at least in China. I guess there's hope for A310 to be ~$79-ish?",
      "no reviews?\n\nwhen will be lifted NDA?",
      "It's also important to note, I think, that the first 3rd Party design features an 8-pin power connecter and a reported TGP of 100W. That's up from the Intel reference design of 75W. GUNNIR is claiming the boost clock is increased from 2000MHz to 2450MHz. That *should* give some very noticeable performance gains.\n\nTo put that in perspective, the RX 6400 has a TGP of 53W, and the RX 6500 XT has a TGP of 107W.",
      "The performance is barely above the 6400 on those numbers, Intel is using performance per dollar and a price of 1030 for the a380 and 1199 for the 6400.",
      "Nope.",
      "Dunno why you're getting downvoted. I guess the people of reddit don't or can't actually read.",
      "When the competition is only peddling out 4GB at this price range, 6GB is a lot better.",
      "I’m going all Intel when they get this shit figured out properly 🤣",
      "Are we getting a 2 slot low profile version for this? That's what I'm excited for.",
      "My 2700U did the same thing. Would go from 3.6GHz to 600 MHz at the most random times. Made gaming feel awful. It wasn’t thermals either.",
      "Intel is saying that. \n\n> Claim: The Intel® Arc™ A380 GPU, with a recommended customer price of 1,030 yuan, delivers up to 25% better performance per yuan than available competitive offerings as measured by performance on a selection of popular games.\n\nhttps://www.intel.com/content/www/us/en/newsroom/article/intel-arc-a380-graphics-available-china.html"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "GTX 1650 4GB  vs  Intel Arc A380 Low Profile 6 GB🤔",
    "selftext": "What do you think … which GPU is better and would last longer? I saw size of 2 fans on Intel Arc A380 and it is kind a disappointing how small they are. Soo I don’t know about cooling performance in long term in Intel GPU 🥵\n\n🟢 MSI NVIDIA GeForce GTX 1650 D6 VENTUS XS OCV3\n- 4 GB RAM \n- GDDR6 \n- video memory frequency = 12 000 MHz\n- memory speed = 1410 MHz and boost to 1 620 MH\n- 128Bit\n- Overclock version GUP\n\n\n🔵 ASROCK Intel Arc A380 Low Profile\n- 6 GB RAM\n- GDDR6\n- video memory frequency = 15 500 MHz\n- memory speed = 2 000 MHz\n- 96Bit\n\n\nI will be using GPU mainly for watching 4k movies in my 165 Hz monitor, multimedia task and some gaming (like Fortnight, CS Go, WoW in low settings) but only occasionally I’m not a gamer. \n\nThis is going to be PC mainly for work (CPU 14700K + 32 GB RAM).",
    "comments": [
      "Are these both new? If you just want the best $100 new card then yeah get the A380. But used you can get so much better, especially considering that you don't need one that has no PCIE power connectors.",
      "Yes I will be definitely buying only new GPU. I am thinking about 99 € to 160€. That would be ideal.",
      "I mean it’s not gonna be anything particularly powerful. i’d look at benchmarks to see how the A380 does especially in fortnite.",
      "In benchmark both can do games like Fortnight in low settings so I guess it will be fine"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "Intel Arc A380 - first look at Intel discrete graphics card",
    "selftext": "",
    "comments": [
      "Crysis.",
      "**Anything you want tested on A380?**",
      "But can it?",
      "74 FPS at 3440x1440 full screen in the FPS benchmark (around 15  FPS when in smoke grenade, 80-100 when no smoke, weird...\n\nOn 1080p the drop is like 20 FPS but the benchmark result is 149,50 FPS.",
      "I did a short test in WoW and it worked in 2 out of 3 spots tested. Still too hard to tell, but A380 is entry-level, and more meaningful results would be from the A700 series cards.",
      "good luck intel",
      "Any game selection and AV1 encoding.",
      "Minecraft.",
      "Isn't Arc advertised to do raytracing IIRC? What's that like?",
      "Destiny 2",
      "As in the benchmarks in the article - 5900X, 4x8GB 3800CL19 RAM. ReBAR on.",
      "Forza Horizon 5",
      "I did tested some DXVK but on Windows for now. The thing is it's quite PC and game specific. Like I did test on AMD and Nvidia dGPU in FF14 and it didn't help while for some people it does help. SC2 on Intel has a big uplift while on AMD/Nvidia does not (and usually those shaders stutter the game a lot). \n\nOn Linux it's a bit different story as prior to Vulkan there was only OpenGL which isn't the hottest API for games ;) Will have to check how to benchmark them properly on Linux.",
      "thank you very much! yes the killer bit of the benchmark is when it’s passing through the smoke. This looks promising nonetheless :)\n\nEdit: May I ask for the rest of the pc spec?",
      "Yeah, you will need to use 6.0 kernel which is mainline, not stable release, also bleeding edge mesa drivers\n\nAlso\n\n>i915.force_probe= module \n\nOption to make GPU work. So yeah, it may be too much work and understandable then, though there is not much reviews so it will take own popularity too!",
      "Simple shade on trees seen very small performance penalty. A city saw a \\~44% one but that's what RTX 2070 lost any time DXR was enabled in the game. Will have to check other spots as it may be that Intel design will avoid slow-downs when there isn''t much or any ray tracing to do.",
      "This might not work as support hasn't been officially added yet, but I would love to see NeatBench results, which is the benchmark for the Neat Video denoise program used for video editing that heavily leverages GPUs:\n\nhttps://www.neatvideo.com/download/neatbench\n\nIn particular the only number that matters in this case is the one labeled \"GPU only\", however if you want to post the whole output to pastebin or something I can add it to [the NeatVideo speed database](http://fifonik.com/nv/).",
      "CS-GO please",
      "Can you please check out Waydroid support on Linux?  \n\nhttps://wiki.archlinux.org/title/Waydroid\n\nTechnically, A380 should work great and have hardware rendering out of the box. It's one rare case in which Intel has great advantage compared to both AMD and Nvidia, and it wasn't tested in Phoronix review either. Thanks!",
      "Rainbow Six Siege at 3440x1440 at very high preset. Perhaps PUBG at 1080p would be cool too!"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "Intel Arc A380 6GB Review: Gunnir Photon including benchmarks, detailed analysis and extensive teardown | igor'sLAB",
    "selftext": "",
    "comments": [
      "Nah, this sub allows criticism of Intel, the only hardware sub where you can't criticise is the Nvidia one",
      "igorslab is about to get banned from this sub",
      "Not too surprising. It was naive to think that a new architecture paired with brand new, hurriedly developed drivers was going to be competitive on day one with hardware and software Nvidia and AMD have been polishing for literally decades. Even if everything went perfect for Intel, at best optimization was going to be spotty and limited to the newest and most popular games.\n\nDrivers will gradually improve, which should help the poor 1% lows, but this is definitely the kind of product where you want to look at where it is 6 months or a year after launch and not jump in on day 1.",
      "> Conclusion:  [A one star review](https://www.igorslab.de/en/intel-arc-a380-6gb-in-test-gunnir-photon-total-benchmarks-detail-analysis-and-extensive-teardown/19/)\n\nNot going to lie, I'm starting to rethink a day one purchase.\n\nI've been waiting to purchase a new graphics card for seven months now. \n\nThe ARC series launch has already been delayed twice because apparently Intel's graphics team is having issues producing stable drivers for this architecture.\n\nI'll hold out a little longer, but I'm not going to pay for the privilege of being a beta tester while Intel spends the next year trying to produce stable drivers.  It really makes me wonder how this could happen to a company like Intel who isn't new to R&D, chip fabrication, and software.  I would have expected that some pre-fab R&D would have had proof of concept code for firmware and drivers working and relatively stable on engineering samples before committing to a launch.",
      "Making graphics cards from scratch is hard...",
      "Over a decade of not caring one bit about their iGPU gaming drivers has caught up with them. I remember the arguments here between the people who insisted Intel drivers were \"rock solid\" (and thus a good sign for Xe) versus the handful who were telling those people the Intel gaming drivers were terrible. *I told you so*",
      "Not sure why downvoted - it takes hundreds of million of dollars and hundreds of engineers to enter this market at this level now.",
      "The truth hurts sometimes",
      "Doesn't matter if performance is up to par as long as it's priced well. The A380 is supposed to be $130 to $140. For the performance and features it provides, that's a great deal imo. Not sure how the A770 stands, but it seems to be at least somewhat competitive with the 3060ti. I'd expect the price to be lower than the 3060ti MSRP, maybe around $350.",
      "Well, I mean if it’s priced to match its performance then it may not be a bad deal, still a bit disappointing though.",
      "not just a new architecture but a forward looking one as well. if you look around the web with the performance hit with not having resize bar support on a intel ARC gpu is massive.\n\nmeanwhile the performance bump from AMD and Nvidia is minimal.\n\nas things stands right now i think with time this gens GPUs will age rather well as driver support gets better and better. but the future if intel keeps going on the GPU can be something that properly smashes Nvidia and AMD with time due to a newer base architecture.",
      "it was a cheap build with a lot of errors made by the manufacturer effectively neutering the GPU. It's mentioned in the conclusion. \n\nThe reviewer clearly stated they did not ask Intel for a card but bought a crappy card somewhere in Asia for this test.",
      "No bad products, only bad prices.",
      "Yeah, they misspelled it. On the card is another typo",
      "Igor's lab notes problems with the partner's build.  [Here is that discussion](https://www.igorslab.de/en/intel-arc-a380-6gb-in-test-gunnir-photon-total-benchmarks-detail-analysis-and-extensive-teardown/3/)\n\nComments relating to build quality and choices of cost cutting on GUNNIR's A380 however do not negate the larger problems relating to driver stability.  Igor's made several comments about the card experiencing microstutering and  [effectively being unusable without Resizable BAR](https://www.igorslab.de/en/intel-arc-a380-6gb-in-test-gunnir-photon-total-benchmarks-detail-analysis-and-extensive-teardown/4/) (rBAR) support.  There were also comments about instability and reboots.\n\nThese comments were similar to those in the Gamers Nexus review of GUNNIR A380 and reflect issues with Intel's drivers or the architecture.\n\nNow maybe Intel manages to resolve these issues by the time they release A750 and A770.  \n\nHowever, looking at this as someone that understands marketing and project management, companies tend to carefully manage perceptions before a product launch to promote the best possible image to maximize revenue potential.   Delaying a product by seven months isn't a good sign.  Letting a bad AIB product be the first public perception of a product launch is also not a good sign.  This tells me that things are probably worse as this is what Intel is permitting to be seen publicly.\n\nIntel engineer Tom Petersen has been sent out to smooth public perceptions.  He is an energetic speaker and definitely was a good choice.  The problem for Intel and Tom is you can only hype or talk favorably so much before reviewers and consumers actually have a product in hand.  At that time if serious flaws exist they will become known.\n\nWe are seeing the tip of the iceberg.",
      "Is 'unknow' a word/saying?\n\nUnknown is how that word should be spelled, but I've seen 'unknow' many times on chicken scratch and just assumed it was a misspelling",
      "Honestly, new GPUs as a whole aren’t worth it imo anymore. Getting used is where the deals are because individual sellers have more freedom than AIBs to drop prices.",
      "> Not going to lie, I'm starting to rethink a day one purchase.\n\nHonestly, I did a LOT of day 1 installs and purchases when I was younger. I was always so enthused to be a part of something new and exciting. Windows Xp 64 bit, Vista, SLI, Ryzen gen 1... and now I think, what on Earth is the point? It's so much better to wait a year or so before bothering with any substantially new tech. You get better prices, better drivers and support, and you aren't tearing your hair out over stupid shit.",
      "First gen always sucks",
      "Intel probably told the chinese to \"come up with the packaging\" and forgot to send them a template with the wording. What a shitshow."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "Sparkle Intel Arc A380 Elf desktop graphics card review - What can you expect from Intel's 129-Euro budget GPU?",
    "selftext": "",
    "comments": [
      "This subreddit is in manual approval mode, which means that **all submissions are automatically removed and must first be approved before they are visible**. Your post will only be approved if it concerns news or reviews related to Intel Corporation and its products or is a high quality discussion thread. Posts regarding purchase advice, cooling problems, technical support, etc... will not be approved. **If you are looking for purchasing advice please visit /r/buildapc. If you are looking for technical support please visit /r/techsupport or see the pinned /r/Intel megathread where Intel representatives and other users can assist you.**\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/intel) if you have any questions or concerns.*"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "ASRock Launches Intel Arc A380 Challenger ITX OC Custom Graphics Card",
    "selftext": "",
    "comments": [
      "> ASRock Launches Intel Arc A380 Challenger ITX OC Custom Graphics Card **In China Only**\n\nFixed title to be more relevant for this sub",
      "These China only releases are weirding me out, man.",
      "In China???\n\nEDIT:  yes. It's also a pretty crazy price at 1299 Yuan. Even without tax, that's more than the RX 6400 price point.",
      "A dedicated hardware AV1 encoder is very, very interesting. Arc's AV1 is faster than NVENC, can do 10 bit encoding, and can do better quality at a significantly lower bitrate... Could realistically be a game changer in the long run for content creators, especially in the world we live in of data caps and bandwidth limitations if you go over them.\n\nRight now many big name streamers use expensive secondary PCs for stream encoding through a capture card. An Arc GPU could itself completely replace that entire secondary PC, halve your bitrate, and give you BETTER quality of stream. That's extremely impressive for ~$130.",
      "It's worth it to a lot of people for the encoder support, or as a basic gpu for certain productivity work. But for gaming it's not worth it, unless you have 100% faith that Intel can gain like 20% performance from drives, and game optimisations.",
      "I wonder where you were when the 6400 launched with no encoder and X4, with reskinned 1650 performance for 160 dollars.",
      "Most hardcore miners ran their GPUs at PCIe x1 (using a x16 riser from MOLEX + x1 PCIe wired through something as simple as an USB cable) without much performance hit.\n\nAlso the x4 and x1 PCIe slots are usually not routed to the CPU directly, but are managed by the chipset, so you wouldn't lose your sweet x16 with that approach *(but other limitations apply depending on how a specific  motherboard is wired, eg. an M.2 slot might  not work anymore)*. Though if you have two x16 slots, you should get at least x8/x8 if your motherboard supports bifurcation properly. And as we know, PCIe 4.0 x8 is enough for any GPU, at least for now.",
      "What were to happen if you were to slot something like this into a x4 pcie slot on a motherboard, with a dedicated GPU in the top slot. Would the main x16 with a GPU in it slot slow down like like x8? \n\nNow I know you technically can't fit this GPU into a x4 slot, but I once helped a totally broke friend to modify a used cheap server system to game on. Cut out part of the plastic (on the mobo slot) obstructing an x16 gaming card from fitting into a pcie x4 slot, and the GPU worked, although maybe 5% slower than it should have.  Point is that I wonder if something like the A310 could just be cut down to x4 pcie so you can just add a 2nd GPU like this.",
      "Where do you even live for it to be new at 49 dollars.",
      "It's the only one on the market, full stop. Using it as a secondary card for encoding purposes only has nothing but benefits as far as I can see.",
      "It's not worth more than $99, this is like old i740 equivalent gpu from 98.",
      "OC will help a lot.",
      "That price included the Chinese sales tax and retailer markup, its isnt Asrocks MSRP.",
      "Do NOT buy. Is full of bugs. Software and even at hardware level.",
      "6400 is worth $49.",
      "Problem is that Arc drivers are so broken that it's not worth messing with them just as an encoder."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "a380"
    ],
    "title": "Intel confirms upcoming Arc desktop SKUs: A770, A750, A580, A380 and A310",
    "selftext": "",
    "comments": [
      "Did they also leak in which decade they'll be released? /s",
      "No way. It's 3070 level hardware with unpolished drivers. Expect 3060Ti real-world performance.",
      "Intel should’ve been very clear about the release timeline. I think they still can be transparent about it.",
      "Do we have any idea how they will compare to existing gpus yet?",
      "IIRC the high end is supposed to be up around 3080 tier",
      "They already went transparent with it. \n\nThey told that early summer desktop GPUs will be launched as OEM only in china. \n\nThen in late summer launch OEM only worldwide. \n\nThen \"later\" launch as standalone GPUs.\n(So one could expect October/November for this)\n\nOverall it seems like they really really want to have most of the driver issues sorted out before releasing standalone GPUs.",
      "This century. Stay tuned.",
      "Good luck guys.",
      "There are low end GPU out there, it is just the value is so bad that people ignore it. With possible bad driver due to first launch, it will be irrelevant",
      "Existing in may 2022 or April 2024 when they are actually available? 🤣",
      "You expect the first iteration of GPU to meet the halo products of established GPU manufacturers such as AMD/NVIDIA?",
      "the lack of vram makes me sad, though i'm not sure what i should've expected from the bus widths. wish 8-12gb was standard because 4gb is proving not to be enough and 6gb is next",
      "And I honestly agree with their way of going right now. Why do people care so much that it only gets released in China. Makes no difference if they release it worldwide now, or later.",
      "Those were likely early laptop performance leaks at reduced clocks.",
      "Yes. It has enough transistors at like 22 billion and already has RT performance better than NVidia or AMD. It's tier 4 vs Tier 3, and Tier 2 for AMD.\n\nIf you include the games where it crashes due to drivers to drag down averages it'll be like 3060ti levels.",
      "It's because Raja Koduri promised stuff. \n\nHe promised the GPUs will be ready early, in Q1 2022. \nTurned out to be a lie.\n\nThen he promised GPUs in the hand of gamers for cheap - again a lie because they prioritize OEM.\n\n\nPeople say Intel GPU will be irrelevant if they launch after Nvidia launches RTX 4000 and AMD Rx 7000. \nI don't think so because all the Nvidia and AMD products from new gen are gonna be 600$+(real price, not msrp).\nIntel's best GPU is supposed to be under 500$.",
      ">He promised the GPUs will be ready early, in Q1 2022\n\nHe actually said that? When?",
      "I don't know for sure if it was him but there were messages from Intel that arc GPU lineup will roll out in Q1 2022."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "A better GPU for my coming i3 12100? - Intel ARC A380 or RTX 3050?",
    "selftext": "I will be graduating soon and my uncle is going to give me an i3 12100 as a gift for my graduation but I'm just thinking if I were going to use it.... it will be for playing games which is not a high graphically demanding games (such as valorant or csgo) and light video editing. I also want to use it for creating illustrations and key animation for my animation portfolio. So what might be the good GPU to pair it with my upcoming quad core beast? The Intel arc a380 or the RTX 3050?\n\n\n\nAdditional note: rtx 3050 and arc a380 are the only gpu covered by my budget",
    "comments": [
      "The rx 6600 should be about the same price as the 3050 and is faster, so if it's available in your region I'd go for that. Otherwise, the 3050 is definitely faster than the A380 and comes with fewer driver issues. Intel needs a bit more experience in discrete graphics before they'll really be ready to compete.",
      "Is the RX 6600/6700 not within budget? Usually a 6600 can be found for about the same price as a 3050.",
      "The Arc A380 is roughly equivalent to an RX 6400, so the RTX 3050 is better in pretty much everything. Is the RX 6600 available in your region? In most places it’s cheaper than the 3050 while also offering better gaming performance.\n\nIf you want an Arc GPU I’d recommend waiting for the Arc 5 or Arc 7 series.",
      "The 3050 is going to be way better, but it’s such a shit GPU. In literally every market I can think of the RX 6600 is 25% faster for the same street price, just get that instead.",
      "The 3050 is like double the price. It's at like AMD RX 6600 pricing but 20% weaker.",
      "The RTX 2060 is legit cheaper than 3050 and much better",
      "Personally I just had my Arc A380 get bricked by ***installing the official drivers,*** so I'd avoid it for now",
      "Go for an rx 6600",
      "No, no, no, no. The 6500 XT should never be bought by anyone, ever. It’s that bad.",
      "> intel said that the Arc 5 or Arc 7 is a limited edition GPU\n\nThat doesn't really mean much. The 'Limited Edition' is basically Intel's branding equivalent of Nvidia's 'Founder's Edition'.",
      "All Intel arc are basically products in \"beta\". \n\nA lot of things are unstable - crashing PC and games, many games run very badly, only select ones run \"okay\"\n\nIf you're not interested in it as \"new stuff to test\" then stay away from it at all costs",
      "Welp I'm expecting it to be a really messed up product from intel due to their driver issues but I'm a little interested on it. I'm also planning to test in on video editing and rendering for blender.",
      "RTX 2060. Same price as 3050 and much faster.",
      "RTX 3050 all day long.",
      "3060 or 6600 my friend.  3050 and 6500 are terribly forcibly bottlenecked.",
      "Where did you find 3050 so cheap it competes with a380?",
      "Thanks for the answer but if i remember correctly intel said that the Arc 5 or Arc 7 is a limited edition GPU so I might go for the RTX 3050 but if i find any RX 6600 in our region I might go for that.",
      "Whoa😲 thanks... others also told me to stay away from arc",
      "Especially when the 6600 is very close in price.",
      "Honestly the A380 seems pretty cool. 3050s are still overpriced as of now. I don't know your exact budget but if I were in this price range I'd go either Intel with a A380 or AMD with a RX 6500 XT.\n\nBut if you find a 3050 near the price of a A380 then def go for the 3050"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "'Overclocked' Intel Arc A380 Shows Impressive Gains",
    "selftext": "",
    "comments": [
      "Neat. If this scales to the higher tier cards as well they might be more competitive than I expected.",
      "Gamers Nexus and other reviewers also need to test ARC cards + Intel drivers against linux performance of games using Wine, Lutris, and Proton.\n\nSome of these projects translate native DirectX calls to Vulkan using DXVK default or implement specialized builds. It would be interesting to see how this ranks against intel's native DX processing.\n\nDon't know if Intel graphics engineers or reviewers lurk here but results could shed light on driver issues and lead to code fixes by Intel Engineers comparing open source code against Intel Drivers.  It could also lead to Intel committing improvements upstream to these open source projects.\n\nMaybe the mods can forward this suggestion to Intel's Graphics team.",
      "Very large share of people writing the open source code work for intel and are paid by intel to write the open source code. The drivers intel GPUs use on Linux would definitely be written by intel regardless of if the source is open.\n\nBy code lines written intel is by far the biggest contributor to Linux kernel updates in general. Intel is also a full member and one of the founders of khronos group that develops Vulkan API.",
      "So… basically Intel just self-owned by throttling their own GPU. That makes this intriguing: who knows how powerful this card would be if Intel just let it rip at 75W?",
      "I will definitely do some overclock testing when I get my A770.",
      "Please include thermal temp readings.\n\nPRO Hi-Tech's review implies their A380 OC had minimal increase on GPU temperatures\n\nhttps://www.youtube.com/watch?v=vrwfHuEb_Gw\n\nMaybe it's just me, but the image processing quality side-by-side looks sharper and more defined on Nvidia.",
      "The recent A380 review by Gamers Nexus implied Intel had poor performance with DX9, DX10, DX11 titles compared to DX12 and Vulkan.  \n\nhttps://www.youtube.com/watch?v=La-dcK4h4ZU&t=15m25s\n\nA comparison is shown where GTAV running DX11 on the A380 had nearly half the performance of a AMD 6400 card. This implies the intel driver is not mature and has issues on Microsoft Windows.\n\nI don't think you followed the intent of my comment.  I understand how the FOSS community works.\n\nMy comments were specifically tailored to Wine, Lutris, and Proton and comparing performance of games while using those DX compatibility layer programs on Linux compared against running the same games on Microsoft Windows under native Intel Drivers. \n\nDirectX is not native to linux. \n\nIf wine, lutris, proton, or DXVK on Linux outperform Intel's drivers native DirectX support on Microsoft Windows, then studying those projects might provide means of optimizing or improving intel's drivers or provide Intel a temporary workaround by translating DirectX 9, 10, 11 to Vulkan.\n\nWine's stack has included DX9 support for a very long time under WineD3D.  It was DX10/DX11 that didn't come until very recently under D3D11.  Typically both have lagged behind native DirectX in performance.\n\nWine is not an emulator, it translates DirectX calls to OpenGL\n\nhttps://www.winehq.org\n\nDXVK is a more recent project which translates DirectX 9, 10, 11 calls to Vulkan.\n\nhttps://github.com/doitsujin/dxvk\n\nProton is a patched version of wine maintained by Valve Software which includes DXVK support.\n\nhttps://github.com/ValveSoftware/Proton",
      "Shhh shhh, not yet. Wait.",
      "Some of this was highlighted in this Gamers Nexus review of the A380\n\nhttps://www.youtube.com/watch?v=La-dcK4h4ZU&t=7m30\n\nIt was then was discussed by Intel Engineer Tom Petersen in this video below.  Petersen didn't fully address if this was going to be fixed when Rebar was off.\n\nhttps://www.youtube.com/watch?v=8ENCV4xUjj0",
      "I watched that.\n\nThe comment made by Tom Petersen was primarily Linux driver support.  \n\nHis comment doesn't really apply here.  DirectX is only native to Windows systems.\n\nDirectX is not native to linux.  That's why programs like Wine, Lutris, Proton, and DXVK exist to translate DirectX calls to OpenGL (Wine) or Vulkan (DXVK).\n\nIf Intel is having performance issues with native DirectX 9, 10, 11 support and if for some reason linux translation programs provide better performance, then Intel should look at those projects.",
      "Budget GPUs that have a low stock wattage are the best overclockers usually already anyway.",
      "Intel on Linux should be fine. [LTT asked specifically about this on the WAN show last week.](https://youtu.be/fDblFRwSZNc?t=1502)",
      "Peterson acknowledged reBAR \"ON\" also having frame time spikes? The reBAR off results are nothing new, but Igor is going to publish something in the next 24 (or so I hear) that goes even further. \n\nI'll be eager to see what he discovers. I really hope it's nothing that can't be fixed. I'd probably trade my 6600xt for a A750 if I had the chance just for the fun of it, and because I have faith there is a massive amount they can still squeeze out of it. But if there truly is something wrong with the silicon, or the driver overhead is as bad, or even worse on the CPU is than Nvidia, I'll probably be forced to stay away as my 4 year old i5 can't take it.",
      "Lol a770 gonna be priced at like 400$, meanwhile miners will be selling rtx 3070s for <400$ at the time of global arc a770 release (if it actually ever happens and Intel doesn't cancel it)",
      "Apparently Igor's Lab is going to drop their review tomorrow (July 20), and they've found really bad frame pacing, and CPU overhead issues on the A380. Even with reBAR on. Might explain why the a750 and A770 don't scale to the performance level they should. No idea if it can be fixed with software alone. Also don't know if that's just DX11 titles he's talking about or DX12 as well."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "a380"
    ],
    "title": "Current state of VR support on Intel Arc (A750, likely also applies for the A770 and A380)",
    "selftext": "I hadn't posted about this here yet, but I am a collector of VR Hardware, and when the A750 and A770 first launched in Germany, I was very eager to try out VR performance on as many headsets as I could.\n\nI personally bought the A750, mostly as I intend on mostly using the card for AV1 encoding in my Server, but might as well try VR on it as well I thought. Here's my findings so far:\n\nThe Intel Arc drivers currently **don't seem to support Direct Display Mode devices at all**. This is a requirement for VR headsets that use DisplayPort or HDMI to receive the image they're supposed to display. That means that I wasn't able to test any of these headsets, and I've tried a lot of them: Oculus/Meta Rift CV1, Valve Index, HP Reverb G2, Vive Pro 2 and the Varjo Aero (tho in the case of the Varjo, there was also a software lock-out, as they don't want their customers running into issues with unsupported GPUs, which also includes ones from AMD).\n\nOfficial Link support with the Quest (both with a cable and wireless) also wasn't working. Quest Link refused to find it's connection (the PC dialog never found the headset's USB connection and I never got the Quest Link prompt in the headset itself). and Air Link straight up threw me an error that the Video Encoder was unsupported. I have no idea what kind of magic other people have pulled off to get Air Link or Quest Link working on Intel Arc, I sure wasn't able to.\n\nI was however able to run VR on my A750 through 2 other means. For one Virtual Desktop basically runs on anything that has a DirectX 11 capable video output and a CPU, so that worked with no issues at all on Arc too, and I was also able to use my Vive Pro 1 using the Vive Wireless adapter.\n\nWith Virtual Desktop performance was quite stellar and I didn't really experience any stutters or similar. With the Vive Wireless Adapter however, I immediately started noticing hitching and stuttering every so often. The more CPU heavy of a game I tried, the worse the stuttering got, hinting that large parts of the stuttering may have to do with GPU drivers reliance the CPU.\n\nFor reference, I'm running a 5900X with 32GB of DDR4 3200MHz CL16 memory.\n\nIf anyone is interested in the performance numbers using the OpenVR Benchmark Tool, I'd be more then happy to provide them later as well. I've already run the benchmark, but the results I have not saved on my main data drive...",
    "comments": [
      "Yeah, this is true. Linus from Linus Tech Tips also faced this issue. Intel has responded saying they are in the alpha stage. They should be able to release beta drivers for VR support shortly.",
      ">The Intel Arc drivers currently don't seem to support Direct Display Mode devices at all.\n\nI am curious if the Direct Mode does work on Linux, since [the implementation is shared with AMD.](https://monado.freedesktop.org/direct-mode.html#intelamd)",
      "No unfortunately not, and I was not aware of it and bought a NUC with arc card to play specifically VR....",
      "I’m running the same specs as you plus an A380 (cause why not see what it can do with VR). Oculus/Meta Rift S didn’t throw me any errors, just said display port was disconnected even tho it was connected.\n\nOn the Intel discord, I’ve brought this up and have been reassured multiple times that VR isn’t software blocked nor hardware blocked. So I’ve been assuming that Oculus/Meta VR and Steam VR hasn’t whitelisted them yet for the direct connection headset. I’m thinking they’re waiting for the rest of the 40 series and 7000 series to release before doing a bulk VR whitelisting of said cards. *People with 40 series have been reporting the same problem so I’ve heard*\n\nFrom what I understand, the wireless alternatives ignore the gpu hardware check and goes by what the gpus can run as you said",
      "I know you probably won't respond but, since then have you tried again? And if you did has oculus link still worked?",
      "I'm curious, at a collector of vr hw, you likely have some great opinions.  I'm looking to buy an older lower cost but still serviceable vr computer to drive an oculus 2.  Only need to do something like Alyx at \"ok\" levels.  It's hard to discern where that sweet spot of age vs functionality vs price is.  If you've thoughts on those lines, love to hear them!",
      "still waiting :(",
      "As soon as a Mesa driver version with Arc support ships with PopOS, I will try that out. I'm just not familiar enough with Linux yes to feel comfortable modifying the system itself through the terminal ![gif](emote|free_emotes_pack|sweat)",
      "40 Series does work with all the headsets. Nvidia changed something with the Framebuffer handling in Ada Lovelace that causes heavy stutters in VR at higher resolutions if not accounted for.\n\nThis whole topic was already discussed in great detail on the official Varjo discord, and Varjo has already released a patch to fix 5hose stutters on the 40-Series GPUs.\n\nIntel Arc isn't blacklisted or anything. Direct display mode devices shouldn't show up to the Windows desktop, but they do on Arc, and usually at the wrong resolution too. The Rift CV1 showed up at the right resolution, but I'm guessing, since it wasn't found by the Oculus software, that there's some flag missing, like HDMIs 3D side-by-side display stuff. The Index shows up as a 640x480 display, and looking into the SteamVR web console it actually says that no display with the right resolution was found, but it does list all displays connected to the Arc GPU. And in the case of the Reverb G2, the display didn't show up on the desktop (it's a built-in Windows driver, go figure), and WMR did actually launch as if everything was working right, but the displays in the headset stayed black. My guess is that yet again, the driver wasn't able to initiate the vorrect display mode (resolution, refresh rate, direct display, 2 DP lanes per screen, etc.) on the G2...\n\nThe tl;Dr is, it doesn't seem to have anything to do with the software, at least for the display connections, it seems to be a driver related issue with the hardware not showing up correctly...",
      "I haven't, tho my intention was to at some point do a 30 day Arc trial, using nothing but my Arc card for 30 days and when I do I can try Quest/Air Link again :D",
      "Honestly, that I can't really say where the sweet spot for price to performance is for VR capable PC Hardware. What I can say is that you'd want a GPU with at the very minimum 6GB of VRAM (which at least all of the Intel A7 cards do fulfill), at minimum 16GB of System Memory and preferably 6 CPU cores with HT or more. I wouldn't to older then an Intel 8th Gen CPU (or the generation after if you're going Team Red) and in terms of GPU not older then Nvidia 10-Series or RX6000.\nIf you have a Quest 2 then that leaves Intel Arc also as a GPU option open for you, as long as you're using Virtual Desktop (as like I've said before Headsets that require a Display connection to the GPU currently don't work on Arc and the Oculus software refuses to work completely on Arc)",
      "They have not fixed it yet!!!?",
      "You could try something more up-to-date like Fedora or endavourOS.\n\nI'd be very interested to see whether this works.",
      "Okay, so there has been some change as the driver updates have been releasing. My experience was with either the launch driver of the update after launch driver which just said *headset is disconnected.* I’ll try launching VR on my Rift S and see if I match what you’ve been experiencing",
      "It still says that, but at least with the CV1 a 2160x1200 display was showing up on the desktop.",
      "Damn. Sorry man"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "Intel Arc A380 (auto-translate captions for English)",
    "selftext": "",
    "comments": [
      "Usually new driver and firmware updates fixes a lot of performance and power consumption problems. It requires a lot of game tuning to perform well. This requires support from game developers and obviously takes time.",
      "Lol. That’s where your wrong.\n\nThe GPU is clearly being held back by its drivers, just look at any previous Intel GPU. Skylake, Iris Xe, etc.\n\nIt’s just like previous AMD GPUs. The RX580 could barely compete with the 1060. Now it’s faster and almost competing with a 1070.",
      "The hardware isn't actually bad, in compute it beats both competitors. \n\nThe reason why gaming performance sucks are the gaming drivers.\n\nIntel is making gaming drivers for like 2 years by now while competition is tuning their drivers 20years already, nothing easy to catch up with at all.",
      "I'm sure the dev team at Intel has been working extensively to optimize gaming performance for the iGPUs whose main focus has been gaming /s",
      "DoA unless it's priced around a used 1070. $150",
      "actually yes , intel has been devoting significant resources to optimize their igpu for many games, even advertized it as a plus, that so many games are playable on their igpu. They have optimized a lot . Sure, not for AAA games , but many esports titles have been really very well optimized (like cs go) .",
      "> Now it’s faster and almost competing with a 1070.\n\nNo, it's not even close to the 1070.\n\nGTX 1070 vs RX 580 https://youtu.be/mvs1iKfcWoU?t=533\n\nGTX 1060 vs RX 580 https://youtu.be/2fLLVWug-c4?t=752\n\nAt 1080p high the RX 580 8GB is ~6%faster than the GTX 1060 6GB, meanwhile the GTX 1070 is ~27.5% faster than the RX 580 8GB.\n\nThey went from the GTX 1060 being 6% faster 5 years ago to the RX 580 being 6% faster 1 year ago, but it's still no where close to a GTX 1070.",
      "So, somehow this 16 CU 2450 MHz card released in 2022 loses to a GTX 1650. It had better be $80.",
      "r/averageyoutuber",
      "120*",
      "The MSRP is 1030 Yuan which is currently ~$153.",
      "89.99",
      "Hmm... If so, then Intel Arc may almost be perfect for me, as I need OpenCL (but no SR-IoV is a shame).\n\nI believe that Intel drivers, similarly to AMD drivers, also are better on Linux than on Windows, which is also to my favor.",
      "I would buy 2 of them lol",
      ">It just means the card can do more with the allocated power budget\n\n.....thats the point",
      "Trash. That's what it is. They better step up the game real soon as 1650 is still a trash card.",
      "This result + Raja at Intel makes me think they simply placed a wrong bet at a wrong time. Vega was a *great* chip, but for compute (i.e. mining) not gaming - it seems Intel wanted to play like AMD/NVidia, print money by making compute-first products to sell to datacenters and miners but get gamers to subsidize its R&D on early iterations by soaking up sub-par silicon. The latest crypto crash *really* came at a bad time for Intel - with AMD and NVidia last gen outperforming Intel effortlessly at lower power consumption *and* flooding the used market i don't see Intel selling any of these at any price...",
      "What a mess.      \nBeaten soundly by GTX 1650 and RX 6400 .        \nDoes manage to beat the 1050ti ( by small margin) .        \nUses much more power than both 1050ti and 1650 .       \n\nbad. just bad.\n\nFor the power consumption, keep in mind that 1650 is built on TSMC 12nm (tweaked 16nm) . \nIntel ARC is using TSMC 6nm.           \n\nARC is being beaten by an nvidia chip built on 12nm and manages to consume more power while doing so. wow .\nAMD Vega vibes all over again.",
      "..intel is making gaming drivers for much longer than 2 years now. \nthey have been releasing gaming drivers for their igpu for a long, long time",
      "i dont dispute that drivers can improve. Power consumption though did not change.release RX580 with todays 580 have same power consumption,despite the perf increase due to drivers.  If the card gains peformance due to driver optimizations, this does not change the card TDP/ power envelope.It just means the card can do more with the allocated power budget... anyway....didnt think that that needed explaining."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "a380"
    ],
    "title": "Intel Arc Alchemist desktop series including A770, A750, A580 and A380 SKUs reportedly delayed till late Q2/early Q3",
    "selftext": "",
    "comments": [
      "Honestly, it doesn't matter.\n\nThey missed their window by not launching during the GPU apocalypse uncontested, so now they have to go against Nvidia and AMD offerings with mature drivers in a health(ier) market.\n\nMight as well take the L at this point and just figure out your drivers.",
      "What they've officially stated is launch in 'summer', compared to mobile 5/7 'early summer'. Anything past 'early summer' puts it in calendar Q3.",
      "They're going to have to seriously discount those cards to sell next to Nvidia and AMD. Can't see many people taking a chance on the first gen of cards especially in the enthusiast market when the top Arc card is only at 3070 level of performance. \n\nAnd then by August and September all of the talk will be on RTX 40 and RDNA3 which will blow these cards out of the water.",
      "This will get steam rolled by Lovelace and rdna3. Should’ve released this in June 2021",
      "Not if it's dirt cheap like polaris. \nBoth lovelace and rdna3 will be starting at minimum 400$ for the lowest model.",
      "Their gonna take an L. The market is getting better and by the time it comes out, amd and nvidia will have their gpus at a few % above msrp. Therefore making these card useless. They won't be faster, they might be a bit less expensive, and their launching gpus close to next gen.",
      "To be honest, even if they launched in June 2021, they'd have probably all been gobbled up by miners.",
      "most of them are probably going straight to OEM.\n\nOEMs will use it as a way to tell AMD/Nvidia to fuck off with their horrid \"incentives\" like priority allocation. Most people buying computers don't know what the hell goes in them so the main problem is drivers. if the drivers continue to be miserable then intel will get nowhere even giving these things away for free.",
      "At least they would’ve sold (Not defending miners). Now they have zero chance of becoming dominant in any market",
      "More competition, more fun.",
      "Well considering the frame stuttering in the arc a350m for mobile that was recently tested, my money is on bad drivers and at this point Intel should just work on the drivers. They missed a critical chance to disrupt the market. Now new gpu prices have fallen down hard. Not yet at msrp but much better. The second hand market will be flooded with rdna2 and 3000series from Nvidia and not mention the rtx4000 series and rdna 3 gpus coming soon. This will be really rough  on Intel.",
      "When will Intel learn that blatantly lying to investors will get them nowhere?",
      "Efficiency wise it's better than GTX 1600, at least the already released laptop 350m which at around 40W has same performance as GTX 1650m at 50W. \nThat's not too bad. Probably on the same level as RTX 3000 as efficiency didn't improve much compared to GTX 1600 and 2000.",
      "Well, thats basically all she wrote for ARC...\n\nI know for a fact Intel won't price them relative to their performance vs other cards.\n\nI hope I am incorrect, as aggressive pricing is the only thing that will save them. That creates an even bigger problem for them though, if they start out low priced, they will remain there for many years. Thats just the way the market works.",
      "They're making them at TSMC so it didn't matter. if these were being made at Intel fabs, well they'd be worse if they were being pushed out on a broken 10nm a year ago or 14nm due to power efficiency.\n\nBeing made at TSMC means when TSMC were starved for capacity these would have been even worse.\n\nUsing TSMC 6nm indications would be that performance is closer to AMD/Nvidia parts that use half the die size. \n\nThe only way these matter in the first gen or two is if Intel does it's Atom/phone/tablet tactic and just firebombs pricing to force their way into relevancy but if the architecture doesn't catch up then eventually when they try to charge real prices they'll be pushed out.",
      "Mmmhmm.",
      "More competition more fun works if wafer supply is ample and performance is competitive. If performance sucks and they eat up supply of incredibly limited wafers then it's terrible for everyone, Intel included. They'll be eating shit selling these at below what they cost to produce and AMD/Nvidia lose out supply to make twice as many gpus from the same wafers and the end user loses out.",
      "Thats just clock gating. Easily fixable.\n\nLook at the GPU speed every stutter, you will see it drop to 1150mhz.",
      "TSMC 6nm is a retooled 7nm, so the density is not that dissimilar.",
      "How is it fixed if u dnt mind explaining?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "The morning after – reflections of the Arc A380 premiere and (no) second conclusion",
    "selftext": "",
    "comments": [
      "I don’t doubt the thoroughness or technical knowledge of Igor’s lab.  He’s great.\n\nHowever without official pricing .. it’s hard to say if this is a ‘pass or fail’ for a GPU ..",
      "You know what, yeah. I want this card to work, and I don’t think it’s *worse* than the 6500 XT and/or 6400… but it sure ain’t much better. This could have been a great little card if the drivers were better, but right now it’s a bad card, and Intel should be ashamed of themselves for loosing it on the Chinese and South Korean markets with a promo consisting of inflated Time Spy scores and little else.\n\nWe need more competition in the GPU space, but if Intel isn’t going to be honest and ethical, they deserve to be blown out of the water.",
      "So basically this can still put some pressure on AMD/NVidia if it's something like $100 MSRP, it can still serve as an entry level barely over iGPU card and give a huge middle finger to NVidia's 1050/1650 price gauging, maybe finally put a stop to those two being churned out and let the entire segment move up in performance instead of just in price... but yeah, it's a failed product at this point",
      "Agree with both of these paragraphs\n\n> What irritated me about making comments, however, is the sudden puppy-protector instinct that erupts whenever you want to protect something small and fragile. Yes, it is Intel’s first dedicated graphics card developed for the consumer market in what feels like ages, but it is neither a sympathetic underdog nor a fragile plant that should not be trampled on. Here, a multi-billion corporation with extensively purchased personnel and know-how (also in the heads) as well as many years of development time and almost infinite seeming resources has failed grandiosely. You cannot and must not call it anything else. The saying that money alone does not score goals, which is often used in soccer, also lives on in IT, and the fact that an unfinished product with even more unfinished drivers is sold to customers for expensive money can and must be criticized.\n\n....\n\n> Of course, you should feel sorry for Intel, but even more for those who have spent their money on such products because the PR machinery produces full-length propaganda at its best and you as a consumer have trusted it. Trust is also exactly the keyword that really matters here. Because I can’t use anything productively that I can’t know if it won’t destroy the work I’ve done up to that point in the next five minutes with a crashing gesture of nonchalance just like that. And there are so many technical bugs and carelessness in the drivers and GUI that you could lose faith in programming mankind. Sure, programmers don’t grow on trees, but Intel didn’t start from scratch. This should never be forgotten, and criticism should also include those who have willingly followed the call of money and have not managed anything that would really be resilient.",
      "Blistering follow up  from Igor's lab. \n\n\nReview:\n\nhttps://www.igorslab.de/en/intel-arc-a380-6gb-in-test-gunnir-photon-total-benchmarks-detail-analysis-and-extensive-teardown/\n\nPrevious discussion:\n\nhttps://old.reddit.com/r/intel/comments/w3im2q/intel_arc_a380_6gb_review_gunnir_photon_including/",
      "I feel like those cards you mentioned aren't even for gaming. And if you're not using it for gaming, isn't the A380 kind of the best at what it does for even $130-$140 compared to the 1050 and 1650?",
      "$129.99 is official pricing",
      "Pretty much my sentiment on this. Appearing on LTT and GN to butter them up along with the viewers doesn't change anything. At all. The end product in systems is all that matters. Remember when RDNA1 ate ass for 9 months with constant black screens and other issues from users, but it beat the competition when it came to price to performance? Same situation. No need to treat them with kiddie gloves. \n\nThey should be almost giving this stuff away, and if they take a loss, then they take a loss. Better luck next time.",
      "It's a fail regardless of any price at all. And you know it's terrible, when Intel sends their employees and not hardware. They're doing damage control at this point and the CPU price raises is solely to mitigate losses created by Arc, as it simply will. Not. Sell. And they know that. Intel fked up hard.",
      "It can still be a great video encoder for home servers. If the Linux drivers are good, which they probably are, because they can build upon the integrated graphics than it could be good in there. No direct x and vulcan should work pretty good on it",
      "Oh give me a break. It's a decent first try in a duopoly market. People beg Intel to enter the market then lambast them the second thier product releases. \n\nAMD and Nvidia have decades of experience. It's actually promising that Intel is doing damage control at all. They could easily just dump the whole thing like Larrabee.",
      "What a botched launch. This card is arguably only better than the GTX 1630 from Nvidia. Anyways, I truly hope these cards end up in laptops so nobody would end up with these cards as desktop cards. Given the enormous amount of capable people and funding Intel has access to, I look forward to much better drivers and tighter execution of their battlemage line up. Please be good.",
      "Yeah, but most of those people bought them 4 years ago. Are you going to buy a 1050 today to game on? Maybe used, if you live in a country that has bad hardware supplies. I've helped some people over at r/buildapc that pretty much only had like 2 year old parts available at stupid prices in their country. Maybe saying they aren't for gaming is wrong, but I'd say you don't buy those cards for gaming anymore. Today. They are like GTX 1030 territory. I consider them display output, or plex server cards.",
      "But it could be $139.99 Tom said. Given some bad reviews it might actually be on the lower end, though. Or is there an actual official price now. It seemed they didn't even know if AIBs will even release the A380 in the west.",
      ">$129.99 is official pricing  \n\nNot a single source has validated that price.  \n\nIts MSRP in China is RMB 1030 which currently equates to around US$150.",
      "Well AIB partners can charge extra money for “premium” card variants so that is kind of a mute point. $129.99 is the base MSRP for the card. \n\nYeah, it is up to AIB partners to release their version of the card in the US, it sounds like Intel will be making their reference cards available everywhere but they are only limited runs.",
      "It needs to start working and stop randomly crashing doing its job first.",
      "It's not decent, it's an alpha stage product released to public. They deserve to be shit on. Overpromised, overcompared, severely underdelivered, region locked, review controlled and issue bombed garbage."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "a380"
    ],
    "title": "Intel ARC A750 and A380 as eGPU for GPD Win Max 2",
    "selftext": "",
    "comments": []
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "My intels arc a380 fans wont keep spinning",
    "selftext": "I have the Asrock intel arc a380 and when I start the computer up the fans spin for how many seconds and then they stop. Then when I log into Windows 11 the fans don't spin at all or nothing so I don't know if this is a hardware problem? All I do is browse the web and I don't game. When I browse the web the fans don't spin. Does anyone else have this problem?",
    "comments": [
      "I don't know if there is a problem with your GPU. But most current GPUs have 0 rpm mode, and the fans won't start spinning unless the GPU get hot enough.\n\nOpen a game, or start a GPU benchmark and check if the fans start spinning when the GPU gets hotter.",
      "Most standard GPU bios I have seen from EVGA don’t even both starting the fans until 58C so it sounds quite normal what it is doing",
      "WTF? Why do you need a 13th gen intel cpu for web browsing? A potato cpu can do that. Just wow. But hey, it's your money.",
      "I know that my GPU usually stays at 53-54 degrees and I guess that isn't considered hot if the fans aren't running thanks for the heads up.",
      "Nothing to worry about, even fans on a A770 may be at 0 rpm if the temps are low. If your GPU is working hard and they're still not spinning, then you have an issue.",
      "Totally normal. Those fans won't run until that card is under enough load to get hot past a specific temperature. No worries.",
      "I was trying to keep the cost down instead of having to pay $319 or $239 for a new Intel 13th gen CPU.",
      "Thanks for the heads up my GPU temps are usually 53-54-55 nothing higher.",
      "People are fucking special man. We had a guy in r/Intel earlier adamantly freaking out that his 30c idle on his 13900k was absolutely unacceptable and OVERHEATING WHY ARE REVIEWERS AT 27C THIS IS UNFAIR.\n\nPeople are stupid, and it needs to be called as it is.",
      "If all you do is browse the web, why did you even bother buying a discrete GPU? You could have gotten a cpu with integrated gpu and call it a day. As mentioned by the other commenter, your gpu fans likely won't start spinning until there is a load on it."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "i cant fiend intel arc a380 driver",
    "selftext": " **i cant fiend intel arc a380 driver its just for Ubuntu why???**  i want for win 10",
    "comments": [
      "https://www.intel.com/content/www/us/en/products/docs/discrete-gpus/arc/software/drivers.html\n\nIntels website is a bit cringe but here are the arc drivers",
      "Are you having a stroke?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "a380"
    ],
    "title": "Portal Prelude RTX working on Intel Arc?",
    "selftext": "Hey Intel peope, I was wondering if anyone has successfully run Portal Prelude RTX on their Intel Arc (GPU, not integrated). I tried using my A380 but got an error message. Given that it's free on Steam, I was hoping more people would give it a shot. I tried posting on the Steam game forums, but didn't get any responses.",
    "comments": [
      "I just tested it myself and recieved an error too. I'm part of the community beta testing program, so I'll file a report about this directly to Intel.",
      "Great!"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "Ray tracing and API scaling on Intel Arc A380",
    "selftext": "",
    "comments": [
      "Pretty interesting. The frame drops when turning on raytracing in WoW more or less match the performance hit, at least proportionally, I see on a RTX 3060 laptop. I'm really interested to see how the higher-end Arc GPUs perform. The low-end one we have so far is pretty impressive for a first outing. I imagine there's still some performance on the table that'll come with driver updates as well.",
      "Battle Mage or likely Celestial. It's not their intent to cover every performance bracket with first gen.",
      "Which exactly are the performances brackets covered then in contemporary competition terms? This celestial you’re on about doesn’t seem like an Ampere contemporary.",
      "There are like 2-3 other companies trying to make a consumer dGPU and as of now, they don't really have anything. Intel using their iGPU base made a dGPU lineup that works. They still have a freakton of things to do with the software, drivers, and likely multiple WTF in the silicon showed up, but they already have first gen and is working.\n\nWouldn't be surprised if next get would be pushed hard in Intel Evo, and with partners that usually do Intel premium designs. Dell XPS, Precission,L LG Gram, Lenovos and HPs - without MX450+ but with B300/500 then Asus TUF with B700 and without RTX 4050/4050 Ti or 4060 to some extent. If you make CPU+GPU intel based you get a discount! OEMs will go for it.",
      "Still 1st Gen.",
      "If they push A700 laptop they should compete with RTX 3060 laptop. A300 laptops out there already compete with MX and alike cards.",
      "So what’s their answer to a 3070ti 100-120w commonly found in 2kg thin and light gaming laptops for around $2000?",
      "This is the second generation",
      "You think those will come out ?\n\nisn’t this an unmitigated distaste for Intel ?\n\nSadly for sure , I was hoping for more competition."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "Arc A380 not displaying with old PC",
    "selftext": "Hello, I have an old desktop that I am trying to get running again. Currently it has a i5 4570 CPU and a Gigabyte B85n motherboard. It was a small mITX build from back in college.\n\nI purchased a Arc a380 and I can't seem to get it to work. If I download the drivers before hand and attempt to install, I get \"No driver was found that coils be installed on the current device. Exit code 8\".\n\nIf I plug in the GPU and boot my PC, I get a black screen. After doing a bit of research, it seems there is an issue with the on-board graphics. I disabled though, and still unable to see anything using either GPU or CPU graphics.\n\nAm I doing something wrong? Or is there a way that I can install the drivers manually, then disable integrated graphics and use the GPU?\n\nThanks",
    "comments": [
      "You’re likely running into issues with the old motherboard - it doesn’t support Resizable Bar which seems to be the fix for a lot of people. I believe you need at least a 10th gen intel processor for that",
      "I got an Arc A770 working with an Intel 8700k on a z370 motherboard, I had to use the motherboards hdmi out and set the (newest) bios for onboard graphics, then install the GPU, then install the Arc drivers, then switch the hdmi to the Arc. Don't know if that'll work for an even older system though.\n\nMy GPU had a quick life though. 3 different systems in 2 months before aRGB broke and had to be RMA'd but worked with cpu's 8700k, 9900ks and a 13500. Worked well while I had it.",
      "I had the issue where it kept blackscreening but would still boot into windows, eventually after restarting a few times it worked.",
      "Did you find a fix for this, or did it just go away on its own? I lose display signal on my A380 regularly and haven't found a solution.",
      "My issue was specifically when first installing my GPU, since then I've had no black screen issues (I *suspect* it had to do with no driver installed then, although I could definitely be wrong about that), although I don't regularly restart my PC (I've probably started my PC at most 5 times since I bought+installed my A750) and never let it go to sleep."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "Intel's Arc A380 Is The First Graphics Card To Support DisplayPort 2.0 Standard But No Monitor Out There To Make Use of It",
    "selftext": "",
    "comments": [
      "So you can play pacman 3D in 16K?",
      "Kinda curious if laptop displays will take advantage of this to save power:\n\nAlongside raw bandwidth improvements, DisplayPort 2.0 also has a few enhancements on the feature front, one of which is Panel Replay.\n\nThis makes the display work more efficiently by limiting the power it uses and decreasing the thermal output. For example, with Panel Replay enabled, a smaller device with a high-resolution display only updates elements that change on-screen. This will prevent the display from using more power to update items that aren’t being shown at the present moment, so when you stay on one webpage, it won’t constantly refresh.",
      "People are sick of waiting",
      "They already do. It's parted of eDP specifications and called panel self-refresh. It's an old feature - my laptop from 2016 supports it. This just looks like the added the feature to the non-embedded part of the specification and rebranded it for whatever reason.\n\nTraditional panel self-refresh (PSR) would only allow a refresh to be skipped if the screen contents didn't change at all. New PSR implementations have a \"region of interest\" and can also update parts of the screen.\n\nPSR requires special LCD controllers with local framebuffer, though. So there's a complexity/cost/power usage tradeoff. You have to consider that LCD panels aren't digital devices. They're analog and work very much like CRT screens. You have to refresh each subpixel regularly, or it will fade. It takes significantly longer compared to a CRT, though, so flicker is unusual. With PSR, the LCD controller will take care of refresh independently, so that the GPU and its memory can power down for a prolonged time.",
      "r/woooosh",
      "On A380? It'll still lag on it."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "I have a new Intel Arc A380 GPU, installed on a Z390-E Gaming MB, and CPU-Z shows its Graphics Interface BUS to be running on PCIe 1.0. The MB is enabled for 3.0. What can I do to fix it?",
    "selftext": "I have a new Intel Arc A380 GPU, installed on a Z390-E Gaming MB, and CPU-Z shows its Graphics Interface BUS to be running on PCIe 1.0. The MB is enabled for 3.0. What can I do to fix it?\n\nHere's a picture:\n\n[https://ibb.co/kKCbjF6](https://ibb.co/kKCbjF6)",
    "comments": [
      "run gpu-z and press button with question mark and you will see actual pcie speed left from ? symbol.",
      "\\*sigh*\n\n*This* thread again?",
      "Likely a problem with the application itself not being updated.\n\nIf your SBIOS is telling you it's 3.0, it's 3.0.",
      "When you put the gpu under load it should say 3.0, if it doesn't you have a problem.",
      "That's normal due to ACM G10 and ACM G11 having a pcie upstream downstream interface between the gpu and the hda controller. It did the same thing when I had an Arc A380. Check with GPU Z or HWINFO64's pci tree.",
      "Playing game still remains at 3.0... Not sure what to do...!",
      "I don't know what that means...",
      "CPU-Z is buggy! It has the decimal in the wrong place. You're actually using PCIe 10 because Intel is so advanced!"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "a380"
    ],
    "title": "Intel GPU labs gets hands-on with Arc Alchemist A380 desktop GPU",
    "selftext": "",
    "comments": [
      "Intel could not plow their face into the ground harder. They are going to release these cards straight into a recession, with disappointingly-bad performance numbers, into a market flooded with old mining GPUs.",
      "I think the missing \"Retail\" in the title really changes the tone of the message.",
      "Pretty bad GPU that is about to be released at the worst time possible.",
      "Intel gets ahold of Intel property.",
      "There are no recessions for data centers"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "Intel Arc xxx - GREAT DIY compute performer ?",
    "selftext": "I was just looking at the first available ARC GPU card specs, [Asrock's Arc A380](https://geizhals.de/asrock-intel-arc-a380-challenger-itx-6gb-oc-a380-cli-6go-a2791643.html)\n\nLeaving graphic driver issues aside for a moment, from pure raw computing perspective, it looks great. 4.61 TFLOPS (FP32), 1.15 TFLOPS (FP64), 186GB/​s for less than €140.\n\nBut the kicker is that its FP32 capabilities are 1/4 of FP64. \nBoth nVidia and AMD in that gaming/commercial segment can do just 1/16 or less.\n\nAnything even coming close to FP32 ( let alone FP64) or bandwidth numbers from A380 costs far more.\n\nSo, one would think that ARC series might find its home in certain uses.\nDoes anyone have more info to share on this matter ?",
    "comments": [
      "We have yet to see, because the only thing everyone seems to care about reviewing is gaming performance on Windows, and getting one out to diss it there as soon as possible.\n\nThen they use those gaming performance on Windows stats to jump to the conclusion that the card can’t do anything else well, like compute, rendering, or even run games well on Linux, and for the later case we can assume it is mostly untrue due to its decent Vulkan performance, which is what most Linux gaming care about.\n\nThen you get people suggesting cards instead of Arc, even going as far as to suggest the 6400 or GTX 970 even when the person was considering it for Plex encoding.\n\nBut yes, it’s probably going to do well in compute. Raja did make Vega, a decent compute unit with terrible gaming performance compared to NVIDIA.",
      "Welcome, beta tester, long journey is ahead us, but fear not.",
      "Yes, it's pretty good for compute.\n\nOneAPI is also an actually useable piece of software, unlike ROCm.",
      "Every DIY project involves a journey.",
      "Question is if teraflops actually converts to actual performance. Nvidia made a video on their site a while ago talking about how latency is a lot more important. You really don't need a massive amount of TF, if you can't feed enough data to the card.\n\nAMD's RDAN3 architecture is supposed to double teraflops. Navi33 is supposed to have like 140% more teraflops of what Navi23 has, and yet will likely only be 30-60% faster. Navi31 is going to have like 220% more TF than Navi21, and yet will likely only be 70-100% faster.\n\nIn some production work, it'll kick ass, I'm sure, because of those FP64 numbers like you mentioned.\n\nThe A770 especially has a massive amount of silicon, and compute numbers compared to AMD. At like RX 6800XT levels almost in FP16 and FP32. It's just weird how it's no where close to it in gaming. The machine learning numbers are also really impressive. I think it's like close to an RTX 3080 in ML matrix stuff.",
      "Good news on the drivers side: [https://community.intel.com/t5/Blogs/Products-and-Solutions/Gaming/Engineering-Arc-8-19-2022/post/1407637](https://community.intel.com/t5/Blogs/Products-and-Solutions/Gaming/Engineering-Arc-8-19-2022/post/1407637)\n\nIntel is owning its problems. They should had delayed it , probably, even considering they were already late. \n\nBut I am sure that eventually the drivers will not be an issue so I understand looking at it from a \"pure raw computing perspective\".",
      ">But the kicker is that its FP32 capabilities are 1/4 of FP64. \n\nSorry to burst your bubble, but as [Intel Arc doesn't support FP64 natively](https://www.tomshardware.com/news/intel-arc-will-not-support-fp64-hardware), [FP64 compute for Arc is simply unavailable under Windows](https://www.pcgameshardware.de/screenshots/1280x1024/2022/10/AIDA64-GPGPU-Benchmark_Arc-A770-16GB-pcgh.png), and although [FP64 emulation is available under Linux, performance is abysmal](https://github.com/ekondis/mixbench/issues/32) (I know that the GitHub issue doesn't link to something explicitly about Arc GPUs, but it can easily be inferred that a similar thing also applies to Arc GPUs given the same lack of FP64 cores). Heck, you probably got the numbers off of techpowerup's website, and if you go [there](https://www.techpowerup.com/gpu-specs/arc-a380.c3913) right now you can see that the FP64 (double) performance section under \"Theoretical performance\" has already been completely removed to reflect what I'm saying here.  \n\n\nSo yea TLDR forget about FP64 compute on Intel Arc and get yourself a consumer AMD dGPU instead if you're looking for good FP64 performance for the cheap (Nvidia has been greatly nerfing their FP64 compute capabilities on their latest gen GPUs, i.e. dialing it all the way down to 1/64th of the FP32 performance, so I won't recommend Nvidia consumer or even pro GPUs for FP64).",
      "Vega is power hungry and sadly needs the hbm to feed it.  The hbm that it needs kills it because of the cost.  But i still love my v56.  It's undervolted and lives in my htpc.",
      "I think all this matters much more for opengl etc than GPU computing.\n\nIn computing, one can tailor timings and code to  much greater degree.\nYou gave the perfect example - while nVidia was on top for gaming, AMD has been big with crypto crowd for very same reason - they were able to put its raw power to a good use.\n\nSo, why couldn't this repeat with Intel Arc ?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "a380"
    ],
    "title": "Intel Arc availability?",
    "selftext": "Aside of the long list of issues and whatnot how is the availability looking - official and unofficial via Chinese resellers? Anyone knows where to buy like a A380 to play around with it? (or when it will become available world wide ? :) )\n\nGunnir does not sell outside China and there is only one overpriced ebay listing, no Aliexpress or Alibaba from what I've searched. Asrock has a similar card but also unknown where and when. Some new floating for MSI and Asus as well. There are also like 2 laptop listings in EU with A370M and A350M but that's a full laptop...",
    "comments": [
      "Would be strange, Intel Graphics [just uploaded a video on Arc A750 performance a hour ago.](https://www.youtube.com/watch?v=6L3JcnBP_jc) If cancellation was on the table I would think they would shutter these videos",
      "Based on rumors, arc (high-end) could be given a thumbs up as early as Aug 5th this friday",
      "All anyone has said is that Battlemage might be cancelled. And that's probably 18 months away. Tom Peterson talks as if a Alchemist refresh is coming if you listen to the PCworld interview on Twitch (YouTube channel got taken down by accident, and can't be found there).",
      "Lol. MLID and their shitty flood of RTX 3060 rumor.\n\nI will believe when I see it, especially when they’re already shipping OEM PCs with Arc in China and laptops with Arc in the US.",
      "They already said the A380 would come to the US around $120-$130 USD. And MSI already sells a prebuild system with the A380 in China.",
      "No one even knows if the A380 will ever come to the west. Intel makes it sound like it's up to AIBs, and they don't know either. Gunnir won't sell here I'd imagine, so it's only Asrock that might. I don't even know which North American supplier outside of Asrock has announced even an A700 card. some are saying ASUS, Gigabyte & MSI might have something but if you were to believe what MLID is saying, then some of them are abandoning that idea, and others are confused themselves. My guess would be the A700 series is launching only from Intel for the first month at least.",
      "There's listings on taobao, so you can probably use a broker like superbuy to import it. It's about $199 before shipping and brokerage fees though.",
      "I've heard Tom say [$129-139](https://youtu.be/AN8ZAf15DrM?t=618). So $130-140 as Steve corrected him by removing the $1. Is there an updated claim of $120-130 somewhere? Also he said \"**If** this this becomes available\", and I swear there were other videos where he makes it sound like he's even more sceptical if the A380 will ever come to the NA. Where you get the $120 to $130 from, and is there actual official confirmation it'll ever come here?",
      "Based on rumors, it might be getting cancelled. Which wouldn't be surprising considering what happened to optane."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "Intel Arc A380 Benchmarked, Full Alchemist GPU Specs Reportedly Revealed",
    "selftext": "",
    "comments": [
      ">A380\n\nSo *that's* why it's so late and the performance doesn't quite meet the expectations! /joke",
      "How is it confusing? No, literally, how?\n\nA5 is better than A3, and A7 is better than A5. It's a lot better than stacking suffixes.",
      "A 75w card with that performance for $150-200 MSRP range would be excellent if they don't gimp the PCIe width like the 6500XT.  Perfect for an upgrade to an iGPU build with low-end PSU, OptiPlex retrofit, low-profile case, etc.  Seems like the perfect 1650 successor.",
      "People with small brains dont understand ☹️",
      "The higher VRAM capacity is more important, after all most of the benefit of the wider PCIE bandwith is if the VRAM capacity is too low.",
      "That's from February 23rd, by the way.",
      "How are you running 5ghz on that cpu? Lol",
      "The issue with the 6500xt is that by limiting the number of PCIe lanes to x4, the effective bandwidth when in an gen3 slot is substantially less than that of the card itself, creating an unnecessary bottleneck.  This is likely a result of the 6500xt starting life as a mobile GPU that would never be used in anything but a gen4 environment, but the limitation greatly hinders it in a desktop environment on older system. As long as Intel doesn't make this same mistake with their low end discrete cards, a low power card like this will still fill a big gap in the market not well served by AMD and seemingly abandoned completely by Nvidia.",
      "https://youtu.be/-EDJXISD6RY\n\nI meant that Hardware Unboxed already proved that if the VRAM capacity is higher the GPU will have not to rely on system memory via the PCIE lanes as much.\n\nIf the 6500 XT was 8GB because of a 128-bit bus or 64-bit with double the VRAM density that would've been the single most beneficial improvement. VRAM is primary in the memory bottleneck the 6500 XT suffers, PCIE lanes are secondary.",
      "That definitely makes sense, but I doubt you'll see much more than 4-6gb of GDDR6 in the sub-$200 price range due to GDDR prices.  Making the bus x8 instead of x4 would be far less expensive to design-in than doubling the VRAM... with the 6500XT, it was already a design limitation of a repurposed mobile GPU not originally intended to be used in anything other than a notebook/laptop with a Gen4-enabled platform.\n\nThen again, I'm one of the few that doesn't think the 6500XT is hot garbage outside of the GPU market as a whole being extremely inflationary... reviewers just seem to cling to where a $200 card USED to sit in the lineup compared to where it does now.  A year from now, I wouldn't hesitate to put a $125-$150 clearance 6500XT in a $500 i3-12100 build for a kid's gaming PC... but the PCIe lane x4 limitation prevents it from being utilized with even more modest components that would otherwise be a good fit in terms of performance and price.",
      "That's a good point, I was only thinking about how the 8 GB would be more effective than the doubling in PCIE bus width, but the PCIE bus width increase probably improves the performance to price ratio with better scaling.\n\n> I'm one of the few that doesn't think the 6500XT is hot garbage\n\nKind agree with that, the GPU die itself is very good from a budget perspective, just mostly crippled by its memory bandwidth problem."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "can i use a intel arc a750 whit i3-10100f?",
    "selftext": "hi guys, i want to buy a new pc (i have a old one whit c2d e8400 and gma 4500 \\[very bad\\]), whatcing some videos and stores i have find an arc a750 and a i3-10100f, next i try to search some bottleneck test (nothing found). Yesterday i found a video about the arc a380 and they says that for installing the drivers yu have to use a IGPU, so i have to use a igpu for installing the drivers of a arc a750 whit a i3-10100f? \\[sorry if my english is bad\\]",
    "comments": [
      "Yea you can use it perfectly no need to worry",
      "Yes. You don't need another GPU just for drivers, microsoft has a default driver for video cards. It makes your dGPU run (but poorly) so you can get a copy of GPU driver and install it."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "Intel Arc A380 MXM Card Surfaces with 50-75 W Power Limits",
    "selftext": "",
    "comments": [
      "Why haven't we gotten USFF GPUs.\n\nThat would be the best market for Arc GPUs.",
      "It's mxm which is/was used in laptops to allow upgradeable gpus. Kinda surprised it's still a thing since Nvidia stopped supporting it.",
      "Is that a ramslot connector"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "a380"
    ],
    "title": "Bykski releases waterblocks for Intel Arc A770 and A380 GPUs - VideoCardz.com",
    "selftext": "",
    "comments": [
      "Why would anybody watercool A380?",
      "Stupid"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "can anyone with an intel arc A380 do a blender 3d benchmark",
    "selftext": "hi, I just want to ask anyone with an intel arc A380 to do a benchmark on the blender 3D benchmarking tool. this will help a lot of 3D artists decide whether they will buy the card or not. if you do a benchmark make sure to put the score you get in the comment\n\nthanks in advanced",
    "comments": [
      "https://techgage.com/article/an-alluring-alchemist-intel-arc-a380-gpu-creator-performance-review/\n\nthis could help",
      "Doesn’t it release on the 12th",
      "Are we going to get HIP on Arc?",
      "it did help , the article has blender benchmarks , thanks",
      "Nope. A380 are available since September on newegg"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "Intel Arc A380 Desktop Graphics Card Leaks Out: Up to 100 FPS at 1080p in Popular eSports Games | Hardware Times",
    "selftext": "",
    "comments": [
      "This is the basic Arc model, nowhere near the flagship. It's a 3 series. There's a 5 and a 7 series aswell.",
      "5600g doesn't do 100FPS in OW even on low all settings at stock speeds and 1080p, it's more around 80 FPS average. And without knowing the exact settings of 'medium/low' this Chinese company is citing it's hard to directly compare, but the A380 is already faster if we compare low to low.\n\nAs for LoL, it's a worthless benchmark, even the 12400 IGP can do around 180FPS, the game isnt GPU intensive enough to differentiate between GPUs.",
      "Ah, 780, my bad. Thanks!",
      "12400f + this just to get similar 5600g performance? this amd apu is 100fps on overwatch, 200 on lol, same as article claims for this dgpu.",
      "Bold of you to assume the performance of an APU that isn’t out yet.",
      "This 7600G will probably do. By the time this card is released.",
      "100 fps in eSports games isn't that impressive to be honest. Isn't the A380 supposed to be the flagship, comparable to a \\~ RTX 3070?",
      "Wow so it has performance like GTX 1650 non-super ahahaha. Basically 1650 but with bugs, glitches and some games not turning on.\n\nWhat a crap GPU..  \n\nI already see how much these Intel GPUs will get bashed by all unpaid reviewers after it releases for having outdated performance and unstable drivers.\n\nThey will get bashed much more than 6500xt."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "a380"
    ],
    "title": "Intel Arc Graphics A380: Compelling For Open-Source Enthusiasts & Developers At ~$139 Review",
    "selftext": "",
    "comments": [
      "Might be fun for open-source people, but for gamers this card is utter trash, and will be until it beats the 1650 Super.",
      "I don’t think it’ll ever get there, it’s trading blows with the base 1650 right now, and the 1650 Super is *35%* faster.",
      "If they manage to beat the 1650 super by improving the drivers, at 139$ this is a pretty good entry-level linux gaming card. Since the drivers are open source, the card should be pretty much plug and play on Linux - as opposed to NVIDIA. On Windows I think they are beating the card by 10% so it should be possible to achieve it. Even though the 1650 is a very old card, I'm happy to find any gpu in that price range.",
      "I wish the test included a comparison with an Intel Integrated GPU. How much faster is it than an Alderlake iGPU?",
      "It's really damn tempting to get one of these just for the encoder, but do we know if we're going to be getting this in the iGPUs for Raptor Lake? Going to be purchasing either the 13900k or 7950x3D and I'm currently leaning AMD because the cache is going to be great for my use case, but a good AV1 encoder might change my mind."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "Intel Arc A380 AV1 encoding",
    "selftext": "Gaming PC with\n13600k\nRtx 3060 \n32gb Ddr5 6800\n850watt PSU\n\nI am planning to add an ARC A380 to my system for AV1 Encoding. I heard about this in a Linus Tech Tips video. Does anyone know of a place where I can find a guide for how to setup/enable this? Thanks",
    "comments": [
      "make sure your mobo has enough PCI slots/lanes, plug it in and select it for the application you want to use its en-/decoding",
      "Ok thanks! I have 3 slots",
      "Wouldn't it just, work out of the box? (obs atleast)"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "How many simultenous encoding sessions can an Arc GPU handle?",
    "selftext": "I need to stream and record at the same time, so I want a card that can do at least 2 encoding sessions simultenously.\n\nIn the spec page of the Arc A380, under \"Supported Technologies\"', there are 2 \"Multi-Format Codec Engines\". Not sure what this means",
    "comments": [
      "> It has a hard time starting them after 3 1080p streams that require conversion\n\nSo it does at least two.\n\nhttps://www.reddit.com/r/PleX/comments/yb4cg3/my_experience_with_intel_arc_a380_plex/",
      "Nice! Thanks for your answer"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "Intel ARC A380 desktop to be 23% faster than ARC A370M mobile - VideoCardz.com",
    "selftext": "",
    "comments": [
      "How shocking",
      "A780 is their high end which matches 3070-3070ti\n\nA580- 3060 to 3060ti\n\nA380- 1650 to 1650 super",
      "Gap is always larger on higher end compared to mid to low end ie 3060 desktop vs laptop is smaller.",
      "Much smaller difference than between 6800 and 6700m or 3080 and 3070m.",
      "I have been told than fire is hot.",
      "Also Water is Wet",
      "Sure. And desktop 1650 (not super) vs laptop 1650 is almost the same. But this intel A380 should be their highest end.",
      "Which is somewhere between 3060 and 3070, maybe it is a decent gap."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "rx 6400 or arc a380",
    "selftext": "",
    "comments": [
      "I would go with whichever is cheaper. They're both pretty matched, with rx 6400 pulling in the lead by a fairly small amount.\n\nEdit: saw the video from the other comment, I was wrong. Rx 6400 is much better, but see if you can find a gtx 1650 as well.",
      "The benchmark testing of both the AMD RX 6400 4 GB and the Intel ARC Alchemist A380 6 GB is more than 3 months old, but I expect the current performance levels to have changed less than 20% from the initial testing, which means that the RX 6400 will still have more than a 30% performance advantage over the A380 rather than the massive over 50% better rating in their initial testing.\n\nBoth are regarded as entry level gaming cards, but the A380 has severe video driver problems, and no matter how good the hardware is, if the software can't play the programs and games you want, then you are out of luck. The much safer perspective is to go with the AMD product, because even if it is a weak offering that requires PCIe version 4.0 installation to maximize its performance, the card is a much more stable product with fewer major video driver issues.\n\nBoth products are better than the GT 1030 or GTX 1050, but neither give enough boost to replace a GTX 1050ti, GTX 1650, RX 470, RX 480, RX 570, or RX 580.\n\nYou are gambling with an A380, while the RX 6400 as well as the even slower RX 6300 cards are not worth buying if you are upgrading from just about anything else.\n\n\nHere's a review of the A380 with the RX 6400 as one of the competitor test comparisons: \n\nhttps://www.youtube.com/watch?v=e3o7tKRGcMY"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "GUNNIR Intel Arc A380 Commericial",
    "selftext": "",
    "comments": [
      "Hope a reviewer gets their hold on one and reviews it in English.",
      "Is it just me or is the video being tagged as [\"potentially sensitive content\"](https://imgur.com/a/zzEpP9M)?",
      "Not just you.",
      "I have no idea why, unless it has to do with playing video games after 9pm or prominently wearing an Apple Watch in an Intel commercial.",
      "cheesy but they do imply its usecase fairly okay; accelerated workload and lightweight gaming. wonder how long will it take for Arc to stabilise, it took iris Xe around a year"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "a380"
    ],
    "title": "Full arc dgpu media capabilities?",
    "selftext": "Hey everyone! I wanna get an arc a310/a380 for media encoding/decoding purposes. Does a sheet like the nvidia nvenc/nvdec matrix exist for arc? I wanna know what it can decode, what it can encode, how many streams of encoding it can handle. Thanks!",
    "comments": [
      "https://github.com/intel/media-driver/blob/master/docs/media_features.md#media-features-summary"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "I have a new Intel Arc A380 GPU, installed on a Z390-E Gaming MB, and CPU-Z shows its Graphics Interface BUS to be running on PCIe 1.0. The MB is enabled for 3.0. What can I do to fix it?",
    "selftext": "I have a new Intel Arc A380 GPU, installed on a Z390-E Gaming MB, and CPU-Z shows its Graphics Interface BUS to be running on PCIe 1.0. The MB is enabled for 3.0. What can I do to fix it?\n\nHere's a picture:\n\n[https://ibb.co/kKCbjF6](https://ibb.co/kKCbjF6)",
    "comments": [
      "Weirdly enough they do that. When it's not doing anything for some reason it falls back to PCIE 1.0. Something is wrong though since it says it's got a max link width of PCIE x1. Is the card plugged into the primary slot just below the CPU?",
      "Yes it is.\n\nIt also remains at 1.0 during any gameplay... :(",
      "Have you tried benchmarks to see if it performs like other a380s?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "a380"
    ],
    "title": "Hardware Unboxed's Intel Arc Alchemist A380 Review Summary",
    "selftext": "",
    "comments": [
      "Why would you not just link the source video instead of some site ripping off their results?  \n\nHardware Unboxed video: https://youtu.be/e3o7tKRGcMY",
      "The video is clearly divided into labeled sections you can very easily click through to whichever part interests you.",
      "Then don't visit the site, then. You have a choice; no one has taken that away from you.",
      "if someone doesn't want to watch an 18-minute long video, they can see the results in much less time by simply looking at the screen-grabs in that post."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "Anyone have experience using ARC A380 for Plex Transcoding",
    "selftext": "How stable is this card for Plex transcoding, or should I go with Nvidia in this case.",
    "comments": [
      "I don’t know if they are supported yet. I have an A380, but not a Plex system currently to test it, but I don’t think it has officially announced as being supported yet."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "Update On My 3.1GHz Intel Arc A380 - SkatterBencher",
    "selftext": "",
    "comments": [
      "It's amazing how quickly GPU clock rates have risen."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "ARC A380",
    "selftext": "Anyone been able to get the A380 here in the UK. I’m still waiting for Ebuyer to get stock. Thanks",
    "comments": []
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "I was surprised by the Intel ARC A380",
    "selftext": "I can't wait for the next release of drivers to see how it improves things on a very capable and affordable card.",
    "comments": []
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "12:51 / 29:37 Intel Arc A380 Gaming GPU Review & Benchmarks vs. AMD RX 6400, GTX 1650, & More",
    "selftext": "[https://youtube.com/watch?v=La-dcK4h4ZU&feature=share](https://youtube.com/watch?v=La-dcK4h4ZU&feature=share)",
    "comments": []
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "a380"
    ],
    "title": "Deciphering the new Intel Arc \"A380\" Naming Scheme",
    "selftext": "It's been reported that the entry level Intel Arc GPU will be called the A380. And this is what I think is the justification for the name.\n\n**A**: Alchemist (kinda obvious)\n\n**3**: Perhaps keeping in line with how we have the i3, i5, i7, and i9. The i3 and A3 are the low end products. \n\n**80**: This is tougher but it might have to do with how GPU naming conventions has xx60, xx70, xx80, xx90 has the more **gaming-focused** vibe to them. The 1060, 2060, and 3060 cards are considered to be the ideal starting point for gaming performance, while everything beneath it like the Nvidia GT 910, 640, etc. is usually considered low performance for laptop meant for basic productivity (i.e. Adobe) and light gaming. The 1050 and 3050 cards are in this 'just decent enough for gaming' area but not recommended in the long term (i'm starting to see games where the bare minimum requirements is a 1060, which sucks for me since I have a 1050 Ti...)\n\nSo with that in mind... the A380 will be the **low-end,** **Alchemist line** but **high-performance-gaming-focused.** At least for that A3 family. \n\nSo, with the other cards... I could see something like an A580. A5 = i5, 80 for that continued gaming-focused branding. The last two numbers could be swapped around because 60-90 is associated with high end gaming, so maybe like an A990 could be the pinnacle Alchemist GPU instead of A980.\n\nAnd yeah, when Battlemage and the others come out, it'll be B380, C780, D980 or so.\n\nAny other speculations?",
    "comments": [
      "Yup this makes total sense to me.\n\n I just only wanted to say these arent the coolest sounding product names in the world imo it sounds like mb chipsets lol.",
      "I felt like the 80 part is just marketing so that it's felt close to top tier card like 2080, 3080.\nIn the I think we will see only a380, a580, a780 maybe.",
      "They honestly should've done word names. Alchemist Alpha/Delta/Gamma/Omega, Alchemist Bronze/Silver/Gold/Gold Philosopher's Stone Edition.. whatever."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "PC Inside Xbox One Case",
    "selftext": "Hello! My name is Kyle and I recently built a secondary streaming/server/portable gaming PC inside an original Xbox One case!!\n\nI have a YT video explaining the build here: \nhttps://youtu.be/4y6Y-KIb09o\n\nSpecs: \ni3 10100\nIntel Arc A380 (I love AV1)\n16GB DDR4 3000\n2tb M.2\nFlex Atx 600 watt (might explode one day)\nNoctua 60mm intake\n2x notcua 40mm intake\n\nTemps under load\nCpu: 94\nGpu: 85\n\nIt was a very constrained build and the back mount for the GPU isn't perfect, but I'm so unbelievably happy with how it turned out!\n\nI will try and reply to as many questions as I can! Thanks!!!",
    "comments": [
      "Super cool! I have a dead Xbox one that would be awesome to do this with!",
      "Pretty neat. In the future, you could get a low profile GPU which would make space for better cooling to keep those temps down. They're spicy hot under load.",
      "I wanted a low profile arc a380, in the video I mention that they don't have them in the US.... for now :)",
      "About 700$ for absolutely everything all said and done. And it's mainly for streaming so it's very overkill in the spec department, I could've even done a pentium and would've been fine, the gpu does all the work",
      "Update 6/7/2023\n\nQuestions answered\n\nThermals: aren't great, BUT I am planning on flipping the cpu fan as exhaust, and hopefully adding a exhaust fan next to the end of the PSU to correlate with the airflow path of the cpu fan being exhaust, I did already flip the intake noctuas to exhaust and it made the GPU worse, but the CPU a little better\n\nPrice: 700$ all said and done, I could've done this for under 500$ but I had parts from my last streaming PC I wanted to keep (gpu, ram, ssd)\n\nSize: xbox one is 7.2 liters \n\nPsu: I want a HDplex PSU, I'm waiting on them to release a higher wattage variant, but it hasn't come out.. yet.. but has been announced! (Will cut down size significantly) also might have to sleeve my own cables since this psu is proprietary \n\nThe back: I know the GPU doesn't look great from the back, the power cable barely presses it up so it looks canted, I plan on adding washers on the standoffs the riser cable is mounted to, which will even it out and is a super simple fix\n\nWhat I cut: I used a Dremel and cut the back of the xbox and the old hardware standoffs in the xbox.. and that's literally it, the power cable cutout in the back I used side cutters which wasn't great because it looks sloppy, and I borrowed the Dremel which I will clean up with in the future\n\nFuture Plans: improve thermals. Upgrade CPU to a 11400 just so I can play more games and do a little more video production,\nUpgrade GPU to rtx 4050/4060 low profile / blower style (I just want AV1 encoding, and being able to have a to go gaming box would be sick)",
      "Pretty dope!!",
      "Rad",
      "That’s pretty freaking cool",
      "How much was this all together?",
      "Bow does it feal to be in the top one per cent of the world.",
      "Cool (even if it's pretty hot)",
      "amazing",
      "I've always wanted to do this with an old console that doesnt work anymore.",
      "Very cool project, been thinking of doing something similar with my old one/the original modded xbox ones I got laying around. I'd be turning them into a NAS though,",
      "Nice! I built a similar rig in 2017 with a 970 and 3570k. It’s honestly the perfect form factor.\n\nYou’ll likely see much improved thermals with a blower style card.",
      "Well done, really good job bro",
      "Use it to emulate Xbox game",
      "Absolutely epic! I am Xbox owner as well, so this clicked with me right away!",
      "Thanks! I could've chose a proper sff case, but having stupid financial decisions comes out to some interesting ideas",
      "For sure share the link to those drive bays if you can :3c"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "a380"
    ],
    "title": "Intel A380 USA Release August 22, 2022?!",
    "selftext": "So Newegg has the Asrock challenger version of the Intel A380 listed for $139.99 on back order with an ETA of 08/22/2022.  It's shipped and sold by Newegg, it's not a 3rd party seller on the site, so this seems pretty legit.  Last time I looked at this listing, it was just listed as \"out of stock\".  So it sounds like it's actually getting a USA release next week, and yes it's actually letting me put it in my cart.    \n\n\nListing is here:  [https://www.newegg.com/asrock-arc-a380-a380-cli-6g/p/N82E16814930076?Description=intel%20a380&cm\\_re=intel\\_a380-\\_-14-930-076-\\_-Product&quicklink=true](https://www.newegg.com/asrock-arc-a380-a380-cli-6g/p/N82E16814930076?Description=intel%20a380&cm_re=intel_a380-_-14-930-076-_-Product&quicklink=true)  \n\n\nIt's actually here.  At $139 I might pick one up for it's AV1 encoding features, I'm optimistic that this can be used as a second GPU to use for AV1 encoding while using a more powerful one for everything else.  If it actually comes out next week, I'm sure we'll find out a lot more soon.  \n\nHaving it released to a larger audience should be useful for getting more feedback on drivers and hopefully speed up the process of those being optimized.  This is kind of exciting, now I want to see the more powerful A750 and A770.",
    "comments": [
      "100% agree. Looks to me like what theyre doing is releasing the card that people will buy and wont really expect any crazy good game preformance out of it and using that to get user feedback on drivers, probably so that when they release flagship there are less driver problems and have a comparatively smoother launch. \n\n&#x200B;\n\nEveryone needs to remember that intel getting into the gpu space is a good thing! more options and more competition driving prices down, and who knows maybe one day you can buy and intel gpu made in america.",
      "what's the alternative? at this point people know what they are getting themselves into buying one. If you don't want to pay to be a beta tester that's totally fine but at least they are persuing this route so that people who are ok with it can use them, help them iron out the bugs and so you, the customer that is waiting, might have to wait less time for a better product.\n\n&#x200B;\n\nIntel is getting an insane amount of shit for these issues theyre having and people need to understand that this will likely be 1st gen bugs (probably rolling over to later generations but not as bad) and that creating a product like this from the ground up is insanely hard.\n\n&#x200B;\n\nWhile i respect your outlook on the situation, you kinda gotta look at the bigger picture. All this negativity generated around these cards isnt exactly going to encourage intel.",
      "like i said, you could but that pushes back your launch date",
      "So, in other words they want people to pay to be beta testers for their drivers.",
      "There is no 8 GB version.",
      "This fits into systems with a PCIe slot, so anyone that has wanted to use AV1 in some capacity now reasonably can. \n\nI  don't think Plex supports AV1 yet, but Jellyfin does, and it would help a lot of people format shift their stuff into a more space and bandwidth-saving container while things are being better prepared on the client side.\n\nAV1 aside, some people might also want to make use of Quick Sync on a desktop that doesn't support it.",
      "100%",
      "That certainly is a lot of projection. The card will have uses other than just GPU and isn't even aimed at any segment Nvidia nor AMD are worried about, related like its encoding, RT Cores and base 96bit interface makes it viable for a lot of older system integration for low cost. While  no one thinks this is where they wanted to be this isn't the worst place just their starting point, which again is a good thing overall for consumers which keeps getting lost  in translation.\n\nCheers",
      "I got a tickle in my Pp when you said gpu made in America",
      "I'm right there with you being unsure of any actual use case, but some people might want to do... something with it.\n\nNot sure what that is but it'll be there soon enough.",
      "Why are people simping for AV1 encode/decode? Literally nothing uses it yet. YouTube, Twitch, Etc. Are all still H264 and will be for the next few years.",
      "Pretty expensive for a last gen card that sucks up a bunch of power and has tons of bugs. But to be fair, that's their entire lineup. They need 3060 (MSRP $329) performance for $189 - but apparently thats their A770 model (with bugs and more power). Maybe we can sell the cards in 20 years to retro gaming enthusiasts, kinda like voodoo cards.",
      "There are no cards on the market that can saturate x8 bandwidth on PCI-e gen 4 or newer.",
      "Lmao",
      "already been done for years, no one is ever forced either.",
      "It can take the load off of other components when encoding streams for example",
      "OBS can use AV1 for recording, that's a big deal for creators.  Encoding matters for recording far more than it does for streaming. With Youtube allowing bitrates up to 51K, the current encoders are sufficient for streaming 4k 60FPS.  So it's not a feature that's really needed for streaming, AV1 however is very useful for recording stuff at higher quality while taking up less storage space.  This can speed up rending times, upload times, etc.  For creators, time savings = money savings.",
      "You can record with it though.  AV1 can deliver a higher quality at a lower bitrate.  This can result in significant savings in storage, reduce rendering times etc.  If you're a creator, saving storage space and time is worth spending extra money for.",
      "It's a bit of a 'chicken and egg' situation. Do sites work to support AV1 before there are any AV1 encoders or do HW manufacturers support AV1 encode even if there aren't any sites that support it (for streaming at least). Intel has effectively broken that 'chicken and egg' situation and now it's on sites to support it.\n\nTwitch doesn't support it currently, but they have [talked about that they are working on it](https://www.nvidia.com/en-us/geforce/news/rtx-30-series-av1-decoding/), and there is even a [test VOD](https://www.twitch.tv/videos/637388605) on Twitch that uses AV1 and demonstrates 1440p 120Hz streaming. It's only a matter of time before Twitch supports it.",
      "So to sum things up.\n\nThere is a chipshortage. \n\nA chip nobody needs (yet) will be shipped.\n\n/s"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "celeron g6900 bottleneck",
    "selftext": "hi guys, just a question, a celeron g6900 will bottleneck a arc a380 at 1080p?",
    "comments": [
      "I did something similar years ago, but I'd really stretch for a 12100f or 13100f. You'll get tons more runway out of them.",
      "It's usually a bad idea to try to game on a 2 core/2 thread CPU these days, unless you specifically only play old/undemanding games. Many games won't launch or won't play properly on 2 core/2 thread CPUs. GTAV is a popular example.",
      "I’d go AM4 at such a tight budget. Used parts are incredibly cheap on that platform and hold up relatively well even the oldest chips",
      "I have both the G6900 and Pentium in machines, if you haven’t already bought the G6900, the Pentium does much better with hyper threading. But in the end a 12100 would be a far better choice with four real cores. The G6900 is ok for basic tasks or a NAS, not much more.",
      "In your case, if you don't mind AMD, I think it makes more sense to go with AM4 since you could go with an 1st/2nd gen Ryzen (You're likely going to be GPU bottlenecked even with these old CPUs) then upgrade to something like a 5600(X) when you have the money.\n\nIf you do go with a g6900, it's going to be a pretty awful experience since it's going to be a stuttering mess, at least until you replace it; The \"bare minimum\" 7-8+ years ago was considered a 2C4T CPU, the G6900 is even worse than that.",
      "You shouldn't pair ARC with a weak CPU, ARC's drivers have higher overhead.",
      "G6900 is not a good idea. 2C2T in 2022, as amazing Golden Cove is, will not do it.\n\nGet at minimum a 12100F, or if you can’t spare it, then maybe one of those 4500 or 5500 that have been going on sale for pretty cheap.",
      "Resizeable Bar (AKA \"SAM\") is a pcie capability, as long as it was added to the motherboard bios, I see no reason it wouldn't work.",
      "Why do you specifically want a380, for AV1 encoding? If you need AV1 encoding, you can get the cheapest ARC GPU and get an Nvidia or AMD GPU for gaming. Otherwise, 12100+a380 is a bad choice IMO. The GPU is forcing you to buy a slower CPU just for its iGPU.",
      "In that case, a380 is the better choice for productivity. But 12100 isn't, it's only 4 cores.\n\n> but in a video i saw that the arc a380 has some problem that if you don't have an igpu you can't install the drivers\n\nI don't know anything about this, couldn't find any info. Maybe it's not a common issue. People have been using arc GPUs on Ryzen systems without issues.",
      "Yes, also why would you buy a G6900 on socket 1700?",
      "I mean I guess OSRS or EQ1 or something not run ok on that chip...maybe.",
      "the is that: i have to buy the parts in physical store and in italy the g7400 cost like 115€ and i3-12100 like 150€ so i want to buy the celeron (only 80€) and take it for 2 months and next i wont to buy a i5.",
      "Would those even work with an A380 that well though since they don’t support Resizable BAR? OP should at least go with 3000+ in that case.",
      "to have the new platform and to be able to update in the future, if it bottlenecks I will take an rx 6400 but it is much more limited, so I was thinking of the a380, so I use this processor for a short time then I want to change to an i5.",
      "Unless the machine is desperately needed... why not just wait until you can afford what you really want?",
      "> i want to buy the celeron (only 80€) \n\nThat's not an insignificant amount of money. If you don't have a big budget, AMD 5600 is the best choice, unless you have intel specific requirements. It is a much better CPU and it costs well under $150 in most regions. You can combine it with a b450 board for around 200€ and nothing can beat that value (physical stores might have mark-ups, so I'm not sure about the price).",
      "yes I know that AMD costs less but they told me that it causes problems, I didn't understand which ones but to avoid the risk...",
      "No problems with ~~560~~Ryzen 5600+b450. It's solid. Only common issue was usb disconnects with \"some b550 motherboards\" but that's mostly solved.",
      "ok i changed the configuration and i managed to put a 12100F on it, but in a video i saw that the arc a380 has some problem that if you don't have an igpu you can't install the drivers, i just can't find a 12100 that costs little, it's true this problem?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "a380"
    ],
    "title": "A380 QSV encodes worked fine, then suddenly \"Failure to initialise thread 'Quick Sync Video encoder (Intel Media SDK)\" error",
    "selftext": "This is somewhat of a cross-post from a similar inquiry on the Handbrake Community support forum, but as the issue seems to be Intel-related, I figure I'll post here as well since I \\*really\\* can't figure out what's going on.\n\nI'll try to be brief, but this is still likely to run a bit long...\n\nI got and installed an Asrock Challenger A380 GPU a couple of days ago and it's worked GREAT with Handbrake QSV encoding (h264, h265, AV1) - MUCH faster than an iGPU.\n\nAfter finishing an h265 QSV encode, I attempted to do another one literally 15 minutes after that last one - and that failed outright. I tried multiple times (after reboots, just to \"clear the decks\"), with the same results. The Handbrake logs point to this error:\n\n*qsv\\_hevc\\_make\\_header: MFXVideoCORE\\_SyncOperation failed (-17)*\n\n*encqsvInit: qsv\\_hevc\\_make\\_header failed*\n\n*Failure to initialise thread 'Quick Sync Video encoder (Intel Media SDK)'*\n\nI upgraded Handbrake to the latest version and tried again, with identical results. I then did  MULTIPLE graphic driver uninstalls / reinstalls (both using DDU and the regular way) - covering the gamut  from SAFE MODE and 'regular' uninstall / reinstalls; all with  corresponding Registry cleanups, to no avail. This is using the latest Intel Driver Support Assistant-vetted driver 'igfx\\_win\\_101.1743'\n\nSo... did the QSV capability on the GPU just \"crap out\" (is that even possible)? It seems to point to that since I also tried QSV encodes in h264 and AV1 and those essentially returned similar 'failed to initialise thread Quicksync Video encoder' errors. I should note that I've also successfully completed h264 and AV1 QSV encodes yesterday on this same hardware.\n\n Any insight would be MOST helpful!",
    "comments": [
      "That feature also uses QSV, it might provide more insight if the issue is with Handbrake or with the GPU itself...",
      "While I can't offer a definitive solution, try the latest beta graphics driver (.3278) rather than the one recommended by the support assistant. The one it suggested is several versions old, despite being fairly recent. They've been pushing out rapid driver releases due to the, uh... surplus of software testing opportunities which Arc has presented.\n\n[https://www.intel.com/content/www/us/en/download/729157/intel-arc-graphics-windows-dch-driver-beta.html](https://www.intel.com/content/www/us/en/download/729157/intel-arc-graphics-windows-dch-driver-beta.html)",
      "Have you tried using the latest handbrake nightly? From previous reviews, Intel's latest updates and fixes are in there.",
      "To follow up on my prior response to your post, I went into Handbrake settings, specifically the \"video\" settings, and UNCHECKED \"Enable Low Power QuickSync Hardware (where supported)\" and it's now successfully encoding in h.265 again.  Will try AV1 after that using QuickSync.\n\nJust FYI, I have Handbrake as well as Handbrake \"Nightly\" both installed.  I opened both programs and changed the settings just in case, and clicked \"exit\" rather than the \"x\" box to close the window, just to make sure that the settings update for Handbrake/Nightly.",
      "A lot can happen in 15 minutes! For example, there was a time (and presumably, still is) where Windows Update was reverting Intel Xe/iGPU drivers without user interaction, as it treated any more recent betas as an old version. There have been similar issues with AMD GPUs as well.\n\nOne other thing to check would be for some kind of software change/bug in \"waking\" the encoder core. In Handbrake, I *believe* there's a low power state toggle under Preferences / Video for QuickSync. If low power encode is enabled, disable it, relaunch and try it one more time to see if it properly gains consciousness.\n\nI remember dealing with something similar a few years back but I don't remember what finally sorted it out. Driver or bus/encoder power state toggle seemed vaguely familiar, though. Hopefully you find a fix!",
      "If it helps, this is a ground-up new-build PC specifically for QSV AV1 encoding. It does NOTHING else, literally just has whatever Win10 comes with + Handbrake.\n\nFoundation specs:\n\ni5-12400 (iGPU disabled in BIOS) | Asrock B660 ITX/AC mobo, latest BIOS installed, REBAR enabled, XMP enabled | 2x8GB T-Force Vulcan Z DDR4/3000 | Crucial P2 NVMe SSD |Asrock Challenger ITX A380 GPU",
      "UPDATE: I managed to snag an A750 and have now installed it in the same system which previously hosted the A380.\n\nDid a fresh Win11 H2 install with latest ARC drivers and using latest nightly Handbrake build.\n\nSo far so good...",
      "Did you tried to record desktop/game using Intel® Graphics Command Center Capture?",
      "I unfortunately have no ideas for you, but am just wondering if you could link your post in the handbrake forum; I'm actually considering doing the same thing as you and am curious what the handbrake folks say about it.",
      "After some (semi)exhaustive testing, I'm 99% sure that \"something\" in the GPU did indeed go wonky for whatever reason. I arrived at this by doing a System Restore to the Day One state of the PC (nothing installed, no drivers, etc). Installed the vetted Intel graphic drivers, installed Handbrake+dotnet, rebooted, then tried QSV encode again - and it fails with the same error. So it's not some rogue software silently installed/updated by windows - recall that the GPU worked fine up until it didn't yesterday.\n\nDid Day One System Restore again, shutdown, unplugged the GPU (so only the iGPU would be active), booted and installed the Intel graphic drivers + Handbrake+dotnet again, rebooted, and tried QSV encode again - and that worked!\n\nThen another Day One System Restore, shutdown, re-plugged the GPU (this also involved re-seating the GPU, in case that was an issue somehow), booted to BIOS and made sure iGPU was disabled, did all the driver and Handbrake installs, tried QSV encode again - and that fails with the same exact error.\n\nUnless I'm missing something, that's a fairly definitive indication (to me) that the GPU has gone wonky for some unknown reason in those 15 minutes when QSV worked, and then didn't.\n\nI've filed an RMA for it, but considering every retailer carrying these cards is back-ordered, who knows when I'll get a replacement.\n\nTLDR: this doesn't seem to be an SDK or software or Handbrake issue, but definitely something with the GPU.",
      "Do you see any error messages in the event viewer in the system category?",
      "I have a similar setup (Asrock B660 + A380) and had the same thing happen; worked one minute \"dead\" the next. When I checked in Device Manager it showed that it lost the drivers for both dGPU and iGPU, and reinstalling them didn't fix anything. I'm curious if you find out anything. The latest thing I tried (didn't work) is installing the intel media SDK, but I couldn't get a hold of the 2022q2 version, just the release before that, even though it is listed on their github. I'm on the latest Win11 btw.",
      "First time posting on Reddit.  I've had a similiar if not identical issue.  Nothing changed on my system, at least not anything that should have changed Quicksync, and now I'm getting the same error message.  I'll copy and paste it below:",
      "Here's the pertinent portion of the log:",
      "JtFYI, I have Handbrake as well as Handbrake \"Nightly\" both installed.  I  \n opened both programs and changed the settings just in case, and clicked  \n \"exit\" rather than the \"x\" box to close the window, just to make sure   \nthat the settings update for Handbrake/Nightly.",
      "Yup, tried that too, no go.\n\nI'll also paste what I posted in response to u/somethingknew123 \\-\n\nAgain, we're talking a span of 15 minutes :)",
      "Yup, did that too, no difference.\n\nAnd not to be ungrateful for the (or any) comment, but I'd still be bothered (if that had somehow fixed it) by the fact that it was chugging along fine with \\_no\\_ changes between when it last worked and when it suddenly didn't.\n\nAgain, we're talking a span of 15 minutes :)",
      "no because that has nothing to do with what I use the GPU for",
      "It's not the forum in Reddit, but at Handbrake themselves. I'm not sure if Reddit will retain the URL but here it is:\n\nhttps://forum.handbrake.fr/viewtopic.php?t=42494",
      "I'm not so sure it's your particular GPU.  Mine was also working fine, encoding great using AV1 QuickSync, h.265 or what have you...basically matching the speed of my 3090 for h.265, but now I'm getting the following error messages:\n\nencqsvInit: MFXVideoENCODE\\_Init failed (-15)\n\nFailure to initialise thread 'Quick Sync Video encoder  \n(Intel Media SDK)'\n\n\\# Job Failed to Initialise. Check log and input settings (3)\n\nIf I understand your post correct, when you physically took the Arc out of the PCIe slot, QSV worked fine (albeit just from the CPU), but when you plugged it back in, it stopped working?  And this is even after a complete restore?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "Resizable BAR Z170 with A380 on Linux (PVE)",
    "selftext": "I installed an Arc A380 6GB in my Gigabyte Z170X Gaming 3 Motherboard with a i7-6700. I was reading there are UEFI mods([REBarUEFI](https://github.com/xCuri0/ReBarUEFI)) to enable resizable BAR in older chipsets, while some don't require it if using linux AND the mobo has the '4G decoding' option. I'm wondering if anyone else has tested this with linux or knows how to check if it's working. My motherboard UEFI has a '4G decoding' option that I enabled, but not sure if that is sufficient. The card works, and I'm using it for transcoding with jellyfin on proxmox. Just not sure if I'm getting the full performance with ReBar enabled. This is the result of my `lspci -v` :\n\n>03:00.0 VGA compatible controller: Intel Corporation DG2 \\[Arc A380\\] (rev 05) (prog-if 00 \\[VGA controller\\])  \n>  \n>Subsystem: ASRock Incorporation DG2 \\[Arc A380\\]  \n>  \n>Flags: bus master, fast devsel, latency 0, IRQ 149  \n>  \n>Memory at ee000000 (64-bit, non-prefetchable) \\[size=16M\\]  \n>  \n>Memory at d0000000 (64-bit, prefetchable) \\[size=256M\\]  \n>  \n>Expansion ROM at ef000000 \\[disabled\\] \\[size=2M\\]  \n>  \n>Capabilities: \\[40\\] Vendor Specific Information: Len=0c <?>  \n>  \n>Capabilities: \\[70\\] Express Endpoint, MSI 00  \n>  \n>Capabilities: \\[ac\\] MSI: Enable+ Count=1/1 Maskable+ 64bit+  \n>  \n>Capabilities: \\[d0\\] Power Management version 3  \n>  \n>Capabilities: \\[100\\] Alternative Routing-ID Interpretation (ARI)  \n>  \n>Capabilities: \\[420\\] Physical Resizable BAR  \n>  \n>Capabilities: \\[400\\] Latency Tolerance Reporting  \n>  \n>Kernel driver in use: i915  \n>  \n>Kernel modules: i915\n\nIt looks like it sees the card is capable, but the 'Memory at' showing size=256M seems to indicate it can't see the full amount",
    "comments": [
      "Did you get to figure this out?\n\n&#x200B;\n\nI'm pretty much having the same config as you, same CPU, MOBO and GPU. but instead of Jelly I'm on Plex.   \n\n\nI havent managed to get the system using the ARC 380  for transcoding :(",
      "What did you have to do to get your PVE host to detect your A380? I'm having a hard time getting mine to be recognized. Currently on PVE 8.1.4.",
      "Well, I know people have more issue with Plex than Jelly with newer hardware. I'm able to use the card to transcode just fine. I'm actually on Proxmox and passing it through to a privileged LXC container running debian 12. My only issue is having the OS/Mobo utilize resizable bar with the GPU to get the full performance. Now, I don't have much to compare it to see if I'm actually taking a performance hit and how much, but based on the attached lspci output, it looks like it is not utilizing the ful resizable bar. I found more info, but it looks like I'll have to do a bios mod to get it working with full resizable bar. Right now I get about 95fps for a 4K HEVC 10bit > 1080P transcode, which is more than enough for my usecase, so I will most likely just keep it as-is and see if I get a boost once I upgrade my server to a newer generation platform that fully supports ReBAR.",
      "I don't recall having any issues with the host detecting it. What are you using to view it?\n\n    lspci -nnv\n    \n    VGA compatible controller [0300]: Intel Corporation DG2 [Arc A380] [8086:56a5] (rev 05) (prog-if 00 [VGA controller])\n\nDo you see this under your lspci?",
      "Oh, did you install any drivers?\n\nI followed the jellyfin guide here: [https://jellyfin.org/docs/general/administration/hardware-acceleration/intel/#lxc-on-proxmox](https://jellyfin.org/docs/general/administration/hardware-acceleration/intel/#lxc-on-proxmox)\n\nThe first step is to install the drivers on the host.\n\n    sudo apt update && sudo apt install -y intel-gpu-tools\n\nThen try:\n\n    sudo intel_gpu_top",
      "In my case, when I'm running the following command intel\\_gpu\\_top it doesn't detect the Arc 380.  Even being in Debian 11 with a back ported Kernel 6.5.9\n\n&#x200B;\n\nroot@bignas:\\~# intel\\_gpu\\_top  \nFailed to detect engines! (No such file or directory)  \n(Kernel 4.16 or newer is required for i915 PMU support.)\n\nroot@bignas:\\~# uname -r  \n6.5.9-zabbly+",
      "You're on baremetal Debian 11 or is it a VM or LXC? I'm running proxmox with kernel 6.2.16-12-pve, which is what the LXC uses as well.",
      ">baremetal Debian 11\n\nYeah baremetal Debian 11. Checking if maybe it's the kernel installed the one causing the problems",
      "OK, I enabled something on the BIOS and now I get some output   \nroot@bignas:\\~# vainfo  \nerror: XDG\\_RUNTIME\\_DIR not set in the environment.  \nerror: can't connect to X server!  \nlibva info: VA-API version 1.17.0  \nlibva info: Trying to open /usr/lib/x86\\_64-linux-gnu/dri/iHD\\_drv\\_video.so  \nlibva info: Found init function \\_\\_vaDriverInit\\_1\\_10  \nlibva info: va\\_openDriver() returns 0  \nvainfo: VA-API version: 1.17 (libva 2.10.0)  \nvainfo: Driver version: Intel iHD driver for Intel(R) Gen Graphics - 21.1.1 ()  \nvainfo: Supported profile and entrypoints  \nVAProfileNone                   : VAEntrypointVideoProc  \nVAProfileNone                   : VAEntrypointStats  \nVAProfileMPEG2Simple            : VAEntrypointVLD  \nVAProfileMPEG2Simple            : VAEntrypointEncSlice  \nVAProfileMPEG2Main              : VAEntrypointVLD  \nVAProfileMPEG2Main              : VAEntrypointEncSlice  \nVAProfileH264Main               : VAEntrypointVLD  \nVAProfileH264Main               : VAEntrypointEncSlice  \nVAProfileH264Main               : VAEntrypointFEI  \nVAProfileH264Main               : VAEntrypointEncSliceLP  \nVAProfileH264High               : VAEntrypointVLD  \nVAProfileH264High               : VAEntrypointEncSlice  \nVAProfileH264High               : VAEntrypointFEI  \nVAProfileH264High               : VAEntrypointEncSliceLP  \nVAProfileVC1Simple              : VAEntrypointVLD  \nVAProfileVC1Main                : VAEntrypointVLD  \nVAProfileVC1Advanced            : VAEntrypointVLD  \nVAProfileJPEGBaseline           : VAEntrypointVLD  \nVAProfileJPEGBaseline           : VAEntrypointEncPicture  \nVAProfileH264ConstrainedBaseline: VAEntrypointVLD  \nVAProfileH264ConstrainedBaseline: VAEntrypointEncSlice  \nVAProfileH264ConstrainedBaseline: VAEntrypointFEI  \nVAProfileH264ConstrainedBaseline: VAEntrypointEncSliceLP  \nVAProfileVP8Version0\\_3          : VAEntrypointVLD  \nVAProfileHEVCMain               : VAEntrypointVLD  \nVAProfileHEVCMain               : VAEntrypointEncSlice  \nVAProfileHEVCMain               : VAEntrypointFEI  \n\n\nroot@bignas:\\~# ls -l /dev/dri  \ntotal 0  \ndrwxr-xr-x 2 root root        120 Nov  3 16:03 by-path  \ncrw-rw---- 1 root video  226,   0 Nov  3 16:03 card0  \ncrw-rw---- 1 root video  226,   1 Nov  3 16:03 card1  \ncrw-rw---- 1 root render 226, 128 Nov  3 16:03 renderD128  \ncrw-rw---- 1 root render 226, 129 Nov  3 16:03 renderD129"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "a380"
    ],
    "title": "Anyone here with an A380 that can test DPC latency with Latencymon?",
    "selftext": "I primarily use my computer for music but do other things with it as well and would like to add a lightweight gpu. Before I pick up a 6600 (AMD is know for low DPC) I would be interested in knowing what DPC numbers are like with ARC.",
    "comments": [
      "Running it now. I'm not very familiar with this benchmark, do you have any specifics for what you want to see? Or, how should I present the results?",
      "I know it’s not specific to AMD. Different gpu drivers can produce different dpc figures (which can interrupt audio tasks). Nvidia drivers are known to produce higher figures in comparison to AMD, and I’m curious as to where ARC stands with this.\n\nhttps://gearspace.com/board/music-computers/1212416-dpc-latency-better-amd-graphic-cards-3-card-comparison.html\n\nhttps://vi-control.net/community/threads/i-swapped-my-nvidia-card-for-an-amd-radeon-and-now-my-daw-is-running-much-better.71482/",
      "Is this the page you're looking for? [Pass1](https://imgur.com/t4j34vA) [Pass2](https://imgur.com/7cwDhP4)",
      "Deferred Procedure Call? If so this doesn't have anything to do with AMD specifically. DPC is a MS operating mechanism allowing for high priority tasks to override low priority tasks as needed. For most folks working in audio and computers DPC can be an issue but its rarely if ever solved by a GPU in of itself. What exactly is the intent and desired purpose/outcome one is looking for here. Without understanding that how are we to recommend or not?",
      "Thank you so much! I know this is a lot to ask but if you have the time to do this - close all apps, let latencymon run for a couple min and then just screen capture the results (two passes preferably). I don’t blame you at all if this is an annoying request.",
      "Just my two cents here, I think he was speaking about DPC with regards to a AMD-GPU, since it's known that AMD has fewer issues with latency as opposed to NVidia, which have been known to be at times horrendous due to unknown changes in their GPU-drivers. It may not solved by a GPU, but for sure a GPU can be the issue for it.",
      "This is exactly it",
      "This is perfect! Thank you very much again",
      "True, Windows 10 itself introduced a nice shipload of especially audio-issues on that.",
      "Speaking of AMD, my monitor going to and out of sleep causes audio stutter.",
      "DPC isn't a AMD or Nvidia thing specifically. It has nothing to do with either in of itself. It certainly can affect various components though. This is why more information is needed as posted otherwise we spin our wheels and no one gets helped."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "A380 World of Tanks Benchmarks",
    "selftext": "Hello,\n\nI'm building my first PC in the coming month or two and I need some advice. I only play two or three games, one of which is World of Tanks. I can't seem to find any benchmarks for it using the Arc A380, however, which is a card I'm very interested in due to its price and its looks/form factor.\n\nI play at 1080p and I'd love to hold a stable 60-80 fps at medium settings. Is this possible with the A380? Or do you know where I can find this information?\n\nAssuming all the other specs of the computer can handle it, and I've ensured that they can.\n\nThank you for your help!",
    "comments": [
      "I don't think you'll be able to achieve 60 fps with that unless you slightly lower the graphics.\n\nThere's an Encore benchmark on YouTube and its usually at 57-59fps.",
      "That test was on ultra, plus the drivers came a long way since then."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "arc a380",
      "a380"
    ],
    "title": "Any A380 users here?",
    "selftext": "I'm undecided on whether to get a Arc A380 or GTX 1660 Super. \n\nThe arc would be brand new, the 1660S would be second-hand.\n\nThe plan is 1080p 75hz gaming. Bigger on JRPGs... And Dota 2. Probably want an Android emulator to run Arknights. Potentially wanna dip my toes into streaming a little, some video editing and I feel the Arc's AV1 encoder could do me good.\n\nBut I know about the Arc's unoptimized drivers. I do plan to delve into some older titles via emulators, so I'm not sure if the Arc would be able to run them decently enough. \n\nI've had a 1660S in the past, it's a great budget card and I know it'll do well gaming-wise. What would your thoughts be on this?",
    "comments": [
      "Get a 1660 super. Intels arc drivers are still buggy and have inconsistent performance. The a380 is not a good card. Nvidia's drivers are way better and also the 1660 super doesn't need resizable bar",
      "1660s is a lot more powerful, but is missing RT and upscaling support, if that matters to you. That said, my A380 did surprisingly well for me when I was messing around with it in a few games. It was even perfectly playable in SOTTR with the medium preset, + medium RT shadows, + XeSS, at 1440p. \n\nBut it's still an entry level card vs. an older low midrange card. The 1660s should wipe the floor with it overall.",
      "a 1660 super walks all over an a380, not even a competition",
      "Overall gaming usecase, I'd say 1660S. \n\nFor Android emulator, if you're in a region where [Google Play Games for PC](https://play.google.com/googleplaygames) beta is available, Arknights should be able to run well regardless of both GPU. Don't use WSA though, that one currently has terrible GPU performance where I can run Arknights at 300-600MHz on Google Play Games with RX 480 while WSA stutters too much even at 1300MHz",
      "1660S, it's just the better card.",
      "1660 Super hands down",
      "generally u want to go nvidia if u are looking to emulate anything. a380 is too new imo"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "a380"
    ],
    "title": "Do I have to have rebar support on server",
    "selftext": "Hey all! Simple question \nDo I have to have rebar support on my media server (running jellyfin) if I want to use an arc card for encode/decode?\nI’m running a 4th gen i5 in it currently and was thinking about grabbing an a380 for av1 video. Will my encode/decode performance get crippled? Will it work at all?",
    "comments": [
      "If you are using it solely for encoding or decoding, in my experience rebar does not need to be enabled.",
      "Will it work at all? Check out this page and follow instructions to see if that option is available in your BIOS.\n\nhttps://www.intel.com/content/www/us/en/support/articles/000090831/graphics.html"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "380",
    "matched_keywords": [
      "a380"
    ],
    "title": "Intel A310/A380 as 2nd GPU for encoding ?",
    "selftext": "Hello everyone, not sure if I'm really gonna get a definitive answer here since my personal research weren't very successful, but I'm asking anyway : do you think an Intel A310 is a good choice for the sole purpose of encoding video, more specifically AV1 ?\n\nI work in the video industry and my machine is already pretty good for editing and encoding what I need. But when I sometimes want to record gameplay sequences on my PC with a good video quality, my poor 3060Ti is often overwhelmed by both rendering the game and encoding in real-time, which leads to choppy footage. Operating a second GPU to encode could free me from this hassle, and not have to worry about limiting framerate/balance settings, and always have a nice clean output everytime I need it.\n\nBesides, I'm also thinking that encoding all my video projects in AV1 instead of the good'ol H.264 would both take less space and offer a higher quality output. Since I have no hardware encoder for AV1, I was thinking it could be worth the investment to go buy a low-end arc GPU for this purpose, preferably the a310. And more generally, make it my go to encoding card for everything I need to render, as it might be faster than my current hardware.\n\nBut this is where I'm doubting if this is a good idea or not. For two reasons :\n1. I've not found any benchmarks focusing on the encoding capabilities of the a310, in term of speed and visual quality. I don't know if the encoder inside of it is as good as the a770 or even a380.\n2. The Arc GPUs are not even available yet in my country (apart the a770 in only one store, at 450€, ouch...), and taking into account the dollar/euro conversion and taxes, it might be a pricy upgrade, or at least pricier than I would want.\n\nNow with that said, what do y'all think ?",
    "comments": [
      "Been curious about this + having the Intel run/process video/streams/youtube on my second/third monitors.  \n\nI haven't dug too deep yet considering availability, but I'll need to see how Nvidia & Intel drivers/hardware play together side by side.  \n\n\\+Ideally the primary card can full idle when this 2nd/3rd monitor workload is active. Should lower total system power? Only time will tell.",
      "Thanks for your answer!\nI would have thought that AV1 was better quality than H.265 for video production, given that it is theoretically superior in term of codec efficiency. However, maybe the encoder in the Arc GPUs is not yet optimized, like what we had with the first gen NVENC that was not that good.\n\nWell it certainly decrease the value of such a purchase for me. But still, AV1 have the advantage of not being a proprietary codec, so in the future I expect it to be way more universal than H.265.\n\nFor the rest, I sadly doesn't have an iGPU, because I run a Ryzen 5900X. So, I could potentially do software encoding, but in my experience, it is substantially less efficient than encoding with my GPU. It is a solution in some games where I'm really GPU limited and have a lot of CPU headroom though. Besides, I've had troubles with OBS studio, often dropping video framerate when the CPU was solicited more heavily in game. I could again dumb down quality settings to have a very stable recording, but this is precisely the type of compromises that had me wanting a A310 dedicated to this type of tasks.\n\nIn the end, I understand that it is probably a bit unnecessary as for now. So, I'll wait, maybe for the next gen? For the moment, I'll just continue to work with what I have, and bear with the frustration ahah",
      "I've never considered this option but theoretically it could be lower than using a big Nvidia GPU for decoding video. I don't think it would be more power efficient than just using one GPU for everything tho, I suppose the idle power consumption of the primary GPU would compensate the gain easily.\n\nIn practice however I have no idea how that would work. I don't know if Windows (no idea if it is possible on Linux) handle very well to have two monitors connected to different GPUs on the same system. I suppose you would deal with some compatibility issues over time depending on the utilisation you make of your two conjoined GPUs, regardless of the screens. This is also one thing I'm afraid of, considering I would be running two very different cards on one PC, even if it is for a specific usage."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "a750"
    ],
    "title": "Intel Arc A770/A750 mark two-year launch anniversary",
    "selftext": "",
    "comments": [
      "I hope they will come up with the next gen GPUs soon",
      "You don't have an Arc GPU if you say this, the improvement has been amazing, the stability and compatibility has been improved so much since launch.",
      ">And they did nothing about the software, Linus in one of his videos told that Intel promised a big revamp, nothing! Liars!\n\nBack to basics. Drivers are software.",
      "Yea they totally care about those 3 people who use linux when fixing drivers of a gaming product (majority of gamers are on windows and the linux statistics are inflated by steam decks)",
      "Can somebody explain the possible reasons they cant launch Battlemage yet?\n\nhardware was ready months ago. And the software is already kinda running on Lunar Lake?",
      "\"hardware was ready months ago. \"\n\nIs there any evidence that this is true?",
      "welp....Ive seen LOTS of people talk nice about the improvements of the drivers.",
      "The architecture is, but that's not the same thing as the whole BMG themselves.",
      "If Arc could one day meet the performance of the 70 or 80 series Nvidia GPUs, I'd consider going a full Intel build on my next gaming PC.\n\nThey've got a long way to go.",
      "And A became B",
      "It will be out soon. We are using it internally",
      "well thats a fair Point.\n\nbut isnt it already running in Lunar Lake?",
      "Why? Statistically at least 15x more people use windows. They'll obviously put all their resources into making functional drivers on windows before making functional drivers on linux. That's like saying app developers should still consider blackberry or windows phones.",
      "My only gripe about my a750 is the lack of VR support.",
      "Arrow Lake using that sweet 2 year old alchemist based igpu.",
      "Update your knowledge",
      "Soon(tm) is somewhat imprecise, but ok...",
      "It will be end of this year",
      "Like you just use Arc control everytime like it is a game, yeah, sure it is annoying that the driver updater not works. But that's pretty much all. I just install the drivers, open Arc Control, change one setting and leave the rest alone.",
      "I do hope Intel delivers Battlemage. Even as Nvidia fans i admit Nvidia have been selling such an overpriced GPU. I don't even doubt if RTX 5000 series will be even more overpriced."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Arc A750 available October 12 for $289",
    "selftext": "",
    "comments": [
      "Arc 750 & 770 come with MW2, and three other games if you join intel access",
      "Copypasting same comment to here as you did on r/hardware it seems. Why would they compare to something with rebate price. That doesn't make much sense.",
      "Some things to note on this comparison. One they're comparing only to the RTX 3060 and not AMD's competition. Two they are taking the **average** price of a 3060 on Newegg and comparing it to the **MSRP** of the A750.\r  \n\r  \nA Zotac 3060 is $380 on Amazon with Prime right now. Likely other even cheaper options too.\r  \n\r  \nI am curious what the numbers would look like if they used the $300 after rebate 6650XT on Newegg as the competition.\r  \n\r  \nAs usual though, wait for independent benchmarks and not Intel's misleading ones which compare price/perf instead of performance.",
      "I guess is the economist on me, but I could not help but smile at their time series chart of prices: 150 dollars for in 2012 they show for a 650Ti, is really equivalent to 200 dollars to todays prices given the latest inflation!",
      "depends per region, AMD and Nvidia still has that \"scalper\" price in my country.\n\n6600XT is at $400\n\nIf this releases at lower price then its better",
      "Makes sense to me to not compare to AMD. The Arc GPUs are more modern like they claim. AI engines and xess, actual ray tracing not done in shaders like amd, and a thread sorting unit. And add to that the way better media encoders and decoders.\n\nAmd has gotten better lately in GPUs, but they're still not as caught up to nvidia as intel is feature-wise, and that's crazy to say.",
      "> 770 basically compares to the 3060 Ti or the 6600XT.\n\nIntel still compares it to the 3060. It's not quite a 3060ti competitor",
      "Is this only if you get an Alder Lake CPU with the GPU? Reading some articles that pointed to this doc [https://softwareoffer.intel.com/Campaign/Terms/617C2EAE-CE89-4199-9805-470E829228A8](https://softwareoffer.intel.com/Campaign/Terms/617C2EAE-CE89-4199-9805-470E829228A8) and it seems like you need a qualifying CPU too unless you're talking about a different offer?\n\nAlso strange that you can't get a Raptor Lake CPU with an ARC GPU in that offer, but I guess it makes sense since it's dated back to August.",
      "Actual ray tracing is amongst the absolute silliest replies I've seen trying to talk up one product against another. It's literally the same code, written and calculated the same. The reality is those 'shaders' are simply capable of doing far more complex and wide ranging calculations and to a greater degree of accuracy while the 'real ray tracing' is just simplified smaller cores that are good for ray tracing and much worse for other things.\n\nYou're implying AMD can't do real ray tracing, real ray tracing was being done back in the 90s on a cpu, just really, really, insanely slowly. Performance is the only metric that matters and we've seen benchmarks and how the Arc gpus really stack up, using a huge die and huge transistor count, power, on a better node all to match dramatically more efficient lower power gpus from Nvidia and AMD.\n\nIn terms of being caught up, this is even more laughable and AMD has pretty much since DX9 been at the fore front of all new technologies, when they are sensible to do so. Ray tracing on a card that performs at this lower midrange level is pretty much worthless and dedicated hardware for it only wastes performance when you don't have RT enabled. That's a bad thing in this range, not a good thing.\n\nAMD don't have dedicated RT cores, because they are still very very wasteful.\n\nAlso did you see the massive range of RT benchmarks Intel pushed to show how much better it was than AMD today, none, because showing 15fps ray traced instead of 60fps without isn't much of a win if your competition's card in the segment gets 12fps ray traced but 65fps without.",
      "I do look for affordable ones like that are on sale or instant rebate (which i never see anymore) but I hate dealing with rebates, always have had terrible luck with rebates so i even pay a little more just to ignore them",
      "Any pics of AIB's?",
      "In the US on NewEgg right now, the 6650XT is on sale for $319, the 6600XT is on sale for $329, and the 6600 is on sale for $239.\n\nWith Arc, you gain:\n\n1. Better encoder \n\n2. Better RT performance\n\n3. Access to the XMX version of XeSS\n\nBut you also lose out on AMD's generally much better driver stability in games specifically. Arc still has issues in this respect, and you can actually see it on some the performance charts they provided.\n\nI think an argument can be made for both GPUs, frankly. At the low and mid ends of the market, stability in games matters. Somebody buying into this section of the market can't just buy another GPU if a game they want to play doesn't work. \n\nBut on the flip side, Arc hold much better potential for the future with all of the extra features it supports, and future driver improvements could also bring signficant performance improvements too. Perhaps in the long run that Arc GPU could compare against a 6700XT/3070. But you're taking a risk by betting on that happening.",
      "A770 is the top end Alchemist SKU, there will be no A780 or A790",
      ">Amd has gotten better lately in GPUs, but they're still not as caught up to nvidia as intel is feature-wise, and that's crazy to say.\n\nLol..you guys..\n\nIts quite crazy how you framed things. They are not closer to Nvidia than AMD. The problem isnt having the features. Its the performance. Period. AMD and Nvidia are launching in a month or two.\n\nIts one thing to praise and encourage intel for what theyve already accomplished and the potential for greater competition in the low end space, but lets be real.\n\nThe targeting Nvidia instead of AMD is because Nvidia is the market leader. They are not going after AMDs meager marketshare. They are going after Nvidias slice of the pie. Intels GPU tech is not superior to AMDs 2 year old tech if we are being honest. At the same time, entering the discrete GPU space and only being 1.5 gens behind isnt all that bad. Its like AMD was with Vega. Its actually a great *starting* point. I mean nobody was expecting them to compete this gen right? They can catch up in 2 gens potentially - coming from almost nothing.",
      "Man, I really hope Intel does well with this, im kinda excited to get one.",
      "Only from Asrock and Gunnir so far\n\nhttps://wccftech.com/intel-arc-a770-arc-a750-custom-graphics-cards-from-asrock-gunnir-displayed/\n\nMSI should have some coming too.",
      "It seems it is you who can't seem to discern between technology and PR/Media speak.",
      "[Offer.](https://softwareoffer.intel.com/Campaign/LearnMore/7c4740d1-7b93-4626-9fbe-d58a7a0a38ef)\n\n[It's a different offer you just need one](https://softwareoffer.intel.com/Campaign/Terms/7c4740d1-7b93-4626-9fbe-d58a7a0a38ef)",
      "Can you blame them though they are new to the dgpu market",
      "Go to eligibility in my second link you just need the gpu"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "a750"
    ],
    "title": "Intel Arc 2024 Revisit & Benchmarks (A750, A770, A580, A380 Updated GPU Tests)",
    "selftext": "",
    "comments": [
      "Arc is starting to look seriously impressive. If the A750 beats the 6600 XT in 90% of games and the latter is still $240, the former is starting to look like the best value card for *everyone* in this price range, not just enthusiasts.",
      "This is really impressive, I thought the a770 was closer to the 3060/6600. Gj intel",
      "you are not missing anything with starfield. To say that this game is mediocre will be praising it. Focus on good games not 5/10 games.",
      "Why do you have such allegiance to a company that doesn't care about you? \n\nAMD, Intel, Nvidia just buy whatever seems better for you",
      "Based on AMDs slow progress to increase performance over the years.",
      "A good video, and a fair comparison.  \n\nIt’s a bummer Stairfield still appears to be having issues on ARC.  \n\nHonestly probably better if Battlemage isn’t launched until another 3-6 months of driver development time has passed overall.",
      "Can we not with the fanyboyism? More competition is good for everyone.",
      "Really pulling for Battlemage. We need a third party option.",
      "Looks like ARC is racing to the top fast. BattleImage will leapfrog AMD and get very close to Nvidia.",
      "What do promises have to do with anything? You can see current performance then realize they are making faster chips that will have more cores as well.",
      "Amd drivers are pretty mature and their driver issues (Intel's too) are very overblown. Their drivers today are about as stable as Nvidia's and are still more reliable than Intel's. I really hope that battlemage will moreso give Nvidia a reality check at the high-end and upper mid-range.",
      "But but but, BIG NAVI",
      "Remember \"RIP Volta\"? What happened after that? AMD shooting themself in the foot with their meme marketing LOL",
      "Nvidia getting out of the GPU space would be more likely than that",
      "The problem with Amd they also still have massive driver issue after years. If Intel keep progressing their driver, it's very possible for them beat Amd in gpu market especially with Battlemage.",
      "I will happily if you can point to anywhere I did that.",
      "I don't understand why Starfield is even used as a benchmark though. It's not even in the top 100 most played games on Steam right now. If they're  using that game to benchmark GPUs, they should use Cities Skylines 2 to benchmark CPUs.",
      "I agree - but it is one game that a lot of well known sites like to benchmark, and it certainly left an impression on many that Intel wasn't ready at launch.   Games hyped that much should be given prioritization for drivers.. \n\nI think we need to see Intel stay ahead on more AAA launches in the future. \n\nI'm optimistic..",
      "I think the driver issues are still common enough that the rx 6600 would be the go-to entry level GPU.",
      "Based on what? Do you have battlemage leaks of performance targets?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Nvidia's RTX 4060 Ti and AMD's RX 7600 highlight one thing: Intel's $200 Arc A750 GPU is the best budget GPU by far",
    "selftext": "",
    "comments": [
      "I ordered one yesterday after seeing the price drop.  It’s not as fast as the newer amd and nvidia stuff but at that price its a great deal.  I’ve been waiting to see if the drivers would get better and it appears intel has gotten a lot of things worked out.  \n\nI also hope intel succeeds here because we need more competition.",
      "That is what I gathered from the reviews I watched. “Here are the new offerings from Nvidia and AMD. And here trailing by 10% or less in performance is the A750 which is going for $200 and $70 less respectively. “",
      "no",
      "Only thing that keeps ARC from being the the choice is the limit to CPUs that supports resizable BAR.",
      "which is basically everything in the last 3 generations from both AMD and Intel.  \n\nif just looking for 1080p and fps per $ then the rx6600/6650xt is where its at.  \n\nI have an Arc card though and its nice to play with and see what you can get out of it.  Good at 1440p or even 4k in some games and can ray trace better than the 3060.",
      "It would be a downgrade",
      "With rebar, the GPU can request bigger data chunks to the CPU, instead of making thousands of small default requests. This way the CPU can load data in vram faster",
      "I mean, was that at launch? They were totally unacceptable then but they're very functional now. I've been daily driving one for months with no issues.",
      "2080 performs better then the RX 7600",
      "For $200 it's finally a strong recommendation, imo. I say that as a guy who bought the a770 le at launch to experiment with and support competition with GPUs. I'd never recommend that GPU, it's not powerful enough to run games at settings that need 16gb VRAM but the a750 at $200 is the best GPU deal we have seen since before the pandemic.",
      "I literally stated that intel will improve their drivers a lot more compared to AMD/NVIDIA and most likely A750 will be a lot closer to 7600/4060 than it's now (while AMD/Nvidia drivers are matured and the gains won't be as big). You are writing like I am stating the opposed, I am confused.",
      "What does resizable bar do?",
      "4060ti is 50% more expensive than 7600, they are in different leagues.\n\nAlso if you are esport player playing in 1080p then maybe 4060ti is ok, otherwise just don't.\n\n7600 is sidegrade or downgride from 2080",
      "For long-term, yes, however many people this just isn't an option because of drivers.\n\nMine idles @ ~40w (ASRock has some sort of guide to \"fix\" this issue, but all that did was cause games to randomly crash, so I had to undo that), I was having some sort of flicking issue when hovering over a youtube video but I don't seem to see it now.\n\nI also have performance issues in Halo MCC Reach (I seem to be roughly averaging around 10-30 or so fps on high but it feels like a slide show, on lowest everything it's around 25-40, but can spike into the hundreds, but it's still a pretty game gameplay experience, the GPU only runs a 600MHz @ ~35% usage; Ironically, Halo Infinite I'm pretty much always had around 110-130 fps on high despite it being far more demanding, but it's a DX12 game).\n\nI was also having pretty bad stuttering in another game, but was able to fix that issue with changing it from DX9 to DXVK, although many games that's not an issue because of anti-cheat.\n\nFor most people, it'll just make more sense to go with an RX 6600, *usually* price is pretty similar, although usually favoring the RX 6600 as being the cheaper card, although right now I'm seeing 3 different 6600's for $200 and there's even one for $180, the A750 has been around $230 for a while with a drop to $200 just a few days ago, but that price went back up, however in the long run I'd definitely expect the A750 to keep gaining performance as its been doing, I also don't mind being on *questionable* drivers since it runs the two games I mainly play fine, I also like having AV1 encoding, it was a large part of the reason I bought my A750.",
      "but compared to AMD/Nvidia you have basically a given big performance jumps via drivers updates. So your GPU will get closer to 7600/4060 in a year vs how it stands now.",
      "Apart from fixing fixing broken CS:GO performance and Forspoken, gains in most games have been pretty modest.\n\n[CS:GO with last month's driver](https://youtu.be/xFLDxMwPcrw?t=342)\n\n[Same performance as driver from 3 months ago, comparison with 3060 and 6600XT](https://youtu.be/00T15aL1pkA?t=371)\n\nForspoken last month has improved over launch and 3 months ago\nhttps://youtu.be/xFLDxMwPcrw?t=366\nhttps://youtu.be/00T15aL1pkA?t=402\n\n\n\n[Cyberpunk 2077 last month results](https://youtu.be/xFLDxMwPcrw?t=200) are the same as [3 months ago](https://youtu.be/00T15aL1pkA?t=162)\n\nSimilar situation as Cyberpunk for Shadow of the Tomb Raider\nhttps://youtu.be/xFLDxMwPcrw?t=294\n\nhttps://youtu.be/00T15aL1pkA?t=297\n\n[Horizon Zero Dawn had modest improvement over launch.](https://youtu.be/xFLDxMwPcrw?t=241) Sits about the same performance as [3060/6600XT](https://youtu.be/00T15aL1pkA?t=215)\n\n\nThing is, people shouldn't expect the same pace of improvements to continue indefinitely. Rate of improvement tends to slow down after the low hanging fruit has been snapped up. Just saying.",
      "Fairly complicated, it allows pcie hardware to renegotiate how much of the device internal memory (vram in this case) is mapped to system address space at once, basically it allows making large concurrent transfers over pcie instead of queuing small ones. \n\nThis doesn’t have direct performance effect in most cases but intel cards and drivers have been designed assuming it works, they really suck without it.",
      "If I have RTX 2080 do it worth upgrade to RX7600?",
      "Not at launch, 3 months ago, it was driving me crazy, arc control doesn't save the settings, old games.. such as mad max, crashed, a lot of PC shut downs, out of the blue, messed up HDR, so many problems that i gave up and bought a 4070 bundled with Diablo 4, i don't regret it.",
      "I had a arc a770 for 4 days, that's how much i lasted before starting to loose my sanity, great looking product, ok ish performance, but when it comes about the drivers and the problems it comes with them, no! They have a duty to fix the driver side, until then, no, no. It was sent back without a second thought."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "a750"
    ],
    "title": "Intel® Arc™ A750 Graphics Benchmarked in Nearly 50 DX12 & Vulkan Games",
    "selftext": "",
    "comments": [
      "These are seriously going to have to be dirt cheap. 3-5% margin in their best suit is no victory in itself. Let's not forget they cherry picked like 6 games to showcase, commanding a 13% lead last time. They've compared it so much to the 3060, but here's the elephant in the room: **Intel is not the first to have to sell their products with a better price to performance compared to Nvidia.** AMD exists. Intel will have to 'out AMD' AMD. \n\nSo right now, the market has basically decided that a 3060 is worth $369, while a 6600XT is $300. These perform very similarly at 1440p. Now where does that leave the A750? It's gotta be under $300. Well under. $200-250 wouldn't be too little with the state of their drivers. They've taken the crown of the third rate player so that's what they must do to survive.",
      "Underclocking their DDR5 from [5200 to 4800C38](https://game.intel.com/wp-content/uploads/2022/08/config-chart-a750-3060-01.png) on the test bed is an interesting choice.\n\nNumbers appear solid, though. One set of outliers caught my eye - Call of Duty: Warzone shows up at 53% faster than the RTX 3060 at 1440p High, but Call of Duty: Vanguard flips it, losing to team green by 30% under the same settings.\n\nGlad they released some useful numbers, though, for a change.",
      "They sorta skipped over 1% lows though.  Let’s wait a little more…",
      "Never belief marketing, wait for reviews.\n\nNo info about fram pacing or 1% lows, which in many reviews of a380 were often bad, despite nice average fps.\n\nAlso in a 1-3 months new gen from competitors should be on the market, maybe not whole series, but high and mid should be.",
      "I want one. Or the A770.",
      "I wouldn't plan on anything yet, considering the numerous driver issues with the A380 and the fact that Arc underperforms in many unpopular titles (especially without reBAR).\n\nAlso, the rumors that Intel will axe its GPU department and the entire Arc project don't give me a lot of confidence in this first gen of GPUs...",
      "Looks pretty good honestly. It's basically an RTX 3060 with an AV1 encoder in the 50 games showed. I'd actually buy the A750 over a 3060 if they had the same price.",
      "The low end amd and Nvidia cards for the next release are way out though. Still not horrible timing",
      "There's also the RX 6700 (non XT) 10GB, which goes for [$369](https://www.newegg.com/sapphire-radeon-rx-6700-11321-02-20g/p/N82E16814202424?Description=RX%206700&cm_re=RX_6700-_-14-202-424-_-Product&quicklink=true) on [Newegg](https://www.newegg.com/sapphire-radeon-rx-6700-11321-02-20g/p/N82E16814202424?Description=RX%206700&cm_re=RX_6700-_-14-202-424-_-Product&quicklink=true) Kinda hard to compete with that in this price bracket.",
      "if its 200-250 yes, if no, why bother with the risk?",
      "If they ever get their drivers sorted these would make pretty good cards. I gotta give props to Intel on a solid first attempt at a modern graphics card.",
      "It's also releasing later this quarter and going to run into the Nvidia RTX 4000 series launch\n\nI think your right about price\n\nIt's going to have to be a bargain to get people to take a risk on a new GPU line",
      "Sure, for DX12 titles.  Not sure I would take it over a 3060 if price the same as many titles are still DX11.",
      "Yep.  Planning on the A770 for a new build.  Unless the 4000 series kicks their butts at similar prices.",
      "> They sorta skipped over 1% lows though\n\nThat's nothing. They skipped DX11.",
      "Yeah old titles may not run as well.  But looking forward to the new titles things look bright already.\n\nIntel will improve its dx11 drivers.  And I don't think they will axe ARC considering how much it's talked up, promoted and forward planned.  If they do I'll buy a 5000 series next time...",
      "The price drops are because of the crypto crash, not because they want Intel to fail. The reviews of the cards and driver suite aren't to flattering to Intel either, this is pretty much a self inflicted wound. These cards were supposed to be in our hands months ago, yet they still aren't. Now bout AMD and Nvidia are bringing out their new cards shortly, who is going to bother with Intel cards aside from tinkerers and curious people. I'd buy a Arc card just for the heck of it, but if I wanted a sure-shot gaming system, I'd go with AMD or Nvidia.",
      "Doesn't matter how hyped up ARC is. Intel needs to deliver fully-working products to its customers and it seems they are not capable of doing so.\n\nIf there is an architecture/hardware flaw within the GPUs, the dGPU sector may be axed. Billions have already been invested and lost for Intel, but how can the company fully commit to the product if it's broken from the start?\n\nOf course, nothing is certain as of right now, but it's important to be ready for this plausible outcome.",
      "You're literally talking about a product they said would ship well over a year ago, after a decade of INtle missing targets on nodes, laptop, desktop and server chips constantly.",
      "i want to preface my comment by saying that it has nothing to do with consumer choice, just my observation on a technical basis.            \nIf this releases at a good price, it will be a great product.       \nNow :   \n3060  :  Samsung 8nm   276mm size,  12 billion transistors.                  \nA750 : TSMC N6  6nm  406mm size , 21.7 billion transistors                 \n6600XT: TSMC N7 7nm, 237 nm size,  11 billion transistors.                            \n\nif i grant that intel will get their drivers working for DX11,DX9 on the level of optimization they have now for DX12 & vulkan, then i would accept that A750 would be slightly faster/better than the 3060 and a bit slower than the 6600XT.                          \nBut, from an architecture perspective, they are spending almost double the transistor budget to achieve a similar result.  3060 is also a tiny die by comparison, even on a much much less dense node, so much cheaper to manufacture for nvidia.             \nSeems that alchemist is an ok first attempt, but needs a lot of work till the future architectures can claim to be on par with amd/nvidia on a perf per transistor basis.              \nRight now, AMD & Nvidia have what looks like a very large advantage .                          \nKeep in mind that ACM-G10 is almost the size of Navi21 and has almost as many transistors (21.7b  vs 26.8b) , but their performance delta is massive."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "a750"
    ],
    "title": "Intel Arc A770/A750 Graphics Card Review Roundup | VideoCardz.com",
    "selftext": "",
    "comments": [
      "I want to buy an a770 not because I want an a770, but because I want a more reasonably priced 50 series and 8 series.\n\n\nCheering for you arc",
      "i'm pretty impressed with the card given its their first attempt, in a few games it was only ~5% behind the 3070, which says that the hardware is at least good. amd and nvidia have had years to optimize drivers for their hardware, so i think intel is almost definitely leaving a lot of performance on the table here due to how behind they with optimization.",
      "Just a PSA, according to Linus the 16gb A770 LE is going to be available in very limited numbers (probably due to low margins). So if that's the GPU you want you need to be ready to buy on launch day.\n\ntimestamp: https://youtu.be/6T9d9LM1TwY?t=701",
      "770 destroy the 3060 in RT at 1440p, and better when xess, go intel go ! \\*in digital foundry review",
      "It’s not about the card, it’s about sending a message",
      "The drivers stack though is brand new from the begining of the year. They tried to port from their mobile drivers and that did not work. Had to start from scratch. This will get better.",
      "We need heroes like you",
      "They'll probably be able to fix it - or at least make it better - with driver updates sooner rather than later.",
      "Why hasn't anyone tested these cards with dxvk for the older titles?",
      "the performance is there, it can compete with its targeted NVidia, Amd counterparts. but without a serious discount why would customers buy intel instead of more tried , proved Nvidia,AMD?",
      "Buy cheap now, wait for driver upgrades while hitting some bumps in the road for a few months, have a card with superior price performance in the near future.\n\nPeople with more money than time can spend two or three hundred dollars more for some guaranteed gaming hours a week. Not everyone has extra dollars and for these people who are looking for maximum performance per dollar and advanced tech like RT and AI optimization, this could be a good buy if they are willing to endure some bumps in the road.",
      "This is 100% not a card to give to a family member who doesn't know how to deal with tech problems. Crash to desktop and system lockups are common across many reviews and they are showstopping problems to average users. \n\nIts fine to believe Intel will fix their drivers, but if you give a device to someone who isnt able to deal with these first (hopefully) months, they will be soured on the PC for years.\n\nThe Apples/Dells/HPs of the world, even with all their BS marketing, will always be attempting to design stable systems first.",
      "As someone on the market for a new GPU, the thing that scares the shit out of me is the rumors that Intel is considering pulling the cord out of its dGPU, Drivers improve with time if effort is put on it, but that is the catch.",
      "Is there a link yet to where it will be sold so I can bookmark it?",
      "The i740 was not a GPU, it was a graphics accelerator. The term GPU didn’t apply until hardware transform and lighting was added to the 3D chip, starting with the GeForce SDR. Larabee was not a GPU and lived on his Xeon Phi. The Arc A7 is the first gaming GPU from Intel.",
      "But dxvk translates dx calls to vulkan, and intel gpus are supposed to be alright with vulkan",
      "Every company at the top plays bad cop. Until competition humbles them.\n\nIntel has had their turn with CPUs. Next it's Nvidia's turn to be humbled. Buy AMD if you have to. Avoid buying the 4000 series for a software update.",
      "I am buying an ARC A770 Limited to replace my wife’s Radeon 6600.   She’ll have an all-Intel build (i3-12100, Intel 660p) :). \n\nThe only thing I (really) don’t like is the high idle power consumption..",
      "Uhhh, why?",
      "Voting with my wallet / to support the brand mainly.  \n\nBut actually usable ray trace performance is a bonus and I think these cards will get faster over time.  \n\nHer card is also the vanilla 6600 so A770 is faster all around."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "a750"
    ],
    "title": "Intel Arc B580 Limited Edition tested in 3DMark, outperforms RTX 4060 and Arc A770/A750",
    "selftext": "",
    "comments": [
      "This is hardly surprising since Intel Arc A770 already outperforms GeForce RTX 4060 in 3DMark Time Spy",
      "i care about game fps comparison more",
      "Now let's see gaming performance.",
      "Huge if true, could be a new budget build king.",
      "Wow 3dmark.\n\nWorthless test thru",
      "Since it might be around 250 bucks. Like there are not many good budget cards eight now. The 4060 is not that bad, just too expensive for its performance (should have been aroumd 250 bucks max)",
      "Is the embargo not over until launch day?",
      "Since when we feel excited for a new card outperforming a terrible card such as 4060? Even NV's ancient 3060ti already done so.",
      "For the price 249$, Intel is king of budget now",
      "https://redd.it/1hbrxdg",
      "Hey SilasDG, your comment has been removed because we dont want to give *that site* any additional SEO. If you must refer to it, please refer to it as *LoserBenchmark*\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/intel) if you have any questions or concerns.*"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Arc A750",
    "selftext": "",
    "comments": [
      "Recently bought the A750 at my local microcenter for msrp (shocking right? lol)\n\nI'm actually pretty happy with it. It plays all my games and all my work needs. I do see the occasional stutter and issue here and there but I'm sure this will all be worked out with future drivers. For the price. You can't go wrong for 1440p gaming.\n\nAND I got a free game! MW2 💪🏻\n\nProud early adopter 👍\n\nEdit...\n\nSo far the games I’ve played.\n\n**Modern Warfare 2**\\- Multiplayer Performance is great and smooth. Especially with having the ability to mess with XeSS. Hard to say if some of the stuttering is GPU related or new game release that still needs some tweaks. Regardless I didn’t feel like it hindered my ability to have fun. I messed with competitive settings and eye candy settings. Competitive without a doubt is smoother. But eye candy with XeSS is impressive. I’m going to try out campaign/co-op soon as it’s a slower play style and you’re more aware of the ‘*little things*’.\n\n**Battlefield 2042** \\- definitely needs to be optimized. There are artifacts. Some graphics don’t load properly. Lighting is horrible... My laser sight literally looked like a light saber and blinded the whole screen. Overall performance isn’t great. Even with competitive settings it wasn’t great. (*I know the game isn’t great in terms of game optimization, I’ve played with other graphics cards so I know what to expect at its best and worst*). I don’t recommend this game at the moment until drivers are better optimized or the game is optimized for the GPU. (*Some games perform better with AMD or Nvidia. So it potentially COULD be an issue for Intel*)\n\n**Planetside 2 -** (DX11 title. Originally was a DX9 title overhauled to DX11). For this title it actually performed better than expected, for an older title. This game is CPU heavy but does like a good GPU paired with it. Higher resolutions provided better performance and I was able to load Shadows with little impact. Before it would tank performance in big battles. I need to test this more as I’m really surprised by the results.\n\n**Halo Infinite** \\- The game plays well with medium to low settings at 70ish average. However optimization is needed. It would stutter and some lighting was a little off. Turning everything to low helps with smoothness but I’m a sucker for eye candy. Comparing it with my previous graphics cards I would consider it under performing and it’s something that could be fixed with future drivers.\n\n**Medal of Honor Allied Assault** (DX8 title) - this game is old but I love it. No impact on performance due to older API. Maxes out FPS and no artifacts. Graphics appear to load just fine.\n\n**Titanfall 2** \\- (DX11 title) probably one of the best optimized games I’ve played. No issues with A750. Maxes out frames on maxed out settings with no stuttering or artifacts (at the moment)",
      "Thank you for your service. Hopefully this paves the way for a 3rd GPU provider to compete with red and green.",
      "We now have three teams. Team Red. Team Green. Team Blue. RGB. Omg we have made full circle ⭕️ \n\n/s",
      "I would consider this card as a replacement for my son's RX480, but for whatever reason it doesn't perform anywhere near as well at 1080p as it does at 1440p, relatively of course.",
      "What do you mean?  There is no bare PCB visible, thus the card has a decorative backplate installed already.",
      "What a beautiful card",
      "how much fps you getting in mw2 in ground wars/invasion mode?",
      "Sounds like an expensive addiction :)",
      "In ground wars with settings set to auto I’m getting about 80-90 fps. \n\nAuto is a mixtures of medium and low with a few high settings. XeSS is set to quality.",
      "It really is. Quality build for the price point. \nThis is the kind of card you want to show off and not keep closed up. \n\n*Starts looking for a new case*",
      "The rgbs are included with the A750?",
      "Just managed to get an order in for a A770 less then an hour ago!\n\nCollectors item for me as i already have a 4090 lol... plus i might also get the XTX.. i might just have a crippling a GPU addiction lol",
      "That's a bummer I was thinking the same thing for my son.  Might have to go amd though",
      "Any artifacts with XeSS that you can perceive with your own naked eyes?",
      "Finally the dream of having amd cpu with intel gpu. And OP missing motherboard backplate.",
      "What would this be on par with performance wise from Nvidia?",
      "Yeah in 1080p for some reason it doesn’t perform well. Well might not be the correct word but like you said relatively speaking it doesn’t perform as expected. But the higher the resolution the better it gets. Kinda minds me of the Vega dGPU series with higher resolutions it performed better. \n\nFor your kiddo I would definitely recommend something more reliable. Tried and true. Something I wouldn’t want to mess with. \n\nIf you like to tinker I recommend this. With future driver updates I’m certain performance will improve. I’m hopeful when afterburner is able to read the bios we can fine tune and unlock what this GPU is capable of.",
      "No not on the a750 model. That is exclusive for the A770 limited edition 16gb model (correct me if I’m wrong).",
      "Oh ok",
      "I agree lol. Very expensive. \n\nBut to a point yes I do agree. I’m glad the A750 functions well enough for me to be happy. It’ll be a Collector’s item in the event Intel quits on the whole GPU thing lol"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Arc A750 vs RX 6600 GPU faceoff: Intel Alchemist takes on AMD RDNA 2 in the budget sector",
    "selftext": "",
    "comments": [
      "The intel arc sub loves to take this article out of context and just read the benchmarks section to then conclude the a750 is the winner\n\nA750 does win in these sets of benchmarks, ray tracing and 4k, not by much and with some outliers in some games. But at ray tracing and 4k, performance is suboptimal at best.\n\nAlso have to note that the arc card still has compatibility issues and stability problems, in some games it performed significantly worst than the other brands(halo infinite as an example). \n\nNotably power efficiency is terrible, consuming as much as 200W unlike the 6600's 130W+-. If that's a concern, the a750 isn't a good choice especially if it's just better in games by a few % more but taking in that much energy to do that\n\nArticle seems to be in favour of AMD unless you have AI applications to run and want to support ARC",
      "Winning by 100% at 4k\n\nBoth cards barely hit 60FPS\n \n4K\nSpider man: A750 17.2 FPS, 6600 8.6fps\n\nBoth are PowerPoint presentations",
      "Right, amd doesn't win \"hands down\", but the a750 winning by 100% in a resolution and setting that both cards struggle to play at isn't what made the 6600 lose out in this comparison",
      "AMD wins hands down. A750 is a good card, but some older games don't work. Rx6600 works for everything. The amd drivers are better too. Intel is great to fill a niche if you're not in that niche, skip it and go straight for the rx6600 for entry level 1080p.",
      "That's not so true anymore. I've been going back and testing games like Fable Anniversary, Hitman Absolution and Space Marine, like they're running really well, over 150 -200 fps 1440p. Then most modern games run great, too. The main issue is new releases where Intel doesn't get early builds of a game to make drivers from. \n\nThat's definitely a problem, but that makes arc a gpu for patient gamers. \n\nI'm using an a770 on a 7800x3d bench, fwiw.",
      "A770 can push halo infinite at 1440p Max settings @ 70-90 fps. What are you talking about instability for halo infinite?",
      "Yup, but this article is comparing products at similar price\n\nNo doubt the a750 is good for what it is, but it's more meant for tinkerers and experienced troubleshooters",
      "The a580 is comparable but cheaper than the 6600 which are probably more comparable products",
      "Counter point. Enemy territory (and et:legacy) work on my a750 in Linux but not on my 6900xt on windows 11.",
      "Just my experience when using the a770. I admit it was few months ago, the driver might've improved it. Had switched to a 6700xt since then",
      "They might be a tie in gaming (except for older titles where the arc is still a bit problematic), but Arc GPUs are much better for video editing, 3D modeling, and other productivity tasks. They're the best for the price by a long shot, even better than Nvidia cards weaker than the 4070. You can check the Blender scores and Puget benchmarks. Not to mention cheap 16GB VRAM for the A770.",
      "Rx 6600 owner here, happy with it at 1080p Flawless experience. I would have gotten the ARC had it been available at the moment I bought AMD. I want Intel to succeed and take Nvidia down from the pedestal. AMD has risen prices because only competition is Nvidia and can mirror prices, but with a 3rd quality player we should get these silicone bits at reasonable price. \n\nI want to move to 2K this fall. I am on the market, so if Intel offers a decent GPU I will back them.",
      "Why would anyone be trying to do 4k with either of these cards? Who cares which ones worse or better at 4k?",
      "I’ve got a a750 in my Linux box. Many games work on it in my steam library. \n\nYou may not get day one support for a new game but it’s working a lot better than at launch.",
      "Counter video. https://m.youtube.com/watch?v=Y09iNxx5nFE&pp=ygUaaGFyZHdhcmUgdW5ib3hlZCBJbnRlbCBhcmM%3D\n\nIf you want to bring up one game that you love to counter, fine. For most buyers out there, this would be a better representation of what's to be expected\n\nIs Intel arc bad? No, they're well on their way to be good. However, I never buy based on potential.",
      "It also ignores older games entirely.\n\nThe situation would certainly change if they threw Detroit: Become Human, Bioshock Infinite, or maybe even CS:GO - but I've heard Intel has improved CS:GO performance significantly since I last tested ARC.",
      "A770 16GB could be found for 270-280 USD. And sure, while that's only 13% cheaper than 7600 XT ($320 at the cheapest). While game performance isn't great, ARC has much better productivity performance, like 1200-ish vs nearly 2000 Blender scores, nearly 4070 level HVEC performance, much better hardware for streaming and video capture, etc.\n\n\nI've never said theyre the best cards for gaming. I usually recommend pure gamers Radeon GPUs, except when they're really cash strapped, in which case A380 and A580 are better values than radeon.\n\n\n6600's competitor would be A580. Gaming and 3D rendering performance isn't too far off from A770 actually. It's a really good card for the price, way better value than A750 even though A750 and A770 are good value already. Plus it's got most of A770's features intact, which means video capturing and streaming are just as good.",
      "I want to upgrade my GPU (GTX 1650) but I don't have a lot of money (not from the USA/UK/Canada/Europe) the a750 is 190 dollars (if I purchase something above 200 dollars in my country they are going to demand taxes, 18% of the product value to be more specific. For example, if I buy the a770 that is 280, they are going to demand 50 more dollars in taxes and instead of 280, I'm going to pay 330. Not to mention the taxes they demand when it arrives and the payment of the weight I have to pay to the courier 💀). Should I buy the a750? Or is it better to get another GPU and pay the 18% tax of the product value?",
      "Yes, but at the a770 16gb prices, it's competition isn't the rx6600 now is it? Arc is way better than Nvidia value wise, but it's a niche card that does do a whole lot for gamers. At a770 16gb prices, it's losing cleanly to the 7600xt. Intel cards simply need to take another 15% to 20% haircut to remain competitive in my book.",
      "what niche ?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "a750"
    ],
    "title": "Hitting the Shelves: Intel® Arc™ A750 and A770 GPUs Release Today!",
    "selftext": "",
    "comments": [
      "OOS @ newegg",
      "Yea this launch has been kind of a mess so far. The A770 and A750 went up earlier than they were supposed to on Newegg and the A770 16gb sold out immediately. The A750 came in stock again on Newegg but now it is also out of stock\n\n**EDIT: THE A770 CAN BE BACKORDERED: https://www.newegg.com/intel-21p01j00ba/p/N82E16814883001**",
      "Aaand .. it's gone",
      "Such a whimper of a launch. They must have made one whole box per country.",
      "True limited edition cards",
      "Complete mess. Release a budget friendly card with all this hype....but don't tell us officially where or when we can buy it until an hour AFTER they sold out of the a770?",
      "Had the A750 in cart and by time I went to check out it went out of stock. Bummer",
      "its my bad, i bought them all",
      "https://youtu.be/-DT7bX-B1Mg",
      "Unironically, I think a *lot* of these LE cards are going to end up as shelf art.",
      "Werent a ton of these things supposedly sitting in warehouses for months?",
      "maybe demand is high?\n\nSounds a little like copium",
      "If they had all this cards since February.... and they are gone... that meas this is it, there should be no more stock.  \n\n\nThis is the first GPU in history to be designed as a collectible item.",
      "So where's that discount code from the HPG scavenger hunt :/",
      "Fucking backorder won't go through. Just keeps putting me back at the secure checkout page.\n\nEdit: It's aggravating. I work on a PC. I had auto-refresh set every 30 seconds since this morning. Missed the first drop. I got up to use the bathroom ONCE and of course that's then they come back in stock lol. Ugghhhh\n\nEdit 2 hours later: Still won't let me add the backorder.\n\nEdit: 5 minutes after previous edit I finally got the backorder to go through. Had to use the phone app (I was able to spam the confirm order much faster over and over). Worked on the 5th try.\n\nETA on the backorder is 10/19....if of course they get the stock in for it.",
      "Man… what a joke of a launch.",
      "They delayed these cards for months in preparation for this launch.   How did they mess this up so bad???",
      "I can’t believe you’ve done this.",
      "Scalpers gonna scalp…the shoe market is slowing and PS5’s are in stock.",
      "Dumb thing to scalp, frankly. These are the higher end models, and there are already better nVidia and AMD cards in the used market. Prices on those and new old stock of 3000-series cards keep coming down. \n\nNo one is going to pay much above MSRP on an Intel Arc card today 😂"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Upgrading from a 1050 to Arc A750. Would it bottleneck?",
    "selftext": " I am planning to upgrade my GPU from a GTX 1050 to a Intel ARC A750.\n\nMY CPU is a Intel Core i5 7400. Would it pair well or would it bottleneck like crazy? (I am also planning to double my RAM from 8GB)",
    "comments": [
      "The problem is, ARC needs Resize BAR, and that requires 8th gen or later CPU.",
      "im glad arc is getting traction.",
      "This is 100% true. Performance drops below the earth if ReBar isn't enabled. I like what Intel is doing though. With XeSS and the way their drivers continue to mature. Watch out, AMD and Nvidia.",
      "6600 is 180, 6600xt is 230, 7600 is 250, 3060 is 280. \n\nIt's fine if you want to buy 750, but these are all great options that perform better than/equal to 750 at lower/equal price.",
      "Not recommend. Your CPU does not support rebar",
      "If you plan on upgrading the CPU soon you can get it. If you want to keep the 7400, don't get the A750.",
      "The A750 is also an 8GB card. So it doesn't have an advantage on that front either.",
      "That'd be a pretty bad pairing imo, 4C4T CPU just isn't enough for newer games while some older games play terribly on Arc, upgrading to a 6700(k)/7700(k) would definitely help a lot, although if you do that and go with an A750 you'll need to force enable ReBar since your motherboard doesn't support it (I think it wasn't until 10th gen when they added ReBar, but it's a pcie feature, so even if it isn't there, it's still supported).\n\nA good example of older games playing terribly, Halo: Reach from MCC I get around 15 FPS on high, changing to low I can get maybe around 30 FPS but it's a terrible gameplay experience, I've also put DXVK into it and it runs incredibly (Depending on where, I can get anywhere around 150 fps to 300 fps) well with the translation layer, however I can't use online features with DXVK.\n\nIf you don't mind not having AV1, a 6600/6600XT/6650XT are good GPUs for the price, although depending on the game you'll lose some performance from being on pcie 3.0 since the GPU only has 8 lanes.",
      "Oh dear, you haven't watched the [reviews for the 7600](https://youtu.be/Yhoj2kfk-x0?t=1028), have you? All the GPUs released this generation are horrible, except for the 4090.\n\nSteve recommends the 6700xt here, it's faster and has 4 GB VRAM extra. Depends on your local pricing though. You can even buy it used (if you think it is worth the headache), since the market is awash with used GPUs from the bitcoin mining rush.",
      "Really? There is a [6700xt for $309 on Newegg right now.](https://www.newegg.com/asrock-radeon-rx-6700-xt-rx6700xt-cld-12go/p/N82E16814930059?Item=N82E16814930059&Description=rx%206700%20xt&cm_re=rx_6700%20xt-_-14-930-059-_-Product) So just $40 more.\n\nBuying an 8GB VRAM card like the 7600 right now is rather risky, more so at 1440p.",
      "[https://pcpartpicker.com/list/KmGkk9](https://pcpartpicker.com/list/KmGkk9)\n\n&#x200B;\n\nWould this be an okay PC for Arc A750?",
      "They need to get the price of their cards down.  I can't justify the extra $100-$200 for maybe 5%-10% performance, not to mention 50%+ TDP increase.  $400-$550 for a $280 rx 7600 competitor? Not a chance.",
      "That looks like a very good list to me.\n\nI would cheap out on the case if I were building for myself, you can get one thats perfectly functional for around $40-50. However if you like the way that one looks maybe its worth the extra $50 to you.  If you do choose to cheap out on the case, you can expect to get what you pay for which means assembly is typically harder because something will be made with bad tolerances and hard to put together.\n\nYou also don't need a 750w psu for those parts, but there isn't much price difference between a decent 650w unit and the one you have picked. Its fine if you like that one and would let you upgrade to something more power hungry in the future.  You might save about $30 dropping to a good 650w unit, but you do not want a bad psu from a no name company.  Unlike the case it isn't something you should ever cheap out on.",
      "No.\n\n7th Gen doesn't support Rebar.",
      "Both the 4060(ti) and 7600 (is it the AMD equivalent?) are horrible value cards.\n\nI had no interest in these cards anyways, but the problem is that with dGPU prices like this, consoles will stay the only budget choice. Which means we will keep getting more and more shitty games that are optimized towards consoles.",
      "How is the 7600 horrible value? What would you buy instead?  I have the 7600 sitting in my cart right now waiting for my next paycheck.  It'll be paired nicely with my 10900K",
      "the fuck are you talking about. the arc 750, at its cheapest, is 250.",
      "That are a whopping 10% faster than previous gen from over 2 years ago, lol. Meanwhile, games have gotten a lot more demanding with some even needing more than 8GB VRAM at 1080p.\n\nIf Intel ups their driver game, there might finally be some proper perf-per-$ gains in the GPU budget segment.",
      "On pcpartpicker, cheapest I can find is $229. You can get a 6650xt same price, 6600s for $179, and 6700s for $279. All of these have better drivers, no rebar needed and less cpu overhead.",
      "They need to put bar on 200 series. It’s been proven you can do it. Takes a custom bios though. And some soldering I believe. Still running a 7700k here"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Arc A750 with Resizable BAR working on i7 6700 using ReBarUEFI BIOS mod",
    "selftext": "",
    "comments": [
      "Oooh, sweet. I keep waiting for an X99 mod I can put on my old system.",
      ">little chance of recovery from error\n\nyou can buy a ch341a bios flasher\n\n&#x200B;\n\nor just trust dual bios like i did on my b75 board",
      "Motherboard manufacturers typically make very little in the way of profit and only have one person handling their in-house BIOS, so how many generations of boards do you want this one person to keep updating? Assuming Intel and AMD release a new platform every two years and we're talking about 2014 platforms here, that's 5 or 6 generations of boards. Every board generation has 2-4 different chipsets and every chipset typically has at least three different boards per manufacturer to hit different price points. Obviously there's a lot of re-use with AMI/Intel/AMD providing the base BIOS and the board partners basically customizing to their liking, but they still need to bake in support for the specific components on their board and add memory support and the like. That's a huge undertaking for a single person IMO.",
      "Why do we have to mod stuff like this in when it obviously works and the manufacturer could just support it. I know it’s a. Money making scheme, but just….annoying.",
      "Have you done any benchmarks to verify that it has had the intended performance increase?",
      "Eh. Most of the mod procedures I’ve seen have been very finicky with little chance of recovery from error. Keep hoping a repository of updated/verified BIOSes eventually gets created!",
      "> Motherboard manufacturers typically make very little in the way of profit and only have one person handling their in-house BIOS, so how many generations of boards do you want this one person to keep updating? Assuming Intel and AMD release a new platform every two years and we're talking about 2014 platforms here, that's 5 or 6 generations of boards. \n\nWith the amount they are charging me for a motherboard these days, I expect much longer support. They can hire more people. If they won't support it, open source the damn thing so the community can do it themselves.",
      "works for any uefi system actually",
      "They just don't want to support really old stuff. Skylake is from 2016 which is now... 7 years ago, geez. \n\nI wish they did, but I can see why they don't.",
      "Some boards also have a BIOS flashback method that works. It’s a specific port that you can insert a USB flash drive into with the BIOS file named correctly and it will rewrite the flash.",
      "Oh, good catch! I didn’t think BIOS Flashback existed on boards that old, but it seems the TUF X99 does actually have it.",
      "This isn't my screenshot but it makes Arc GPUs go from unplayable stuttering to performing as should.\n\n\nOn my RX 580 it does give me a ~10% fps boost in DX12 games and improves the 1% lows alot",
      "No that won't work because the PCI initialization has to be done by UEFI. Maybe it is possible to make it do the PCI initialization but idk",
      "First, motherboards today are in no way worth the money being charged for the “tiered” features they add(see $500 dollar motherboards, cost more than the cpu it can’t run without). Second, If it is possible and modders have acheived it, then how hard could it be for a manufacturer? Btw, BIOS support tends to suck terribly after the major push of the original hardware. They only update old stuff if it’s due to security. As far as platform support and length, as long as it’s viable. If a prebuilt from an OEM like Dell and HP can be trusted to do this, then mobo manufacturers should be even better.\n\n To address the single person, this isn’t some dude that is a hobbyist. If his one job is to work on BIOS, he aught to be pretty good and efficient at it. Also, why have one dude to write BiOS, what happens if he’s sick or dies in a car crash. “Whelp, Dave’s dead. We might as well give up the board business!!!”",
      "Open source you say, but that might cost us money!!!! This is right there with right to repair.",
      "Sadly, no dual BIOS on my TUF X99 Sabertooth.\n\nWhich is actually a bit ironic considering the “military grade” marketing of the Sabertooth back then. The even thing came with complete dust shields for every port and slot. But, I need a GPU for that system and it would be nice to put an Arc in save for the Rebar issue.",
      "As far as business sense, no reason to do this. This is to be read, best solution for business. As far as it being an exercise for nerds, or power users, or maybe just the frugal, it’s just pure fun or free performance. You’re kinda asking why did AMD offer backwards support for so long on CPUs. Well, they didn’t want to, but the potential loss to Intel over breaking their own word could have cost more than losses from some people not buying new CPUs. Some people remember things like this and remain loyal. Some people, like me, don’t care about that and chase whatever is best ftm. To ignore public sentiment and pretend it doesn’t matter is just head in the sand. Even court cases depend on public perception and emotion. Guilty can be set free and innocent imprisoned for their perceived traits and behavior. CPU/motherboard sales are far less important than law. Or are they ;)",
      "Its not legacy. This is not due to some limitation of the hardware. The board cpu/chipset is capable of the tech. It simply isn’t able to be toggled on. \n\n In computing, a legacy system is an old method, technology, computer system, or application program, \"of, relating to, or being a previous or outdated computer system\", yet still in use. Often referencing a system as \"legacy\" means that it paved the way for the standards that would follow it.",
      "I can't seem to get it working on my kabylake g4560 CPU with rx 5500 XT in an MSI gaming motherboard. In bios I can enable above 4g Decoding so I thought it might work but there's probably some other limitation I'm unaware of",
      "There were uefi bios workaround for old platforms like x58. You basically used a usb drive to boot i to a uefi loader. Wondering if you could piggyback this on top of that to make it work. Probably not."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "My Arc Experience",
    "selftext": "I purchased an arc a750, upgrading from a gtx 1660. It is paired with a r5 5600 and 16gigs of ram at 3200, all of this on windows 11. No overclocking, I do have resizable BAR enabled. I play mainly popular games, call of duty, battlefield, forza, rocket league. those games run perfect for my use case (1440p at 120hz) when the drivers work.\n\nHowever, the drivers are some of the most temperamental and pissy things I have ever dealt with. I cannot go a single day without my drivers failing, needing a reboot or reinstall. I have older titles that just don't work (civ V, medal of honor airborne, old NFS games just to name a few), even a newer game like ready or not crashes frequently. I cannot use VR at all because the drivers have no support on Oculus. to say the drivers make the product hard to justify to a friend is an understatement. It has gotten to the point where if I have any issue at all, I chalk it up to Arc and give up.\n\nI understand this is a new generation of product with challenges I don't have the technical knowledge to pretend I can fix, but when people say arc was half baked coming out the door, it was an understatement. I have used both NVidia and AMD gpus and have never experienced anything this intrusive in my gaming experience ever before.  I would love to hear what others have experienced, maybe I'm the unlucky one here.\n\nps, this was typed in a fit of rage after the third crash within two hours, so forgive the lack of structure and thorough explanation.",
    "comments": [
      "I have had an a380 for 2 months and I have only played warzone, warzone 2 and mw2 and I’ve had almost zero issues. First few days of mw2 I kept getting an error status thing but it must have been fixed quick. I can’t remember the last time I had an issue of any sort.",
      "You say youve never had these kinds of issues with amd cards.... ima say you never used a Radeon VII or a 5700XT at launch lol",
      "I have done a lot of testing and fiddling with an A380, A750, and A770, and I have very rarely run into game-stopping issues. If you haven't yet, I would recommend using DDU to remove the Nvidia drivers, as sometimes having multiple different graphics drivers can cause some weird issues.",
      "Radeon 7, literally every reviewer ever said \"card borderline unusable due to drivers\" even for a few months after launch.",
      "This is why being a pioneer of the first generation tech is always a risk. Want stability? Get NVIDIA.",
      "Even AMD is in a good state relative to intel driver wise. I know people crap on them a lot too, but honestly I've had both good and bad experiences with nvidia and AMD over the years. However, I know my friends with intel IGPs have had trouble in the past with certain games, and I know that the ARC GPUs seem temperamental as fudge. \n\nI'd like to see arc mature and get to a point where they're more on par with nvidia and AMD, but right now it's like...yeah no. This just isnt working. I dont have a ton of money, i want something that works reasonably well and provides a good bang for my buck, and buying into an extremely experimental technology that either works or doesn't is just not something i wanna do.",
      "I had crashes on Radeon VII while overclocked and stock, that card was beautiful build wise, but a horror story performance wise.",
      "I actually did a fresh install of windows 11 on this PC, so it has no other drivers on it. However I might ddu the current drivers and start anew, maybe a bad file is in there somewhere. I see lots of big YouTubers with similar videos where it works fine. My issue is i use it every day, so maybe I'm more prone to finding these issues.",
      "I had issues at first, but most of all my issues were resolved with the most recent drivers, [3975](https://www.intel.com/content/www/us/en/download/729157/intel-arc-graphics-windows-dch-driver-beta.html). However, when installing the drivers I literally install the drivers ONLY, no software.\n\nEver since I've had no issues. Even with prior drivers, most of my stability issues went away with ONLY installing the driver.\n\n&#x200B;\n\nMy main games I play with no issues:\n\n* Modern Warefare 2 (DX12)\n* Planetside 2 (DX11)\n* Overwatch 2 (DX11)\n* Medal Of Honor Allied Assault (DX8)\n* Halo Infinite (DX12)\n* CS:GO (DX9)",
      "Darn I wish I was in your shoes. Glad you have had a good time with it.",
      "Windows stock drivers might have downloaded themselves. Its really annoying",
      "[Unrelated to Intel/Arc driver issues] but a 3600Mhz set of ram + running the infinity fabric @ 1800 (1:1) will help with 1%/0.1% lows. While the memory controller on Zen3 is far better than previous gens, from my experience I've noticed there's still performance on the table when running ram a bit beyond the \"rated\" speed in tandem with matching it with the IF speed",
      "I appreciate your word of advice, I am aware that the ram isn't perfect, but I am sooner to blow 300 on a card that doesn't suck my cock",
      "I will say I have seen lots of comments here and the acr specific subreddit saying they have zero issues. I may be the worst case I have seen. The performance is honestly great when it works. And the feature set with av1 is great.",
      "Fairly redundant but true",
      "Dawid Does, did this already ( he wasn't alone )....",
      "Hmm I saw you posted about battlefield blue screening on the previous card.  Is this system definitely stable? \n\nThe no VR support is good to know and disappointing :(",
      "which driver version are you using?   \nmay need to use ddu (Display Driver Uninstaller) to uninstall drivers and install the new ones.\n\nI haven't had many problems if any with my a770 16gb version",
      "If you have any driver related issues take notes of what they are which game what it's actually doing and happened and report all driver bugs and faults to Intel so they can fix it.",
      "I have A770 for a months now and no problem so far. Everything runs without problem, just 1-2 older game with a little stuttering but nothing else. (12700K)"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "ASRock Challenger Arc A750 GPU Drops to $179 at Newegg",
    "selftext": "",
    "comments": [
      "Now that's a killer price.",
      "Night and Day difference from the launch, my main issues are older games that the developers won't release patches for like Fallout 3 - requiring user tweaks for them to run.\n\nDetroit : Become Human is the only newer game that I really have problems with anymore. Intel says it's up to the developer to fix it *shrugs*",
      "A750 is just slightly slower than the a770.\n\nAnyway still wouldn't buy given the rx 6600 is also $180.",
      "Which NVIDIA card is this comparable to in terms of performance?",
      "RTX 3060",
      "Went up $10 to $190 apparently but...\n\nhttps://www.newegg.com/asrock-radeon-rx-6600-rx6600-cld-8g/p/N82E16814930066",
      "How’s the driver support? I know the Arc cards were pretty glitchy at first. Any improvement on that front?",
      "I thought the a770 was comparable to rtx 3060?",
      "Nope! But it’s better to recoup some money than none.",
      "They both are more or less comparable to the RTX 3060, the gap between the A770 and A750 is much smaller than the gap between the RTX 3060ti and RTX 3060.\n\nThe 3060 will run better most of the time in older titles, ARC will run better in most newer titles.",
      "That’s a good deal. Comes with Starfield too. Thanks!",
      "There's like no way Intel is making money off of them",
      "Damm that is a great deal!",
      "Practically giving them away 👀👀👀",
      "\\>me having just gotten a non-returnable $165 sapphire RX 6600 from ebay yesterday\n\nOh well I use Linux anyway and Arc is way worse there for now compared to Windows. \n\n*For now*\n\nFor most Windows users the a750 is now the better buy at this price though.",
      "Beat bang for the buck GPU on the market now, by a large margin",
      "Detroit : Become Human doesn't work period with Intel Graphics",
      "if you're having weird stutters on the steam version of detroit become human, you have to go into the steam folder and delete the vulkan overlay files which are automatically generated every time you boot up steam",
      "Fallout 3 also runs badly on AMD. It's one of those games you need Nvidia for and I don't think that will change. \n\nI have dxvk for it on an AMD build and it still loses out to an old 1060. I've tested a 3060 ti and 4090 with it, and it's smooth there.\n\nAlso, I always love seeing a fellow FO3 fan, have you tried out the HD Overhaul mod? It was released really recently, looks absolutely fantastic, I have it loaded after NMC just in case NMC does anything it doesn't (but it probably has everything), but it also covers the DLCs and is just flat out more detailed and amazing. Looks like a modern game again.",
      "Does anyone have an a750 and can give a real world run down as to whether or not this is worth it? Currently my wife’s machine is a 2060S and believe it or not THATs more than she needs so I’d like to use that card in a frankenvuild for a friend if the a750 is worth it. She plays a lot of like build it type games like civ, tycoon games, Minecraft with the fam on game nights etc…"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Intel Arc A750 & A770 Meta Review",
    "selftext": "- compilation of 11 launch reviews with ~2240 gaming benchmarks at all resolutions\n- only benchmarks at real games compiled, not included any 3DMark & Unigine benchmarks\n- geometric mean in all cases\n- standard rasterizer performance without ray-tracing and/or DLSS/FSR/XeSS\n- extra ray-tracing benchmarks after the standard rasterizer benchmarks (at 1080p)\n- stock performance on (usual) reference/FE boards, no overclocking\n- factory overclocked cards _(results marked in italics)_ were normalized to reference clocks/performance, but just for the overall performance average (so the listings show the original result, just the index has been normalized)\n- missing results were interpolated (for a more accurate average) based on the available & former results\n- performance average is (moderate) weighted in favor of reviews with more benchmarks\n- retailer prices and all price/performance calculations based on German retail prices of price search engine \"Geizhals\" on October 9, 2022\n- for the full results plus some more explanations check [3DCenter's launch analysis](https://www.3dcenter.org/artikel/launch-analyse-intel-arc-a750-a770)\n\n&nbsp;\n\n1080p|Tests|6600|6600XT|6650XT|3050|3060|3060Ti|A750|A770LE\n|:--|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n[ComputerBase](https://www.computerbase.de/2022-10/intel-arc-a770-a750-limited-test/)|(10)|-|-|_124%_|81%|114%|143%|100%|107%\n[Eurogamer](https://www.eurogamer.net/digitalfoundry-2022-intel-arc-7-a770-a750-review)|(8)|-|116.4%|-|-|101.6%|131.2%|100%|108.5%\n[KitGuru](https://www.kitguru.net/components/graphic-cards/dominic-moass/intel-arc-a750-limited-edition-review/)|(10)|95.1%|_110.8%_|-|-|_97.6%_|128.0%|100%|108.4%\n[Le Comptoir](https://www.comptoir-hardware.com/articles/cartes-graphiques/46698-preview-intel-arc-a770-le-16-go-a-a750-le.html)|(10)|93.8%|-|_115.5%_|-|101.8%|135.3%|100%|109.2%\n[PCGamer](https://www.pcgamer.com/intel-arc-a770-limited-edition-review-performance-benchmarks/)|(9)|99.8%|119.3%|-|78.4%|106.8%|-|100%|109.9%\n[PCGH](https://www.pcgameshardware.de/Intel-Arc-Grafikkarte-267650/Tests/A770-A750-Test-Benchmarks-Preis-Release-1404382/)|(20)|-|112.7%|118.0%|72.9%|100.3%|-|100%|107.1%\n[PC Watch](https://pc.watch.impress.co.jp/docs/column/hothot/1445247.html)|(10)|-|-|-|-|_104.2%_|-|100%|110.9%\n[PCWorld](https://www.pcworld.com/article/1341464/intel-arc-a770-a750-graphics-card-review.html)|(11)|98.7%|-|-|-|99.3%|-|100%|106.0%\n[TechPowerUp](https://www.techpowerup.com/review/intel-arc-a750/)|(25)|100%|116%|-|76%|104%|132%|100%|106%\n[TechSpot](https://www.techspot.com/review/2542-intel-arc-a770-a750/)|(10)|99.7%|112.1%|119.1%|75.3%|104.7%|130.6%|100%|105.8%\n[Tom's Hardware](https://www.tomshardware.com/reviews/intel-arc-a750-limited-edition-review)|(8)|95.4%|111.5%|113.7%|72.6%|98.8%|128.4%|100%|111.9%\n**average 1080p performance**||**98.4%**|**113.8%**|**118.4%**|**74.6%**|**102.5%**|**131.6%**|**100%**|**107.9%**\n\n&nbsp;\n\n1440p|Tests|6600|6600XT|6650XT|3050|3060|3060Ti|A750|A770LE\n|:--|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\nComputerBase|(10)|-|-|_112%_|74%|107%|137%|100%|109%\nEurogamer|(8)|-|104.6%|-|-|95.8%|126.0%|100%|108.7%\nKitGuru|(10)|86.6%|_102.4%_|-|-|_93.6%_|124.5%|100%|110.9%\nLe Comptoir|(10)|85.0%|-|_104.2%_|-|97.1%|130.6%|100%|110.1%\nPCGamer|(9)|92.3%|111.5%|-|74.8%|103.7%|-|100%|112.6%\nPCGH|(20)|-|104.2%|109.6%|69.5%|97.0%|-|100%|108.8%\nPC Watch|(10)|-|-|-|-|_101.7%_|-|100%|114.4%\nPCWorld|(11)|86.9%|-|-|-|94.2%|-|100%|108.2%\nTechPowerUp|(25)|87%|103%|-|69%|96%|125%|100%|107%\nTechSpot|(10)|86.6%|98.3%|105.2%|68.7%|94.4%|123.8%|100%|106.9%\nTom's Hardware|(8)|85.7%|102.0%|104.1%|69.1%|95.4%|126.7%|100%|112.7%\n**average 1440p Performance**||**88.4%**|**103.3%**|**107.8%**|**69.4%**|**97.0%**|**127.2%**|**100%**|**109.4%**\n\n&nbsp;\n\n2160p|Tests|6600|6600XT|6650XT|3050|3060|3060Ti|A750|A770LE\n|:--|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\nEurogamer|(8)|-|93.4%|-|-|92.9%|124.3%|100%|110.2%\nKitGuru|(10)|75.8%|_89.0%_|-|-|_96.8%_|132.0%|100%|120.5%\nPCGamer|(9)|80.9%|99.0%|-|68.9%|97.2%|-|100%|112.6%\nPCGH|(20)|-|96.5%|102.2%|69.4%|99.8%|-|100%|117.6%\nPC Watch|(11)|-|-|-|-|_104.5%_|-|100%|123.6%\nTechPowerUp|(25)|74%|88%|-|64%|92%|122%|100%|109%\n**average 2160p Performance**||**78.5%**|**93.3%**|**~98%**|**67.0%**|**96.4%**|**127.3%**|**100%**|**114.6%**\n\n&nbsp;\n\nRT@1080p|Tests|6600|6600XT|6650XT|3050|3060|3060Ti|A750|A770LE\n|:--|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\nComputerBase|(4)|-|-|_84%_|74%|115%|148%|100%|111%\nLe Comptoir|(10)|60.1%|-|_73.7%_|-|101.4%|138.9%|100%|107.3%\nPCGH|(10)|-|80.2%|83.8%|73.7%|103.5%|-|100%|119.4%\nTechPowerUp|(8)|67.1%|78.5%|-|67.2%|93.2%|120.7%|100%|107.6%\nTom's Hardware|(5)|62.1%|73.9%|76.1%|65.2%|93.0%|125.0%|100%|114.3%\n**average RT Performance**||**66.5%**|**76.7%**|**80.5%**|**70.3%**|**100.1%**|**131.8%**|**100%**|**112.3%**\n\n&nbsp;\n\n&nbsp;|6600|6600XT|6650XT|3050|3060|3060Ti|A750|A770LE\n|:--|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\nGen & Mem|RDNA2 8GB|RDNA2 8GB|RDNA2 8GB|Ampere 8GB|Ampere 12GB|Ampere 8GB|Alchemist 8GB|Alchemist 16GB\n1080p Perf|98.4%|113.8%|118.4%|74.6%|102.5%|131.6%|100%|107.9%\n1440p Perf|88.4%|103.3%|107.8%|69.4%|97.0%|127.2%|100%|109.4%\n2160p Perf|78.5%|93.3%|~98%|67.0%|96.4%|127.3%|100%|114.6%\nRT@1080p Perf|66.5%|76.7%|80.5%|70.3%|100.1%|131.8%|100%|112.3%\nU.S. MSRP|$329|$379|$399|$249|$329|$399|$289|$349\nGER Retail|290€|380€|380€|300€|380€|470€|~350€|~420€\nPrice/Perf 1080p|119%|105%|109%|87%|94%|98%|100%|90%\nPrice/Perf 1440p|107%|95%|99%|81%|89%|95%|100%|91%\nPrice/Perf 2160p|95%|86%|90%|78%|89%|95%|100%|95%\nPrice/Perf RayTracing|80%|71%|74%|82%|92%|98%|100%|94%\nofficial TDP|132W|160W|180W|130W|170W|200W|225W|225W\nIdle Draw|4W|5W|~5W|9W|13W|10W|40W|46W\nGaming Draw|131W|159W|177W|129W|172W|202W|208W|223W\nEfficiency 1440p|140%|135%|127%|112%|117%|131%|100%|102%\n\n&nbsp;\n\nSource: [3DCenter.org](https://www.3dcenter.org/artikel/launch-analyse-intel-arc-a750-a770)",
    "comments": [
      "Tbh Props to Intel for making a card better than a 3060 as their first gpu",
      "Last gamers Nexus benchmarks the A770 was right behind the 3070 in some cases. So once Intel fixes the driver issues I  really want to see how it shines",
      "Better seems like too much of a blanket statement, especially with the long list of caveats for the A770.",
      "These cards were manufactured Q1 this year (based on the GN teardown video). Drivers have been worked on since then (if not earlier). And the reason these were delayed as much as they were was because of the drivers. I'm gonna go out on a limb here and say that \"a few months of driver work\" have a high chance of amounting to nothing. It could go either way. I am hopeful, but don't count on it.",
      "Getting it into the hands of users is the key to making \"game-ready\" driver updates.\n\nSome things will not fix the issues with pre-DX12 games/engines. However, I will at least give Intel props on this - they've been clear they're looking forward with this platform. \n\nDoes that hurt adoption rates in the short term? Yes. But Intel has been pretty clear that these cards aren't for everyone, but that the development of the platform and the drivers is a forward-looking project.",
      "So all in all, A770 is just beating RTX 3060 and the RTX 3060 Ti smacks them both around.\n\nSounds about right to me. Hopefully they're able to get drivers better, but I don't have any hope for non-DX12 games.",
      "Even if the drivers get sorted, I don't think that solves the old titles issues? I know they are emulating directx9, so I imagine that will take more than driver optimizations to sort out. Next intel cards might be out before that is fixed.",
      "With a few months of driver work it'll FineWine(tm). In some games it's almost 3070 levels and in 1 or 2 compute tests it was hitting 3080 levels.\n\n16GB variant could be an ML monster for the price.",
      "Holy shit, the idle draw. I've missed that part up until now. That's a no thank you from me. That and a bit too much power draw in general.",
      "Yeah [just look at what 2 years did for their DX11 driver!](https://media.discordapp.net/attachments/682674504878522386/999402021474009198/unknown.png?width=1595&height=897)\n\nOh wait.",
      "Valve's own Proton compatibility layer operates in similar fashion to whatever Intel is using, and Proton is sometimes even capable of out-performing native support. I'm 100% confident Intel can make improvements, it just takes time.",
      "same, I noticed it recently too. I thought it's just inefficient at gaming and for casual use it would be ok. Can't buy with this idle / multimonitor draw. If you're European then when long term running costs are considered, it's straight up 6700xt/3070 price point competitor.",
      "True. Maybe 50% fps disadvantage for Arc A700 on older games. But still way over 100 fps.",
      "I don't necessarily think it's locking out older gamers. A decent-spec modern PC with a higher-end Arc card should hit 150+ FPS in CS:GO. Keeping in mind most people also don't run a monitor with a refresh rate higher than 144Hz, I don't think this will make the card completely out of reach for budget/mid-range gamers.",
      "Well now they're under market pressure with a real release out and \"many eyes\" reporting bugs.\n\nTake a look at how stable and fast they are under the open source linux drivers, for example. That's the driver where some tests were hitting 3080 levels.",
      "As much as I want Intel to succeed in the GPU market, and as much as the feature suite is extremely compelling and fully competitive with Nvidia, there’s really no reason to buy either of these cards when the RX 6600, RX 6600 XT and RX 6650 XT are all such amazing $200-300 options.",
      "Right, and I think it was Steve (GN) that said alot of the older games where it falls behind, it's still plenty fps avaliable.",
      "from the same page no, 6750xt jumps from 7w to 39w (6700xt 33w). it's recent chip so recent drivers too. yet still below arc. video playback is 20w for 6750xt, while arc is at 50w...\n\nhttps://www.techpowerup.com/review/intel-arc-a770/38.html https://www.techpowerup.com/review/asus-radeon-rx-6750-xt-strix-oc/35.html",
      "Right, I am i Europe. So I'd much rather run an undervolted 6700xt for efficiency.",
      "Both Ryzen 1000+ and 8th gen Intel support Rebar. Alder Lake represents a fair leap over ol Sky Lake, and each Ryzen generation improved quite significantly."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Im very concerned about buying an arc a750.",
    "selftext": "I'll just ask a couple of questions to get some of my confusion away from everything.\n\nThe fact is, i've got a killer deal on a prebuilt with an a750, a rebar and sam compatible mobo and an i3-12100f.\n\nI'm just scared because these arc cards are apparantly still in their infancy and not fully developed.\n\nHere are my questions;\n\n\\-I will probably play alot of newer AAA titles (which from what i've seen they all work fine) the thing is, i also want to play alot of older games as well (like perhaps far cry games, crisis games, just cause games, bioshock games and more). The reason i'm confused about this is because apparantly games which use older versions of directx run poorly on these cards. So shortly, will games with older dx versions run as well? (especially with all the driver updates the cards have been getting)\n\n&#x200B;\n\n\\-My second question is about some games i personally play. I want to make sure that, TF2,CSGO and potentially LoL all run well. I'm pretty sure they do, just that i couldn't find any benchmarks on tf2 or league.\n\n&#x200B;\n\n\\-Lastly, as newer drivers were released the cards had performance increases and better compatibility. Are these still occurring? If so at what kind of rate?\n\n&#x200B;\n\nThanks for anyone who took the time to answer my questions. To be honest, if i can't get this card i'll probably settle with an rx 580 and at worse a 1650. Hoping i get a good turnout, once again.. Thanks",
    "comments": [
      "Then simply don't buy it.",
      "The last driver release for the ARC video cards made HUGE improvements, even for older titles.  It actually was enough to open the door up for Intel to be a legit contender in the video card market, even right now with the current lineup.  It also brings hope to the future intel cards being more competitive.",
      "I am having good results with my A770 16gb. I've mostly been playing newer games so idk about older DX versions.",
      "source engine games like csgo and tf2 run fine. LoL runs fine too\nif you couldn't find LoL running with an arc card, it's because you didn't look very hard.\nthis is 3rd result when searching \"intel arc lol\" on youtube\nhttps://www.youtube.com/watch?v=KM9xTcq1MTU\n\ncsgo\nhttps://youtu.be/DMscPJ_ojSU?si=ngbt450205EBMdID&t=563\n\n\ndont settle for a 1650 or rx580, the arc a750 is much more powerful than either of those two cards. if you're going to settle for a similar tier card, it'd be a 3060 or 6600xt",
      "To be fair, no drivers we/are ready for Starfield.  It was yet another premature failed game launch.  They're rushing these games out before they're ready to appease the shareholders of the company.  People need to stop paying these companies money until they have a game ready and deserve your money.",
      "Intel didn't get a review copy to make drivers with, so it's not entirely their fault there.",
      "I used an A770 for (iirc) ~6 months; in that time the drivers went from being literally unusable like 25% of the time, to being just a tad buggy in some games\n\nCS:GO ran great for me, but pretty much everything runs that great; I never tested tf2 or LoL\n\n\nI did swap away from intel GPU(s) because of driver issues in certain older games (bioshock being one of them); otherwise Arc GPUs are pretty good for the price",
      "Even in the worst case scenarios, ARC a750 will still be much better than a RX 580. Go for it.",
      "\"Does anyone have information on how these games play on this card?\"\n\n\"If you're worried about it, don't buy it.\"\n\n10 upvotes. Okay, reddit. Okay.",
      "I would be more worried about the 12100F than the GPU. if you're playing new titles the ark gpus aren't too bad at all and are completely different than what they were when they released, don't let a bunch of people who have never used touched or even seen one in person dissuade you if the price is right. I personally have a 4080 but I have high hopes for battle mage and can't wait to see what Intel can do now that they have a little experience",
      "Well Arc has XeSS which amounts to the same thing - except like DLSS it needs to be baked into the game (unlike FSR) and adoption has been slow .\n\nI'd have a little more faith. Intel has MASSIVE resources to throw at this particular problem, and if it wasn't for the fact that I want to have a usable 1440p experience, I'd have bought an A750 by now. The incredible media capabilities it has are the icing on top.\n\nThey're playing catchup for sure, but this is no friggin Noble Dragon Hung Low silicon effort. It's Intel bruv!",
      "as an addition i'll be upgrading from an i5-7400 with no graphics card (integrated)",
      "I've been gaming on an Intel Arc A380 @ 1080p / 60fps Med-High settings since release and everything has been fine, driver keeps improving.",
      "With current triple A titles, I wouldn't reccomend anything less than an i5 and an A770. I bought 3 A770s for testing and at the time shortly afyer release, they were crap. But as I understand it now, they are pretty dang good with all of the driver updates. But I wouldn't go below that i5 and a 16G  A770 range for longevity and playability reasons. You might get away with it at low to medium settings for a kittke while, but not for long.",
      "In general, arc gpus are still not recommended for users who don't want to encounter problems. If you are one of those users who want to fiddle with things and early adopters, it's definitely for you. I personally would stay away from it until it's good to use in general. For now, I'll stick with Nvidia as I use the gpu for work.",
      "Pretty sure that Intel arc has been high tiering themselves for giving older games incredible FPS boosts and overall quality in games like AC unity for example, I think Intel are dedicated and is trying to get these cards to a standard against money scammers r us Nvidia and simplistic AMD",
      "I’m using an a750 in Ubuntu. It even works well in Linux. I can play games that don’t work on my windows pc with 6900xt like enemy territory.",
      "Then don't buy arc, arc is cool but it's basically a beta, I'd u don't have much cash, buy amd card",
      "I don't know what specific deal that is, but the RX7600 has gone on sale for around Arc A750 prices. In Europe Its consistently around Arc A750 pricing to begin with. When these cards are similarly priced, the RX7600 outperforms it overall. Not only handily in DX12, but also by a larger margin in DX11.",
      "I play several DX11 and DX9 titles and they all work, but sometimes a little tweaking is necessary. \n\nI have owned an A770 since December last year, and while I have experienced some issues on older games, they are (almost) always fixed with DXVK. Basically you just drag the same set of files to each older game's directory."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "a750"
    ],
    "title": "ASRock launches Arc A770/A750 Challenger SE graphics cards",
    "selftext": "",
    "comments": [
      "article says they will be available in japan on the 12th. Other places \"mid july\" so yeah, wouldn't expect them to be available anywhere right now.",
      "doesn't look like it. The source for it posted it on the 5th, and asrock will be shipping the cards starting this month.",
      "yeah, these are new it seems. It makes sense to think it was old though since a770 and a750 are old hat at this point. The article starts out with \"with no battlemage in sight...\". It doesn't sound good to me to have the next generation nowhere to be found and for partners to be launching new versions of the existing stuff this late in the stage.",
      "Can’t be found anywhere, except all the articles talking about the performance gains. They probably bought left over chips for a discount and released some budget friendly cards.",
      "Oh, i probably confused it with some other ASRock arc cards",
      "Isn't this old news?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Intel Arc A750 Graphics Card Demoed In Control at 1440p High, Faster Than RTX 3060 & RX 6600 XT",
    "selftext": "",
    "comments": [
      ">The specs for the Intel Arc A750 Limited Edition graphics card include a cut-down ACM-G10 GPU with 448 EUs, 3584 ALUs, and 12 GB of GDDR6 memory running across a 192-bit bus at 16 Gbps, and a TGP around 200W.\n\nWell that's just plain wrong. It's 8GB across a 256 bit bus. Also, I think they STILL have not given us the official EU and Xe core count. 448 is what MLID is saying, and it could be right based on the fact he was right about it being 8GB, which Intel has already confirmed themselves. That also means the A770 at max could be 14% faster, but realistically will probably only be 10% ahead of this.",
      "No. They'll show that off later. No idea when. Some say these are supposed to launch in 2 days on the 5th, so I don't get how.",
      "It’s doable with the 3060 with dlss.",
      "And with Ray tracing on?",
      "I don't understand the issue with the naming.  \n\n\nA is for the generation (Arc), 7 is the model line and the last two are version of that model line.  \n\n\nNext Gen will be B###, then C### etc.",
      "Why is it that when I shared the direct youTube video it got downvoted to 0, and when Wccftech just shares an article with false information it gets upvoted. I'm confused.",
      "When XeSS launches in the coming month it should be doable. The RT hardware in Intel's GPUs could be close to Nvidia. Technically it's more advanced on some levels, but that does mean it's as good since I think they have dedicated less die space to it than Nvidia. Way ahead of AMD most likely, though.",
      "If it can barely do 1440p60, raytracing probably isn't a great idea.",
      "Nah, I disagree. First, the letter for a generation. Then the usual 3/5/7/9 level. Then the position within said level. Sounds almost better than what nvidia and AMD have to me. A rare thing with intel - naming that actually makes sense.",
      "Assuming not 100% bandwidth limited, A770 could clock higher too due to better binning making the max more like 20-25% higher.",
      "Oh, I don't know where to start. If you don't notice how bad it is... then I don't even know how to begin to explain it honestly. 1035G4, L13G4, 10510u, 1065G7, L16G7, 10610u - all in one gen beside each other. Everything mixed up together, no coherent meaningful naming whatsoever. Tiger lake H (35) and H (45) processors with vastly different performance under the same letter. It's a minefield which you can orient in only if you are an enthusiast or if you look up every 'i<somethingmeaningless> model' and compare directly.\n\nIn comparison with that ARCs naming is a breath of fresh air, straightforward, no Ti / Super / RTX 20 GTX 16 coexisting / XT bullshit.\n\nEDIT: Also. \"K\" means unlocked, right? You don't need overclocking you think so you get a 12600. Intel says fuck you - less cache and 4 fewer cores to you!",
      "Depends on the volume of a770.  It's possible 750 is the big sku for board partners and 770 is Intel only.  \n\nI'd expect at least an extra 5% clock speed though on top of 14% more units, similar to 6900xt vs 6800xt.",
      "Linus literally just touched one settings twice. Ona a borked early software. Is this really how we measure oc potential now?",
      "Going from A380 tests, it sits neatly in the middle of AMD and Nvidias efforts in raytracing, so a great first effort if it scales up.",
      "I actually kind of doubt that RT performance is held back much by drivers. I think it's probably straight forward. Same with machine learning. A 770 should be at RTX 3080 levels of or even 3090 levels. \n\nI actually would not be shocked if just the rasterization part if broken, or unoptimized on A770, and underperforming by like 30% of what they were expecting, and RT performance is acting like intended. In a way the card might be like an RTX 3060ti in rasterization, with the RT and tensor corers of an RTX 3080 bolted on.",
      "Yeah, The A in A380 etc isn't for ARC it's for Alchemist. Next gens model will be the ARC B380, where B will be 'Battlemage'",
      "Decent hardware that will be handicapped by awful drivers. Hopefully Intel can get that sorted out faster than AMD with the 5000 series cards.",
      "Is Intel Alchemist discrete GPU available in China yet? When does it come to North America?",
      "And were they allowed DLSS and FSR?",
      ">A rare thing with intel - naming that actually makes sense.\n\nCan you explain where this comes from? Intel naming scheme for consumer products is dead simple, and has been like that since forever. Mobility chip naming could be called wonky I guess, but it still gives you enough information about the product."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Acer Arc A750 8GB GPU is now available for just $169 - VideoCardz.com",
    "selftext": "",
    "comments": [
      "Problem is the A750 is competing where around the 6600/6650XT in performance, considering how cheap those GPUs have gotten, the A750 has to be cheaper because of the drivers (To this day, I'm still having driver issues, specifically the driver crashing randomly when using the hardware encoder, maybe about 5 months ago I never had issues with drivers crashing, but one of the games I played back then was unplayable).\n\nYou can get a 6600 for $185 and 6650XT for $210.",
      "Does Intel have so much overstock that they need to sell them at a loss? There is no way they are making any money at this price.",
      "Makes me wonder what the battle age price point is going to be.",
      "Not this low if it's a good product. They aren't going to sell the B series at a loss if they can avoid it.",
      "Most likely. People dont want to \"beta\" test them so they can have good, stable drivers in a couple of years. Was and still is the same with AMD gpus.",
      "Lately I've been pushing near RTX 3070 levels of performance in some games, which is darn nice for my A770 16 GB.",
      "They still have an uphill battle of market penetration.\n\nAlchemist isn't selling well and has an *abysmal* reputation over driver issues.\n\nBattlemage should be undercut vs the competition, finally geared with decent drivers, and spend its cycle gaining trust.\n\n*then*, and *only then* can celestial be priced more in line with the competition, per tier.\n\nMaybe even wait till druid to hike prices.",
      "Yeah I’m getting between 3060 and 3060ti levels (though usually closer to 3060) of performance with my A750 over the last couple months",
      "That's is genuinely a good price for an 1080p baller build",
      "Oh, they'll still undercut AMD and Nvidia. They won't have a choice. But they won't be selling Battlemage at a loss if it's good. Maybe only 10% to 20% margins instead of 50% to 100% margins like AMD and Nvidia.",
      "So is this a significant upgrade coming from a 2060?",
      "HUGE upgrade. It was a much bigger upgrade from my 2060 than I expected. Especially at 1440p",
      "Hmm. Should I sell my 2060 and get the ARC 750 then? Or would you recommend 770. Not sure how much more it is",
      "The A770 isn’t THAT much better than the A750, just a huge improvement in VRAM. For best value and bang for buck the A750 is the better value. There are a few posts in r/intelarc that lay it all out there better than I could. But definitely recommend. I would snag an A750 first and get it installed and updated and live with it for a few weeks before you sell your 2060 just in case you decide it’s not for you. But I’ve loved mine",
      "Still too much."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Intel Arc A750 Limited Edition Graphics Card Performance Showcase",
    "selftext": "",
    "comments": [
      "I know the state of Intel GPU’s is on super rocky ground and the drivers are broken etc. \n\nBut man do I REALLY want them to succeed.  A third competitor in the GPU space would be HUGE even if they spend the first few gens sticking to low/mid tier brackets.",
      "i am more worried about the alleged talks of cancelling the project. i know it's from MLID and all, but still, he does appear to have *some* source at intel at least.",
      "The 3060 Ti seems to be 10-20% faster in nearly the exact same scenes that they showcased. Quite a few dips below 60, and that's just the beginning. It gets much worse in clustered combat. So it checks out to probably be a little bit faster than a 3060 here.  \n\nThey really have to put up or shut up. No more teases, dangling the same 5 cherrypicked games while deliberately hiding the rest. No more Mr. corporate nice guy coming onto Techtubers show to butter people up. Get the card into real people's hands so they can do some actual benchmarking.",
      "Arc will succeed for 2 simple reasons, it will be in damn near every mobile system & pre-built PC.",
      "I just want intel to disrupt the market, it does not matter if they can or cannot beat team green and team red, if they have better price offerings, i am going with them",
      "He said they'll beat Nvidia by 20% in fps per dollar in these select titles, so this better be $299. 10% faster than a 3060 and 10% cheaper.\n\nEven at that price you're taking a leap of faith going Intel. This thing could be 3060ti or even 3070 performance in a years time in new dx12 titles, or it could be 3060 levels forever.",
      "Yeah looks like they're desperately trying to prop up this semi-broken turd before it hits the retail. The card might not be as bad as it looks, but goddamn do the drivers suck major knob at the moment. \n\nIf I was Intel, I'd make sure the e-sport titles work flawless on those cards first then move onto other stuff.",
      "He probably just latched onto the faintest rumor and peddled it as if it was the largest possibility instead of just a few executives grumbling about mismanagement. Probably was angry that Intel outright discredited him, which he made a rant about in his video peddling this rumor.\n\n&#x200B;\n\nAt most, the gaming side would be cancelled, but compute and iGPU would still be developed. Even then, this is unlikely, as all the components required to make a dGPU would already be there if you have to cover those two categories anyways.\n\n&#x200B;\n\nIt's wildly absurd to think that Intel would cancel such a product, as a lot of R&D has already gone into making it, and talent needed to be hired. And before you point out Optane, that has been ongoing for 5 years before it got canned and was struggling for years.",
      "Comparing their GPU prices to Nvidia is ridiculous, since Nvidia is hugely overpriced at the moment. AMD's ~~rx 6600~~ rx 6600 XT beats the rtx 3060 handily in everything except ray tracing. So even if this Intel GPU is beating Nvidia by 20%, it will be at best comparable to AMD's offering ... which is really bad for Intel. People who want the best possible GPU with ray tracing will go Nvidia. People who are conscious about their budget will go AMD.",
      "Using Jarrod’sTech’s RX 6600 vs RTX 3060 comparison, this card does appear to be meeting Intel’s performance claims at least in this one game, but the drivers are still absolutely horrible.",
      "It was hilarious when he noped out and hurriedly scrolled back up as soon as they got to the ray tracing settings at the bottom of the menu (which were all disabled).",
      "It doesn’t beat the 3060 in anything, according to basically every comparison of the two by a major TechTuber. The 6600 is still easily the better buy, what with the cheapest one being just $260 right now, but it isn’t a faster card than the 3060.",
      "I'm a regular consumer of MLID, but without anyone jumping down my throat, I'm just gonna have to wait and see. He's had some good stuff on Intel since at least Alderlake, but the thing is that he seems really upset that Intel people were insinuating he was a liar for saying there'd be a A780, or 512 eu Arc, while others in January were saying that their intel sources hadn't heard of sych a product. He may be open to being biased to the ultra negative side of sources he has but yeah. I really hope Arc pulls through.",
      "You know i dealt with another version of you a few months back. [Guess how his take aged....](https://www.reddit.com/r/hardware/comments/skd37h/comment/hvlkkr3/?utm_source=share&utm_medium=web2x&context=3) relevant links in there too",
      "You're thinking of Krzanich , previous previous CEO the fu*k up who basically collected premiums on previous engineering work and did fuck much else ... Keller only joined Intel in 2018, and left in 2020, and he's a tech guy, he worked on Zen in AMD, basically he kinda did run Intel into the ground sort of /s",
      "I'm subscribed to this channel on YouTube and I can even see it on their channel page but for whatever reason this video does not appear in my subscription page.",
      "their mistake was teaming with raja, who cant fit in the same room with his own ego. He beefed with the interim/acting CEO and was so toxic that Jim Keller, who was even doing some oversight on the GPU, bailed from the toxicity.",
      "No, once they cancel Battlemage it's all over. No celestial. That's what the rumors are.",
      "What the heck are they gonna price these at? Its new tech on a rocky driver base. It has to be cheaper than a 3060 I.E. sub $300 to succeed.",
      "I'm still pulling for you, Arc. Get them drivers where they need to be."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "a750"
    ],
    "title": "Intel Arc A380, A750 and A770 8GB GPUs price slashed, A380 now listed for $120 - VideoCardz.com",
    "selftext": "",
    "comments": [
      "Man, I realy wish to have your prices in Europe",
      "Never buy a promise. The A380 *might* be better than that by now, but there’s no 2023 review data for it, so we really don’t know. At any rate, it has a long-ass way to go before it hits GTX 1650 Super performance, which is really the minimum you should get for this price in 2023.",
      "Wish there was a arc a380 low profile",
      "The A380 still isn’t even remotely cheap enough, considering that it loses to a GTX 1650. That class of product should really be, like, $70 by now.",
      "Cheapest gtx 1650 on newegg is $170 and amazon for $160. A380 at $120 is not bad at all, especially with more RAM, AI cores, and a way better encoder with AV1 support.",
      "It’s nonsensical to blame increasing tech prices on inflation when tech markets have found ways to exponentially improve price/performance regardless of inflation all throughout their history. The A380 is at best *identical* value to a GTX 1650 Super from four and a half years ago. That’s abysmal.",
      "So far this is true, but nobody knows what tommorow will bring.",
      "Uk prices have been comparable with the cheapest European markets (ie Germany & Netherlands) for years  and they still are. Nowt to do with Brexit. The relative strength of the dollar due to their interest rate policies are why prices are higher in europe, including the UK.",
      "meanwhile in brexit britain A770 16gb acer bifrost is gbp 427 = 484 euro = 518 usd .....  or could just get an asus strix oc RX6750 instead.",
      "Brilliant!",
      "will only buy it under $50",
      "Let’s just remember that the GTX 1650 Super launched for $160 *four years ago*, and outperformed the A380 by ~35%. That’s the only benchmark worth comparing new budget cards to, because everything else that’s available in that segment right now is trash.\n\nI’m in full agreement that the A380 might well be the relatively best option in the hellhole that is the post-2020 sub-$200 market, but when compared to the market we should have, it’s a detestable waste of sand. Sub-$200 price/performance hasn’t improved in *four and a half years*.",
      "And if they dont? It's also perfectly possible performance could go down with more stable drivers.",
      "yes indeed. even worse when you compare it to the rx 480 which launced for around €200 7 years ago, and still should be around 25% faster on average.  \nthe rx 400 and rx 500 series gpu's also where insanely good at raw performance/compute tasks compared to other gpus. since while in gaming the difference is only around 25% back then a single rx 480 could easily beat and sometimes even double the performance of a gtx 1080 in cad software and similar compute heavy things that wheren't optimized for a speciffic set of hardware and instead relied on raw performance.  \n\n\nso actually the last 7 years there hasn't really been much advancement in gpu's in some cases the ai or raytracing can be usefull however. but ofcource we have to see how well it works on low end cards, since if it works bad then the raw performance of the old 480 might still manage to beat it in such things.",
      "I mean, Alchemist seems similarly compute-heavy, but point taken.",
      "perhaps it is indeed, I didn't yet see as many benchmarks from it outside of gaming and don't own one right now.  \nif that fully is the case, there might actually be a lot of improvement in price per performance next gen, or alchemist gpu's mught be capable of much more performance(probably won't really see that for most people, but some might experience it).  \nit makes sense since typically raytracing cores can be used quite much like cuda on nvidia, so they are typically capable of quite some raw performance.  \n\n\nso sad about performance per price not going up, but intell arc indeed seems like quite much a good trend in the gpu market, since they push the prices less insanely high. perhaps next gen or such might be a lot cheaper per performance since after all this was the first gen, and so it likely had by far more reasearch and producion cost into it, due to much more reasearc being needed for a completely new line of products, also early on optimizations for reducing cost are also limited. so I by far am more angry at amd and nvidia now.\n\nopenly I hope that Intell actually uses this, since amd and nvidia increased prices so insanely much that even their first gen was a quite good or the best competor on the market despite the pricing being as high as  7 year old gpu which performed the same. this could reduce the amount of money loss on the first gen or possibly even generate a lot of proffit and name loyality so also driver support, which might make a next generation much better in price for performance. I hope.",
      "Yeah, I think Arc has a really high ceiling in terms of raw compute performance, but I doubt it’ll ever get particularly close to that ceiling in games. It feels like a Polaris-type architecture, with more features.",
      "Your arguments make no sense, even not taking features into account. And zero goalposts were moved except by yourself.",
      "You manifestly could get something equal to or better than an A380 in… hm, probably early 2020 for $100. Also, GTX 1650 Super, for $160, 35% faster than the A380, available everywhere. We still haven’t beat that, you know, and cheap cards are supposed to be *better* value than expensive ones.",
      "And if the drivers improve?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "a750"
    ],
    "title": "Intel Arc Graphics A750 + A770 Linux Gaming Performance Review",
    "selftext": "",
    "comments": [
      "They seem solid. We just don’t get insane frame rates on ancient games.",
      "Pulling for Intel, currently have an all AMD build but I plan on upgrading once I get a 4K monitor that meets all my personal requirements.. hopefully by then (a few years) Intel will have a high end card out and their drivers are fully sorted out, I’d love an all Intel build.",
      "I'm really rooting for Intel, and it feels good to see some competition, as the 3060 is mainly crushed and being sold for way more. However, AMD has the high ground on ~$300 with the 6600 and 6600 xt's, that's some crazy value for the money that can hardly be beaten.",
      "That performance in dirt rally is weirdly high",
      "I don't think anyone buying a budget card should care about raytracing.",
      "Wednesday the 12th from the looks of it",
      "Buying parts with the promise of future updates is almost always a bad idea \n\nHowever, in the case of ARC where the biggest issue seems to be drivers I would imagine these cards will age well. But we’ll see. \n\nMaybe they’ll copy AMD’s old FineWine technology lol",
      "Unless you actually like Raytracing. That's one place the ARC series seems to justify its cost vs AMD",
      "launch day is october 12th",
      "tbh,i think the cards could go even higher than 6600xt.",
      "Raytracing is not a gimmick - it's not an Nvidia creation, it's a lighting technique that's been around for some time (e.g. CGI film creation) but only relatively recently been possible to render in real time. \n\nWhether it's good value or not is entirely different, and certainly less relevant to lower priced GPUs, but it is something that is here to stay and will be improved upon by all 3 companies over time.",
      "I’d like to know how dx9 titles run as on windows it uses an translation layer IIRC so how would something like proton compare?",
      "I was watching the LTT Livestream, and it did give me hope, but it did also highlight issues with frame consistency.",
      "Then it wouldn't be constructive criticism. The article seems to answer some questions I had, therefore it was valuable to me. I have issues with Phoronix's website though:\n\n1. site stopped loading for me on page 4. Honestly, it could be Internet or my phone. But I suspect the website.\n2. I in the past I tried to give Phoronix some money, but their website makes it a hassle to do so. You have to do a bunch of steps, that I just aint going to do. I like things like Patreon sites. I recall I set up a forums account but ran into issues.  And loss interest in figuring it out.\n\n![gif](giphy|sDcfxFDozb3bO)",
      "Also the AV1 codex encoding support seems to be a selling point for a handful of people who want to stream.  I believe the RTX 3060 decodes but not encodes. The RTX 40xx also encodes. I imagine AMD's next card will also.\n\nOf course, i didn't read the whole article to see if Phoronix tested this with Mesa (page wouldn't load). It would suck if you buy if for that and it doesn't work.",
      "Considering AMDs OpenGL compatibility has been a mess through the entire time I have owned my rx580 I don’t have particularly high hopes for older APIs",
      "Raytracing is a gimmick from Nvidia to sell things for higher price. With the current market, it doesn’t make sense for buyers of mid range cards to care about raytracing.",
      ">Unless you actually like Raytracing\n\nRay tracing is only relevant at the high-end unless you enjoy playing games at 20fps. \n\nMore realistic, AV1 support is a nice thing to have.",
      "on linus tech tips they ran some games on a770 recently and TF2 and Half Life 2 were same performance as RTX 3060.",
      ">With the current market, it doesn’t make sense for buyers of mid range cards to care about raytracing.\n\nEspecially on AMD'S cards, where it might as well be broken."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "New hardware day! i5-12490F and Arc A750.",
    "selftext": "",
    "comments": [
      "as a 12400f owner I want to know what the hell is 12490f?",
      "now I am even more mad that China is getting exclusive CPUs",
      "https://www.tomshardware.com/news/intel-core-i5-12490f-review-chinas-exclusive-black-edition-gaming-chip",
      "Chinese market. Clocked like the 12500.",
      "Wtf is a 12490F?",
      "https://www.tomshardware.com/news/intel-core-i5-12490f-review-chinas-exclusive-black-edition-gaming-chip",
      "And with a black box, which is all what really matters! :)",
      "Off meta build",
      "Interesting",
      "If ever curious, look at South Korean eBay stores too and you sometimes find pretty interesting SKUs or larger inventory of harder to find western SKUs just chilling out.\n\nThat's how I nabbed a 3300X for MSRP when they were either OOS or inflated in NA.",
      "Congrats, enjoy!.",
      "Are these Made from an Intel factory in China?",
      "I ended up getting one of these. Have it on the Port Royal Hall of Fame right now 😁.\nHow you liking it?",
      "I reject the meta completely and follow my heart, no matter how weird it is!",
      "Has the cache of 12600k though. Should be basically a 13400 sans e cores.",
      "Going with a vertical ultrawide display setup? ;P",
      "Do we know if the i5-13400 will have e cores?",
      "Tie fighter with two vertical ultrawides and a central 4:3 CRT actually (no)",
      "Every leak points to a 6+4 so it looks that way."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Intel Arc A770 16GB and Arc A750 8GB available now on Newegg",
    "selftext": "Looks like the listings are now active, and the GPUs can be added to your cart and checked out. Have fun, everyone!\n\nA770 16GB is $349.99, with the A750 8GB at $289.99.\n\nEdit: It's now showing out of stock for the A770, but I'll leave this post in case they bounce back in and out throughout the day.",
    "comments": [
      "Grabbed one!",
      "Already sold out for me.  I hate this so much.",
      "I think 750 would be the card to get if you wanna support intel. However, if you are on a tight budget and want the most from your pc, 6700xt is 360 now, and 6650 is 270.",
      "https://www.anandtech.com/show/17263/intel-arc-update-alchemist-laptops-q1-desktops-q2-4mil-gpus-total-for-2022\n\n\"Intel is expecting to ship over 4 million units/GPUs for 2022\"\n\nThat was published in feb, the same month TSMC production ramped for these. Meaning it's probably an accurate accounting sourced from Intel.",
      "They made millions of units.\n\nTBD if they just sold through them all or if it's a trickle.",
      "([Availability showing on my end](https://imgur.com/a/db6j5mH))\n\nLink to Newegg landing page, if you didn't grab it from the other posts:\n\n[https://www.newegg.com/promotions/intel/22-1809/index.html](https://www.newegg.com/promotions/intel/22-1809/index.html)\n\n[Link to A770 16GB](https://www.newegg.com/intel-21p01j00ba/p/N82E16814883001?Item=N82E16814883001&Tpk=14-883-001)\n\n[Link to A750 8GB](https://www.newegg.com/intel-arc-a750-21p02j00ba/p/N82E16814883002?Item=N82E16814883002&Tpk=14-883-002)",
      "No Canadian Newegg joy for me :(",
      "I'm seeing the same thing. It's possible that they're going to go in and out of stock, as I have a hard time believing it would have completely sold through this fast. Unless the stock consisted of whatever Ryan Shrout could fit in his car's trunk to drop off at Newegg HQ this morning. There are also the ASRock A770 and A750 cards which have not yet appeared in listings, although the LE cards look much cleaner.\n\nETA: ASRock cards are listed, but not live yet.\n\nA770 **8**GB: $329.99\n\n[https://www.newegg.com/asrock-arc-a770-a770-pgd-8go/p/N82E16814930077](https://www.newegg.com/asrock-arc-a770-a770-pgd-8go/p/N82E16814930077)\n\nA750 8GB: $289.99\n\n[https://www.newegg.com/asrock-arc-a750-a750-cld-8go/p/N82E16814930078](https://www.newegg.com/asrock-arc-a750-a750-cld-8go/p/N82E16814930078)",
      "It makes no sense to me either. I mean, there can't be this much demand, especially when Nvidia/AMD cards are all in stock. Something doesn't seem right, unless they launched with 10 cards in inventory.",
      "I can see the A750 and that hasn’t gone out of stock at all.  I was able to add A770 to my cart but went out of stock again.",
      "Had one in my shopping cart but it was gone before I pulled my credit card out of my wallet.",
      "I was able to place a back order too but the 19th has come and passed with no change to my order’s status. Any luck for you?",
      "To be fair that was including both the laptop arc cards and their add in cards for prebuilts(like NUCs). I really don't think there is really as much volume left over for DIY as we would hope.",
      "How is this out of stock??? I've been checking since 8:15 est and did not see the a770 listed as available even one time....\n\n Intel didn't even officially announce where they were selling these units unitl 9am est this morning.....\n\nWhat a joke of a launch",
      "I managed to place an order for an A770 when it was listed as a back order. Newegg took my order.  My order says \"release date: 10/19/2022.\" Maybe that's when Newegg thinks they'll get the next restock.",
      "Are there any retailers up here who have this up yet?",
      "I agree with you. I woke up this morning a little late (Intel never said what time it was launching) I think it was around 11AM and there was nothing. No way these cards are that popular. Intel probably made 100 total for the whole US.",
      "My suspicion is either they have yield issues, which would explain why there seemed to be many more A750s available, or they’re purposely slowing supply so they have more time to improve the drivers.\n\nThey won’t be slammed with as many bad reviews and tech support problems if they release the supply slowly.",
      "Yeah, there's some BS going on. How is it sold out, can't be that popular when there are tons of AMD/Nvidia cards in stock.",
      "Canada Computers has it listed, but OOS for now: https://www.canadacomputers.com/index.php?cPath=43\\_557\\_5769"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "bad fps on arc a750 any help?",
    "selftext": "im getting like 15 to 40 fps on rust and i dont know why im on the latest update and it just feels like nothing a changed for this game since October.....",
    "comments": [
      "Is this on the Ryzen 2600. It doesn't support resize Bar which Intel Arc needs to function properly. With that you should be getting ~90 fps at 1440p on high graphics settings.",
      "Welp that definitely isn’t on the box when I bought it so I wouldn’t think it’s a unfinished product that the are selling",
      "Yes it on a b350 motherboard I have the option to have it enabled so I and it says it enabled in the arc control settings",
      "I think Zen 2 CPUs are too weak for current driver overhead for the ARC GPUs. I read of another user on a Ryzen 4K series APU having performance issues on Cyberpunk 2077.\n\nTheir driver has bottlenecks even in modern APIs such as DX12 and likely even Vulkan. The Golden Cove cores in 12th and 13th gen are stronger than Zen 3 nevermind Zen 2, so low end 12th gen/13th gen/Ryzen 5K/Ryzen 7K is good.",
      "Thanks man appreciate the help I ended up getting an r5 5600x and some ddr4 3600 from 3200 and I went for a choppy 40 to 60 to 100+",
      "Intel graphics card are product with unfinished drivers, by buying it you agreed to be a beta tester for Intel. \n\nSo report this issue to Intel through their website.",
      "So update took out the intel gpu and gave up on it im going to sell it or trade it and just get a 3070",
      "My test bench is currently out of action until my new CPU cooler arrives, otherwise I would test it out, but Rust is a DX11 game I believe, so it's possible that it has performance issues on ARC. \n\nYou can report issues [here](https://github.com/IGCIT/Intel-GPU-Community-Issue-Tracker-IGCIT/issues).",
      "I installed a r5 3600xt still getting max 50 fps",
      "Yeah like I can average 60+ on my rx 470 so I was just confused on why the performance is below 40 fps most of the time. Now that I know that it was an unfinished product I wish I didn’t buy it now and I could have used the 300 dollars on and actually good gpu… I guess going off of specs isn’t something I should do with Intel",
      "It looks like some motherboards are able to enable it, but it doesn't actually function properly on 2000 series.",
      "What's your ram lookin like",
      "idk why the downvote on this comment but its the reality, even polaris were cursed for its dx11 perf, they still tank alot on vulkan, dx12, and recently, opengl.\n\nfun fact you could use resizable bar on polaris gpu with 8gb of vram.",
      "are you still using arc a750? how is it performing now?",
      "Thanks man I would have gone on forever trying to get it to work right I have no problem with the cpu power it runs every great but I guess I’m being forced into upgrading",
      "24gb at 3200",
      "It’s gotten better with the gpu update no getting massive drops into 15 to 30 range I stay between 50 and 70 after some more test",
      "I love it has awesome ray tracing I get any where from 80fps to 120fps depending on the game’s settings and capabilities, but I’ve been amazing can’t wait for the next line up. Definitely sticking to intel if they keep the prices good",
      "Just picked up a r5 3600xt so hopefully",
      "I’m new I thought ram only play a part in like loading speeds not fps too lol i have to much into the build as is so looks like I’m giving up on the arc"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Rtx 3060 or Intel arc a750/rx6600",
    "selftext": "Rtx 3060(12gb) costs me $325 and\nIntel Arc a750(8gb) and RX 6600(8gb) costs me $240. \nI have i31200f cpu. Which gpu should I go for? ",
    "comments": [
      "Following this because in on the same circumstances, but just rx 6600 VS arc750.\nThe 6600 is it usd 200 brand new rn on ebay, and the a750 can be found for usd 180 brand new as well, but it looks quite more robust than the rx for the price/performace, the drivers thing seems like a concern though.",
      "rtx 3060 12 GB, because it is the only card of the 3, that has enough vram.\n\nalso the intel card would be out, even if it had enough vram, because of the driver issues."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Intel floods European market with Arc A750 GPUs, French retailer has 1233 in stock",
    "selftext": "",
    "comments": [
      "> WoW dragonflight\n\nThe game engine supports DirectX 12 and Arc GPUs run DirectX 12 games flawlessly, performance should be good.",
      "I frequently see the 750's but why not 770?",
      "Anyone have data on WoW dragonflight performance of one of these? Might be a worthy upgrade from a 1070",
      "Are you located in the EU?\n\nTheres a marginal difference between the a750 and the a770 in gaming, I want to say 10%? But I need to go back into the benchmarks by GN\n\nThey probably did a market analysis and thought the a750 would sell better in those markets\n\nThe “limited edition” is just in the name of the product and is by no means limited in terms of production capacity",
      "Anything poorly optimized is going to run horribly. I can put square tires on a car and it will run like crap regardless of how tuned that car might be and how flawlessly it will run with round. \n\nThat doesn't take away from the fact that it will run DX12 games well.",
      "That's obviously not an Arc issue, it's a specific-game optimization issue https://www.youtube.com/watch?v=KbOyz9W3aUU",
      "Probably due to yields almost all produced dies qualify for the A750 bin but markedly less are 100% perfect as needed for A770.",
      "Sorry, let me revise that. I guess you could say Dead Space DX12 runs ‘errorfully’ on Arc.",
      "Probably because the 770 is supposed to be the \"Limited Edition\", I guess so",
      "Hopefully we see some offers with this much offer",
      "My guess is nobody buys 750s.",
      "It's quite obviously an Arc issue seeing that other manufacturers cards work perfectly fine.",
      "And it doesn't run that DX12 game well, so the DX12 API is not a guarantee that Arc will run well. If AMD and Nvidia ran Dead Space at an unplayable level everyone can be suffering together in misery, but here Arc is the odd man out. \n\nThat's a good analogy. Putting an Arc gpu in your pc is like putting square tires on your car.",
      "> flawlessly\n\n[Hmmm..](https://www-computerbase-de.translate.goog/2023-01/dead-space-remake-benchmark-test/3/?_x_tr_sl=auto&_x_tr_tl=en&_x_tr_hl=en#abschnitt_das_testsystem_und_die_benchmarkszene)"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "How stable is the Intel Arc A750 for normal PC (non-gaming) use?",
    "selftext": "Anyone using the Intel Arc A750 with Lightroom Classic?\n\nThis is the last piece to my new i7 13700, Z790 Gigabtye Elite AX & 32 gb DDR5 photo editing build.\n\nI need a GPU for a photo editing build using Lightroom/photoshop. It  will do NO gaming and NO video editing. Photography editing is 2D  computations.\n\nSo I read a lot of negative reviews/remarks about the A750 with respect to gaming performance as well asstability. Is that irrelevant for my use and is this  a better value GPU than the RX 6600 for MY usage? Should I care if it  can't run certain games that I never heard of before? or would it impact  my use of Lightroom/photoshop? I also wonder if there is even more  improvement upcoming as they continue to refine the drivers? I doubt the  RX 6600 is going to see improvements. Both of these GPUs are on sale  for roughly the same price. Need to pull the trigger tomorrow and get it  built!",
    "comments": [
      "The Arc should work fine I haven't heard any complaints about it aside from gaming so you should be good to go",
      "It’s definitely better than the RX 6600 for photo editing - hell, it’s even consistently better in gaming now.",
      "Thanks...glad to see there were no issues while doing non-gaming tasks",
      "From my experience the two main areas you would see a benefit from the GPU in photo tasks are in preview rendering on higher res monitors, and some passive tasks like filters. Do you feel like there’s any improvement / disadvantage in regards to those?",
      "Early days.....I purchased the a750 and put it in the new rig I had built 7 days ago - absolutely NO issues so far....I have processed photos in LR- superfast - and watched videos on youtube. So I have not stressed it and with my usage I never will. It has been solid and a great value purchase....test will come when I update the next driver updates.",
      "At the start it was a bit unstable had random blue screen and didn’t wake up from sleep and so on. They seemed to have ironed this stuff out I have not had a blue screen for a long time. It is very stable now. The games I run have not crashed on me specially mw2 and god of war, halo infinite, apex legends.",
      "It's still a little too soon to depend on it professionally in my eyes. The 6600 will certainly be more reliable.",
      "If you end up getting it I would love to hear an update on how it works out for you! Using a lot of Photoshop/LR as well.",
      "Last time I checked? About as stable as a one legged tight rope walker. But it's been three or four months since I last checked. Bit from what I understand, things have gotten much better since then. \n\n And about three years since I lost my leg, but I can still walk a tight rope! Just not real well.",
      "I'd be more concerned about that 32 GB of RAM than a discrete GPU. GPU will barely matter, but RAM on the other hand...",
      "That's good news....trying to design a new build after 11 years with my current rig was quite an ordeal. It seems every review of components is from the POV of a gamer or overclocker since they probably are 95% of the people that build their own PCs.....most photo enthusiasts don't have the stomach to learn about pc components and shell out big money for some nice MACs.",
      "I've seen very few reviews of the A750 (maybe 4) with a lightroom/photoshop benchmark but ALL indicate it is great for 2D photo rendering - i was just scared by the ton of driver related warnings and hoped they wouldn't be an issue with my usage. THANKS!",
      "Early days.....I purchased the a750 and put it in the new rig I had built 7 days ago - absolutely NO issues so far....I have processed photos in LR- superfast - and watched videos on youtube. So I have not stressed it and with my usage I never will. It has been solid and a great value purchase....test will come when I update the next driver updates.",
      "I'd hope so considering it costs as much as a 6600xt",
      "So you bought it before the recent driver update? I don't game. I do photo editing (for a hobby, not income) and I don't want to lose a day's work because of instability.",
      "Will keep you posted if I have the nerve to buy it!  ;)",
      "Do you have any real experience with the A750?",
      "The GPU WILL matter if it is unstable, thus the query.\n\n32 gb of RAM should suffice for photo editing (NO video).",
      "There are a few uses of it in Lightroom - exporting uses it a bit. But you are right, it is of benefit more for high -res (especially multi monitor set ups) - which I don't have. I am thinking that this is a GREAT low budget GPU for those who don't game or render video.",
      "I tried looking into things to figure out why Lightroom didn't let me check the box for my 3080 ti for\"GPU acceleration\" in Lightroom. I can't remember exactly what I found, but it had to do something with the CPU is mostly going to do the heavy lifting for edits anyway, so the GPU doesn't really seem to make much difference in the end. \n\nI wish both classic and CC versions were more smooth in windows at least. I use a11800h laptop and a12900k i9 and on both computers it will still stutter and not be smooth."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "intel arc a750 triple setup",
    "selftext": "someone who knows how to adjust the monitor setup in the software for the Intel Arc A750",
    "comments": [
      "This subreddit is in manual approval mode, which means that **all submissions are automatically removed and must first be approved before they are visible**. Your post will only be approved if it concerns news or reviews related to Intel Corporation and its products or is a high quality discussion thread. Posts regarding purchase advice, cooling problems, technical support, etc... will not be approved. **If you are looking for purchasing advice please visit /r/buildapc. If you are looking for technical support please visit /r/techsupport or see the pinned /r/Intel megathread where Intel representatives and other users can assist you.**\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/intel) if you have any questions or concerns.*",
      "Just go to display settings in windows"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Arc A750 with Ryzen 7 2700x ?",
    "selftext": "I  have an Arc A750 GPU on order, and despite the  requirements/recommendations with minimum compatible processors not  showing this gen of CPU's, everything I read seems to point to the need  for ReBAR being the reason. I have an ASUS TUF Gaming X570-Plus (Wi-Fi)  motherboard which has been flashed with the latest bios and has let me  enable ReBAR in the bios even with my 2700x (and I have set all the  proper settings to allow this)\n\nI  have used GPU-Z to confirm this as being active. Should there be any  reason given this is the case that the A750 still wont work with my CPU?  The only thing I might guess would be if the currently shaky Intel  drivers simply won't work with it for some reason beyond the need for  ReBAR.\n\nFrom reading up on SAM and  ReBAR it seems some companies boards were locking people out of turning  on this feature if they detected certain CPU's but others had no issue. I  obviously feel I am in the latter camp. Short of intel drivers not  playing nice with it for whatever reason, should the A750 have no issues  in terms of functioning properly so long as ReBAR is enabled on my system?",
    "comments": [
      "So did you ever manage to benchmark this? I've a 2700x and an MSI B450 Tomahawk that has ReBAR support with the latest BIOS. But the question is would an Arc card run like crap with it. I really don't feel like upgrading both the CPU and the GPU, especially considering I barely have any time to game these days. The 2700x is still excellent for work and productivity on a budget.",
      "I have my 2700x working with ReBAR currently despite thoughts that it may be too old for some boards. I am thinking in my case it will run due to the fact that it is enabled and seems to be functioning. I checked this with gpu-z to make sure. Though I will certainly agree a newer processor wouldn't hurt, particularly to get pcie 4.0 vs 3.0 https://imgur.com/a/cDQr1ej",
      "There have been issues with Arc Control showing ReBAR as being disabled even though GPU-Z, AIDA64, Windows Device Manager and other software list it as enabled. It seems to just be a cosmetic problem that needs to be fixed.\n\nhttps://www.reddit.com/r/IntelArc/comments/zsepcv/rebar_issue_gpuz_vs_intel_arc_software/",
      "Let me guess, broke/fell out of the pcie slot?",
      "2700X is a decent productivity CPU, and was, in its time, a \"decent\" gaming CPU. But zen+ is a bottleneck these days.\n\nEven zen 2 is a bottleneck these days. Lots of benchmarks from techtube back this up. digital foundry, HUB, gamers nexus, etc. Even at 4K, if you game at such a res.\n\nZen 3 or intel 10th(ideally, 12th) gen are the minimum starting points you want to be at. and luckily 5600non-x and 12400f are cheap as dirt lately.\n\nSince you have the X570, just drop a 5600 non-x or 5600X in it. It'll run circles around the 2700X.",
      "OP, listen. You do NOT have Rebar support. You need at least a 400 series motherboard AND a 5000 series CPU. You need both to support it in order to utilize it. Just because you enabled it in the BIOS doesn't mean you have it.\n\nThe Arc runs like crap without Rebar. Therefore, unless you plan on upgrading the CPU (which you should do anyway, because a 2700x is, what, 5 years old and missing relevant technology), then DO NOT buy the Arc. Intel CEO even said that if you dont have Rebar, don't buy it. \n\nGo with a 3060ti instead.",
      "Is it for gaming or office ?",
      "I have not yet. I still do have my 2700x, but on some level now that it has been safely removed and is stored in the 5600x's plastic case and box I am tempted to just get it sold and avoid having to deal with removing the annoying cpu fan and reapplying thermal paste on both processors for a test. It would be pretty useful for the community though. I did manage to read some other stuff about older processors frame timing? being janky. Like the FPS is smooth other than various moments where it hangs and then zaps back to smooth. So consistent but with jumps and hangs?  \n\n\nI am sort of glad I upgraded my processor though, because in all honestly there are still quite a few issues and bugs to work out of these cards, and probably will be for a while. It is nice to know that any of the issues I experience are worth investigating/reporting for a fix.   \n\n\nI feel like if I was still running the 2700x, I would never be sure if an issue I am having is due to the unsupported processor, settings in my software, or legitimately a problem with the card. I did manage to figure out how to get the idle power draw down on my motherboard, despite it missing the settings to do so. So that was quite the useful trick that I hope ends up helping someone down the line at least.  \n\n\nI will be sure to update this post if I get bored and decide it would be worth running some tests for science sake. It was kind of annoying to have people try to dismiss running it with a 2700x after all. And I only just barely happened to be able to swing a new processor on top of everything else.",
      "Problem is I have no money for a new CPU atm or I certainly would. I just moved across the country and my motherboard and rx580 gpu both took a shit in shipping. New parts were not in the budget by any means, but I had to drop $220 on the new board already \\*cries\\*.   \n\n\nI was watching some benchmarks comparing the 2700x and 5600x and it seems once you went up to 1440p/4K that the differences closed in FPS as the GPU took over the heavy lifting. This doesn't of course address frame stability or whatever unfortunately which I am sure would also be much better on a new cpu.\n\nThere certainly was a noticeable gap at 1080p with the card they were using for the comparisons, no doubt about that. I won't need to go above 60fps on the TV I am using (fresh is 60hz) so I figure so long as it works at 30 to 60fps on anything I toss at it I would be happy for now. Getting a better CPU would be next on my list when I can though I totally agree, hell... if even to get pcie 4.0 vs the 3.0 I am stuck with atm.",
      "Why would windows and gpu-z show it, and why would my motherboard (an x570) allow it to be enabled if my processor doesn't support it. I have found upon further research that other people in other posts show improvements on frame rates with processors that supposedly are not supported. They showed screen shots.   \n\n\nIt seems like the processors were only software locked. I am open to the idea that even with it enabled (and everything in my system settings and gpu properties showing its on) that the processor will still handle the job too slowly to function properly, but by all reports it seems to be on according to windows and bios. They just added the 3000 series of processors after a driver update so I would think its entirely possible that its functioning.",
      "It is kind of my all in one machine. I do use it for office stuff when I am actively running my (sorta) business. Mostly sales on social media, printing, shipping, payments, etc. But it lives in my bedroom and I use it for all my entertainment as well like movies, music, gaming, graphic design (which is business as well as pleasure).   \n\n\nI would also use it for any work from home jobs that might come my way as well. The only other computer I have is a shitty half working dell laptop from 13 to 15 years ago, so I tend to want this PC to work and work well for all my needs for a few years at a time. I used my last setup for about 8 and a half years before I built this one 3 years ago. I love getting mileage from decent budget builds :) I am super impressed at how good the rx980 GPU I had was and how long it has lasted me. I was only just starting to hit acceptable to me limitations but thought I had at least a few more years to squeeze out of it.",
      "That's not right. Having it enabled doesn't mean anything if the software won't take advantage of it (for many reasons). For example, you can enable ReBAR and get all the indicators that it's enabled on AMD RX 5000 GPUs yet gain no performance since the driver doesn't support it.\n\nRefer to the [official Intel statement](https://www.intel.com/content/www/us/en/support/articles/000091128/graphics.html), not your intuition or random people on Reddit. If it worked, they would probably list it there.\n\nIf you want to verify it yourself, use your current rig to run a game that has measurable (>10% fps) gains from SAM - eg. Forza Horizon 4/5.",
      "While you may have it enabled in your BIOS, that doesn't mean you have it. Your processor doesn't support it. Therefore, you don't have it.",
      "2700X are bidding above 100 on ebay. A 5600 is $130.\n\nYou got 30 bucks laying around?\n\nThe difference is huge at 4K and 1440p. Ignore avg fps charts and look at frame time graphs, stutter, 1% lows.\n\nI game at 4k60 and saw a huge, HUGE improvement in smoothness going from a 3800X to a 5600X. Even in cases where fps is locked at 60, frame pacing issues were fixed.\n\n>  I have no money\n\nIf true you shouldn't even be here. Sell what you have, focus on survival, and buy back in when you have spare cash to pull trigger on hardware buys.",
      "Thanks for the lookout! I could see that throwing me for a major loop. I will have to run some benchmarks and see how well its running compared to others with slightly newer processors.",
      "Yeah I have a 6500xt atm I could probably do a test on something like forza like you mention. That and/or maybe some kind of benchmark utility. \n\nI can also try tossing in a Ryzen 5 5600x, that I have very temporary access to, and see if there is some night and day change at 1440p/4k with the arc card vs my 2700x. If ReBAR isn't actually functioning/providing any benefit with my 2700x CPU it should hopefully be clear by the vast difference in FPS since there seems to be little to no difference in fps improvements at these higher resolutions based on the cpu.",
      "People are reporting it as working for them in getting higher frame rates even with processors such as mine (I have found upon further research). It seems like it is indeed functioning. While I can't be certain the older processors are actually efficient in doing it, or it technically works but with little to no improvements due to some form of slow emulation is another story.   \n\n\nI have a 5600x on very temporary loan that I may try after running some benchmarks with the 2700x and see if they don't seem to meet what others are experiencing. At least with 2k to 4k resolutions where the processor power matters less and its all about the gpu. Or as another user suggested, to turn it on and off with my current card to see if it changes anything for the better as is.",
      "Hehe worse on some level! It looks totally fine... But trying out a new power supply, processor, ram, and even a new video card on the board wasn't working. Could get no POST screen at all. And when I put a new video card in the old board, it acted like it wasn't even plugged in (beep error codes). Tried everything 5 times. I finally bought and tried a new board, and it shot to life, but my original video card was throwing scrambling purple and green lines down the screen like an old dirty nintendo (even in POST/bios screens). New temp card worked flawlessly.\n\nI tried cleaning its contacts properly in case it was somehow dirty and such, but no dice. The reason I say \"almost worst\" is because of how frustrating it was to trouble shoot everything else but the main board, the waiting and wondering aspect. If it had just broke/fell at least I could have taken care of everything much faster. I now know to take my shit apart and wrap it up individually if I am going to ship something like that.",
      "My PC was damaged in shipping from a cross country move. My main board and the GPU both. So here I am... I spent $200 something already on the board, and don't want to waste more than half the cost of a decent gpu that can carry me into the future with a crappy card. My computer is an essential item to have running at least semi decently (even if gaming technically isn't). I just wasn't expecting to have to replace a MB and a GPU randomly but they needed it or else, no computer...\n\nI am not disagreeing with you that stutters and performance will improve with a better cpu, just that I don't really have more money or time to sell mine and not have a functioning machine while I wait for the other. But if I can sell my 2700x on ebay for $100 that will make holding onto a 5600x a lot easier. Though on amazon I see them for more like $165 after tax. ![gif](emote|free_emotes_pack|grin)",
      "The ebay idea for the 2700x is a great idea though, It could easily pay for more than half of a new cpu... making obtaining one much more possible."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "a750"
    ],
    "title": "Intel Arc Graphics A580 / A750 / A770 Linux Performance For Early 2024",
    "selftext": "",
    "comments": [
      "This subreddit is in manual approval mode, which means that **all submissions are automatically removed and must first be approved before they are visible**. Your post will only be approved if it concerns news or reviews related to Intel Corporation and its products or is a high quality discussion thread. Posts regarding purchase advice, cooling problems, technical support, etc... will not be approved. **If you are looking for purchasing advice please visit /r/buildapc. If you are looking for technical support please visit /r/techsupport or see the pinned /r/Intel megathread where Intel representatives and other users can assist you.**\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/intel) if you have any questions or concerns.*"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "12Gb RTX3060 Vs 8Gb Intel Arc A750. Need only for video editing .    ",
    "selftext": "Which is good for video editing \nCpu. I5-14500 \n",
    "comments": [
      "We don't talk about the Intel Arc."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "a750"
    ],
    "title": "Intel confirms upcoming Arc desktop SKUs: A770, A750, A580, A380 and A310",
    "selftext": "",
    "comments": [
      "Did they also leak in which decade they'll be released? /s",
      "No way. It's 3070 level hardware with unpolished drivers. Expect 3060Ti real-world performance.",
      "Do we have any idea how they will compare to existing gpus yet?",
      "Intel should’ve been very clear about the release timeline. I think they still can be transparent about it.",
      "IIRC the high end is supposed to be up around 3080 tier",
      "They already went transparent with it. \n\nThey told that early summer desktop GPUs will be launched as OEM only in china. \n\nThen in late summer launch OEM only worldwide. \n\nThen \"later\" launch as standalone GPUs.\n(So one could expect October/November for this)\n\nOverall it seems like they really really want to have most of the driver issues sorted out before releasing standalone GPUs.",
      "Good luck guys.",
      "This century. Stay tuned.",
      "There are low end GPU out there, it is just the value is so bad that people ignore it. With possible bad driver due to first launch, it will be irrelevant",
      "Existing in may 2022 or April 2024 when they are actually available? 🤣",
      "You expect the first iteration of GPU to meet the halo products of established GPU manufacturers such as AMD/NVIDIA?",
      "It's because Raja Koduri promised stuff. \n\nHe promised the GPUs will be ready early, in Q1 2022. \nTurned out to be a lie.\n\nThen he promised GPUs in the hand of gamers for cheap - again a lie because they prioritize OEM.\n\n\nPeople say Intel GPU will be irrelevant if they launch after Nvidia launches RTX 4000 and AMD Rx 7000. \nI don't think so because all the Nvidia and AMD products from new gen are gonna be 600$+(real price, not msrp).\nIntel's best GPU is supposed to be under 500$.",
      "the lack of vram makes me sad, though i'm not sure what i should've expected from the bus widths. wish 8-12gb was standard because 4gb is proving not to be enough and 6gb is next",
      "And I honestly agree with their way of going right now. Why do people care so much that it only gets released in China. Makes no difference if they release it worldwide now, or later.",
      "Those were likely early laptop performance leaks at reduced clocks.",
      "Yes. It has enough transistors at like 22 billion and already has RT performance better than NVidia or AMD. It's tier 4 vs Tier 3, and Tier 2 for AMD.\n\nIf you include the games where it crashes due to drivers to drag down averages it'll be like 3060ti levels.",
      ">He promised the GPUs will be ready early, in Q1 2022\n\nHe actually said that? When?",
      "I don't know for sure if it was him but there were messages from Intel that arc GPU lineup will roll out in Q1 2022."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "[GamersNexus] Intel Arc A750 GPU Hands-On, Driver Challenges, & Overclocking",
    "selftext": "",
    "comments": [
      "This was a really good overview, and the Intel guy seemed pretty open.  I just wish they had revealed some real hints at performance and release dates. GamersNexus did a good job (as usual) with what they have..",
      "Absolutely loved the entire video, no BS and people that were excited to talk and work with each other.",
      "Performance is an easy guess.\n\nthe 8 Xe core GPU's confirmed performance is ~1050Ti.\n\n32 Xe cores would be 4x that in compute, but it also gets a wider memory bus, so picks up a few more % fps.\n\nOn napkin math, that puts it somewhere around a 3060/3070, which is in line with leaks from last year.",
      "Pretty insane to think about all the hard work behind the current performance of arc GPUs, even if we think its lackluster compared to Nvidia/AMD.",
      "It's their first effort at a \"real\" GPU. Nobody expected them to flounce on Nvidia and AMD out of the gate given their history in the iGPU space.\n\nBy their C or D gen they should have the knowledge under their hats to be equals though.\n\nAfter they get parity, it'll just depend on who has the better process technology and architecture.",
      "> increase competition in the marketplace.\n\nPour one out for the boys (Matrox, 3dfx, S3)",
      "I'd be conservative and say more likely closer to a 3060 in performance than a 3070 but who knows tbh.\n\nIt was never realistic for Intel to compete with Nvidia's high or even upper mid range cards with their first generation. Important thing is that they got something out of the gate and probably learned a lot in the process to be more competitive going forward.\n\nI probably won't buy an Intel GPU for a long time (until they reliably compete with Nvidia's high end at least) but I do want them to do well to further increase competition in the marketplace. A three way competition between Nvidia, AMD, and Intel will be great for consumers.",
      "If it honestly matches a 3060 or 3070 that is very impressive as a first attempt. It's like people are expecting them to match the best of the competition straight out of the gate. If the price is right based on performance, these can sell well",
      "This. \n\nTBH you’ll probably see 3060 levels from it most of the time with optimized “game ready drivers” pulling fine wine 3070 perf out of it.",
      "Intel does at least have years worth of iGPU driver fixes in place for older titles. It may not be 100% optimized for Xe but the major papercuts are gone, and it's not like those old engines need to maximize their usage of a beefier GPU than anything that existed during their launches.\n\nMy Xe iGPU (which shares drivers with dGPU) runs FNV great, for example. I can even load ancient obscure dx8 stuff like Live For Speed and it runs perfectly.",
      "> It's their first effort at a \"real\" GPU.\n\nThat was Larrabee like 10 years ago.",
      "One thing we haven't seen yet is RT performance, that might be up being Intel's drawcard.",
      "Yep. The right price, good drivers (coming along slowly, but surely), and they've got most of the market served.\n\nHigher end cards are nice, but in terms of market saturation they don't get very far because they're priced out of most consumers hands.\n\nIntel is 100% gunning for mass market mindshare saturation before ever targeting the high end. If they went high end out of the gate they'd flop, especially with A-gen driver quality.\n\nThey need millions of people going \"hell yeah I can play fortnight (insert any wildly popular eSports or zoomer game) at 400 fps on intel!\" not a few thousand neckbeards going \"Hell yeah I can do Cyberpunk at RT 60!\"",
      "Think Intel is gonna skip the western markets for the alchemist generation and try a proper release for battlemage?\n\nHigh end Lovelace and RDNA3 is around the corner, current Ampere and RDNA2 availability is high, with prices on a downward trend. It will be difficult to carve a niche unless the AIB and Intel are willing to lose money to differentiate themselves significantly.",
      "It's really hard to go from having no high power GPU at all to having one. All the other graphics chip manufacturers gave up fighting Nvidia and AMD decades ago.\n\nJust glad to see competition isn't just two companies anymore. Miss the glory days when you had 3dfx, Nvidia, ATi (now AMD), Matrix, S3, etc. all producing graphics cards. After Nvidia started the GPU era they really whittled down the field because most companies just couldn't put the resources in to build a competitive GPU",
      "Don't make any moves right now. That RX 6600 is still good and Arc, IMO, should only weigh in your purchasing if it's cheap and exceeds what you have. And to know that, it has to launch and get reviewed and have pricing announced, land on shelves, and actually test market pricing. I don't think there's any outcome where the Arc GPU will sway a current RX 6600 owner to switch, as it won't be *that* much faster.\n\nNothing wrong with grabbing a 4060 later, but the pricing on that may end up shocking us.\n\nRegardless of the shakeout, having 3-way competition is exciting."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "New Arc A750!",
    "selftext": "",
    "comments": [
      "I really love the looks of the Arc GPUs.\n\nCables could use some time tiding up.",
      "Yeah, they're clean.  The gamer stuff certainly has an audience - and that's fine - but I doubt its even 50% of the GPU market.  Really wish manufacturers would tone it down more for at least 1 or 2 models.",
      "The Arcs are beautifully made GPUs. Great looking rig as well. I think in less than 5 years, Intel GPUs will rival Nvidia/AMD in compatibility and performance, possibly even top them.",
      "Enjoy your new toy!",
      "Yeah, so far I haven’t run into any major compatibility issues, but there was a blue screen with some Graphics error code while playing RDR2 at 4K. Besides that, I have literally no complaints",
      "Sweet pick up",
      "I recently switched my mounting positions from horizontal to vertical. I think it is a lot nicer looking at the whole card itself, especially with the RGB. I only did this because, with my new motherboard, I could not mount my GPU  horizontally for some damn reason. Luckily I had a spare riser cable that I wasn't using, and now the whole setup is super nice.",
      "Really wish AIBs would follow suit and stop with the cringy \"gaming\" designs. I like all the reference cards from green/red/blue but i fucking hate all the bling and slogans of the AIB custom models.",
      "Yeah I would totally do that, except this card doesn’t glow :( it only have a white “Intel Arc” logo on it, and nothing else. Not complaining, it just is what it is",
      "I agree, I have a reference 6750xt and love the look and these Arc GPU’s look even better.. but AIB will stop designing them like that when consumers stop buying them."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Arc a770 or Arc a750?",
    "selftext": "I'm looking at building my first pc and I want to go with an intel arc graphics card. I don't need crazy performance, but I want something that can run the latest games and will last for a decent amount of time. \n\nWith that being said, I'm debating between the Arc a750 and Arc a770. I can buy the a750 for $220 on Amazon and the a770 for $460 on Amazon (also have a bid on ebay open box with a maximum bid of $350). What is your recommandation?\n\nThank you!",
    "comments": [
      "The A750, it’s not even a question. You’re getting 92% of the performance for half the price.",
      "Just get a750, there is no point of 770 . a750 is 90% of a770",
      "The problem with that is the RX6700XT exists for the price of the A770. The A750 is easier to justify the cost of.",
      "Fair enough, my A750 is unbeatable in price/performance.",
      "A770 should be around 350$ and I'd go for it, I already have an A750 and I feel like I'm gonna need the extra VRAM in a not so distant future.",
      "If you want longevity and insist on an Arc card then id suggest the 770 over the 750",
      "I have the 750 and like it a lot but the only game I play is a modded GTA V with video quality maxed out. I still get 40fps",
      "I went for the 750 with my Ryzen 7 5700X as it had the two items I was looking for. 1) An 8-core processor, and 2) good 1080p graphics.\n\nWhat is now being raised as an interesting question is how much 8MB VRam is useful in today's GPU - but for low spec gaming - I can't see how that will affect me for the next 2 years",
      "Out of the two, I'd reccomend splurging on the A770. But either way, be prepared for possible issues with different apps, including games. I was an early adaptor if the A770, bought two different models, and had so much trouble I sent them both back. But from what I understand, Intel has come a long ways since then and they're looking like a pretty good option with the understanding that you may run into some snags along the way. Good luck and have fun Mate!",
      "A750 is way better since it's almost all the performance for a much cheaper price. But 16GB of VRAM on the A770 I think is mandatory for a new card in 2023 unless you're planning on playing in 720p.",
      "When you say low spec gaming, do you mean 1080p?",
      "Yes, when compared to 4/5/8K @ 150fps average.\n\nOne thing that I didn't include in the op was my need to be as efficient as possible with the power.\n\nHaving an 8-core at 65W met my requirements for the CPU, however, researching power draw on graphics I found that the higher resolutions require more energy. \n\nI then had to make a choice based on voltages against price - and price won out with the 750."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Will it be downgrade if I sell Arc A750 and buy Quadro P4000?",
    "selftext": "I currently have Intel Core i7-6700 system with Arc A750 using custom Resizable Mod on motherboard. The only problem is that my Arc A750 makes a huge framedrops during the gameplay like PUBG or Call of duty which I've noticed on my previous RX580.\n\nRecently, I found a very good condition Quadro P4000 for $120 on local used market. Will it be the downgrade if I replace my A750 with P4000?",
    "comments": [
      "According to benchmark testing about a year ago and recent video YouTube reviews of the performance of Intel ARC video cards 3 months ago, the Intel ARC A580 8 GB and Quadro P4000 8 GB had a performance boost about 20% above that of the AMD Polaris RX 580 8GB. They had FPS in the 50 to 70 range while the ARC 750 8 GB had 70 to 90 and the ARC A 770 16 GB at 90 to 110.\n\nBased on these reported numbers, the P4000 is likely to give you less or equal gaming experience to the A 750 at best. All of the reviewed video cards have performance targets in the 200 to 300 dollar range where they are in the same gaming category as a GTX 980 to GTX 1660. All of them have difficulty outperforming an RTX 3050 8 GB or an RTX 2060 6 GB card. An RX 6600 8 GB card outperforms all of these cards except for the ARC 770 16 GB GPU.\n\nIf you want to make sure you purchase a better video card than an Intel ARC A750 8 GB, then look at GPU hardware that is better than a GTX 1660 Super. The ones you should be looking at are only the 8 GB memory capacity or higher video cards such as the GTX 1070 8 GB, RX 6600 8 GB, RTX 2060 Super 8 GB, RX 6600XT 8 GB, RX 6700 10 GB, etc. Please note that most of these are currently selling above 200 dollars in the used market and they rarely drop below 150 at this time. It will be another year before the products regularly sell below 200 dollars once the next generation of video cards releases at the end of 2024.\n\nThe GTX 1070 8 GB is probably your best bet. It does not have DLSS nor ray tracing but the card is selling used for between 80 and 150 dollars on eBay. Its processing power is similar to an RTX 2060 which is above an RTX 3050.",
      "A750 is 2-3x faster than Quadro P4000, so yes it's a huge downgrade."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Intel Arc A750/A770 will be available at 6 Micro Center locations",
    "selftext": "List:\n\n-\tDallas, TX\n-\tHouston, TX\n-\tChicago, IL\n-\tWestbury, NY\n-\tDenver, CO\n-\tOverland Park, KS\n\nSource: https://game.intel.com/story/intel-arc-graphics-release/\n\nEDIT: Looks like the A750 only, I don't see any A770s even listed at these locations.\n\nEach location seemingly only received seven A750s and no A770s. So the entire Micro Center chain received 42 Arc 7 GPUs lol",
    "comments": [
      "What went so drastically wrong that it seems only a few hundred GPUs were made available? Intel certainly is no stranger to product launches, I wonder if there was pressure just to launch something at this point.",
      "Yep that seems to be the case. The fact that they only had stock at a handful of Micro Center locations is a pretty big red flag. Meanwhile my single Micro Center location had over 120 4090s this morning.",
      "All these cards were made back in Q1, only 4 million arc cards were made overall and that covers the entire stack from a380 to a770 (not sure if that is just discrete cards or whether that includes laptop gpus as well).\n\nIt's pretty much a case of get these cards from somewhere now because they ain't coming back (great for gpu collectors though)",
      "About par for Intel with how the Arc GPU is going. So many people want them to succeed after being fleeced by Nvidia and AMD over the last couple years, but that just can't deliver on the demand that is there waiting... I almost went to my local Micro Center in Fairfax, VA but glad I didn't waste my time",
      "Wow, that's a lot of 4090s...who has the money to blow on a 4090?! I was hopeful with the 770 to finally get a GPU with good specs at a reasonable price. Gonna have to hope my 1070 holds out a little longer.",
      "No 770 at overland park.. i hope there are some",
      "Nice, did you receive a software bundle email from Micro Center?",
      "In the UK, only seen listings so far for just one major online retailer and the big pc one (overclockers) , haven't heard anything from them. There is zero volume but hopefully it's the sort of thing where it'll be available on off for a few weeks on Newegg for you guys",
      "It seems I did. I looked back at the receipt and I got COD MWII for Free.",
      "I hope they have more than 100000 gpu in stock.",
      "I stopped by the NJ Microcenter in Paterson. They said that they just did not receive them yet and to check back tomorrow. I also called their support line and it seems like they don't really have any answers.\n\n\nEdit: Spoke with a store manager and they confirmed that Intel barely sent any stock. They suggested to keep an eye out on the site and to check back in on delivery days (wed,fri). They also said they did not receive any promotional material, what’s going on Intel!?!?",
      "What a 🚀 launch Intel!",
      "I actually just got one. They just received some at the Dallas Store. The funny thing is that no one here seems to want one.",
      ">certainly is no stranger to product launches, I wonder if there was pressure just to launch something at this point.\n\ni heard they didn't want to sell directly to people because there is no margin. The plan is to try to bundle with systems, and cpus to make money.\n\n\\>> The source is moreslawisdead and some one line articles. Couple. Nobody is on the record but is the likely explanation. \n\n&#x200B;\n\nMy take:\n\nSo i'm cool with companies making money and such, but they annoying thing is this isn't' how ti was marketed, and i'm kind of annoyed about it tbh. wasting our time with a marketing gimmick if true. \n\nMy guess: it's all about money though they have cards. they are trying to look like a release, part marketing stunt, and part use it as a loss leader as part of the stunt to sell more computers etc. Right because once you bundle the idea is it can help sell some chips.  I can see the point.   \n\n\nSo weird to have a release of product like this that you don't intel to really sell, so you don't believe it in but your customers do. Never seen anything like this i can think of.",
      "That's a lot of foot warmers.",
      "that's what i feared, i would happily buy one if I could get a 770",
      "Just to put this all into perspective, Nvidia + AMD shipped together something around ~50 million desktop units for 2021. This is likely 4 mil all in, including mobile.",
      "So, if they produced these back in Q1 is production done or what? Surely, we'd see more form AIBS etc later on. I have seen rumors about how the entire gpu division is done for. I am having a hard time reading the market and seeing if they are in this for the long haul or not. I ask bc what's the point of fixing this if they aren't?",
      "No, just walked in and got the card no bundles.",
      "When gamers nexus did his teardown, confirmed that that die was produced in Q1 and the rumours have been of a warehouse full of alchemist dies waiting for the software to catch up.\n\nAs far as I'm aware in terms of alchemist, that initial Q1 run was it, especially considering 6nm is in much hotter demand than it was back at the start of the year.\n\nThe gpu division isn't done for but I think it's going to be a very very long time before we see the arc division on parity with someone like amd where obviously they have the cpu side of the business, while also being able to make half a dozen different gpu dies for their products stack. Current rumours are Intel focusing on data centre and laptop (which makes a lot of sense) and then just keeping some sort of presence in the discrete market, so maybe just one gpu die for the battlemage launch to keep costs low. This is a awful market to be launching a new gpu into when supply is far outpacing demand.\n\nSo long story short, Intel is in the long haul for the graphics business but no one including Intel has a clue about them being involved in the discrete gpu space long term"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Intel Arc A750 Freezing",
    "selftext": "Recently I just got the intel Arc A750 and it works fantastic, but one thing I noticed is there is some momentary freezing with the latest drivers I have installed. I was wondering if this freezing is caused by the older drivers still being installed or if the new ones are having some issues?",
    "comments": [
      "Try rebooting to safe mode, then display  driver uninstaller, then install the driver again. \n\nThis is how you make sure that nothing was left from previous install",
      "I have an odd issue where the screen frequently freezes.  I have to turn off the monitor and turn it back on to \"unfreeze\" the screen.  It will freeze again in 5 or so mins.  I used DDU and then reinstalled the latest driver.  Still unresolved.  Driver is from 10MAY2023.  Cant enable BAR as my computer is PCI 3.0.",
      "I enabled ReBAR and that worked! But I did recently install the new driver. Are you suggesting that I uninstall any of the old drivers on the arc gpu?",
      "I see. The problem is rather frustrating but are you absolutely certain you cannot enable Resizable Bar? I think 3.0 boards do support it. What generation cpu are you running?",
      "It's not a bad idea to use a ddu, especially if you had a different gpu before without a fresh windows install. That said rebar was probably just the issue.",
      "If it's already fixed then you don't need to do anything",
      "It is not available on the X399 motherboard.  Yeah, still rockin' the Threadripper 2950X.",
      "Appreciate the help and suggestion! Something else I noticed is that in some games such as ROBLOX the screen is flickering? Any suggestions on how to fix that?",
      "What brand motherboard do you have?",
      "https://rog.asus.com/uk/motherboards/rog-strix/rog-strix-x399-e-gaming-model/",
      "Checked out the motherboard, it does seem that ReBar support is compatible.",
      "I double checked in the BIOS…\n\nIt's not there on the STRIX X399.  Seems other companies did include it.  Typical.  😑\n\nThanks for your help though.  ☺️",
      "Do you have the most recent bios? I only au that because I also have an older mobo by then (z390-e) and they did release the rebar support. I do suggest you upgrade to the latest bios. Without Rebar your gpu is rather slow and hard to use. Rebar is the foundation for ARC performance. I do know of a program that gets rebar on older motherboards..may help",
      "I also do say this because if the motherboard is compatible with windows 11, it may be likely that you’re behind on a BIOS and it has ReBar."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Need Help: Persistent Driver Crash Issue with Sparkle Titan OC Edition Intel Arc A750 GPU",
    "selftext": "https://preview.redd.it/i20sfwnchkzb1.jpg?width=3000&format=pjpg&auto=webp&s=2d648370c4493f796746e84a9503e1b7b4f089b9\n\n \n\nBody:\n\nHey fellow Redditors,\n\nI recently built a system with the following specs:\n\n* Sparkle Titan OC Edition Intel Arc A750 GPU\n* ASUS H610M-E D4 Motherboard\n* Intel Core i5 12th Gen\n* 16GB RAM\n\nHowever, I've been encountering a frustrating issue – persistent driver crashes in every game. I've tried various troubleshooting steps, including proper driver reinstalls, updates, and even a fresh Windows installation. Unfortunately, the problem persists.\n\nI've also reached out to the help center, but so far, we haven't been able to identify the root cause or find a solution. The crashes happen regardless of temperature and frame rates.\n\nHas anyone else faced a similar issue with the Sparkle Titan OC Edition or Intel Arc A750 GPUs? If so, were you able to resolve it, and how? Any insights or suggestions would be greatly appreciated.\n\nThanks in advance for your help!",
    "comments": [
      "Submit your ticket to intel on their ticketing website. They are pretty good with escalating issues. Good luck.",
      "I've been noticing more crashes as well, although it's usually OBS crashing (Replay buffer using AV1 encoding+2 fairly lightweight games running), although was playing Halo Infinite yesterday and it crashed on me twice before it stopped crashing.\n\nLast month or so has been pretty bad for crashes for me, however I can't say for certain for me if it's related to Intel Arc drivers or OBS since besides Halo Infinite crashing yesterday, the crashing has only occurred when OBS is running (OBS would crash, which would follow with other stuff crashing).\n\nYou could try an older driver, I personally haven't tried older drivers since for me it isn't so bad that it's every game, sometimes a few times a day, sometimes can go multiple days without a single crash.\n\nPersonally, I suspect the crashes are from a botches bios updated that happened a few months back, before then I don't remember any crashes (Besides bad performance in some games and some weird visual bugs, it was fairly stable for me for many months since I bought my GPU up until then), but around then I remember seeing a post about them pushing a bios update through the driver update and someone mentioned it on the [arc community forum](https://community.intel.com/t5/Intel-ARC-Graphics/bd-p/arc-graphics); Although if it is the bios update that is the issue, then going back to older drivers probably won't help.\n\nAlso, nearly forgot to mention, my A750 is the ASRock Challenger D, so it's not specific to Sparkle A750's.",
      "Yes I'm going to be that guy. You've bought a brand new GPU and it's giving you this much trouble... Return it.\n\nBuy something from AMD or NVIDIA and only consider Intel when they've got their drivers in order. This is not acceptable from a multi-billion dollar company.",
      "can you suggest me a stable driver?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "What CPU will be considered balanced well with a Arc A750?",
    "selftext": "I made a post earlier on r/intelarc asking for Arc GPU advice with the i5-12400 because that's what I settled on before.\nI don't think I need a high core count because I will be doing little or no multitasking. So am I better off with a 12th gen or 13th gen i3? And since the 13100 and 12400 support both DDR4 3200MHz and DDR5 4800MHz, do I go with DDR4 to keep prices low or DDR5 for future proofing?",
    "comments": [
      "There is no price difference between 12600kf and 12400 around here in NA. Maybe look for that? I am sure every market is different though so can't say but its a way better get.\n\nI3 12100 has terrible frame times even though avg looks good typically. Its fine but when it's $100 for it vs like $170 for 12600kf I would pay the extra.\n\nIf you are going turbo cheap  on all parts then I get it. You can get a $80 h610 board something like that for 12100. Still 12600kf works ok on these cheap boards too as long as you are not running cone bench for hours. You dont need a z board.",
      "12100-13100 are the same 4 core 8 thread cpu (little bit higher clock on the 13th gen one), true raptor lake is from 13600k up, even the 13500 still has alder lake cores. 12400f would be a fine choice for your build it has 6c 12t.",
      "Both 12th Gen and 13th Gen will do the job for an Intel Arc A750, might as well get whichever is cheaper. Might not hurt to wait for Black Friday sales when retailers will be trying to empty their stock of 12th and 13th Gen chips to make room for 14th Gen, if you can wait about a month.",
      "Here the 12400 is $167, the 12100 is $125 and the 13100 is $143 not a large difference and they all can run on a B660 or B610 motherboard.\n\nI don't want to get those suspiciously cheap boards as they might die on me a few weeks out of warranty.",
      "What do you mean by terrible frame rates?\nI have played 80% of the newest AAA games and they run at 100+fps with a 12100F on high-ultra settings. The only game where i am noticing some choking in the most intensive parts of the map is Cyberpunk.\n\nGot it for €100 here in Europe and its the best €100 i have ever spent. Wipes the floor completely with my old 4690K.\n\nMost of the games nowadays get boring after a short time or are complete garbage. IMO not worth to spend a lot on a PC rn.",
      "There is no black Friday in my country but price should drop after 14th gen releases"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "a750"
    ],
    "title": "Current state of VR support on Intel Arc (A750, likely also applies for the A770 and A380)",
    "selftext": "I hadn't posted about this here yet, but I am a collector of VR Hardware, and when the A750 and A770 first launched in Germany, I was very eager to try out VR performance on as many headsets as I could.\n\nI personally bought the A750, mostly as I intend on mostly using the card for AV1 encoding in my Server, but might as well try VR on it as well I thought. Here's my findings so far:\n\nThe Intel Arc drivers currently **don't seem to support Direct Display Mode devices at all**. This is a requirement for VR headsets that use DisplayPort or HDMI to receive the image they're supposed to display. That means that I wasn't able to test any of these headsets, and I've tried a lot of them: Oculus/Meta Rift CV1, Valve Index, HP Reverb G2, Vive Pro 2 and the Varjo Aero (tho in the case of the Varjo, there was also a software lock-out, as they don't want their customers running into issues with unsupported GPUs, which also includes ones from AMD).\n\nOfficial Link support with the Quest (both with a cable and wireless) also wasn't working. Quest Link refused to find it's connection (the PC dialog never found the headset's USB connection and I never got the Quest Link prompt in the headset itself). and Air Link straight up threw me an error that the Video Encoder was unsupported. I have no idea what kind of magic other people have pulled off to get Air Link or Quest Link working on Intel Arc, I sure wasn't able to.\n\nI was however able to run VR on my A750 through 2 other means. For one Virtual Desktop basically runs on anything that has a DirectX 11 capable video output and a CPU, so that worked with no issues at all on Arc too, and I was also able to use my Vive Pro 1 using the Vive Wireless adapter.\n\nWith Virtual Desktop performance was quite stellar and I didn't really experience any stutters or similar. With the Vive Wireless Adapter however, I immediately started noticing hitching and stuttering every so often. The more CPU heavy of a game I tried, the worse the stuttering got, hinting that large parts of the stuttering may have to do with GPU drivers reliance the CPU.\n\nFor reference, I'm running a 5900X with 32GB of DDR4 3200MHz CL16 memory.\n\nIf anyone is interested in the performance numbers using the OpenVR Benchmark Tool, I'd be more then happy to provide them later as well. I've already run the benchmark, but the results I have not saved on my main data drive...",
    "comments": [
      "Yeah, this is true. Linus from Linus Tech Tips also faced this issue. Intel has responded saying they are in the alpha stage. They should be able to release beta drivers for VR support shortly.",
      ">The Intel Arc drivers currently don't seem to support Direct Display Mode devices at all.\n\nI am curious if the Direct Mode does work on Linux, since [the implementation is shared with AMD.](https://monado.freedesktop.org/direct-mode.html#intelamd)",
      "No unfortunately not, and I was not aware of it and bought a NUC with arc card to play specifically VR....",
      "I’m running the same specs as you plus an A380 (cause why not see what it can do with VR). Oculus/Meta Rift S didn’t throw me any errors, just said display port was disconnected even tho it was connected.\n\nOn the Intel discord, I’ve brought this up and have been reassured multiple times that VR isn’t software blocked nor hardware blocked. So I’ve been assuming that Oculus/Meta VR and Steam VR hasn’t whitelisted them yet for the direct connection headset. I’m thinking they’re waiting for the rest of the 40 series and 7000 series to release before doing a bulk VR whitelisting of said cards. *People with 40 series have been reporting the same problem so I’ve heard*\n\nFrom what I understand, the wireless alternatives ignore the gpu hardware check and goes by what the gpus can run as you said",
      "I know you probably won't respond but, since then have you tried again? And if you did has oculus link still worked?",
      "I'm curious, at a collector of vr hw, you likely have some great opinions.  I'm looking to buy an older lower cost but still serviceable vr computer to drive an oculus 2.  Only need to do something like Alyx at \"ok\" levels.  It's hard to discern where that sweet spot of age vs functionality vs price is.  If you've thoughts on those lines, love to hear them!",
      "still waiting :(",
      "As soon as a Mesa driver version with Arc support ships with PopOS, I will try that out. I'm just not familiar enough with Linux yes to feel comfortable modifying the system itself through the terminal ![gif](emote|free_emotes_pack|sweat)",
      "40 Series does work with all the headsets. Nvidia changed something with the Framebuffer handling in Ada Lovelace that causes heavy stutters in VR at higher resolutions if not accounted for.\n\nThis whole topic was already discussed in great detail on the official Varjo discord, and Varjo has already released a patch to fix 5hose stutters on the 40-Series GPUs.\n\nIntel Arc isn't blacklisted or anything. Direct display mode devices shouldn't show up to the Windows desktop, but they do on Arc, and usually at the wrong resolution too. The Rift CV1 showed up at the right resolution, but I'm guessing, since it wasn't found by the Oculus software, that there's some flag missing, like HDMIs 3D side-by-side display stuff. The Index shows up as a 640x480 display, and looking into the SteamVR web console it actually says that no display with the right resolution was found, but it does list all displays connected to the Arc GPU. And in the case of the Reverb G2, the display didn't show up on the desktop (it's a built-in Windows driver, go figure), and WMR did actually launch as if everything was working right, but the displays in the headset stayed black. My guess is that yet again, the driver wasn't able to initiate the vorrect display mode (resolution, refresh rate, direct display, 2 DP lanes per screen, etc.) on the G2...\n\nThe tl;Dr is, it doesn't seem to have anything to do with the software, at least for the display connections, it seems to be a driver related issue with the hardware not showing up correctly...",
      "I haven't, tho my intention was to at some point do a 30 day Arc trial, using nothing but my Arc card for 30 days and when I do I can try Quest/Air Link again :D",
      "Honestly, that I can't really say where the sweet spot for price to performance is for VR capable PC Hardware. What I can say is that you'd want a GPU with at the very minimum 6GB of VRAM (which at least all of the Intel A7 cards do fulfill), at minimum 16GB of System Memory and preferably 6 CPU cores with HT or more. I wouldn't to older then an Intel 8th Gen CPU (or the generation after if you're going Team Red) and in terms of GPU not older then Nvidia 10-Series or RX6000.\nIf you have a Quest 2 then that leaves Intel Arc also as a GPU option open for you, as long as you're using Virtual Desktop (as like I've said before Headsets that require a Display connection to the GPU currently don't work on Arc and the Oculus software refuses to work completely on Arc)",
      "They have not fixed it yet!!!?",
      "You could try something more up-to-date like Fedora or endavourOS.\n\nI'd be very interested to see whether this works.",
      "Okay, so there has been some change as the driver updates have been releasing. My experience was with either the launch driver of the update after launch driver which just said *headset is disconnected.* I’ll try launching VR on my Rift S and see if I match what you’ve been experiencing",
      "It still says that, but at least with the CV1 a 2160x1200 display was showing up on the desktop.",
      "Damn. Sorry man"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Intel Arc A750: The Best Value For Sim Racing in 2024?",
    "selftext": "",
    "comments": [
      "This subreddit is in manual approval mode, which means that **all submissions are automatically removed and must first be approved before they are visible**. Your post will only be approved if it concerns news or reviews related to Intel Corporation and its products or is a high quality discussion thread. Posts regarding purchase advice, cooling problems, technical support, etc... will not be approved. **If you are looking for purchasing advice please visit /r/buildapc. If you are looking for technical support please visit /r/techsupport or see the pinned /r/Intel megathread where Intel representatives and other users can assist you.**\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/intel) if you have any questions or concerns.*"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "I wanna replace my RTX 2060 with an Arc A750, is it worth it.",
    "selftext": "Hello, i am currently using a 2060 6GB and i have an I3-12100F. I want to get an Arc gpu, since they seem like a pretty good value for money and they offer better performance than my 2060.\n\nMy concern is ReBar. I just enabled it in my BIOS, and it didnt switch out of CSM mode (I cant boot in UEFI mode for some reason). It stays in CSM mode, even though it says that ReBar is enabled. \n\nSo is it worth replacing my 4 year old 2060 for an A750?",
    "comments": [
      "It’s at least 20% faster than your 2060 - probably a little more since drivers have improved so much, but not more than 35% faster. That’s not worthwhile.",
      "I'm quite sure ReBar isn't enabled for 20xx serie, your mobo can support it but the nvidia driver and firmware just won't let you use it, or better, nvidia marketing reasons.",
      "No.",
      "A750 is faster although I normally only upgrade for 2X performance jumps which I am pretty certain this is not",
      "Yeah, i decided to not get the A750. Maybe a 3070 would be a better choice.",
      "The 3070 is alright but 8gb is quite limiting at its performance tier the 6700 xt is almost as fast as doesn't require texture settings to be dropped in some of the games where the 3070 needs them dropped\n\nIt's out of stock in my area and more expensive in some but in places like the United States it's cheaper and arguably better",
      "In my opinion no.",
      "CSM sounds like an Asrock mb? If it’s set you have windows installed on mbr rather than gpt partition. If you need to be in uefi mode for some reason and changing it stops you booting you need reinstall windows (or transition the partition table to gpt, possible but not sure if officially supported)"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Intel arc a750 freezing problem",
    "selftext": "I recently replaced my gtx1650 with an intel arc a 750 and now it keeps freezing at random moments it only refreshes when i replug the hdmi connector i already tried the ddu and i already installed the latest firmware and i enabled rebar also i already updated the drivers can sombody help me\nPc specs:\nCpu:Amd 3800x\nMobo:Gigabyte aorus b550 pro ac\nRam:16gb 3600mhz g.skill trident z\nGpu:Intel arc a750\nPsu:Be quite 750watt \nSsd:1tb samsung 980\nCase nzxt h550",
    "comments": [
      "Sounds like a sporadic driver issues I would take a video of this and send it to Intel as a driver bug report",
      "So this is not like application or OS freeze but screen is not updated until you unplug /plug HDMi connect again ? If this is so can you try Display Port or different HDMI cable ?",
      "I will try it thanks",
      "I will try it thanks for the support",
      "Thats a last resort option",
      "btw it is still crashing",
      "Return gpu, get another one.",
      "Also, see if there is a newer driver for your monitor.",
      "Wait does a monitor have drivers?",
      "Contact intel gpu support, visit forums.",
      "Yes, most devices have drivers. Windows typically automatically downloads a functional compatible one that works out of the box. Sometimes when you have shit like this going on, it is worth looking into if there is a more recent driver. I do agree with the other guy though, try display port if you haven't."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "a750"
    ],
    "title": "Intel Arc Alchemist desktop series including A770, A750, A580 and A380 SKUs reportedly delayed till late Q2/early Q3",
    "selftext": "",
    "comments": [
      "Honestly, it doesn't matter.\n\nThey missed their window by not launching during the GPU apocalypse uncontested, so now they have to go against Nvidia and AMD offerings with mature drivers in a health(ier) market.\n\nMight as well take the L at this point and just figure out your drivers.",
      "What they've officially stated is launch in 'summer', compared to mobile 5/7 'early summer'. Anything past 'early summer' puts it in calendar Q3.",
      "They're going to have to seriously discount those cards to sell next to Nvidia and AMD. Can't see many people taking a chance on the first gen of cards especially in the enthusiast market when the top Arc card is only at 3070 level of performance. \n\nAnd then by August and September all of the talk will be on RTX 40 and RDNA3 which will blow these cards out of the water.",
      "This will get steam rolled by Lovelace and rdna3. Should’ve released this in June 2021",
      "Not if it's dirt cheap like polaris. \nBoth lovelace and rdna3 will be starting at minimum 400$ for the lowest model.",
      "Their gonna take an L. The market is getting better and by the time it comes out, amd and nvidia will have their gpus at a few % above msrp. Therefore making these card useless. They won't be faster, they might be a bit less expensive, and their launching gpus close to next gen.",
      "To be honest, even if they launched in June 2021, they'd have probably all been gobbled up by miners.",
      "most of them are probably going straight to OEM.\n\nOEMs will use it as a way to tell AMD/Nvidia to fuck off with their horrid \"incentives\" like priority allocation. Most people buying computers don't know what the hell goes in them so the main problem is drivers. if the drivers continue to be miserable then intel will get nowhere even giving these things away for free.",
      "At least they would’ve sold (Not defending miners). Now they have zero chance of becoming dominant in any market",
      "More competition, more fun.",
      "Well considering the frame stuttering in the arc a350m for mobile that was recently tested, my money is on bad drivers and at this point Intel should just work on the drivers. They missed a critical chance to disrupt the market. Now new gpu prices have fallen down hard. Not yet at msrp but much better. The second hand market will be flooded with rdna2 and 3000series from Nvidia and not mention the rtx4000 series and rdna 3 gpus coming soon. This will be really rough  on Intel.",
      "When will Intel learn that blatantly lying to investors will get them nowhere?",
      "Efficiency wise it's better than GTX 1600, at least the already released laptop 350m which at around 40W has same performance as GTX 1650m at 50W. \nThat's not too bad. Probably on the same level as RTX 3000 as efficiency didn't improve much compared to GTX 1600 and 2000.",
      "Well, thats basically all she wrote for ARC...\n\nI know for a fact Intel won't price them relative to their performance vs other cards.\n\nI hope I am incorrect, as aggressive pricing is the only thing that will save them. That creates an even bigger problem for them though, if they start out low priced, they will remain there for many years. Thats just the way the market works.",
      "They're making them at TSMC so it didn't matter. if these were being made at Intel fabs, well they'd be worse if they were being pushed out on a broken 10nm a year ago or 14nm due to power efficiency.\n\nBeing made at TSMC means when TSMC were starved for capacity these would have been even worse.\n\nUsing TSMC 6nm indications would be that performance is closer to AMD/Nvidia parts that use half the die size. \n\nThe only way these matter in the first gen or two is if Intel does it's Atom/phone/tablet tactic and just firebombs pricing to force their way into relevancy but if the architecture doesn't catch up then eventually when they try to charge real prices they'll be pushed out.",
      "Mmmhmm.",
      "More competition more fun works if wafer supply is ample and performance is competitive. If performance sucks and they eat up supply of incredibly limited wafers then it's terrible for everyone, Intel included. They'll be eating shit selling these at below what they cost to produce and AMD/Nvidia lose out supply to make twice as many gpus from the same wafers and the end user loses out.",
      "Thats just clock gating. Easily fixable.\n\nLook at the GPU speed every stutter, you will see it drop to 1150mhz.",
      "TSMC 6nm is a retooled 7nm, so the density is not that dissimilar.",
      "How is it fixed if u dnt mind explaining?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Acer Predator BiFrost Arc A750 OC desktop GPU review - Intel's second-strongest gaming graphics card has only been given 8 GB VRAM",
    "selftext": "",
    "comments": [
      "I wonder what's the rationale behind that cooler. Seems easier to just have dual axial fans.",
      "The blower keeps it super cool"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "I found arc a750 for very cheap price. I will be using it for gaming and solidworks. Is it performing badly? Anyone using this graphics card for Solidworks?",
    "selftext": "",
    "comments": [
      "Maybe someone will help you, but seem to be possible to fix this with registry edit. https://community.intel.com/t5/Intel-ARC-Graphics/Solidworks-on-an-ARC-A770/m-p/1425976",
      "How much of an estimated performance increase does it provide?",
      "Here's an updated and more in-depth chart: \n\n[https://techgage.com/wp-content/uploads/2023/03/Dassault-Systemes-SolidWorks-2160p-Viewport-Performance-Subtests.jpg](https://techgage.com/wp-content/uploads/2023/03/Dassault-Systemes-SolidWorks-2160p-Viewport-Performance-Subtests.jpg)\n\nFrom here:\n\n[https://techgage.com/article/specviewperf-deep-dive-february-2023/](https://techgage.com/article/specviewperf-deep-dive-february-2023/)\n\nIntel has workstation-focused Arc Pro cards that are said to improve performance in multiple CAD-like workloads, but I've yet to be able to benchmark one.",
      "I haven't ever used Solidworks or Catia, but my experiences last year with the A770 for pro/compute workloads were dreadful, usually \\~20% behind a 6600 at twice the power, so that image tracks. Not to mention the some of the truly lateral thinking required to get things running at all.\n\nI'm quite glad that I returned it for a secondhand 6600 - you may be well-served by doing similar, if that chart is reflective of your workload.",
      "https://community.intel.com/t5/Intel-ARC-Graphics/Solidworks-on-an-ARC-A770/m-p/1481553#M3829\n\nOne person stated that with the change he made in the registry, it went up to 125 fps.",
      "A750 looks much more powerful in tests",
      "My flair?",
      "Damn, if only it was a little more than that"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Can I use a 1650 super 4gb and a arc a750 in the same pc",
    "selftext": "I’m buying the arc a750 and is wondering if I can use another gpu I have lying around",
    "comments": [
      "It's possible to use several video cards within the same computer, but the guidelines as to how and why it works or not is based on whether you can assign programs and workloads to specific hardware.\n\nThere's various conditions on why some setups work and why others don't. You probably need to go to a multi-GPU usage forum to determine what you can get away with.\n\nCan a GTX 1650 Super 4 GB card work in the same computer as the Intel A750? As long as you have both video drivers installed and you assign with non-video output processing to one of them, then you can use one for video output, while the other one is crypto mining. Please keep in mind that two or more video cards likely means you do not have enough power supply juice to provide cabling to each video card.",
      "Ok thanks"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "This thing is a beauty, Acer Predator Arc A750.",
    "selftext": "Just go from recent Tiktok shop black friday deal of $129, this thing is such a beauty.\n\n[Box art](https://preview.redd.it/5xwdmc910n3c1.jpg?width=1280&format=pjpg&auto=webp&s=a496053da633b151f342f75868f30162b27e63b2)\n\nOpen the box, it's well protected by foam.\n\nhttps://preview.redd.it/coi4ca150n3c1.jpg?width=1280&format=pjpg&auto=webp&s=0ca32fa05acf3f21f93e9d9496bce6615b9fbb58\n\nThe card is well made, I'd say it's on level of med tier card, with axial fan in center is very old fashion, reminds me of AMD or Nvidia founder edition.\n\n&#x200B;\n\nhttps://preview.redd.it/qj7o3kkt1n3c1.jpg?width=1280&format=pjpg&auto=webp&s=95313c387ffb09f3b7c8eab401f695c08bf81e71\n\n&#x200B;\n\nhttps://preview.redd.it/rbx9ccpq2n3c1.jpg?width=1280&format=pjpg&auto=webp&s=7306e7df77842971aafada595137ed22d5abb2ac\n\nThe backplate is also made of metal and thermal pad can be seen from the gaps.\n\nhttps://preview.redd.it/yhhh33e52n3c1.jpg?width=1280&format=pjpg&auto=webp&s=e81f6e209dd85c47e02d88c46d70bc5cb9d19d9c\n\nThere might be LED lights on the side, besides the logo:\n\n&#x200B;\n\nhttps://preview.redd.it/t61wg9lc2n3c1.jpg?width=1280&format=pjpg&auto=webp&s=dda1179bbcaf8b7a0755f748705ce4749cafefca\n\nA peak into the cold plate, there is a very visible \"tail\" on the side, which also means it's cooled by vapor chamber.\n\n&#x200B;\n\nhttps://preview.redd.it/pmslfe2o2n3c1.jpg?width=1280&format=pjpg&auto=webp&s=fd615226056b814f98e5026284f505e6dddea639\n\n Just hold it, I'd say it feels premium in hands, can't wait to install it later in the weekend.\n\n&#x200B;",
    "comments": [
      "Nah it was for Black Friday and ended around cyber Monday. Infinite 30% off coupons",
      "Is the deal still on?\nCan you tell me more about seller? I noticed that the newegg is selling through the tictok."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Intel Arc A750 Limited Edition 8GB + Assassin's Creed Mirage on sale for $180 on Amazon",
    "selftext": "",
    "comments": []
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "i5 9400f with Arc A750",
    "selftext": "i know that arc cards need ReBar to get full use of them and 9th gen don't have that feture in bios.\nbut recenty i was looking in my bios and my mother board (msi H310M pro) has ReBar option.\n\nso will it work on 9th gen cpu, because it's very affordable in india compare to RTX 3060 or my next choice RX 6600\n\nplease let me know if anyone has tried this with ReBar.",
    "comments": [
      "8th and 9th gen fully supports ReBar now, just update your mobo's bios to the latest and you should be able to use it.\n\nI also have it on my ASUS Prime H310M-K R2.0, it's just that my GPU doesn't support ReBar hence why I'm not using it at the moment.\n\nbut I did turn it on, it just didn't make a performance difference since my GPU doesn't support Rebar.\n\nyou should be clear to use ReBar now that 8th and 9th gen platforms are capable of using it.",
      "You have obsolete cpu for this gpu.",
      "It did when I ran an A770 on an EVGA Z370 Micro with a 9600k.",
      "> i know that arc cards need ReBar to get full use of them and 9th gen don't have that feture in bios.\n\nYou can force enable it, since it's a pcie feature it doesn't matter if your motherboard does or doesn't support it.\n\nAlthough Arc does still have driver issue, they're definitely being slowly patched out, but if you don't want to deal with some games being much slower than you'd think, you might want to skip ark (Really depends on if you're fine with some games being nearly unplayable for a while).",
      "Rebar works fine on 8/9th gen provide the board (like yours) supports it.",
      "Beta bios E7B25IMS.2B1 or newer should have “support” for rebar, but your mileage may very, many people see stability or crashing issues with it enabled.",
      "You are looking at 20% to 50% performance loss without rebar depending on the title. Also the 1% lows are also severely impacted which is far worse for fluid gameplay.",
      "so it should work right?",
      "i know that but i can turn on ReBar from bios will it work on 9th gen cpu that is what i want to know",
      "Nope."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Will the Intel arc a750 be able to outperform other cards?",
    "selftext": "So far the arc has outperformed the 3060 and 6600 in many instances, but will it be able to beat other cards? Such as the 3070 or 6700/xt",
    "comments": [
      "Not even close to those two. Might see it challenge the 6600XT in some games.",
      "The A750 already surpasses the 3060 in some titles.",
      "Why?",
      "I never said that the A750 outperforms a 3060. Also the main point if my comment wasn’t the \"some\" but the \"already\". I also used \"some\" because I wasn’t sure how many games already run better on the A750. The 3060 only has ~5-7% more average performance depending on sources. Tom's hardware for example has around 6% but it isn’t easy to tell since his chart only goes in 20 steps. So assuming the 3060 has around 70 fps on 1080p max and the A750 has 66 fps so the difference is 6%. In another comparison on YT there are 20 different games and the difference is around 5% on 1080p. So drivers should definitely close this little gap at least. For comparison: the 3070 was a bit better than the 6700 XT until late 2022 drivers update. Now there on par at least. \nAlso what’s \"Y'all Arc people…\"? Am I an \"Arc people\" just because it’s right? Are you a Nvidia people then? \nAlso your \"Oh but in a year bla bla\" doesn't make sense in this case because this is basically OP's question. \"WILL the intel arc a750 be able to outperform other cards?\" OP literally wants to know what could be in the future. \nAlso nice car and crush comparisons altough they don’t make sense in this debate.",
      "RTX 3070 or the RX 6700 XT? hell no. the GTS 450/GeForce 210? hell yes",
      "its a great card, problem is intel may be ditching arc.",
      "they fired there lead engineer",
      "I know from owning many NVidia, AMD cards and system boards that drivers are what make those parts really go. also, I think once intel gets the driver to what it should be the 750 and 770 will benefit huge as it has in the past with both NVidia when it had motherboards and still has video cards that users can increase performance using driver alterations such as nvclean install and such. even better if one can program. S I think intel has a way to go same as AMD for a very long time and still some issues. The issue I have with NVidia is their video card Biosses. Pend that much on a card one should have full control just like motherboards. Hope one day they will give us that option and all cards come with the parts on them to allow that kind of adjustment. Hang in there as I am going to get the a770 and work on the drivers myself as the cars comes with everything in hardware to compete with the 3080, just the drivers suck but better than they were.",
      "From where do you know that the Arc A770 has the hardware like a 3080? I only heard that the A770 should have the hardware of the 3070 which is still very impressive since it’s a lot cheaper and has the double amount of VRAM.",
      "I didn't say it has hardware like the 3080 as they work very differently, It has hardware that is capable of competing, until they get drivers up to par it won't use what it does have in hardware fully. AMD is still working on the same thing. Really haven't seen any major change in the drivers for NVidia for 8 years, now game settings such as RTX, DLSS and related stuff for individual games have been added. but not huge performance gains since those add ons. Intel will get those gains but will take a while. the A770 has the hardware to compete with the 3080.",
      "\"Some\" games, is not outperforming a different graphics card. My old Saab can also reach 250km/h, but it doesn't outperform a comparably priced Mercedes.\n\nDrivers will not push FPS in games much beyond what it is now. Y'all Arc people need to get that out of your head. \"Oh but in a year this card MIGHT outperform a 3060 on average\" Nobody can use that to anything. That's like saying your crush might eventually be yours, but she has to get used for an entire year beforehand.\n\nThe card performs on average, worse than a 3060. The 770 16GB performs on average, between 3060/3060 Ti. Regardless of singular games performance, the average is what is useful. Telling new buyers otherwise is just stupid and will eventually kill any enthusiasm connected with the cards.",
      "No. And it is not outperforming 3060 either."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "GUNNIR launches new Intel Arc A750 Index graphics card - VideoCardz.com",
    "selftext": "",
    "comments": [
      "That actually looks very clean. Wish more brands would follow suite instead of some of those gamer aesthetics."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Arc A750 vs RX 6650 XT",
    "selftext": "Looking to get a gpu to pair up with my R5 5600X, options are XFT SWFT 210 RX 6650 XT and Intel Arc A750 lmk which I should get",
    "comments": [
      "The a750 was faster than a 3050, 3060 8gb and the Rx 6600 but I am pretty sure the 6650 xt is faster.\n\nAlthough the a750 has better raytracing and av1 so it depends what your specifically doing",
      "I’m pretty sure the 6650 XT is still faster overall.",
      "If you plan to  play old games it is safer to go Radeon. If you dont really touch them, intel should be decent and has really impressive ray tracing.",
      "In addition the the above comments, xess is better than fsr",
      "The Arc should be a fair bit cheaper for similar performance (and a lot cheaper for much better performance when using RT/XeSS if one wanted to compare it that way)"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Will Intel Arc Gpu work on Amd Ryzen 7600",
    "selftext": "I am creating this post want to get feedback, bought an Intel Arc A750 along with Ryzen 5 7600 since the X version was not available am preparing to build a pc, and not yet ready for 4k. Also have the A770 limited edition as plan B, & very willing to try their Intel gpu.",
    "comments": [
      "Yes it'll work. What makes you think it won't?\n\nAlso the A770 is barely faster, only get it if you absolutely need the 16GB VRAM.",
      "First time building it myself, and asking those who know what they are talking about appreciate the response.",
      "As long as your CPU supports Resizeable BAR, you should be fine with Intel GPUs. Which the 7600 does. You may as well use the A770 since it has more VRAM since games even at 1080p these days seem to be gobbling up as much VRAM as you can throw at it.\n\nI actually daily drive a 3700X and A770 and the experience has been decent so far. You're actually way better prepared for future games though than me since PS5/Series X games tend to make CPUs cry no matter the resolution.",
      "VRAM is video RAM, which is how much graphics data your GPU can store. Bigger is better, because it lets you play the game at higher resolutions and texture quality. Doesn't mean that smaller is necessarily worse though. The performance will be about the same at lower resolutions when the compute power of the smaller VRAM GPU equals that of the higher VRAM GPU.\n\nArc 770 with it's 16GB of RAM is good for 1440p gaming and even 4K in some cases. Arc 750 is only good for 1080p gaming.",
      "yes, but if he wants to the silicon to go a little further, the 770 is better for the 16gb along - also, plenty of things will increasing use above 8gb (RTX for example) as time goes on.\n\nWe need to know his price difference."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Has anyone tried to use Arc with a b450 amd motherboard?",
    "selftext": "I may or may not have bought an arc a750 and am running to massive problems. I just checked the website and apparently you need a 500 series motherboard. Obviously I made a mistake but has anyone else gotten it to work? Or any suggestions.",
    "comments": [
      "Do you have one? I've had an A770 since launch. No actual issues with the hardware, just issues with games. They have slowly been ironing that out though. I would make sure your mb has the latest bios and has resizable bar (amd smart access) enabled. I have been running the A770 with an Intel 12400 and a 670 chipset mb without issue. I haven't tried it with one of my AMD systems yet, but to say it doesn't run on any CPU or mb is just plain false.",
      "I tried an Arc A750 LE on an Asus B450i Strix with a Ryzen 5 3600 back in Oct and besides the drivers being much worse than they they are now, it worked with resizable bar enabled.\n\nTBF, Arc has issues with anything, it all comes down to the drivers.",
      "ARC still has issues on any motherboard and any CPU, it's mostly due to drivers. Unfortunately, it's not a reasonable choice for active gamers.",
      "Some people have reported you might need a bios update for the motherboard, so it starts working.\n\nYou might want to send the manufacturer a question about it. Arc hasn't been tested on everything. You're participating in what is essentially a GPU beta. Product is promising, but software is still flaky.",
      "Like said, yes it has issues with games due to drivers but you can't say it has hardware compatibility issues with any motherboards and cpus.",
      "yes if it's not a gigabyte with zen1/2. you have to mod bios in that case",
      "Works for me with rebar enabled, although arc control center says rebar is not supported. A770 with asrock b450 itx. However still facing many bugs. Every time PC sleeps I lose HDMI audio until a reboot",
      "Hmm maybe it’s something else that’s wrong then. It hasn’t stayed in windows long enough for me to install any drivers. Most of the time it doesn’t even get past the loading windows screen. BIOS works perfectly fine though.",
      "It's a shame, cause if the A770 wasn't having driver issues *even worse* than the 5700 XT (at launch) it would almost be competitive. That said, Alchemist/1st gen Intel GPUs are definitely having some growing pains but might become more worth it over time. Hopefully Battlemage/gen 2 will fix a lot of the problems. Or at the very least make Arc control center less trash lol",
      "Could you clarify what “mod the bios” means? Luckily BIOS does work fine for me",
      "I am using Gigabyte b450 ds3h and am on the latest bios update.",
      "A couple of guys I know bought one each and suffer from issues in many games, even popular ones like Battlefield 1. And yes, they know how to update drivers. But dozens of games they've tried worked horribly (crashes, artifacts, etc)",
      "Who cares why? You buy a GPU, you want to be able to play games, period."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Should I buy a arc a750 graphics card for productivity purpose??",
    "selftext": "My computer specification is Processor Ryzen 5600x ; Motherboard b550mds3h ; Ram16gb 3200mah; SSD 250gb nvme. ; Hdd 1tb ; Psu 650w bronze I want to do productivity work is it ok to get Intel ARC A750 GRAPHICS CARD with this computer specification?",
    "comments": [
      "Depending on what you’re doing specifically. But I’ve seen the a770 as a popular choice due to the 16gb. But the A750 will do just fine.\n\nOf course there are other GPUs to consider when they will provide the same or better performance, but at a price.",
      "Depends on the programs you’ll be using it. At the current price point the A750 is a great buy. You’d have to buy a Rtx 3060 to have somewhat better performance but the margins aren’t that static to really justify (of course depending on your use case). There are a bunch of reviews check them out.",
      "The a750 is better than the 3060 8GB model. \n\nThe 8GB model of the Rtx 3060 has a smaller memory bus and capacity. This has an impact on its performance versus the 12GB model.",
      "Which graphics card will give me same or better performance at this price?",
      "Is rtx 3060 8gb version is better than Intel Arc A750 or not??\nWhat you suggest rtx 3060 8 gb or intle arc a750??"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Arc A750 or RX 6700XT",
    "selftext": "So as title says I am thinking about upgrading my gpu to a750 or RX 6700XT  \nI game on 1440p  \n\n\nRX 6700XT 470 Euro  \nArc A750 350 Euro  \n\n\nWhich should I get and is it even worth it going with RX 6700XT due to 120 euro difference and not to say arc A750  \n probably isn't at its max due to not the best drivers from intel \\[as we've seen with latest driver update]\n\nEdit: I plan using it primarily for gaming maybe some occasional streaming/recording.\nAlso i won't even consider nvidia as 3060 is over 1k euro let alone anything better (in croatia)",
    "comments": [
      "Can't speak for the Arc but I've been rocking the RX6700XT  for over a year now and it has been an excellent experience. \n\nMost games on high/medium settings and well over 100fps@1440p. Have not experienced any driver issues or anything.",
      "Id say 6700xt , but I’m most likely going to buy the arc as my next upgrade. I want to support new competitors, especially with such an impressive first prototype.",
      "If you can hold on and wait for the RX 7600XT, that potential video card is likely going to be priced in the 300-400 US dollar range while holding 8-12 GB of DDR6 video ram and performing somewhere between an RTX 3060ti to RTX 3080 or above an RX 6700XT and potentially up to an RX 6800XT.\nIf the RX 7600XT contains 12 GB, I think AMD will try to push performance to the RX 6800 gaming level which is what an RTX 3070 would game at.\n\nThe RX 6700XT has performance similar to an RTX 3060ti or an RTX 2080 Super, while the ARC Alchemist A750 needs further driver upgrades because the performance is usually somewhere above an RTX 3050, but it struggles to routinely perform above an RTX 2060 and definitely below the RTX 3060 12 GB. The A750 is not better than an RX 6600 8 GB card which you can routinely purchase in the 210 to 240 US dollar range in America, while the RX 6600XT is usually priced below 300 on Newegg.\n\nIf you are risk averse, then just buy any RX 6000 series card you can afford and/or wait if the RX 7600 or RX 7600XT is priced well enough to tempt you to get better performance. The RX 6700 10 GB card should also be for your consideration, since, it often has been seen selling just above 300 dollars.",
      "There's a significant performance difference between the two. The arc 750 is on par with a rx6600. If you can't afford the 6700xt just get a rx6600 or 6600xt, whichever is cheaper",
      "If you're not going with tried and true mainstream nvidia quality, you might as well go with the untested cheaper intel. I don't see any reason to ever get AMD when they price their cards the same as nvidia. You may regret not having DLSS for gaming on resolutions over 1080p on such a weak card though. But if you are not going to use any such \"gimmicks\" (RT too; but for RT you need nvidia and only nvidia) anyway, Arc looks pretty compelling for just it's raw performance. Obviously, driver support and compatibility will be the big question going with it.\n\nPersonally, if I was shopping for anything less than 4090, I'd just get a used nvidia (e.g., 3080 for 500-600). Although for the price of Arc (300-400 bracket), the Arc still looks like one of the best options, even compared to used. In the cheapest bracket, Arc really looks like the best deal, even if it is a potential gamble on what it will run as well as expected and what it may not run at all.\n\nOn the other hand, neither Nvidia, nor AMD has released the newest series direct low-end competitors for the Arc, so there may be reason to just wait for those. Although, seeing the pricing on the upper bracket, I have a feeling there may be no offers at all in Arc's low price range.",
      "alright noted thx",
      "exactly but I hope arc gets even better before I am ready to upgrade",
      "I appreciate it but sadly in croatia there is no discount on new/current gen stuff and I can't order from amazon as that is atleast another 100 euro for delivery plus you can't pay with cash and rx 7600 is selling for over then 2k euros here on most websites and on others its hardly under for e.g. a ps5 in croatia sells at 800 euro used and abt 800-1700 brand new \\[no games 1 controller\\] also an rtx 2060 still sells at 800 and rtx 3060 is usualy double the price if not more",
      "God I envy north americans for pc building. Saw an Intel i5 12400 for $130 and a ryzen 6700xt for $320 on amazon but shipping from NA to Europe was $40-70. Same prices asocal stores.",
      "yeah I understand your reasoning i have an rtx 2060 rn but I am planing on building another pc and making this one a family pc and here in croatia as I said nvidia gpu's skyrocketed in pricing",
      "They already have. Had a 70% increase in gaming experience from one singular driver update and more are to come. And, realistically, that is ONLY with old games using DX9. if you only play newer games, you quite literally have nothing to worry about",
      "Unfortunately, the way the distribution scheme for the world is structured, countries, governments, and businesses negotiate lower shipping costs and fees to North American locations in order to take advantage of a larger market that buys more products. This happens on almost everything. This means pianos that are made in Singapore costs more to ship and sell within a hundred miles of that location, but the same product sold in North America that has a lower price will have lower taxes and little to no shipping cost.\n\nMost people travel to America and buy a ton of stuff to ship back to their own country, because it's cheaper to do it this way. Some locations in North America have tax holidays where sales tax is reduced or not charge, and many products and services have frequent sales or discounts based on various excuses such as friends and family sales, flash sales, bulk discounts, early release promotions, clearance sales, discontinued sales, leftover items sales, etc.\n\nPlease note the AMD RX 7600XT 16 GB video card is going to be released near the end of January 2024 at around 330 US dollars while several Ryzen 7 and 9 CPU chips are currently dropping prices for the third generation series between 150 and 350 US dollars."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "For those having problems with ARC cards.",
    "selftext": "So I bought a Arc A750 and it has been great however when I went to plug in my secondary display it did not work no matter how hard I tried. However I tried replacing the cable with a newer HDMI cable it worked perfectly fine. The old cable still worked I just think that the newer version of HDMI did not like the older cable. Hope this helped!",
    "comments": [
      "![gif](emote|free_emotes_pack|upvote)",
      "What monitor is your secondary display? HDMI should be backwards compatible, just with the limiting factor of bandwidth depending on the older HDMI cable.",
      "Had the same issue with my 4070 and display port actually.",
      "Interesting it might just be HDMI 2.1 having some weird problems."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Intel's new desktop graphics card Arc A750 surpasses RTX 3060",
    "selftext": "",
    "comments": [
      "Is not confusion is follow their CPU naming convention. A IS the generation, 7 is tier of the GPU like i7 CPU and the other 7 or 8 is the tier inside that bracket.",
      "I made [this](https://imgur.com/a/k9w7O7C) very rough guide to best understand how the nomenclature scheme relates to AMD and Nvidia.",
      "This looks amazing,\n\nCan't wait for A770 and A780, wish they would follow nvidia/amd xx50 xx60 xx70 xx80 xx90 naming scheme. The current naming scheme is very confusing IMO",
      "Fuck, I hope. Nvidia deserves to be ruined for what they did to gamers in 2020 and 2021. I’ll honestly let the 40 series rot on shelves for as long as I can justify it.",
      "How those compare price wise?",
      "looking forward to AV1 benchmarks",
      "Intel has decades of brand history.\n\n*MOST PEOPLE* aren't fully aware (or care) that Intel iGPU is why their laptop sucks at gaming, they just know \"hurr I didn't buy a GAMINGZZZZ LAPTOPSZZZZ\"\n\nThe instant Intel drops a billion dollars on marketing \"OMG RGB GAMING LAPTOP!\", stuffs store shelves with them, etc, it'll fly off shelves.\n\nNvidia's dominance isn't because of skill or a superior product(they've often had the inferior products). It's because they *vastly outspend* AMD on *marketing*. Marketing is *the* lynchpin on which this industry turns and will be the only determinant between success or fail. And Intel can 100% outspend Nvidia on marketing.",
      "nVidia are being greedy, no doubt but they will pay for that this year. Word on the street is, AIB partners want nVidia to delay the launch of the 4xxx series because they still have piles of unsold current gen GPU's.",
      "[Here's the video that this stuff is from in case anybody wants to watch it.](https://www.youtube.com/watch?v=uctDN9uYcXI) Happy to see Ryan Shrout in the spotlight again. Loved listening to the PC Perspective stuff while he was there.\n\nI think it's pretty impressive. Would have liked to have seen ray tracing relative performance numbers, but I suspect those aren't great or they would have shown them off. Also, that [Limited Edition cooler](https://cdn.videocardz.com/1/2022/07/ARC-A750.jpg) [looks fucking awesome](https://cdn.videocardz.com/1/2022/07/INTEL-ARC-GPU.jpg).\n\nI don't doubt that Intel has technology that can compete, but they're gonna have to price it carefully. They can't expect consumers to pay Nvidia/AMD money for an unproven product with no brand history, especially with Nvidia expected to release new hardware over the next 6 months.",
      "Looking forward to it, most people don't need a card better than the 3060 anyways. Lets hope that they work without crashing.",
      "Not just marketing, but volume sales and O.E.M connections as well.\n\nIntel can Easily strong arm Nvidia out of the mx450~gtx 1650 mobile market segment.\n\nSince they're running with 6gb of gddr6 as well, they can also steal market share away from the 4gb RTX 3050(Ti) laptops.",
      "They can claim anything... It doesn't mean it going to outperform 3060...",
      "Hope it does full AV1 transcoding at 1080p @30FPS at least…",
      "Maybe they could ship DXVK with their drivers?\n\nSeems like that could be a stopgap measure for a lot of their driver issues at the moment, seeing as their Vulkan performance is better than DX11.",
      "They could, that’d be an intriguing option. Even so, Intel missed the boat: Alchemist is basically a dead generation now, which may be an intriguing beta test but certainly won’t be anything more than that.",
      "How much do you wanna bet it’s a 3050 in everything else? I don’t wanna go there, but the driver troubles are so much worse than I thought, and this isn’t like Zen, where AMD punched a lazy and complacent Intel in the mouth, because Nvidia and AMD are both at the absolute top of their game right now (in a features sense for AMD and in a cynical corporate price-gouging sense for Nvidia).",
      "These things dont even have a release date as far as we know, even if this is released I wonder how much available they will have."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Arc Control not responding",
    "selftext": "I own an Intel Arc A750. \n\nThe Arc Control center isn’t responding when I try to do anything on it. \n\nThis is not a matter of old hardware or old drivers. \n\nI am on the most recent beta driver (31.0.101.4335) I installed the beta driver due to someone from another post saying it wasn’t working and posts inside that post suggesting the beta driver fixed their issue. Likewise I am running a brand new system. Ryzen 5 7600 32gb ram and Arc a750. I’ve had no issues with the gpu itself but the control center is another story. \n\n",
    "comments": [
      "Report it to intel gpu support team.",
      "The gpu itself works fine. This control software is another story",
      "Have you tried ending it via Task Manager (May need to use the \"Details\" tab) then restarting it?",
      "Hi! I had the same issue. Could you try to disabling and enabling again you resizable bar option in your motherboard bios? It worked for me :)",
      "Can you tell me how to do that? I googled “Intel arc gpu support” with no results",
      "Yeah. That’s the only way I can close it.",
      "Which is…still a problem. If someone wants to return a GPU because the provided software doesn’t work whether it be the control software or the drivers themselves, then why shouldn’t they?\n\nNo idea why that dude is being downvoted.",
      "I ended up returning the card, too many problems to worth keeping it, Intel need to get their drivers right"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Intel Confirms Arc A770 & Arc A750 Launch For 12th October, A770 Limited Edition For $349 US, A770 8 GB For $329 US, A750 8 GB For $289 US",
    "selftext": "",
    "comments": [
      "Great, especially for GPU computing apps.\nCompared to nVidia & AMD, these things have especially great F64 performance.",
      "Bah, when’s the last time that ever stopped us. If we’re lucky we can also use it to contribute to heating the house :D",
      "I was under the impression that Intel gaming gpu's don't support F64, see toms article [https://www.tomshardware.com/news/intel-arc-will-not-support-fp64-hardware](https://www.tomshardware.com/news/intel-arc-will-not-support-fp64-hardware)",
      "I think we all know an unreliable source for leaks :)",
      "I hope they will perform well. I need a new affordable GPU",
      "Who’s gonna tell him…",
      "Also have open source drivers on Linux, which Nvidia does not.",
      "How limited will it be? 16 gigs will be perfect for CGI work",
      "Those prices are actually pretty damn good. I can't wait for the next gen chiplet design.",
      "The A770 pulls 55W more than the 3060, and 25W more than the 3060Ti."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Intel ARC A750 GPU | Here comes a New Challenger !",
    "selftext": "",
    "comments": [
      "This is old news.",
      "New for India, or that I got access to a card only now. 😬"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "a750"
    ],
    "title": "[Digital Foundry] Intel ARC A770 / A750 Graphics Review: Here Comes A New Challenger",
    "selftext": "",
    "comments": [
      "The one i was waiting for.",
      "It's actually crazy how well Doom Eternal runs that you could buy a $350 graphics card and get a locked 4K 60 Ultra experience. And it's not even a tier 1 game.",
      "Can we get clarification about the free copies of MW2 and Gotham Knights? \n\nI see on Intel's website they want a purchase of 12th gen 12600K or greater. I am willing to get the A750 on launch for media applications because I was going to buy those two games anyways and the value works for me - but not if I need to buy an intel 12th gen CPU.",
      "https://youtu.be/a-KmhAEuHXw?t=748",
      "Shrout clarified on twitter that you'll get the free copies regardless of CPU purchase \n\nhttps://twitter.com/ryanshrout/status/1576198656428244992?s=20&t=Jru4IGNddradHgzc2LBGdw",
      "1440p BEAST",
      "The moment intel can compete, the prices will become comparable, the only reason they are cheaper now is because no one would buy them if they weren't due to their performance",
      "I am very impressed by arc 770 RT performance.If their perf target for next gen is something faster than 6800XT(i have one now) and they keep good price i might swap my 6800XT for that. I am sick of AMD/NV playing DUOPOLY and price fixing last 8+years.",
      "More supply is more competition. The only issue is that all three of the companies are using TSMC right now. \n\nIt would be better (for competition and supply) if Nvidia would have stayed at Samsung and Intel would use their own fab i guess. Right now TSMC is the big winner with 3 companies competing with each other but also buying their services and products from TSMC.",
      "Lowkey don’t take digital foundry’s hardware stuff seriously anymore, look at what happened with Turing and Ampere…"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Is Arc a750 compatible with MSI b450 gaming plus max motherboard??",
    "selftext": "I have a ryzen 5 3600x with a MSI b450 gaming plus max motherboard and a 600w psu. Can I use intel arc a750 with this configuration or do I need to upgrade my psu and/or motherboard?",
    "comments": [
      "If you have rebar in bios then yes.",
      "Yeah it will work well\n\nCheck for resizeable bar support as that can have a big impact on arc graphics\nhttps://youtu.be/eYsk7U3vYRk?si=cMZjDi9P8ayP3gdK"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Gonna buy an Intel Arc a750, what should i look out for? Is it even possible to use it in my build?",
    "selftext": "My specs:\n\n\n\n\nProcessor: AMD Ryzen 5 3600\n\n\nMotherboard: ASRock B450 Pro4",
    "comments": [
      "Eh theyre kinda twitchy and you need a new CPU to get the most out of them, but I'd say given what nvidia is selling at their price range,t he A750 is worth looking into. I'd still prefer the 6650 XT I actually bought though. \n\nNvidia is just way overpriced atm and has no decent options under $300. Heck even under $400 if you're going by value. \n\nThe A750 or 6650 XT would compete directly with a 3060. Might even win a lot of the time.\n\nArc is still kinda immature so id expect a lot of \"edge cases\" where things dont quite work well, but when it works, it works. \n\nIdk, I mean, given the market, Arc is an option worth considering at the current price. I'd still prefer AMD though tbqh. THey've been making GPUs for longer, they (generally) know what they're doing, and I'd expect it to \"just work\" tbqh. \n\nYes yes, AMD has driver issues time to time. So does nvidia, but no one talks about that. They all do. Only reason im giving arc a hard time is because for a while they just flat out ran older games worse than my old 1060.",
      "You might go for the Arc a770 as the uplift is much higher and you can also enjoy double the amount of vram. (Be aware there are also arc a770 8gb vram models)\n\nBesides that definitely enable Rebar (Resizable BAR) in your bios. Because if it's disabled the gpu won't work properly or at it's full capacity.\n\nOther than that make sure to download latest intel arc drivers from their official website.",
      "Alright! Thanks a lot, u bet there are tutorials on tebar somewhere on youtube.\n\n\n\nAnd the 770 isn't really interesting to me in this build at least, as the pc is a gift to someone who mainly plays Sims 4 on 1080p xD",
      "Are the intel arc gpus any good compared to Nvidia “budget” gpus and amd budget gpus?",
      "Tutorials aren't really needed. Go to bios and search for Rebar (Resizable BAR). Or if you prefer research before it, look first online with a google search (your mobo) enable Rebar, etc.",
      "The A750 is very competitive for its price. A770 less so but still pretty decent.",
      "Thanks a lot for your kind help 🙏",
      "No problem!",
      "With a B450 you *may* need a bios update to get ReBar.",
      "Alright, i dont worry too much about that as i did these 2 or 3 times already, never had problems :)"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Intel ARC A750 and A380 as eGPU for GPD Win Max 2",
    "selftext": "",
    "comments": []
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "a750"
    ],
    "title": "Intel arc won’t POST or hangs if it does",
    "selftext": "So I literally was intel’s hype man. I recommended A750 as a great buy but with the caveat of driver issues.\n\nI did not expect this much headache! It’s in a new build and when PC didn’t POST first boot, I had a heart attack not sure what it was.\n\nAnyway luckily ryzen 7900x comes with integrated graphics, and the minute I plugged DisplayPort there everything worked, except the A750 isn’t even detected in device manager.\n\nHours of troubleshooting later and I give up. I’ve tested this card in 3 systems, B650, B550, 8th gen intel, all the same result. Black screen with signal, or showing no signal, or it’ll work for a second then nothing.\n\nI’ve flipped rebar on and off, manually set PCIE x16 slot 1 to gen4. \n\nDid not try single ram (well I did, just not thoroughly). Did not try secondary x16.\n\nWhy is this card such a POS, like how can they even release a card that can’t reliably do basic video out.",
    "comments": [
      "What are the chances I just got a bad card? I like a 3rd company in the GPU market but idk if I want I’ll just get burned buying another arc.",
      "There's chance for a GPU to arrive dead or damaged, probably like 0.01%\n\nYours seems to be dead.\n\n\nIf a GPU doesn't do display out when correctly plugged in 2 PCs it's just dead, no point holding onto it and trying whatever sorcery",
      "RMA.",
      "My A770 didn't show up in device manager or anything else until i installed the drivers for it. Although i did get a display signal out of it.",
      "You make sure the pcie power cable is seated all the way, on both sides?",
      "Had the Same...its toasted..rma...the new Card works now..."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Intel ARC A750 GPU | Here comes a New Challenger !",
    "selftext": "",
    "comments": []
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Arc A750 or RX 7600",
    "selftext": "I was wondering that should I get the A750 or RX 7600. The RX 7600 is 330$ and the A750 is 310$(other gpus are very overpriced) here. I was thinking of paring it with a R5 5600 and a B550 mobo. So whats the best option?",
    "comments": [
      "The performance of the Intel A750 is between an AMD's RX 5500XT 8 GB and an RX 6600 8 GB. The A750 games above a GTX 1650 Super, but usually below that of an RTX 2060. It's performance is better on newer games that use DX12 protocols, and it finally can play older games at adequate gaming rates due to using emulation software for titles that require DX 9 through 11 commands. The stuttering is still there but tolerable. The price in the States is currently in the 200 to 220 US dollar range.\n\nThe preferred video card is the RX 7600 8 GB that performs about the same as an RX 6700 10 GB card. The current price of the RX 7600 8 GB is about 270 US dollars while the RX 6700 10 GB is around 280 dollars. Both cards perform at the same level as an RTX 2080 and above the RTX 3060 8 GB or 12 GB versions.\n\nWhich is better? The RX 7600 is more than 20% better with less stuttering. But the initial production of the new video cards have some brands may have bad power cable connectors.\n\nIf you do not use the latest triple A games and stick to less than 1440p or 2k resolution, then either card will be okay. Basically it's your decision between budget to spend and gaming quality.",
      "Id go 7600 for the more mature software.",
      "I see the 7600 for $270 US.",
      "Yeahh amd rx 7600 go for",
      ">ands m\n\nI wont be having any problems with the bad power power connectors because I will also buy a new PSU."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "I betten buying a Rtx 3060, Rx 6600 or Arc A750. Which one is better?",
    "selftext": "For me all 3 graphics cards are the same price, but i'm not sure which one is better. I use my pc mainly for gaming, 3d renders and sometimes for vr. Which card do you guys recommend.\n\n&#x200B;\n\nAlso the Rtx 3060 is the 8g model and I know the Arc A750 cant run vr, I can wait when they add support",
    "comments": [
      "Pricing video cards for the RTX 3060, RX 6600, and the Intel ARC A750:\n\nCurrently, the RTX 3060 8 GB is as low as 250 while the RTX 3060 12 GB is in the 280 dollar range. I don't understand why someone would not be willing to spend an extra 20 or 30 dollars to get double the amount of video memory for an RTX 3060. This card is more than 10% to over 20% better than either the RX 6600 8 GB card or the Intel ARC A750.\n\nThe RX 6600 8 GB is priced in the 210 to 240 dollar range, while the A750 is around 240 give or take 20 dollars. The RX 6600 can beat the RTX 2060, but it's more than 10% weaker than the RTX 3060. \n\nThe Intel A750 does have the processing power to beat the RX 6600, but it cannot compete well, because it has video driver issues and a manufacturing design defect in the chip that slows processing down. It's rated gaming experience puts it above an RTX 3050 8 GB card and below an RTX 2060 8 GB for games that use DX12 (API). Games that use DX11 or older have performance levels that are 20% lower because many games are played using an 'emulation mode' that makes playing the game possible. Most people using the GPU will not notice the lower performance because they have no experience in comparing if they are actually getting a better value.\n\nMy recommendation is to either get the RTX 3060 12 GB card which is the best performer of the three choices, or get the RX 6600 8 GB which is the cheapest, and it's better than the Intel A750 8 GB.\n\nIf you are going to spend at least 260 to 300 dollars, just get either the RX 6600XT 8 GB, RX 6650XT 8 GB, RX 6700 10 GB, RX 6700XT 12 GB, or RX 7600 8 GB, because all of these AMD video cards are more than 10% better than the RTX 3060 12 GB card while all of them are currently selling between 250 and 300 dollars.",
      "If theyre the same price, I'd go the 3060.\n\nEDIT: NOT THE 8 GB MODEL THOUGH.\n\nOkay, now I'm inclined to say 6600. The 12 GB 3060 is better than the 6600 but vs the 8 GB version (which sucks) go 6600.",
      "Don't listen to anybody else. If the ram is the same go for geforce you will have tue oat support for games. Ie performance will be better",
      "Thanks man! This helped me a lot."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "arc a750 / a770 idle power draw",
    "selftext": "According to [techpowerup.com](https://www.techpowerup.com/review/intel-arc-a750/38.html) the idle power consumption for the new Arc a750 and a770 is at 39W and 44W. Intel gave instructions on how to fix this issue found [here](https://www.intel.sg/content/www/xa/en/support/articles/000092564/graphics.html?countrylabel=Asia%20Pacific). Has anybody tried this? did it reduce your idle power consumption?\n\nHonestly, this is the only issue I have with Intel's new graphics cards.",
    "comments": [
      "Correct. I mentioned that before I had gotten my card and realized there was native power monitoring built into the arc control panel.",
      "Same here on Intel ARC A770 LE.  \nMy MSI-B550-PRO motherboard doesn't seems to have those settings.  \nAnd the Widows setting did nothing.",
      "Just try it.",
      "Remind me to try this tonight. My A750 comes in, and I’ve got a Kill-a-Watt meter I can test with. \n\nI’m not holding my breath though, because on high refresh rate monitors the clocks usually stay up (Intel even admits it). Same thing happens on my 6800XT.",
      "My Z390 Master's BIOS (F11n) has some corresponding options:\n\n* Platform Power Management\n* PEG ASPM\n* PCH ASPM\n* DMI ASPM All of them could either be enabled or disabled and neither of them did anything. I've also got the corresponding Windows power setting set to *Maximum Power Savings*, yet idle consumption remains \\~40 W.\n\nCuriously, GPU-Z reports the Bus Interface as `PCIe x 16 4.0 @ x 16 3.0`, which is expected on my Z390 platform, but with the 2080 I had previously installed, it would switch down dynamically to `@ x16 1.1`. Maybe I should reinstall it at some point to see if the 2080 still does that, though I am not very keen to uninstall the A770 (it's the 16GB LE) again just yet.",
      "Z490M-ITX Asrock. Native ASPM = Enabled and no other BIOS setting along with the Windows settings has made my idle power consumption as low as 10W per GPU-Z. Typical is 15-20W.",
      "Sorry, I should of been more clear. I don't own one and want to see if the idle power consumption fix works before I buy one.",
      "be reminded",
      ">Kill-a-Watt meter\n\nDoesn't that test whole PC's power draw? IMO you can instead see the GPU power draw in some program like HWInfo64 instead. Mine draws \\~40W on idle.",
      "I couldn’t find the BIOS settings, but changing the PCIe power saving did nothing. Average idle before and after was within margin of error sadly."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Intel Arc A750 Flickering",
    "selftext": "So I’ve noticed when playing games like ROBLOX the Intel arc A750 is flickering? Any suggestions on how to fix that?",
    "comments": [
      "Report it to intel support.",
      "Latest drivers? Try running it in windowed mode, Roblox seems to have troubles with full screen",
      "Definitely will do, thanks for the tip!",
      "Latest drivers and windowed mode both were already in use, I wonder if it’s just a problem with ROBLOX and the optimization for the arc? Every other game seems just fine?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "a750"
    ],
    "title": "ASRock Intel Arc A770 8GB and A750 8GB GPUs in stock at Newegg. Go!",
    "selftext": "[https://www.newegg.com/p/pl?N=50001944%20100007709%208000&Manufactory=1944&SrchInDesc=arc](https://www.newegg.com/p/pl?N=50001944%20100007709%208000&Manufactory=1944&SrchInDesc=arc)",
    "comments": [
      "A770 now showing OOS. Pain.",
      "I'm really curious, if they are selling like hotcakes or if supply is low, if i remember correctly its been said these cards had already been made and sitting in warehouse while they optimize the drivers",
      "An Intel employee in their discord confirmed that AIB buyers will also be eligible for the game bundle, not sure on the process though. (whether it's automatic or not)",
      "I don't know for certain, but the T&C aren't clear on it either. They reference Intel Arc products, which in this case could be both a brand or a manufacturer. Might have to literally tweet Ryan Shrout to find out for sure.\n\n[https://softwareoffer.intel.com/Campaign/Terms/7c4740d1-7b93-4626-9fbe-d58a7a0a38ef](https://softwareoffer.intel.com/Campaign/Terms/7c4740d1-7b93-4626-9fbe-d58a7a0a38ef)\n\nETA:\n\n[https://twitter.com/fsucory/status/1580256023096721408](https://twitter.com/fsucory/status/1580256023096721408)\n\nSomeone's ahead of us. No answer yet though.",
      "I did. I mean I'm cool either way, but the added benefit is nice (plus I could cancel my MW2 preorder...)",
      "Haha, well then I gave your reply a like. ;)",
      "\\*whispers\\* That's me...lol",
      "Is the deal for the four games and software included? Not showing up on the page for either card",
      "Which discord is that? Do you have a link?",
      "It's the Intel Insiders Discord: https://discord.gg/r6Az4q6D"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Should I get a used 3060 or brand new arc a750? For 1080p vr and ray tracing",
    "selftext": "I’m planning on getting it in march so hopefully prices are cheaper but I can get a used Asus Tuf 3060 12gb for £310 or a brand new arc a750 8gb for the same price what would be the better option for 1080p vr and ray tracing? Or should I save my money and get a rx6600 for £240?",
    "comments": [
      "6650XT new for £299.99 and it outperforms the 3060. Not as good for ray tracing but a better buy IMO https://www.overclockers.co.uk/msi-radeon-rx-6650-xt-gaming-x-8gb-gddr6-pci-express-graphics-card-gx-38s-ms.html",
      "Um with all those u not really gonna be able to do raytracing but if I would to pick I would say 6600 xt",
      "Good RT only exists at the high end, it's overrated at this point IMO, but don't buy a low end card if it's that important to you.\n\nThe 3060 12gb is no better than the 6gb version, it's not powerful enough to use the extra memory.\n\nThe only reason to get the Intel is if you need av1 encode.",
      "Yeah lol \nWe don't even know the CPU... but we do know exactly what they're getting with Nvidia/AMD. Unless Intel were like 50% cheaper for similar performance it makes no sense to even consider them.",
      "3060 cause its made for gaming",
      "why would anyone buy the intel card",
      "This.\n\nAnd ray tracing at this performance point is just not good. It barely makes sense at the high end.\n\nFSR 2 / 2.1 is getting pretty close to DLSS, too.",
      "They’re all made for gaming, the ones he listed at least lol.",
      "AV1 encoding... nostalgia one day...\n\nThat's about all.",
      "lol exactly"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Should I can buy Intel arc a750 graphics card for productivity purpose??",
    "selftext": "My computer specification is Processor Ryzen 5600x ; Motherboard b550mds3h ; Ram16gb 3200mah; SSD 250gb nvme. ; Hdd 1tb ; Psu 650w bronze I want to do productivity work is it ok to get Intel ARC A750 GRAPHICS CARD with this computer specification?",
    "comments": []
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Should I buy arc a 750 graphics card for productivity purpose??",
    "selftext": "My computer specification is Processor Ryzen 5600x ; Motherboard b550mds3h ; Ram16gb 3200mah; SSD 250gb nvme. ; Hdd 1tb ; Psu 650w bronze I want to do productivity work is it ok to get Intel ARC A750 GRAPHICS CARD with this computer specification?",
    "comments": []
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Will buying an Arc graphics card be worth it with my AMD Cpu?",
    "selftext": "I'm currently running an AMD Ryzen 7 5800x on a MSI Tomahawk B550 Motherboard, and am really considering upgrading my GTX 1070 to an ARC A750. I've been watching all the reviews and they say I need to enable Smart Access Memory too take advantage of the full power of the card but I don't think I can since I have an Nvidia gpu. Would I be able to enable it with the Arc card, and my 1070 is just too old or something?",
    "comments": [
      "B550 should support SAM, so that part would be okay. Not sure if I would recommend an Arc Graphics card for gaming, if you use your GPU for something else then that would depend on your workload.",
      "I have that motherboard with a 3070 and sam enabled. They released a bios update to support it so if your bios is still original from factory that will probably need to be updated\n\n[https://www.msi.com/Motherboard/MAG-B550-TOMAHAWK/support](https://www.msi.com/Motherboard/MAG-B550-TOMAHAWK/support)\n\n&#x200B;\n\n1070 wont have it as it only started at 30 series.\n\n&#x200B;\n\nhttps://www.nvidia.com/en-au/geforce/news/geforce-rtx-30-series-resizable-bar-support/",
      "6600XT. Unless you want to do free QA for Intel.",
      "Resizable BAR is supported on ARC, RTX 3/4000 and RX 6000 series cards, and SAM is only supported on RX 6000 series cards paired with Ryzen 5xxx and 7xxx series CPUs.\n\nRebar should work on your proposed setup, but I would think carefully about your use case before buying an ARC GPU.",
      "Don’t, just buy a 6650 XT. Arc just ain’t it right now.",
      "Yeah I just can’t get SAM to enable on my 1070 so I wanted to make sure I could on the Arc. And I use it for gaming and blender. I really just want to see these intel cards succeed",
      "Thank you!!!! This answered my question perfectly.",
      "I'd say go for it but expect DX9/11 problems. I suspect the interim solution to that is to either run Linux in a partition on a Arch/Fedora based disto to take advantage of Proton and the built in DX9-12 translation to Vulkan for problem games. Assuming that doesn't scare you.\n\nOr figure out which particular DXVK DLLs you will need to make the game run on Windows and setting startup parameters needed and locate the appropriate folder in the game to place the DLLs in to override DX9-11. Certainly a bodgier solution than letting SteamOS or the Linux Steam client sort it out for you (DXVKs target is Linux and in particular the SteamDeck) but with some fiddling you could use that to get some potential performance back.",
      "Arc gpu are good cards but they will need a time and lot of work to make them use their HW potential especially for older games so if you accept to be open minded during the voyage then sure, if you are faint of the heart then choose green or red league."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Should I buy a Intel arc a750 graphics card",
    "selftext": "My computer specification is Processor Ryzen 5600x ; Motherboard b550mds3h ; Ram16gb 3200mah; SSD 250gb nvme.  ; Hdd 1tb ; Psu 650w   bronze I want to do productivity work is it ok to get Intel ARC A750 GRAPHICS CARD with this computer specification?",
    "comments": []
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "a750"
    ],
    "title": "Should I buy ARC?",
    "selftext": "Am considering buying a750 or a770 second hand for 450 AUD, new ones like 650$+ but wanted to know if its a good idea with my specs as they are a bit dated. i7 8700k, PCIE3 mobo, \n\ni heard stuff like resizable bar and dunno if that's really gonna work with my stuff or do I need to look for Nvidia. i play at 1080p60, do i really need the 16gb model for arc gpu?",
    "comments": [
      "My Asus ROG Strix Z370-I Gaming mainboard with i7-8700K got ReBar support through a BIOS update. Kudos to Asus!\n\nWorks fine with the Arc A750, apart from Intel's bugged drivers...",
      "Iirc the i7 8700K motherboards don’t have the option for REBAR in the BIOS. I think 9th gen (Z390 chipsets) was as far back as they went. Due to this I would not get an Arc card as they are designed to use REBAR and not having it obliterates the performance of the card.",
      "Some z370 boards have beta or regular bios updates that enabled resizable bar.\n\nYou need to check with your motherboard manufacturer to see if that’s available. If you cannot enable resizable bar then I wouldn’t bother with arc",
      "Ah, thanks for correcting me. I wasn’t completely sure how far back they went with it.",
      "Seems like a good price! I'd buy it if I could.",
      "A litte late on I know, but some gen8 have it. Running z370-p with 8600k and rebar",
      "FYI, ReBar is only available on 10 series and newer CPU's. Therefore, your 8 series CPU doesn't have that capability. The head of their GPU division said himself that if you dont have it, then dont buy the Arc. It's whole architecture is built on it.\n\n\nhttps://www.guru3d.com/news-story/intel-arc-to-require-at-least-core-10-or-ryzen-3000-cpu-otherwise-performance-issues,6.html#:~:text=Minimum%20system%20requirements%20call%20for,versions%3B%20this%20feature%20is%20mandatory.\n\nBuy a 3060/3060ti instead if you aren't willing to upgrade the CPU/mobo."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "a750"
    ],
    "title": "Intel, why not do a showing of Unreal Engine 5's Matrix City Sample project to sell your ARC GPUs?",
    "selftext": "Tom Peterson and Ryan Shrout have mention multiple times how they are targeting \"next gen features\", and \"new standards\" when it comes to GPUs. As well as touting how great it is in DX12 titles.\n\n\"*Among game developers,* ***48%*** *of announced next-generation console titles are powered by Unreal*,\" - Epic Games CEO Tim Sweeney said.\n\nIf this GPU is targeted towards the *future*, and the selling point is how Intel will clobber the competition in performance per dollar in DX12 titles, prove it. You're claiming you're beating an RTX 3060 in [Fortnite](https://cdn.videocardz.com/1/2022/07/ARC-A750-vs-RTX3060-1200x675.jpg) already, which is using UE5. I feel like nothing represents next gen gaming more than that engine does.",
    "comments": [
      "Does anyone in this sub understand that intel just started from zero with DGPU business and only time will tell how much success it will have, major obstacles are:\n\n1. Drivers, they will need 1-2 year to get optimized and more people working in driver development/department.\n2. Prices should be 20-30% lower than equivalent counterpart from Green/Read team, yeah they will lose money in beginning but that doesn't matter they need to be competitive.\n3. If they fall to achieve 1&2 then they are fucked.",
      "Probably makes them look bad. It's easy to market your product when it has some kind of performance win.",
      "> Now, according to ~~available information~~ **speculation**, they are running into hardware scheduler issues...",
      "What sucks is that the GPUs actually have an FP64:FP32 adder ratio that’s significantly higher than any of their competitors. On the Intel architecture it’s 4 FP32 for every 1 FP64 ALU, where as on Nvidia it’s 64 FP32 to 1 FP64, and on AMD it’s something like 16 FP32 to 1 FP64. If you know how to write your software you can achieve a MASSIVE speed up, especially for things like scientific simulations. But for gaming only FP32 is really used…",
      "Because it would probably run at like 10fps or just insta-crash on current drivers",
      "My understand that I got from someone that seems to know a lot about graphical optimization, is that a massive amount on even DX12 titles falls into developers hands. Because DX12 is so light on the driver, and close to the metal, it's mostly in the game developers hands to figure out how this architecture works and to squeeze it to get more out. \n\nThat kind of sucks for Intel because the majority of work needed for new DX12 titles isn't really in their own hands. And game devs don't have the budget or time to figure out how to use Intel's hardware to it's fullest. They have no incentive to do so because hardly anyone even uses the hardware. Intel has to pay developers to even bother. And they probably are already. I'm sure the with Epic and CD Project Red had done wasn't free.",
      "Maybe. But since they got Fortnite to run well in DEX12 mode at least, they must be working with Epic. They got a few weeks to figure it out before launch.",
      "Year it depends on how much effort the game made to work on their GPUs. DX12 puts more responsibility on the developer to work close with the hardware to fit their game specifically. It should be better than DX11, but the worst ones won't be by much. But there could be titles where a dev actually tries, and puts a few hundred man hours in, and an a750 could be pushed to 3060ti performance easily.",
      "When it’s coming from Intel AIBs it’s not “speculation” lmfao. If Nvidia was tight lipped about a design flaw and every one of their distributors came out in unison saying the exact same flaw and issue, then it’s not speculation lmfao.\n\nEdit: I think I know who you’re talking about though. Moore’s Law is Dead is not a legitimate source of information, I agree. It is also evident though from their performance figures that it is a scheduling issue. Look at the A380, no matter what you lower the resolution to, the card performs identically.",
      "What happened to \"Rapid packed Math\"? There were like half a dozen games announced years ago when they were hyping up Vega. Are games still using that? Far cry 5 was one of them.",
      "> Look at the A380, no matter what you lower the resolution to, the card performs identically.\n\nDespite what MLID says, [this is just categorically not true](https://www.igorslab.de/en/intel-arc-a380-6gb-in-test-gunnir-photon-total-benchmarks-detail-analysis-and-extensive-teardown/5/). MLID built that entire argument on [JUST the 1% lows on a single test from a single reviewer at a single resolution between an overclocked A380 and an non-overclocked A380.](https://youtu.be/DH2s5HeZzs8?t=528) His reasoning is incredibly poor.",
      "From what I understand it’s still there, you just have to program to use the different data types. If you program an entire engine using 4-byte Floats, then you need to adjust areas where you can go down to 2-byte half floats, but you can’t just convert every single float to a half float because you lose precision, especially on large pieces of geometry. Instead now they use Variable Rate Shading to utilize the included hardware. It’s funny because you saw massive VRS gains on Turing and RDNA2, both of which saw a doubling of FP16 from FP32. Ampere brings it back down to a 1:1 ratio, so it loses almost all of the performance implications…"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Arc A750 (colored dots on the screen)",
    "selftext": "Hello, since yesterday I started using the Arc A750, but today I noticed that some colored dots appear on the screen. I tried to solve it by reinstalling the driver, I checked the cables and some settings, but I couldn't figure out where the problem would be.Hello, since yesterday I started using the Arc A750, but today I noticed that some coloured dots appear on the screen. I tried to solve it by reinstalling the driver, I checked the cables and some settings, but I couldn't figure out where the problem would be, and I wonder if anyone has any idea about this problem.\n\n I want to point out that I didn't have any kind of problem with the previous video card. (GTX 1660 TI)\n\nThank you.\n\n&#x200B;",
    "comments": [
      "Dots coming from a GPU is an artifacting issue that is likely due to a defective video card.\n\nThe problem might be if the GPU is stressed beyond a certain point where whatever is broken inside the video card becomes evident because that section of the GPU or a component of the video card needs to be used or stressed beyond its capabilities. You might try to under clock the video card to see if the issue disappears or you can reduce the work load of the GPU by lower the settings within the software you are using. At this point we do not know if the problem lies inside the GPU chip, a memory component, a circuit weakness, a VRM or capacitor defect, etc.\n\nYou only say dots appear but you give no other information such as when, where, all the time, or only on one game or all games, or during turn on or when the computer is on 10 minutes, an hour, longer, or the temperature, resolution, etc. As I said, You only say you see dots. If it's all the time then the problem is the video card. If the problem only occurs inside one program then the issue is a bug or software issue and not the video card. Problems in video output as to when they occur usually signal what area of a computer is causing the problem.\n\nI suggest you go to an Intel video card user forum to see what other like minded users encounter and request assistance from them.",
      "Thank you!"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "a750"
    ],
    "title": "Intel Arc A770/A750 GPUs get a €248 water block - VideoCardz.com",
    "selftext": "",
    "comments": [
      "This seems silly. I’m glad Intel have made their entry but it doesn’t seem like it would warrant relatively exotic cooling…",
      "Putting a waterblock on an arc card is just a flex. Nothing more, nothing less",
      "Yeah the A770/750 are actually kind of big for their relative performance.",
      "Water cooling has really fairly little to do with performance. EK blocks are too expensive for this card but if someone wants this card in a water cooled pc there are very few options.",
      "the cards dont get nearly hot enough to ever need the use of a waterblock, nevermind one that costs over half the actual card",
      "So roughly it's gonna cost nearly as much as a Rx 6900 xt and u can also get rx6800xt which is cheaper and beats it 😆",
      "It would be a meme for sure. Or you just want to be some (non LN) benchmark champion for this particular model"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "csgo on arc won’t open HELP",
    "selftext": "hello! i made a post last night regarding csgo not opening on my new arc a750, i am coming from a 3070 ti and i just bought an arc to experiment with, i just wanted to make a new post with what troubleshooting steps i did take (game still won’t open)\n\n1) used drivers 4091 and beta 4123.\n2)DDUed both drivers twice and reinstalled after failed attempts of opening game.\n3)DDUed old nvidia drivers.\n4)ran windows update.\n5)disabling things for CSGO like fullscreen optimizations.\n6)performed an sfc /scannow test in cmd.\n7)tried opening game with resizable bar on and off in bios.\n8)flashed mobo bios to latest.\n\nthings that have kinda worked\n\n1)opened the game on an alternate gtx 1650 that is in the computer along side the a750, game opened perfectly fine with that.\n2)put a fresh windows installation on an alternate ssd and put only csgo and arc drivers on it and it opened perfectly fine.\n3)reinstalled game on main OS as a last attempt, the game did open but the minute i tabbed out the game crashed and never to be opened again.\n\nthis card does seem to be defective in some sort because when i use hdmi to one of my secondary monitors, i get really bad artifacting and anomalys on the display. displayport-ports seem to be fine though. i have a feeling it can be a windows build because it is working on another one but the build of windows is only a few months old and has stayed on 22h2 the whole time \n\ni’m just really frustrated and i want to root for intel because this can be a great product \n\nspecs\n5900x\ngigabyte aero x570\n980 pro 1tb\nrm850 psu",
    "comments": [
      "okay i solved the issue, i forced the game to open in 640x480 by manually finding the game and checking that option off under properties, then i put the game into windowed and reloaded with the setting checked off and i was good to go!",
      "Didn't they just release a new driver that optimized CSGO? Weird. Anyway, glad you got it fixed.",
      "Glad that you got it fixed. Mind if i ask about your experience with the card so far?",
      "performance is solid espically on esports titles like csgo, valorant and overwatch, these are really the only games i have played on it but for the price it’s hard to beat. i do wish there was a manual highlight feature with a key bind to record like the past 3 minutes of gameplay like amd and nvidia cards do, but the device is 3 months old so they get a pass, also not having a custom resolution creator is a major miss on this card, i use 1440x1080 4:3 on csgo and valorant and this is such a dealbreaker cause u have to use a stupid 3rd party program do to so, intel used to have it in their graphics settings a long time ago but they took it out. i do have to RMA the card because the hdmi port doesn’t work, every time i use it all i see is a flickering screen, lines, or like half of the display doesnt appear right, also my card struggles running 3 monitors properly but i think it could just be the card cause my 3070 ti has no issues with it. 7/10",
      "Thx for the feedback. I've seen mixed feedbacks about the a750 so i was quite hesitant between it and rx 6600. A750 is about 12% cheaper than an rx 6600 here. \nUsage wise, mine is not much different from yours, mainly eSports title and possibly some non 3a title (my current laptop cant handle it). Often uses davinci resolve for 1080p videos too. From your pov, do you think the 12% price difference worth the purchase?",
      "nah not yet its not ready, maybe if youre are a content creator cause it has an av1 encoder but the software is crappy, itll get better soon tho it seems like intel is putting alot of time in it"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "a750"
    ],
    "title": "i5-10400 + Arc A770 or A750?",
    "selftext": "Hi, I have to upgrade my graphics card. This is my current setup:\n\ni5-10400, B460 MoBo with ReBAR enabled, 2x8gb DDR4 2666, 512gb SSD NVMe.\n\nI'll use the new gpu for 1080p gaming on a 24.5\" 240Hz monitor (games like Far Cry 6, Horizon Zero Dawn, Cyberpunk 2077). Should I pick the A770 380€ or the A750 280€ would be enough?",
    "comments": [
      "In all of those options i would go with a 6650xt, 330 isn’t a bad price, I personally don’t have any experience with intel Gpus. I would shoot for the 6650xt. But for -2 to 5% performance you can save money and go for the 6600xt. But regardless of which you decide, all with provide a very nice 1080p gaming performance. My current set up explicitly is also a 10400f and a 6600xt",
      "Is there a 6750XT in the $370 range? That would be my pick. However, to truly enjoy Maxed out Ray tracing in Cyberpunk, RTX 3060Ti will be your best bet. That's $400.\n\nAlthough I'm unaware how well the Intel cards are at ray tracing at the moment with the new drivers.",
      "Are you needing an arc gpu?, i ask because the 6600xt matches it or surpasses it in most cases. Plus i just got a power color red devil for 230 on eBay. But if you need the intel counterpart i would go with the 770.",
      "A770",
      "So, I've heard the arcs have improved a lot thanks to the new drivers and the A750's become a good deal. Anyway, my current options and prices are A750 280, 6650XT 330, 6700 10gb 360, A770 380. Which one would you pick?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Intel arc a750. No video when installed.",
    "selftext": "I purchased an intel arc a750 to install on my hpz420 workstation, but when i made the swap, there was never any video when it was in the main GPU slot.\n\nThe computer hardware:\n\nPSU: Delta electronics 600W switch mode power supply (DPS-600UBA) (with 1x8 PCI upgrade)\n\nProcessor: Intel Xenon E5-2670 (2.60GHz) 8 cores\n\nRam: Generic 32GB DDR3\n\n(Stock)GPU: NVIDIA quadro 600\n\nHDD: Segate Barracuda 2tb (ST2000DM008)\n\n&#x200B;\n\nHere what I tired:\n\nOS: Linux Manjaro stock BIOS (Arc750 - no signal, Quadro600 - normal)\n\nOS: Linux Manjaro stock BIOS, NVIDIA drivers purged (Arc750 - no signal, Quadro600 - normal)\n\n&#x200B;\n\nOS: Linux Ubuntu 22.04, stock BIOS (Arc750 - no signal, Quadro600 - normal)\n\nOS: Linux Ubuntu 22.04, stock BIOS, NVIDIA drivers purged (Arc750 - no signal, Quadro600 - normal)\n\nOS: Linux Ubuntu 22.04 (22.10), stock BIOS, New kernel installed (from intel's tutorial) (Arc750 - no signal, Quadro600 - normal)\n\nOS: Linux Ubuntu 22.04 (22.10), stock BIOS, New kernel installed (from intel's tutorial), Nvidia drivers purged (Arc750 - no signal, Quadro600 - normal)\n\n&#x200B;\n\nOS: Windows 10, stock BIOS (Arc750 - no signal, Quadro600 - normal)\n\nOS: Windows 10, reset BIOS/CMOS reset (Arc750 - no signal, Quadro600 - normal)\n\nOS: Windows 10, BIOS Updated (Arc750 - no signal, Quadro600 - normal)\n\nOS: Windows 10, both NVIDIA drivers and device pointers removed (Arc750 - no signal, Quadro600 - no signal ofc)\n\n&#x200B;\n\nI have never gotten a signal from the arc a750, However the fans still spun, the lights still turned on, just no signal. All the PCI power connectors were connected. Each test was booted with the different GPUs and each of the output ports was tested, both booted up with it plugged in, and repluged when the machine was on.\n\n&#x200B;\n\nI started noting everything about the latter tests:\n\nThe ARC GPU got hot for the first 3 min of operation, then returned to normal.\n\nThe monitor blinked the \"no signal sign \" whenever I  unplugged or plugged in.\n\n&#x200B;\n\nI am not going to use the arc750 for high performance graphics, it is mostly going to do stream processing, so even wildly inefficient solutions are welcome.\n\n&#x200B;\n\nEdit: more testing:\n\nArc in the main PCI slot, Quadro in the second slot, monitor connected to Quadro:\n\nI would get video if and only if the Arc was not powered. This means that the Arc is recognized, it just won't give video.",
    "comments": [
      "Did you try setting the primary GPU in bios?",
      "I had the exact same issue when I installed my A750, windows would boot and everything would work, but no video, after restarting a few times it worked.",
      "Each test was done with only 1 GPU at a time."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Should i buy arc a750? for a video editor here",
    "selftext": "Hey guys,Should I purchase an Intel arc a750 8gb as a freelance video editor,I would be using Davinvi Resolve as my editor and i cannot increase my budget at all...? Is there any better GPU in this segment? I dont think so...\nAlso i am a passionate gamer and i know how epic this gpu is on gaming.\n\nPlease help mates.Thank you",
    "comments": [
      "https://www.pugetsystems.com/solutions/video-editing-workstations/davinci-resolve/hardware-recommendations/\n\nIf you cannot increase your budget at all, I suggest you try to find a used RTX 3060. Depending on where you live, they can be found for around the same price as a new Arc a750, and you'll have a much better experience using Davinci Resolve."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Intel ARC GPU runs very bad on a game that is only taking 40% load",
    "selftext": "I used to have an Radeon RX 570 before upgrading and it would run Spider-Man (Miles Morales) at 30-60 fps. now I am getting 20-30 fps with at avg load of 40% (not due to any bottleneck) This is the only game I experienced this on.\n\nCPU: Ryzen 5 5600x    GPU: Intel ARC A750 (8gb)\n\nCan anyone help?",
    "comments": [
      "did you turn on resizable bar",
      "Try restarting. Try both the latest whql and beta drivers. Try DDU to wipe all amd and intel drivers in windows safe mode.",
      "Getting the same problem on Horizon Zero Dawn. ~40 fps at 1080p max on the benchmark--I was getting over 90 with a 3060. GPU rarely goes over 40% utilization. The HZD benchmark gives separate CPU and GPU FPS: GPU says 99 (in line with what I'd expect) and CPU says 47. I would think it's a CPU bottleneck but my CPU doesn't go over 60/70% utilization. \n\nR5 3600, Win 10, ReBAR on, GPU running in PCIe Gen 3",
      "yes."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "intel arc a750 fps drops",
    "selftext": "i recently bought a intel arc a750. I really enjoy it, but I get huge fps drops in Warzone 2. Does anyone know how i can fix this problem?\n\n(I enabled resizable BAR and updated the drivers.)",
    "comments": [
      "Have you waited for full Shader Compilation?\n\nIf so, the advice is sad, but: Radeon or NVIDIA."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Intel Arc A750: Turn off the logo light?",
    "selftext": "That's a very bright light! I would rather like to turn it off. \n\nApparently you can control the logo light on the A770, but the A770 LED control software refuses to run without detecting a A770.\n\nAny chance to get rid of that light without taping it over?",
    "comments": [
      "I personally have taped it over after a few days. I didnt know about the LED control software tho",
      "I figured that much by now. It's really a shame, just a simple DIP switch on the side of the board offering maybe 0%-33%-66%-100% would such an appreciated quality of life improvement.\n\nI don't mind the branding, just don't make it a glowing Sun. It's taped over with some transparent but really dark packaging film I had laying around now (subjectively blocking 90%+ of the light): https://i.imgur.com/FPixW0M.jpg I like it - it's fine.",
      "The A750 doesn't have controls for its LED lighting. Black tape is my suggestion.\n\nOtherwise, you'll have to open up the card and pull out the LED cable thats powering it.",
      "I agree. I mean, what's the point of putting LEDs on a gpu if you won't let users control it. Or at the very least an on/off toggle."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "can i use a intel arc a750 whit i3-10100f?",
    "selftext": "hi guys, i want to buy a new pc (i have a old one whit c2d e8400 and gma 4500 \\[very bad\\]), whatcing some videos and stores i have find an arc a750 and a i3-10100f, next i try to search some bottleneck test (nothing found). Yesterday i found a video about the arc a380 and they says that for installing the drivers yu have to use a IGPU, so i have to use a igpu for installing the drivers of a arc a750 whit a i3-10100f? \\[sorry if my english is bad\\]",
    "comments": [
      "Yea you can use it perfectly no need to worry",
      "Yes. You don't need another GPU just for drivers, microsoft has a default driver for video cards. It makes your dGPU run (but poorly) so you can get a copy of GPU driver and install it."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "[TechPowerUp] Intel Arc A750 & A770 Unboxing & Preview",
    "selftext": "",
    "comments": [
      "More of a preview than a review, I didn't know what flair to use. \n\nI love the Arc branded toolkit they're giving reviewers for teardowns",
      "Looks like the gamers nexus toolkit.",
      "Looks promising but lower end than I was hoping.  If it was on par with a 3070 I'd jump.  Still might simply because I can probably afford it!"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Intel Arc A750 Flickering",
    "selftext": "PART 2) Owner of an Intel arc a750, been trying to figure out how to stop screen flickering on games? Any tips on how to stop the flickering?",
    "comments": []
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "a750"
    ],
    "title": "Intel 11700K + What ARC?",
    "selftext": "Hi there!\n\nI'm looking at buying an ARC gpu to have an all-blue setup. I have a 11700K and I'm wondering if it could handle the A770 16gb or if I should go for the A750 8gb instead. What do you guys think?",
    "comments": [
      "What are you planning on using the card for? Gaming or productivity?",
      "Should be fine.",
      "Assuming gaming, if you're doing 1080p A750 will be perfect, you won't need an A770  \nIf you do 1440p gaming, bumping up to A770 will help.",
      "Ik it's late. I have a i7 11700k and it had a 2060 super in it and my cpu hardly ever goes over 35% usage on any hard gameing max settings 1440p 75hz. AAA titles. I'm going to buy a arc intel a770 16gb soon. Did you buy one? How did it do if so. Also I belive our cpu can handle like a 3090ti.",
      "Both actually, and I would say less gaming. I have my own business and do a lot of video editing, but also 3d modeling, graphic design, web development, etc.",
      "I would say less gaming and more things like video editing, 3d modeling (Blender, Solidworks), graphic design (Adobe CC), web development, etc. I have a 34\" 1440p ultra-wide screen, so then maybe the A770 would be the best fit? (meaning workload + occasional gaming)"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Intel Arc | Intel Arc A750 Limited Edition Graphics Card Showcase - VRR/HDR/HDMI",
    "selftext": "",
    "comments": [
      "> Any adaptive sync certified display will do the same and should work with Intel Arc graphics, but we’re validating 100+ top VRR displays to make sure you have an amazing experience when the Intel Arc A700 family of cards launch.\n\n> Finally, we can discuss HDMI standards and what you can expect on Intel Arc products. While the A-series of GPUs supports HDMI 2.0 natively, partners and OEMs can build in support for HDMI 2.1 by integrating PCONs that will convert DisplayPort to HDMI 2.1. Our Intel-branded Limited Edition cards, both the A750 and A770 variety, will support HDMI 2.1 through this method. Other add-in cards, and notebooks, will support it if integrated.\n\nhttps://game.intel.com/story/intel-arc-graphics-vrr-hdr-hdmi/",
      "So now I'm wondering if my use case would still be a good one for Arc. HDMI 2.1 is a must have so I can get 4K120, but I would also like FreeSync over HDMI (which my monitor *does* support officially).\n\nIt has to be HDMI because the \"monitor\" is actually an OLED TV which doesn't have DisplayPort.",
      "Folks should keep in mind that Intel is likely only supporting the official standards for VRR. This means for Intel Arc, \"FreeSync over HDMI\" monitors that **do not** have HDMI 2.1 - or used with an ARC card without the converter - may or may not work with VRR.",
      "Now that ARC reviews are released, do you know if HDMI 2.1 forum VRR is part of the official release spec for A750/A770? Has anyone one tested this yet?\n\nUse case is for 4K high refresh displays with only HDMI input (e.g. OLED TVs)"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Intel Arc on HDMI 2.1 TV - Blank Screen",
    "selftext": "I just got my Arc A750, and I’m having a weird problem with the HDMI out that makes me think Intel might have been lying about the HDMI 2.1 support on LE cards.  \n\nI have a Samsung S95B, and with Input Signal Plus turned off it works fine (albeit with detecting HDR as unsupported and capping at 60hz). But when I try to use Input Signal Plus to get 120hz and HDR it’s a blank screen. It flickers like it’s trying to get a signal but ultimately doesn’t come back. \n\nShot in the dark considering the cards are a week old, but has anyone else run into this?\n\nEdit: it seems like the card doesn’t like my HDMI switch. I would attribute the issue to the switch entirely, if not for it having no issues with a PS5, Series X and 6800XT all using HDMI 2.1.",
    "comments": [
      "I am planning to do an HDMI 2.1 test on my card when I get the chance",
      "Do you see any warning/error messages in the Windows event viewer related to Intel?",
      "Is that a HDMI 2.1 rated  hdmi cable?",
      "Nothing that I can see.",
      "The one I switched out wasn’t, but the others are. Even with both being 2.1 certified (and working on three other HDMI 2.1 devices) I still got disconnects with the switch.",
      "Yes intel is lying it's probably hdmi 1.0."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Games keep crashing with ARC A750 and get KERNEL_MODE_HEAP_CORRUPTION someone pls help",
    "selftext": "",
    "comments": [
      "Whatever happens, just know that we are proud of your sacrifice to have bought an intel gpu.",
      "https://www.intel.com/content/www/us/en/support/contact-intel.html#support-intel-products\\_80939:227960"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "EK Launches Water Blocks for the Intel ARC A750 and A770 GPUs",
    "selftext": "",
    "comments": [
      "https://gfycat.com/coolfluffyboaconstrictor",
      "When you spend more on your waterblock than on your GPU.",
      "Maybe I'll try it next year. I am up for a challenge water cooling a Formd T1.",
      "Looks really pretty, maybe get a bit of extra performance out of it. Mostly for aesthetics though. Can also see some potential for ultra slim form factor builds."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "750",
    "matched_keywords": [
      "arc a750",
      "a750"
    ],
    "title": "Have any of you played Squad with Arc a750?",
    "selftext": "",
    "comments": []
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Arc A770 Available 10/12 for $329",
    "selftext": "",
    "comments": [
      "329$ is pretty cheap for *modern days standards* specially if this competes even closely to the 3060ti. Hopefully they just can get their drivers sorted out.\n\nAnyways nice to see finally a 3rd competitor in the gpu market.",
      "Pat's rocking that \"dad at Disney World\" look to contrast with Jensen's \"uncle who thinks he's cool but actually isn't\" look.",
      "Team **R**ed vs Team **G**reen vs Team **B**lue\n\nRGB 🤔",
      "I mean... you can already get an RX 6700 for $360 which competes with the 3060 Ti.\n\nAnd actually works well with OpenGL/DX11 and below. And it also has known functional drivers",
      "To be fair Nvidia's 3060 \"MSRP\" doesn't exist, the cheapest one thus far (excluding the very limited number of MSRP models at launch) has been approximately $370 for a low tier MSI model. We'll have to see how Intel Arc pricing plays out in reality and whether we get a card at $329. I agree with the your AMD point, their pricing is very good right now.",
      "HOLY MOLY WE HAVE A DATE!\n\n&#x200B;\n\nAND A GREAT PRICE!?",
      "And uses less power.",
      "great price, just keeping fingers crossed for performance",
      "Truly is.\n\nI don't get the people saying \"its overpriced\" \"the drivers are bad\" \"this is awful not buying it\"\n\nLike this is amazing actually, Intel joining the competition is nothing but perfect for us consumers. We get better and cheaper GPUs in the long run. With the way Nvidia is looking right now it's a good thing that eventually they will be forced to make things cheaper. Only a matter of time before Intel, AMD, and Nvidia are neck and neck\n\nCompetition is good",
      "Great news for everyone.",
      ">​AND A GREAT PRICE!?\n\nNot realy, 13% faster then a 3060 with the same MSRP does not work well with the drivers being at their current state and higher power draw then both AMD 6600xt and nvidias offering the card is DOA. 259-299$ would have been a maybe but at that price it's a flop. Maybe they can discount it then it has a chance",
      "It was meant to be.",
      "and is less experimental phase. Yeah, I would absolutely pay extra for either RX 6700 or 3060 Ti. The value proposition doesn't seem to be there based on my understanding of the expected performance of Arc.\n\nThat said, do we have any benches yet?",
      "In some other timeline Intel released this one first, with better drivers, and effectively created a modern RX580, which would have been a great way to enter the market.",
      "Yeah, this is still a bit too much, with 6600 XTs widely available at $300 now.",
      "No, Intel ruined the naming scheme. I was kinda hopeful they wouldn’t.\n\nR = *R*adeon\nG = *G*eforce\nB = … *A* rk",
      "and stability",
      "Inteeeelllllllll gpuuuuuuu",
      "Guys don't order it. I need to get my order in first :).",
      "They are old tech geeks.  Seems quite appropriate.  I'm sure I'd look just as daggy trying to cool on stage in front of the world..."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "arc a770 defo won the sexiness test so far",
    "selftext": "",
    "comments": [
      "I really hope Intel's GPUs succeed, if only so we can finally have options for aesthetically appealing GPUs",
      "Why would you buy one to replace a 3070 though?",
      "I hope they are persistent on fixing bugs and improving performance in next 1-12 months that will decide fate or 2nd gen arc gpu.",
      "Looking impressive! I am not a fan of RGB. But I will make an exception with Arc. This is how RGB should be done. Also the clean look. Wow!",
      "A 3070 without a backplate? Didn't know those existed.",
      "So far benchmarking wise the arc is identical to the 3070 on a 1440p ultrawide",
      "Hopefully Battlemage will turn out great",
      "EVGA Black edition it was only available at launch kinda was bummed about it but I couldn't really complain at least I had got a GPU then",
      "The 3070 beats it in gaming, pretty big difference",
      "It was evgas msrp card they had a lot lower stock of them",
      "I have been running actual games I was getting around 106-135 fps on the Witcher 3\n40-65 fps on cyberpunk 2077\n139 fps on GTA V\n200-540 Fps on CSGO\n1440p Ultrawide",
      "not completely true in cyberpunk 2077 my 3070 would get 75 fps on high settings and the  arc would get 40-65fps so really the drivers need to be ironed out",
      "Oh damn that means they already fixed a bunch of stuff. Csgo used to run really badly.",
      "I bought to tinker with and test with for a little bit might switch back to the 3070 in month or two",
      "Tastefully refined LEDs to the rescue!",
      "Idk why you're being downvoted.\n\nLike, look at that diffuser. Its terrible. You can literally count each LED.\n\nWhats the point even having a diffuser at that point?",
      "I’ve heard the Arc cards are extremely good in 4K, but you should be running actual games, synthetics are a completely unrealistic best-case scenario for Intel.",
      "Because hes an enthusiast",
      "why change a 3070 for a 770?  \nis a downgrade for you",
      "True, though they have incredible thermals the only reason I buy them, sad to hear they dropped out of the GPU market"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Arc A770 16Gb matching top Ampere GPUs in Hogwarts Legacy",
    "selftext": "",
    "comments": [
      "This article has the Arc A770 16Gb slotted in between the RTX 3080 and 3090 in **Ray Tracing** performance.\n\nLooks like Intel made sure to be prepared for the release of this title. This is the first time the A770 16Gb has really lived up to the potential of it's hardware. This title also has a really good implementation of Xess as well.\n\nEdit to specify I'm talking about RT performance.",
      "Promising results. Intel went and put the hundreds, hell probably thousands, of hours into optimizing for this one game knowing it was really the first hot release since the cards came out.\n\nNow, the question is can Intel consistently get big Wins like this with new games? That remains to be seen.",
      "Performs the same as a 3090 in 4k RT. Crazy good scaling, and XeSS will make that playable.",
      "Its worth noting that the performance its getting is in Ray Tracing scenarios only, in a raw raster scenario its still placing between a 6600 and 6700 xt, and well below the 3080 and 3070, but in a RT scenario it leaves both of its AMD competitors in the dust. Heck, its leaving a 7900 XTX in the dust in a RT scenario. Pretty wild.",
      "Yup, that's the most important question, 1 game means nothing.\n\nTBH it seems that they massively improved since launch, but there is still lots to do, seems like they are making a progress.",
      "That's true. I guess I was just blown away by seeing Arc perform anywhere near 3080 / 3090 in any scenario and I didn't look at the fine print.\n\nAccording to this [video](https://youtu.be/beT2EBXDPpY), the Xess implementation is doing a good job of boosting the raw performance too.",
      "This tracks with some of the early benchmarks which slotted the A770 up with the 3080 in edge cases, and the original Intel expectation they had a 3080/3090 competitor on their hands before driver quality gave them a dose of cold water.\n\nAs the drivers improve these could legit become 3080 class cards, or at the very least, 3070Ti.",
      "Intel is new to the GPU scene and is already fucking AMD in the ass.",
      "This makes me excited to see what Battlemage is like.",
      "question based on your flair:\n\n do you have the RTX 3070 and the Arc A380 in the same system? I was thinking since Intel iGPU has been around forever that their co-existence would be just fine in the same system, for encoding/streaming purposes, etc :-)",
      "Its very impressive, it means that per dollar, Arc is getting the most performance in raytracing there is on the market right now. I'm not sure many people value RT that highly yet, but it shows how well the Alchemist architecture is handling modern games and technologies.\n\nWhats wild is at its price point, 350 dollars, there is no other GPU in its price range that is creating what I would consider a 'playable' experience in 1080p raytraced, (absent dlss/xess ect), able to consistently stay above 30.",
      "I'm not sure, but I think it has to do with building credibility.\n\nLet me jsut shoot my shot here.\n\nIf Intel had some in with a 600-700 dollar GPU that launched running as poorly as the a770 and a750 did, I think they would have been written off as a waste of money.\n\nBut with something as cheap, especially now, as an a750, I think it opens people up a lot to trying something different out, giving them a chance, and if it works really well, thats awesome, if its really bad, well its improving. It appeals to a different market, but when they release an enthusiast series GPU with battlemage, its much better to have people go 'The Alchemist GPUs are actually pretty good, now its worth jumping in' for that bigger ticket item.\n\nOr maybe its a total cope and they just didn't yet have the experience to make an enthusiast gpu they were comfortable releasing, idk.",
      "i’m telling you, if intel keeps this up with their graphics cards division, i may have a team blue system when it’s time to replace my card",
      "I mixed a 2080 and a770. No problems there",
      "yeah, I may redo this thread so I can specify I'm talking about RT performance in the title.",
      "AMD's raytracing is tragic.",
      "Getting a little too excited here!",
      "Yeah, same system. Nothing uses AV1 yet though, but quicksync is pretty good and helps take load off the 3070. Can't say the drivers play too nicely with each other though.",
      "In a 2 slot form factor also, not 2.5 or 3 slots. This is really impressive for the size of the card compared to the competition.",
      "Yes they did - but there’s a side effect that optimization often is more general and helps more than one title. A rising tide lifts all boats type of thing"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "a770"
    ],
    "title": "“Dark ARC” 12600K | a770 build",
    "selftext": "Sharing my latest build. Enjoy.",
    "comments": [
      "Really good looking clean build",
      "This is super clean! I'm building one with an a750. It's my first build so I'm pretty stoked!",
      "What AIO is that?",
      "This makes me want to get rgb fans.",
      "Deepcool LT720",
      "The 750 is a great card! Happy building",
      "Deepcool LT720 infinity 360mm",
      "It’s absolutely stunning. The camera cannot pick up all the variations in the colors. It’s more wow in person",
      "Ultra sexy. Very nice",
      "DeepCool LT series, I think the LT720",
      "Love the color theme",
      "I love this build! Very tasteful RGB and the modern Intel case badges are a nice minimalist design.",
      "Glad I'm not the only one still using the stickers!",
      "That's really clean. Arc GPUs look sick",
      "Ty. I’m a Systems Integrator, most of my work gets sold. I build more than I get to enjoy- most of the time. It can be torture sometimes because I basically want to keep every build for myself",
      "CPU:\n Intel Core i5 12600K 10-core/16-thread @3.7GHz/4.9GHz Boost\nRAM:\n32GB DDR4-3200 Kingston FURY Beast (CL16) \nMotherboard:\nMSI Z690-A PRO Series WiFi DDR4\nGraphics:\nIntel ARC a770 16GB Limited Edtition\nStorage:\n1TB Samsung EVO 970 \nOperating system: \nWindows 11 Professional w/key activated \nPower:\nSegotep GM 750w 80+ Gold ATX 3.0 (12vHPWR ready)\nCooling:\nDeepCool LT720 360mm Infinity All-in-One CPU cooler\n5 Asiahorse 9002 PRO HSP aRGB fans\nConnectivity:\n2.5G Ethernet, Wi-Fi AC6 & Bluetooth on-board\nAccessories:\nBlue ATX/PCIe power cables\nExtra 12vHPWR PSU Cable, Extra PCIe Cables for future upgrades\nCase: \nHYTE Y40 Panoramic case (Black)",
      "Very clean build. Enjoy :)\n\nI enjoy building rigs for my friends and myself more than actually playing on them 🥲😅",
      "Clean af looking build bro 😎 cool that people can now have complete intel builds! interested to see benchmark results",
      "What's that cooler?",
      "please share full parts list man"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Alleged Intel Arc B580 Battlemage listed in shipping manifest, Arc A770 drops to $269",
    "selftext": "",
    "comments": [
      "… Does it outperform the 6650 XT now? It needs to have a clear win over that card to be worth it.",
      "it's not a duopoly, nvidia has been beating tf outta amd in gpus for a while now. Nvidia has almost a 90% market share in the gpu space and rdna3 was so bad it lost amd market share.",
      "Sometimes, but other times nope. Even if it was a bit faster than the 6650 XT in every game, for $70 more then what it goes for nowadays I still wouldn't buy it.",
      "in my humble opinion, the one percent lows are the most important metric",
      "There should be something like a B770: [https://www.guru3d.com/story/intel-arc-battlemage-graphics-cards-expected-to-launch-in-december-ahead-of-ces-2025/](https://www.guru3d.com/story/intel-arc-battlemage-graphics-cards-expected-to-launch-in-december-ahead-of-ces-2025/)",
      "I wish that intel cards were better, breaking the duopoly would be sick.",
      "269$ for 16gb of vram is wild.",
      "No. It’s a budget card. Lowest performance.",
      "B770 looks like it could be juicy. I finally have a monitor that is Freesync rather than GSync, so now I can pick and choose from any manufacturer for my next upgrade. As long as Intel can figure out their HDR support they could be considered.",
      "For the coming few generations, yes. AMD has said that they will cater to the low to mid range to build market share.",
      "Is this the highest sku they gonna make, if it is that's disappointing",
      "What? \n\nJust because a product sells well too doesn't make it good, mindshare is definitely a thing (and arguably a greater factor for Nvidia than Intel).",
      "Idk why you insist on putting words in other peoples mouths and making false equivalences lol. Where did I claim AMD made the better product?",
      "For a first attempt they were okay (okay is not good enough in the current market). They certainly didn't hit their goals but no1 does in the first attempt.\n\nThe question is if they decide to stop the whole consumer line and just focus on enterprise, considering their current financials. Ideally you want both for full integration but that's reality, and then they can maybe steal share from AMD that's already like 10-15% of the market. Tackling Nvidia is another thing entirely......",
      "Not everyone cares about those features.  In my view the only problem on amd gpus is the terrible rt performance.  I don’t care about dlss downgrade tech.",
      "To be honest: no clue. I do have a few suspicions though. Right now they rare still working on their consumer gpu chiplet design, and it wasn't fully working, so they decided to go back to 'monolithic' for the time being, until they work it out.",
      "Wtf? AMD midrange and overhyped cpus? The 9800x3d is the gaming king at this moment. Maybe intel leads in productivity sometimes slightly, but more often than not AMD beats them aswell because of the lackluster performance of Intel Core ultra 200. \n\nAMD doesn't pay people. It's just the fact that Intel is down now, and AMD released a gaming beast that people are excited about. That's why they leave positive reviews, and on Intels parts negative ones. That's been happening since zen 2. That's when AMD really started to pop off.\n\nAMD is taking up more market share by the quarter (up from 23.3 to 24.2). They are the more bought, more performant, more efficient, easier to cool cpu. It's just plain simple. \n\nIdk about the NVidia cpu part, but I reckon it's going to take a few generations to catch up again (just take a look at Qualcomm, they haven't fixed their x86 emulation for arm yet.)",
      "I want a flagship 😭",
      "Yeah amd is really getting slammed. Only really good if you are on a budget and never use new features. I have an amd gpu and the driver issues and fsr are both big downgrades from an nvidia card. Literally have to use a driver that is 3 versions old because amd cant fix their shit.",
      "What’s “bad” about it?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "New Build (Core i5-11600k & arc a770 16gb)",
    "selftext": "",
    "comments": [
      "Why would you buy a new 11600 at this point in time",
      "If you're mainly concerned about the price, why didn't you buy a 12400F? It's likely cheaper and performs even better than the 11600K.\n\nGoing with AMD's ryzen 5600 will be even cheaper, and again perform better than the 11600K.",
      "You unfortunately missed out on a huge generational performance increase. And you only needed to pay an extra 50 euros.",
      "Damn, that is pretty close to the price of a 13400F. If you already had a motherboard for 11th gen then I can see why it's a good choice for you, but otherwise I don't understand why you didn't go for a 13400F or 12400F. Buying a \"higher end\" model of an older CPU is usually a very bad idea because you can get a \"lower end\" model of a new generation that is even better and also cheaper.",
      "Intel chips all still supports DDR4, it's not necessary to use DDR5 and you only lose some few percent of performance by not using DDR5. It's 14th gen chips that will be DDR5 only.",
      "This was poorly informed and you should do your own research not just listen to mates",
      "Finally an i5 and not a 7 or 9",
      "11th gen may be maligned, but the i5 variants were pretty good relative to 10th. it was only the i9 that was a waste of sand/regression vs its previous i9.\n\nCould you have done better? probably.\n\nBut you could have done far worse. It's not a *bad* cpu.",
      "Don't wanna ruin your day but a 12400F or a 5600 shits all over the 11600.",
      "He could have a 400 series chipset on his motherboard, those support 11th gen intel with a bios update.",
      "Enjoy the build 🙌",
      "Mostly pricing xD",
      "He bought a new motherboard, a B560, and paid €170 for it.",
      "thats when you buy a 5 pound gpu",
      "Which motherboard did you get and how much was it? Personally I would've bought a 13400F and the cheapest motherboard I could find since I care only about performance/price, but if you really really need to have the integrated graphics and you want a \"good\" motherboard then the 11600K is not a bad choice.\n\nI don't really get that integrated graphics argument though, you can buy a very old used GPU for about €10 to €20  if you ever need a backup gpu for any reason.",
      "The 5600 is significantly less expensive. I’m pretty sure the 5600X is also less expensive now after its price has dropped, but I wasn’t even talking about that cpu.",
      "In case you didn't realize, 12400 and 13400 non-F SKUs exist just as the 11400 does. So if you wanted an iGPU, you can still have it with a budget Alder Lake/Raptor Lake part.",
      "Your friends gave you poor advices here. The benefits of having the gpu in the CPU is absolutely negated and destroyed by the performance difference you gave up. Besides you can get a 20$ gpu if and when you need. Which in my experience is extremely rare for 99% of people.",
      "Ryzen 5 5600x is worse than 11600k, not better. And it's more expensive.",
      "5600g is slower than the 5600x in gaming."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Intel Arc A770/A750 mark two-year launch anniversary",
    "selftext": "",
    "comments": [
      "I hope they will come up with the next gen GPUs soon",
      "You don't have an Arc GPU if you say this, the improvement has been amazing, the stability and compatibility has been improved so much since launch.",
      ">And they did nothing about the software, Linus in one of his videos told that Intel promised a big revamp, nothing! Liars!\n\nBack to basics. Drivers are software.",
      "Yea they totally care about those 3 people who use linux when fixing drivers of a gaming product (majority of gamers are on windows and the linux statistics are inflated by steam decks)",
      "Can somebody explain the possible reasons they cant launch Battlemage yet?\n\nhardware was ready months ago. And the software is already kinda running on Lunar Lake?",
      "\"hardware was ready months ago. \"\n\nIs there any evidence that this is true?",
      "welp....Ive seen LOTS of people talk nice about the improvements of the drivers.",
      "If Arc could one day meet the performance of the 70 or 80 series Nvidia GPUs, I'd consider going a full Intel build on my next gaming PC.\n\nThey've got a long way to go.",
      "The architecture is, but that's not the same thing as the whole BMG themselves.",
      "And A became B",
      "It will be out soon. We are using it internally",
      "well thats a fair Point.\n\nbut isnt it already running in Lunar Lake?",
      "Why? Statistically at least 15x more people use windows. They'll obviously put all their resources into making functional drivers on windows before making functional drivers on linux. That's like saying app developers should still consider blackberry or windows phones.",
      "My only gripe about my a750 is the lack of VR support.",
      "Arrow Lake using that sweet 2 year old alchemist based igpu.",
      "Update your knowledge",
      "Soon(tm) is somewhat imprecise, but ok...",
      "It will be end of this year",
      "Like you just use Arc control everytime like it is a game, yeah, sure it is annoying that the driver updater not works. But that's pretty much all. I just install the drivers, open Arc Control, change one setting and leave the rest alone.",
      "I do hope Intel delivers Battlemage. Even as Nvidia fans i admit Nvidia have been selling such an overpriced GPU. I don't even doubt if RTX 5000 series will be even more overpriced."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "a770"
    ],
    "title": "Intel Arc 2024 Revisit & Benchmarks (A750, A770, A580, A380 Updated GPU Tests)",
    "selftext": "",
    "comments": [
      "Arc is starting to look seriously impressive. If the A750 beats the 6600 XT in 90% of games and the latter is still $240, the former is starting to look like the best value card for *everyone* in this price range, not just enthusiasts.",
      "This is really impressive, I thought the a770 was closer to the 3060/6600. Gj intel",
      "you are not missing anything with starfield. To say that this game is mediocre will be praising it. Focus on good games not 5/10 games.",
      "Why do you have such allegiance to a company that doesn't care about you? \n\nAMD, Intel, Nvidia just buy whatever seems better for you",
      "Based on AMDs slow progress to increase performance over the years.",
      "A good video, and a fair comparison.  \n\nIt’s a bummer Stairfield still appears to be having issues on ARC.  \n\nHonestly probably better if Battlemage isn’t launched until another 3-6 months of driver development time has passed overall.",
      "Can we not with the fanyboyism? More competition is good for everyone.",
      "Really pulling for Battlemage. We need a third party option.",
      "Looks like ARC is racing to the top fast. BattleImage will leapfrog AMD and get very close to Nvidia.",
      "But but but, BIG NAVI",
      "What do promises have to do with anything? You can see current performance then realize they are making faster chips that will have more cores as well.",
      "The problem with Amd they also still have massive driver issue after years. If Intel keep progressing their driver, it's very possible for them beat Amd in gpu market especially with Battlemage.",
      "Amd drivers are pretty mature and their driver issues (Intel's too) are very overblown. Their drivers today are about as stable as Nvidia's and are still more reliable than Intel's. I really hope that battlemage will moreso give Nvidia a reality check at the high-end and upper mid-range.",
      "Remember \"RIP Volta\"? What happened after that? AMD shooting themself in the foot with their meme marketing LOL",
      "I will happily if you can point to anywhere I did that.",
      "I agree - but it is one game that a lot of well known sites like to benchmark, and it certainly left an impression on many that Intel wasn't ready at launch.   Games hyped that much should be given prioritization for drivers.. \n\nI think we need to see Intel stay ahead on more AAA launches in the future. \n\nI'm optimistic..",
      "I think the driver issues are still common enough that the rx 6600 would be the go-to entry level GPU.",
      "Nvidia getting out of the GPU space would be more likely than that",
      "Lunar lake will have integrated graphics based on Battlemage",
      "I don't understand why Starfield is even used as a benchmark though. It's not even in the top 100 most played games on Steam right now. If they're  using that game to benchmark GPUs, they should use Cities Skylines 2 to benchmark CPUs."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Flagship Arc Battlemage specifications leak with reduced clock speed, 75% more shaders vs Arc A770 and increased die size than previously rumored",
    "selftext": "",
    "comments": [
      "Old article, but I didn’t see it here.  \n\nFlagship Battlemage will retail for around $449 and will give you roughly 4070ti performance.  If intel can do this, I’m ready to dump Nvidia.  The market needs a new competitor.  \n\nAnyone else looking to get one of these?",
      "> Anyone else looking to get one of these?\n\nIf performance is close to what these rumours say and drivers are stable, then yes I'll think about getting one. But if they can't reach clock speed targets then it may not get there",
      "Yes, but the release date matters a lot. If these won't appear until a quarter or so from the 50 series, it would be unpragmatic to not wait for a 5060/70. I can't wait for a Celestial -80 challenger, and maybe a Druid -90 challenger.",
      "My concern is power consumption, if it's doing 4070ti performance but at 300+ watts 2 years later that's a bit of an oof. And I would like XeSS to start getting adoption rates the same as DLSS. But I'm running an A770 for now so I'll see how my experience goes over the next year or so.",
      "Midrange 50 series is still well over a year out. Releasing 6 months before high-end is 9+ months ahead of midrange.",
      "Intel doesn't need a 4080 or 4090 killer. It just needs to be good enough to play most games decently at a decent price.\n\nIf it can meet the 4070 and the AMD equivalent performance for under $500, it's going to be a winner. I think everyone's tired of the ridiculous prices for GPUs and it's going to take something like this to shake the market up.\n\nI'd like to get back into desktop PC gaming, but I just can't afford it now at the current GPU prices, where a decent on costs as much as my CPU, motherboard and RAM combined.",
      "?? There’s plenty of games where even the 4080 can struggle to hit 4K 120 FPS at maxed out settings. And that should be the target if you’re buying a card that’s over $1000.",
      "my a770 has thrown me for a loop incredible for the price and honestly feels faster than a lot of cards i’ve used. 3060, 3050ti(m), 3070(m), 3080 ti, 4060 is it better no at some thing i felt that it did better job but 300.00 it is amazing can’t wait for battlemage!",
      "> and by all accounts AMD has been more price competitive than intel with RDNA2 GPUs\n\nHave we been watching the same market? Don't get me wrong, amd has much higher top end products, but the 770/750 are well priced against the 6750/6700/6600. Intel is just still held back on drivers which is super expected. This entire exercise hinges on their drivers evolving faster than their hardware, but it's only possible if they get more and more arc field data to get this exercise going.",
      "If the flagship Battlemage isn't on par with the RTX 4080/SUPER in games that will be disappointing in my opinion.",
      "Bruh if they build a 4070 ti at the same cost or less than Nvidia, IM NEVER GOING BACK.. Now that I know you guys are \"cool\" we need intel to start talking about frame generation or they are going to get left behind.",
      "Arc in laptops should see to the adoption of XeSS. Arc will eventually become the most used graphics solution overall since it'll be in like 90%+ of all laptops and office PCs going forward. Should make adoption of software technology basically guaranteed. \n\nThe actual hardware itself is the harder part to get right, but if it's priced right people can overlook an inefficient architecture.",
      "What? there are plenty of games which even max out a 4090, at least at 4k. Of course, you do need a fast CPU and rest of the system for that.",
      "I have a 3080, so this wouldn’t be it for me. If they could compete with a 4080 in gaming I’d buy one.",
      "I do not see 4070ti performance for $449 happening\n\nFirst gen ARC GPUs haven't undercut Nvidia by that much, and aren't especially competitive with AMD. I don't see them undercutting more with their second gen\n\nAMD hasn't undercut Nvidia by that much this gen, and by all accounts AMD has been more price competitive than intel with RDNA2 GPUs",
      "They need to push BM out before H2 2024 however I see them have to wait till the end of 2024 and at that point it will be another Alchemist \"late to the party\" scenario...",
      "lol wtf?  There’s plenty of VR stuff I play that would gladly use 2x or 3x what my 4090 can do",
      "\"30-40 million people use VR daily but because I don't own one it's not mainstream\" got it",
      "Well if I didn’t have a 4090 I’d consider it, depending on release date. If it’s 3 years late to the party like arc was… well nvidia and amd will have some better cards out",
      "As long as Intel is keeps the VRAM-frequency locked down overclocking isn‘t worth it as I oced my A750 to 2512mhz Core clock and the VRAM is currently the bottleneck."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Intel's Taped & Glued Arc A770 GPU: Tear-Down & Disassembly of Limited Edition Card",
    "selftext": "",
    "comments": [
      "Thanks Intel.\n\nTwo big takeaways:\n\nThis thing was really expensive to design and assemble, but with no good reason or benefit. It's just inefficient and dumb. Where did Intel get the people who designed the assembly for this card?\n\nThe February manufacturing date on that plate. There were lots of reports that Intel had tons of cards manufactured and sitting in boxes doing nothing for months over the summer. I guess those were true. That card (or at least most of it), was manufactured back in March/April/May and has been sitting in a warehouse all this time.",
      "The sitting on a warehouse thing might be explainable if they didn't have the software side ready. As even now drivers seem hit or miss.",
      "This is probably the most nitpicky video I've seen from steve\n\nThen again, his nitpickiness is based in much more knowledge and years of experience than I do so his opinions are probably valid\n\nIt's probably rooted in the fact that he knows the potential that intel has to make a great product, and he wants them to do better because they can be a serious competitor in the space if they iron out some big kinks",
      "They tried. They decided to delay until they had something decent. I respect that more than shipping it anyway.",
      "I agree that this is very nitpicky. This isn't a product people are going to tear down because it's a mid range card.\n\nFor example the fact that the backplace is taped on isn't a big deal because it looks like it's still sticky even after being removed. Even if it wasn't sticky you don't actually need to tape it back on because the cards works and looks fine with it removed.\n\nAlso most of the pain in the ass when disassembling and the \"complicated\" design is because they added that extra board with the RGB controller. This means the ARC cards that don't have the RGB stuff are likely going to be easier to open which is something that he doesn't mention.",
      "I like Steve, but sometimes he’s truly obnoxious lol",
      "Oh no Steve is so biased against Intel there's no way he'd make content like this about, say, the RTX 2060 FE.",
      "The regular user should at least be able to replace the fans. Fan failure is the most common RMA. \n\nA sapphire fan replacement takes 2 minutes with a couple screws. This one it seems like you have to go through the whole disassembly process to get to the fans. It’s an anti-consumer design.",
      "Because the 4000 series utterly trounces it, at a wildly incomparable price point, and the 7000 series hasn't been fully revealed yet. The 6600 and XT are on the market, right now, and are Intel's direct competition. What is your point?",
      "The problem is rn it's kinda a 6600/6600xt with less features and worse drivers (for now) which is why people aren't recommending it.",
      "So was amd when they released their first card and it was ass\n\nNow look at them\n\nThis stuff takes time",
      "Its so over engineered to the point I want one.",
      "I feel like this review came from a guy who made up his mind before he reviewed it.  He must have said \"screwless design\" like 5 times, whereas the intel slide he showed clearly said \"screwless shroud\" -- an aesthetic choice which is obviously going to have some ease of disassembly trade offs. \n\nNow, I think his right to repair/ease of service arguments are valid, but they're more valid coming from someone who at least can appear to be giving something an unbiased look.  He was whining when the interior contained screws... which *obviously* it is going to, he would have bitched if it hadn't contained screws, because that would have basically meant zero serviceability and would have been a terrible decision.  Damned no matter what.  Barely acknowledged at all in the whole video that the shroud was an aesthetic choice and was going to have some trade offs for function.\n\nPersonally I'd prefer serviceability just like he apparently does, I would have loved to see magnets instead of tape to hide screws.  However, it's hard to watch a video like this and assume that you're getting a fair unbiased take of any positives or trade offs that might exist, the whole thing just reeks of confirmation bias made manifest, not journalism.",
      "Decent dx9 support",
      "Could be by the guys who design their NUCs, which is why its so overengineered. Probably will be a lesson learned for their next interation.",
      "Toyota is one of the biggest auto manufacturers in the world, but I bet if you asked them to make a tractor the first one would be so-so.\n\nLoads of experience in a closely related field doesn’t translate to a perfect first-gen attempt.",
      "I very sincerely doubt that he is being paid for anyone, he is a highly credible voice in the tech space\n\nThat being said, I have noticed LTT (with the exception of his latest video being the review of the A770) and GN being hyper hyper critical of intel lately",
      "Sometimes?",
      "FR, all I want to know is when I can drop an A770 or two in a Plex server and how many transcodes from 50Mbps to 10Mbps can each handle concurrently.\n\nSome see Steve as being nitpicky but the number of *different* fasteners alone is worthy of his ire.  Personally, I think of the design, fit, and finish as truly deluxe but the PITA involved with a disassembly and reassembly is enough to probably enough to open a gyro shop.",
      "Issues would be when you need to clean or repaste in a year or so. \n\nYou kinda want it to take apart easily and come back without needing any adhesives.\n\n[EDIT] I cant believe having ease of assembly/disassembly would be a controversial or even bad idea for some people...."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "My ARC A770 cooling :)",
    "selftext": "",
    "comments": [
      "How did it get that hot. That cooler is overbuilt for the workload. Something might not be right with the card itself.",
      "Lol bro cooled his intel card with amd cooler",
      "because the card is 90 degrees",
      "Why?",
      "Or his case airflow is garbage. One of those fashion over function designs. It's possible.",
      "I would RMA it then, my overclocked card has hit a high of 81c. Although, my case is fully meshed.",
      "need to use intel heatsink for extra -5c",
      "I like that along the front part of the GPU the light is still blue (Intel) while it suddenly turned red (AMD) on the rear where the Ryzen stock cooler was placed",
      "Hell no. The back plate is plastic",
      "But, why on the cooler?  Are you thinking conduction on the cover plate?  Is it even Aluminum?",
      "This is a joke…right?",
      "Look at the orientation of the cooler pins/screws... it's upside down ontop of the GPU backplate.",
      "Disgraceful. You used the wrong stock cooler.",
      "They would never know, that's like thinking Intel or AMD have never given warranty replacements for gamer's overclocked processors, unless they are psychic and know what people do with them.",
      "That does nothing",
      "What settings are you running that your card is that hot? I’m pushing mine hard and barely hits 70*C",
      "My question is...does this work?",
      "Upside down?",
      "This seems a little unnecessary. The backplate coolers were primarily for GDDR6X cards, specifically the 3090, since it had memory modules on the back of the PCB.",
      "Pat Gelsinger is very disappointed in you"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "a770"
    ],
    "title": "Intel Arc B580 Meta Review",
    "selftext": "- compilation of 12 launch reviews with ~3660 gaming benchmarks at 1080p, 1440p, 2160p\n- only benchmarks at real games compiled, not included any 3DMark & Unigine benchmarks\n- geometric mean in all cases\n- standard raster performance without ray-tracing and/or DLSS/FSR/XeSS\n- extra ray-tracing benchmarks (without DLSS/FSR/XeSS) after the standard raster benchmarks\n- stock performance on (usually) reference/FE/LE boards, no overclocking\n- factory overclocked cards were normalized to reference clocks/performance, but just for the overall performance average (so the listings show the original result, just the performance index has been normalized)\n- missing results were interpolated (for a more accurate average) based on the available & former results\n- performance average is weighted in favor of reviews with more benchmarks\n- power draw based on numbers from 13 sources, always for the graphics card only\n- current retailer prices according to Geizhals (GER/Germany, on Dec 15) and Newegg (USA, on Dec 16)\n- performance/price ratio for 1080p raster performance and 1080p ray-tracing performance (higher is better)\n- for the full results and some more explanations check [3DCenter's launch analysis](https://www.3dcenter.org/artikel/launch-analyse-intel-arc-b580)\n\n&nbsp;\n\nRaster 1080p|7600|7600XT|6700XT|3060-12G|4060|4060Ti-8G|4060Ti-16G|A770-16G|B580\n|:--|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n&nbsp;|RDNA3 8GB|RDNA3 16GB|RDNA2 12GB|Ampere 12GB|Ada 8GB|Ada 8GB|Ada 16GB|Alchemist 16GB|Battlemage 12GB\nComputerBase|99.3%|-|113.0%|-|106.2%|126.2%|-|84.5%|_100%_\nGamersNexus|93.3%|-|108.9%|80.0%|98.5%|120.4%|-|88.7%|_100%_\nHWCanucks|90.0%|96.9%|-|90.1%|105.4%|-|-|84.5%|_100%_\nHardware&Co|87.7%|100.8%|-|80.4%|95.7%|120.0%|119.8%|81.2%|_100%_\nKitGuru|87.1%|96.4%|-|77.7%|91.9%|114.7%|-|85.4%|_100%_\nLinusTT|85.3%|-|105.9%|78.4%|87.3%|-|109.8%|79.4%|_100%_\nPCGH|87.2%|100.3%|-|78.3%|92.9%|-|-|90.2%|_100%_\nQuasarzone|96.2%|100.4%|-|84.6%|101.1%|121.8%|-|85.5%|_100%_\nTechPowerUp|87%|96%|106%|83%|95%|121%|122%|86%|_100%_\nTechSpot/HUB|92.2%|94.8%|97.4%|81.8%|93.5%|119.5%|120.8%|80.5%|_100%_\nTom's HW|79.3%|100.9%|-|-|99.7%|-|-|83.1%|_100%_\nTweakers|-|99.4%|-|81.2%|96.6%|119.4%|-|83.7%|_100%_\n**avg 1080p Raster Perf.**|**89.2%**|**97.7%**|**105.3%**|**81.9%**|**95.8%**|**119.2%**|**-**|**84.6%**|**_100%_**\n\n&nbsp;\n\nRaster 1440p|7600|7600XT|6700XT|3060-12G|4060|4060Ti-8G|4060Ti-16G|A770-16G|B580\n|:--|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n&nbsp;|RDNA3 8GB|RDNA3 16GB|RDNA2 12GB|Ampere 12GB|Ada 8GB|Ada 8GB|Ada 16GB|Alchemist 16GB|Battlemage 12GB\nComputerBase|83.9%|-|108.4%|-|95.8%|118.4%|-|88.7%|_100%_\nGamersNexus|94.0%|-|114.5%|82.2%|96.5%|118.7%|-|97.2%|_100%_\nHWCanucks|83.2%|89.6%|-|86.0%|95.7%|-|-|87.7%|_100%_\nHardware&Co|77.6%|95.4%|-|76.8%|91.0%|115.0%|113.6%|82.9%|_100%_\nKitGuru|81.4%|91.8%|-|75.2%|86.4%|108.6%|-|88.0%|_100%_\nLinusTT|76.3%|-|98.7%|75.0%|81.6%|-|102.6%|80.3%|_100%_\nPCGH|79.7%|94.5%|-|75.1%|81.5%|-|-|89.8%|_100%_\nQuasarzone|88.9%|93.4%|-|80.2%|92.9%|114.5%|-|87.4%|_100%_\nTechPowerUp|80%|91%|103%|81%|92%|116%|118%|88%|_100%_\nTechSpot/HUB|82.5%|89.5%|94.7%|77.2%|87.7%|110.5%|114.0%|82.5%|_100%_\nTom's HW|68.6%|96.4%|-|-|92.0%|-|-|84.7%|_100%_\nTweakers|-|94.2%|-|79.2%|91.3%|116.0%|-|86.0%|_100%_\n**avg 1440p Raster Perf.**|**81.2%**|**92.1%**|**102.7%**|**78.7%**|**89.1%**|**111.3%**|**-**|**86.7%**|**_100%_**\n\n&nbsp;\n\nRaster 2160p|7600|7600XT|6700XT|3060-12G|4060|4060Ti-8G|4060Ti-16G|A770-16G|B580\n|:--|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n&nbsp;|RDNA3 8GB|RDNA3 16GB|RDNA2 12GB|Ampere 12GB|Ada 8GB|Ada 8GB|Ada 16GB|Alchemist 16GB|Battlemage 12GB\nPCGH|70.6%|88.9%|-|73.5%|71.2%|-|-|91.5%|_100%_\nTechPowerUp|62%|84%|97%|78%|84%|98%|106%|87%|_100%_\n\n&nbsp;\n\nRayTracing 1080p|7600|7600XT|6700XT|3060-12G|4060|4060Ti-8G|4060Ti-16G|A770-16G|B580\n|:--|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n&nbsp;|RDNA3 8GB|RDNA3 16GB|RDNA2 12GB|Ampere 12GB|Ada 8GB|Ada 8GB|Ada 16GB|Alchemist 16GB|Battlemage 12GB\nComputerBase|81.1%|-|111.5%|-|97.8%|114.4%|-|89.9%|_100%_\nGamersNexus|77.1%|-|89.8%|85.6%|108.0%|135.7%|-|91.5%|_100%_\nHardware&Co|32.8%|63.1%|-|80.2%|101.6%|129.4%|131.0%|81.5%|_100%_\nKitGuru|61.3%|76.4%|-|85.7%|103.1%|129.4%|-|85.9%|_100%_\nPCGH|58.0%|81.0%|-|84.4%|87.7%|-|-|86.5%|_100%_\nQuasarzone|44.9%|74.9%|-|91.1%|117.6%|138.1%|-|83.9%|_100%_\nTechPowerUp|45%|71%|79%|81%|97%|124%|128%|86%|_100%_\nTechSpot/HUB|64.4%|64.4%|57.8%|-|124.4%|166.7%|168.9%|95.6%|_100%_\nTom's HW|54.3%|68.8%|-|-|90.9%|-|-|79.5%|_100%_\n**avg 1080p RayTr Perf.**|**55.4%**|**73.0%**|**78.2%**|**86.7%**|**102.2%**|**128.9%**|**-**|**87.6%**|**_100%_**\n\n&nbsp;\n\nRayTracing 1440p|7600|7600XT|6700XT|3060-12G|4060|4060Ti-8G|4060Ti-16G|A770-16G|B580\n|:--|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n&nbsp;|RDNA3 8GB|RDNA3 16GB|RDNA2 12GB|Ampere 12GB|Ada 8GB|Ada 8GB|Ada 16GB|Alchemist 16GB|Battlemage 12GB\nComputerBase|58.3%|-|106.1%|-|87.7%|102.1%|-|87.4%|_100%_\nPCGH|52.8%|80.3%|-|84.8%|83.1%|-|-|91.0%|_100%_\nTechPowerUp|38%|65%|73%|78%|78%|93%|123%|86%|_100%_\nTechSpot/HUB|59.4%|59.4%|53.1%|-|115.6%|162.5%|165.6%|103.1%|_100%_\nTom's HW|44.4%|65.1%|-|-|84.4%|-|-|85.7%|_100%_\n**avg 1440p RayTr Perf.**|**~50%**|**~70%**|**~75%**|**-**|**~90%**|**~113%**|****|**~91%**|**_100%_**\n\nNote: Due to the small number of figures, the performance index was rounded to whole numbers in this case.\n\n&nbsp;\n\nRayTracing 2160p|7600|7600XT|6700XT|3060-12G|4060|4060Ti-8G|4060Ti-16G|A770-16G|B580\n|:--|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n&nbsp;|RDNA3 8GB|RDNA3 16GB|RDNA2 12GB|Ampere 12GB|Ada 8GB|Ada 8GB|Ada 16GB|Alchemist 16GB|Battlemage 12GB\nPCGH|45.5%|79.9%|-|88.9%|81.1%|-|-|99.6%|_100%_\nTechPowerUp|30.5%|64.0%|71.4%|74.8%|76.5%|92.0%|113.2%|87.6%|_100%_\n\nNote: I excluded one game from TechPowerUp's results (Alan Wake 2), because it destroys the index with it's super-weak performance of the Arc B580 there (0.3 fps).\n\n&nbsp;\n\nAt&nbsp;a&nbsp;glance|7600|7600XT|6700XT|6750XT*|3060-12G|4060|4060Ti-8G|A770-16G|B580\n|:--|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n&nbsp;|RDNA3 8GB|RDNA3 16GB|RDNA2 12GB|RDNA2 12GB|Ampere 12GB|Ada 8GB|Ada 8GB|Alchemist 16GB|Battlemage 12GB\navg 1080p Raster Perf.|89.2%|97.7%|105.3%|~111%|81.9%|95.8%|119.2%|84.6%|_100%_\navg 1440p Raster Perf|81.2%|92.1%|102.7%|~108%|78.7%|89.1%|111.3%|86.7%|_100%_\navg 1080p RayTr Perf.|55.4%|73.0%|78.2%|~82%|86.7%|102.2%|128.9%|87.6%|_100%_\navg 1440p RayTr Perf.|~50%|~70%|~75%|~79%|-|~90%|~113%|~91%|_100%_\nTDP|165W|190W|230W|250W|170W|115W|160W|225W|190W\nReal Power Draw|160W|190W|219W|221W|172W|124W|151W|223W|163W\nEnergy Eff. (1080p&nbsp;Rast.)|91%|84%|78%|82%|78%|126%|129%|62%|_100%_\nMSRP|$269|$329|$479|$549|$329|$299|$399|$349|$249\nRetail GER|266€|342€|_EOL_|329€|274€|294€|388€|313€|294€\nPerf/Price&nbsp;GER 1080p Raster|99%|84%|-|99%|88%|96%|90%|79%|_100%_\nPerf/Price&nbsp;GER 1080p RayTr|61%|63%|-|74%|93%|102%|98%|82%|_100%_\nRetail US|$250|$310|_EOL_|$369|$280|$300|$410|$230|$250\nPerf/Price&nbsp;US 1080p Raster|89%|79%|-|75%|73%|80%|73%|92%|_100%_\nPerf/Price&nbsp;US 1080p RayTr|55%|59%|-|56%|77%|85%|79%|95%|_100%_\n\n* performance just interpolated based an older benchmarks (6750XT is nearly constant +5.1-5.4% faster than 6700XT)\n\n**Interestingly, the Arc B580 achieves a significantly stronger performance/price ratio in the U.S. than in Germany.** Apparently, Intel prices in the USA are lower, but nVidia prices are higher.\n\n&nbsp;\n\nList of Arc B580 reviews evaluated for this performance analysis:\n\n- [ComputerBase](https://www.computerbase.de/artikel/grafikkarten/intel-arc-b580-limited-edition-test.90585/)\n- [Gamers Nexus](https://gamersnexus.net/gpus/intel-arc-b580-battlemage-gpu-review-benchmarks-vs-nvidia-rtx-4060-amd-rx-7600-more)\n- [Hardware Canucks](https://www.youtube.com/watch?v=kumOux-zD_A)\n- [Hardware & Co](https://hardwareand.co/dossiers/gpu/test-de-l-intel-arc-b580-le-renouveau-des-cartes-graphiques-sous-les-300-e)\n- [KitGuru](https://www.kitguru.net/components/graphic-cards/dominic-moass/intel-arc-b580-limited-edition-review/)\n- [Linus Tech Tips](https://www.youtube.com/watch?v=dboPZUcTAW4)\n- [PC Games Hardware](https://www.pcgameshardware.de/Arc-B580-Grafikkarte-280998/Tests/Benchmark-Review-Preis-vs-4070-A770-Limited-Edition-kaufen-1461374/)\n- [Quasarzone](https://quasarzone.com/bbs/qc_bench/views/92504)\n- [TechPowerUp](https://www.techpowerup.com/review/intel-arc-b580/)\n- [TechSpot](https://www.techspot.com/review/2935-intel-arc-b580/)\n- [Tom's Hardware](https://www.tomshardware.com/pc-components/gpus/intel-arc-b580-review-the-new-usd249-gpu-champion-has-arrived)\n- [Tweakers](https://tweakers.net/reviews/12768/intel-arc-b580-grote-stap-voor-intel-kleine-stap-voor-gamers.html)\n\n&nbsp;\n\nSource: [3DCenter.org](https://www.3dcenter.org/artikel/launch-analyse-intel-arc-b580)",
    "comments": [
      "I check the website now is sold out, no stock\nLooks like intel finally make it into gpu. It is cheaper as well more vram.\n\nThe performance it is still not final, usually intel will release the drivers we can get 10-20% perf gain from current.\n\nHope next years intel release the gpu to take 4090 or 5090 with cheaper as well more vram since nvidia 5090 rumors is 32gb vram",
      "A bit faster but also cheaper and has more VRAM. That's what people like about it.",
      "We found one of MLID ALT accounts.",
      ">you’re just overpaying for bad drivers and increased power\n\nBad drivers I agree somewhat, Intel Arc still performs in games than in benchmarks. Power efficiency is good though? Not as good as Nvidia, but Nvidia is the best in dGPU while Intel is a newcomer, you can't catch up in power efficiency that fast(2nd gen). Intel already beats AMD in power efficiency for Battlemage GPUs, which holds true for the iGPU version(Lunar Lake) too(Lunar Lake consumes less power than AMD RDNA 3.5 for same performance). This is already excellent, it shows Intel is catching up. Or at least, AMD is worse than Intel in power efficiency, which is disappointing for a long-time GPU maker",
      "Has anyone made reviews of B580 running on DX9 to DX10 games? So far I have only seen TES IV review and it seemed to run well but it is only 1 game so far.",
      "Do you expect more games to integrate Xess2 in the coming years? If so, then you can expect the B580 to increase its benefits over time. \n\nDo you like spending more money for the same performance? NVIDIA and AMD cards may have been on the market for a while, but they are still on the market. Consumers are choosing between them and it doesn’t matter to them who best who to the market. It’s about what is the best value for their use case.",
      "Sadly, this is not sustainable for Intel, chip is too big for that amount of performance with the node they are using, this ain’t making them money. I understand that they had to do this for their share holders and fulfill their agreements but I hope their next gen is more competitive, lord knows a third competitor would be great.",
      "Thanks a ton for this review aggregation!",
      "Thanks, I'm very curious to see what the results would look like for DLSS, FSR, or XeSS, especially in terms of super resolution and frame generation at 4K",
      "How do these new intel GPU's do with things like Autocad and Revit?",
      "Vex was seeing 10-15% higher performance with an oc so at 1440p you're legit getting a 12gb 4060ti/2080ti. It's also way more comparable to Nvidia cards than if it was an AND one seeing the feature set is much closer unlike amd. Hardware Xess is much closer to dlss than fsr, xe low latency is already in more games than anti lag 2 so a better competitor to reflex (and their driver level variant of XELL for games where it's not implemented seem to be the best out of the 3 actually) , ray tracing is way closer as well and video encoding/decoding is actually slightly better than even Nvidia on Intel unlike amd which is far behind them.",
      "Hopefully, HUB makes their 45-game test as well for B580.",
      ">Sadly, this is not sustainable for Intel\n\n\nTransistor count is about the same between the RTX 4060 and B580.\n\n\nThe main reason the B580 utilizes such a large die is because AD107 (RTX 4060 die) is about 42% denser than BMG-G21 (B580).\n\n\nWhich is kind of crazy when you think about it. Since they're using a similar node.",
      "Doesn’t that make it worse though? Using a lot of expensive silicon wafer and not achieving the same level of transistor density - whilst also using a more expensive process node?",
      ">Doesn’t that make it worse though?\n\n\nOn an architectural level? Absolutely.\n\n\nBut you don't buy products because of their architectures, you buy them because of their price to performance ratio.\n\n\nPeople don't buy a Toyota Tacoma and wonder why it can't hit the same speeds as a Bugatti Veyron.",
      "Average consumption is one part of the picture. Power consumption while idle,vsync, video playback, etc. are another part of the picture.",
      "4060’s are in stock and ready to ship. Can occasionally find one for $279. B580 out of stock and Xess 2 nowhere to be found other than F1 2024. DLSS 3 is mature and has widespread game support. The B580 was hyped to the moon.",
      "They're in the room with Digital Foundry, Daniel Owens, Techpowerup, and basically everywhere I looked. Myself, I already have a problem with Intel's integrated solutions. Can't use portrait mode correctly for my side monitor. A big pain in the ass. So the last gpu vendor I would ever trust is Intel. Dropping settings, buying used, and paying a few extra dollars is worth it.",
      "Keyword: would. Being less than 5% faster than a card that launched almost 2 years ago is not enough for the vast amounts of glazing going on. $250 cards will be few in the wild. It’s just a title grabber. They won’t even be in stock until next year.\n\nSure, you’re just overpaying for bad drivers and increased power for the same performance 😝",
      "> 4% faster than 4060, 22% over 3060 \n\nWith the way it’s being treated, you would think that the B580 is literally twice as fast. Luckily for everyone, the $250 models are nonexistent, and will basically start at $270. I can probably count on one hand the number of reviewers are actually fair towards the cards instead of trying to craft a story. The mods here are a joke pinning some techtuber on the front page."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Intel Arc A770/A750 Graphics Card Review Roundup | VideoCardz.com",
    "selftext": "",
    "comments": [
      "I want to buy an a770 not because I want an a770, but because I want a more reasonably priced 50 series and 8 series.\n\n\nCheering for you arc",
      "i'm pretty impressed with the card given its their first attempt, in a few games it was only ~5% behind the 3070, which says that the hardware is at least good. amd and nvidia have had years to optimize drivers for their hardware, so i think intel is almost definitely leaving a lot of performance on the table here due to how behind they with optimization.",
      "Just a PSA, according to Linus the 16gb A770 LE is going to be available in very limited numbers (probably due to low margins). So if that's the GPU you want you need to be ready to buy on launch day.\n\ntimestamp: https://youtu.be/6T9d9LM1TwY?t=701",
      "770 destroy the 3060 in RT at 1440p, and better when xess, go intel go ! \\*in digital foundry review",
      "It’s not about the card, it’s about sending a message",
      "The drivers stack though is brand new from the begining of the year. They tried to port from their mobile drivers and that did not work. Had to start from scratch. This will get better.",
      "Why hasn't anyone tested these cards with dxvk for the older titles?",
      "the performance is there, it can compete with its targeted NVidia, Amd counterparts. but without a serious discount why would customers buy intel instead of more tried , proved Nvidia,AMD?",
      "They'll probably be able to fix it - or at least make it better - with driver updates sooner rather than later.",
      "We need heroes like you",
      "Voting with my wallet / to support the brand mainly.  \n\nBut actually usable ray trace performance is a bonus and I think these cards will get faster over time.  \n\nHer card is also the vanilla 6600 so A770 is faster all around.",
      "The fast sum up is that the hardware is mostly ready but the software is not. My guess is if you get an A770 you’ll get additional 10% performance on average when drivers are more mature. But Intel needs to fix stability issues for the top 100 games played on steam.",
      "Buy cheap now, wait for driver upgrades while hitting some bumps in the road for a few months, have a card with superior price performance in the near future.\n\nPeople with more money than time can spend two or three hundred dollars more for some guaranteed gaming hours a week. Not everyone has extra dollars and for these people who are looking for maximum performance per dollar and advanced tech like RT and AI optimization, this could be a good buy if they are willing to endure some bumps in the road.",
      "Uhhh, why?",
      "But dxvk translates dx calls to vulkan, and intel gpus are supposed to be alright with vulkan",
      "As someone on the market for a new GPU, the thing that scares the shit out of me is the rumors that Intel is considering pulling the cord out of its dGPU, Drivers improve with time if effort is put on it, but that is the catch.",
      "Is there a link yet to where it will be sold so I can bookmark it?",
      "I am buying an ARC A770 Limited to replace my wife’s Radeon 6600.   She’ll have an all-Intel build (i3-12100, Intel 660p) :). \n\nThe only thing I (really) don’t like is the high idle power consumption..",
      "The i740 was not a GPU, it was a graphics accelerator. The term GPU didn’t apply until hardware transform and lighting was added to the 3D chip, starting with the GeForce SDR. Larabee was not a GPU and lived on his Xeon Phi. The Arc A7 is the first gaming GPU from Intel.",
      "Intel A770 is only 20-30 use cheaper than Nvidia 3060. At this price its not worth the troubles."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Core i5 13600k + Arc A770 go big blue 🤌🏼🦾",
    "selftext": "",
    "comments": [
      "Since the new driver i cant say anything bad. Works like a dream for the price! 🦾",
      "i really hope they manage to bring great stuff  to the market, gpus cost to mutch these days maybe the competition will help...",
      "How's the Arc treating you so far?",
      "Awesome! I think Intel will see great future success in the GPU market.",
      "whoa... is this an AIB Arc GPU? I thought there was like only one style of Intel GPUs?",
      "It is from ASrock, i did not mange to find a referenc card...",
      "Probably for the better, since your AIB looks to have much better cooling vs the reference card.",
      "I don’t really know how big is the difference between them. You can go now whit the 12600k and in a few years upgrade to a 13700k when you save some money 🤷🏻‍♂️",
      "ASRock, Gunnir, Acer, and Gigabyte are all Intel AIB (Add-In Board) partners.",
      "Skip the 13600k unless you're chasing frames\n\nThe 12600k will be around 10-15% slower in maximum theoretical gaming performance, but it also uses less power.",
      "Is that the good old NH-D14?",
      "12600K is gonna be enough for your needs and it has igpu which doesn't hurt to have",
      "But for 80 canadian dollars maybe you are better of whit the 13600kf or k so yo have the integrated gpu 🤷🏻‍♂️",
      "Might I ask why this could be an issue? I have mine on the wooden vinyl floor.",
      "Same. I have the A770 LE and it’s like 👌",
      "Is it worth getting a 12600k for $299(canadian) or 13600kf for $379",
      "sorry i have a small desck it will live down there for now...",
      "13600K is $420 😬….i think a 12600k should be good for 1440p 144hz for a good while. Ill overclock too",
      "Yea for the price i really recommend it 🤘🏼",
      "I just got a new pc w/12600k + rtx3070 and can certify that the 12th gen part works like a charm - no complaints so far. I play at 1440p and am looking for anything between 60-144fps (depending on the game obviously) and this fits right in with all games I've tried to so far from APT: Requiem to RDR2 almost maxed out and new titles like High on Life. Very happy."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Acer Arc A770 Predator custom GPU with 16GB VRAM is now available for $349 - VideoCardz.com",
    "selftext": "",
    "comments": [
      "16gb of memory and a decently good GPU as well for $350 is a steal.",
      "GREAT card for Hogswart’s Legacy.",
      "I have so little faith in Radeon that I think Intel ARC, even with the drivers the way they are now, are more likely to eventually catch Nvidia than Radeon is.\n\nOnly time will tell though. If Intel sticks with it they’ll definitely beat Radeon eventually. Don’t know if they’ll stick with it, or just put it on a skeleton crew though.",
      "If the 6700xt didn't exist and this was facing off against the Rtx 3060ti only it would be such a win.\n\nExcept... the 6700xt does exist unfortunately for Intel.",
      "Offering less features and lower quality features was fine, when they were selling RX 480 8GB for $229 vs Nvidia’s GTX 1060 6GB for $299. That was ~30% price difference.\n\nNow that they’re basically trying to price match Nvidia, that crap doesn’t fly anymore. At least not with me.",
      "Damn. If only it had linux support...",
      "AMD have had their GPU division for the last decade yet the drivers are still awful, Intel has had less than 2 years in the dedicated gaming market yet are improving massively, give it time and they'll surpass the Radeon team.",
      "...glances at my Fedora 37 box with A770...\n\nAlthough support will be better OOTB with March/April distro releases.",
      "Watched them get their asses kicked for over a decade, more or less. They used to at least be significantly cheaper, but for three generations now they’ve just slotted into Nvidia’s lineup.",
      "Yeah rdna 3 is a big disappointment. The 7900 xtx isn't even that much better than the 4080. And for ray tracing the 4080 is better. Price wise they're way overpriced.",
      "AMD's reluctance to fully embrace RT/AI at the hardware level is going to see them lose #2 position if intel continues to improve.",
      "> I have so little faith in Radeon that I think Intel ARC, even with the drivers the way they are now, are more likely to eventually catch Nvidia than Radeon is.\n\nWhy is that?",
      "gaining ground where? even 3080s sold more than the cheaper 6000 series cards",
      "It's supported well on newer kernels but yeah you need a newer kernel for that and can be terrible for something like Debian unless they've backported it to an older version",
      "> The 6700xt is definitely better than the 3070 as of today, the opposite was true in 2020. The 3080 10gb is starting to look really crippled, so the 6800xt is making ground on that too.\n\nPlease tell me that you are not drawing these conclusions based on how that broken mess of Hogwarts Legacy is performing right now.",
      "Yep, can’t wait until it has food debian server support, because I will pull the trigger on it just for AV1. Lucky man with a rolling release distro though…",
      "That's not what I saw in benchmarks. Can you post a link?",
      "Lol except when you turn on ray tracing then that 6700xt is no where to be found",
      ">AMD GPUs still have no hardware accelerated ray tracing\n\nWhy tell such an obvious lie?\n\nYou are really gonna tell everyone with a straight face 7900XT achieved [over 90% RT performance ](https://freeimage.host/i/HEUZmwN) of 4070 Ti with zero hardware acceleration.\n\nSo by your logic Nvidia's 3rd generation \"accelerated\" RT on 4070 Ti is so bad it's only less than 24% (normalised for non-RT performance) better than 7900 XT with no hw acceleration. Nice.",
      "I don’t care about the underlying architectures. I compared R9 300 to GTX 700, not Kepler to GCN. If you release a product, even a rebranded product, I expect a minimum amount of support for it to make it worth the money.\n\nIf AMD named it “R9 291” instead of “R9 390” I’d cut them some slack, but they didn’t so 🤷.\n\nI personally got burned by this because I recommended the R9 390 to someone back in the day."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Intel Arc B580 Limited Edition tested in 3DMark, outperforms RTX 4060 and Arc A770/A750",
    "selftext": "",
    "comments": [
      "This is hardly surprising since Intel Arc A770 already outperforms GeForce RTX 4060 in 3DMark Time Spy",
      "i care about game fps comparison more",
      "Now let's see gaming performance.",
      "Huge if true, could be a new budget build king.",
      "Wow 3dmark.\n\nWorthless test thru",
      "Is the embargo not over until launch day?",
      "Since it might be around 250 bucks. Like there are not many good budget cards eight now. The 4060 is not that bad, just too expensive for its performance (should have been aroumd 250 bucks max)",
      "Since when we feel excited for a new card outperforming a terrible card such as 4060? Even NV's ancient 3060ti already done so.",
      "For the price 249$, Intel is king of budget now",
      "https://redd.it/1hbrxdg",
      "Hey SilasDG, your comment has been removed because we dont want to give *that site* any additional SEO. If you must refer to it, please refer to it as *LoserBenchmark*\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/intel) if you have any questions or concerns.*"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "[HUB] ARC A770 & A550 Review and Benchmarks",
    "selftext": "",
    "comments": [
      "A lot of what I'm seeing here is immature drivers. AMD and Nvidia have a super refined driver that has squeezed plenty of of the gains out. Intel will likely see much larger gains overtime vs the competitors as they refine the driver.",
      "How are they can potentially destroy AMD since Intel's 3070 competitor is losing to low end AMD from 2020? Intel right now is 2 gens behind. Unless you count AMD only targeting gaming in which case by this logic to multithreaded apps AMD has killed Intel already.\n\nAMD provides much better value and is destined to take on series 40.",
      "Seriously some of what intel has shown the hardware seems to punch well above a 3060. I’m curious if it won’t be getting compared to the 3060ti-3070 within 12 months. Looks like specific driver issues causing the relative perf issues in both modern and old titles alike depending on specific instructions used. Imagine if they fix all that and suddenly games can use the full ability of the ARC architecture. I’m just wondering how GTA 6 will run. Oh and starfield.",
      "A reviewer shouldn't give a fuck how much it costs to develop the product.",
      "Wauw your pretty clueless what AMD as a company has done for GPU's....\n\nFirst to HBM  \nFirst to Compute GPU's  \nFirst to Mantle(DX12)  \nFirst to AUDIO acceleraterd GPU\n\nFIRST chiplet GPU incoming",
      "Also:\n\n- AMD (or ATI back then) was also first to GDDR\n\n- First to Tessalation,\n\n- First Terraflop GPU\n\n- First Eyefinity (or scaling rendering across monitors).\n\n- First Re-bar support.",
      "\\+1 for effort.\n\nBut for gaming only it's a not a good card vs competition. 6nm, larger die and higher memory bandwidth plus higher power consumption it only matches a 3060 that was also a bad card at launch. Can't beat the better price per perf 6600xt. Only faster in select games where it is \"optimized\". Yeah it has better RT performance than AMD but at this price range, raster performance is a better indicator than RT.",
      "On average, this is *okay* value, and would’ve been good six months ago. (Hell, these cards have a better launch price than most cards this gen.) The thing is, the 6600, 6600 XT and 6650 XT are so cheap right now that there’s really no reason to go Intel.",
      "For what it offers it's pretty impressive honestly.",
      "If they had released a year ago, I would've given them the benefit of the doubt even with crappy drivers. But at this point, by the time they figure out their drivers, we'll be looking at GeForce 4050 and or Radeon 7500 cleaning the floor with these. \n\nTheir redeeming quality isn't gaming. AI dev? Go ahead. Media encoding? Go ahead. Linux? Go right ahead. Gaming? Only for 100 bucks less at the top end.  \n\nThe only card that entices me is the A310 for less than 100 usd. It's been a while since we had a usable and cheap discrete GPU.",
      "So the Finewine^(TM) argument. What happened to we shouldnt buy things based on the future?\n\nIve always been indifferent to the finewine argument personally and I do want Arc to be successful *enough* to allow Intel to continue with it as I believe, in a couple of gens, Intel GPUs can be truly competitive and more competition is great, however, I see great irony in the narratives being spun in this sub in particular. It feels like the logic flip flops depending on whether Intel is the underdog or the market leader. I thought Finewine^(TM) was a meme ie mocked here? I also thought GN was the most trusted reviewer and LTT is considered trash in this sub? \n\nCan we admit that this sub is no better than AMD and Nvidia subs? Although I do like how the rules are actually enforced here.",
      "I mean I doubt it, but here I am trying to get an A770. The fact is that you’re underestimating AMD a tad much.",
      ">AMD has always been lazy when it comes to GPU innovation.\n\nRDNA 3 is a revolutionary chiplet design. Nvidia doesnt have that.",
      "The \"fine wine\" argument doesn't really make much sense anymore at this point. Arc GPUs were delayed basically forever, while the hardware was in developer's hands for a long time already. If they haven't figured things out by now, they probably won't figure it out in the near future either. Maybe they never will.",
      "> Potentially even completely destroying AMD if they stay with competitive prices.\n\nYeah nope. AMD has atleast a decade head start on the overall dGPU, regardless of AMD's shortcomings on its drivers. \n\nAnd there's a reason why Intel includes Nvidia only on their marketing speak about Arc. Because AMD is still superior in terms of price to performance ratio.",
      "Also IMO HUB is already more \"lenient\" compare to others like GN.\n\nYet people just assume HUB is somehow biased because Steve don't always say what they want to hear.",
      ">The fine wine argument is relevant for people with less money to spend who want to have the best hardware they can get within their budget.\n\nThe justification has already begun. Was AMD not the budget option before when Finewine was argued? Why was it ridiculed then? Because it is hopes and dreams and pretends that all AMD or Intel need to do is fix drivers and that's it. It's putting fantasy over reality because current reality does not fit with desires and expectations. \nYou can not guaranteed that finewine will make any GPU worth its launch price sometime in the future. It might be that by the time those improvements are seen, the whole industry has moved on to new performance levels and prices and it will then be too little too late.\n\nThese GPUs are being launched at the end of current gen for AMD and Nvidia and the only reason the prices are competitive against 2 year old tech is because Nvidia decided to go with scalper prices for next gen and not planning to release 4060s for sometime. AMD is still a question mark. \n\nThese GPUs are like 6-8 months too late imo. That's the objective reality. They should be priced even lower to stay a legitimate option for budget for the next year.\n\nThis is all besides the point though. The irony of this sub lending legitimacy to the finewine argument after ridiculing it for years and arguing for considering it in purchase decisions is not lost on me. \n\nRemember, when finewine was a thing AMD did not have a competitor to the 2070ti and their GPUs were the value option if you were willing to deal with worse drivers and software.",
      "I mean, I hope you're right, but I still wouldn't recommend someone get a card with insanely all over the place performance and stability when alternatives exist because it MIGHT not perform terribly some day.",
      "Yeah, exactly. I had such a bad experience with the 5700xt that I got rid of it and told myself I'll never buy an AMD card again.\n\nThat card had a lot of problems but it was functional, those Intel cards don't even work half the time if you watch the GN review you would know.",
      ">I might still get it because being in on the ground floor might be interesting.  And i want to support a 3rd player.  But it's not the best decision for most people.\n\nExactly. As an enthusiast, this I understand. I wanted to get in on the ground floor of 5800x3D for this reason as well even though I dont really game as much these days."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "MKBHD builds a new PC with an Intel Arc A770",
    "selftext": "",
    "comments": [
      "Because they get crossover viewers this way from MKBHD’s massive audience",
      "What does LTT get out of sponsoring this guy? Why not do it themselves?",
      "They’ve collaborated before and it expands the advertising reach of the screwdriver and LTT Store in general.\n\nMKBHD doesn’t do PC builds and focuses more on phones and Apple products. However, his videos don’t necessarily shy away from technical details when relevant and I suspect there’s some overlap between audiences as a result.",
      "Kind of out of left field. MKBHD doesn't really do PC gaming related stuff sans maybe monitors every now and then.",
      "It's kind of perfect. MKBHD has a clean million more subs than LTT, and he's techy, but just out of sync  in focus enough that if you're his core audience, you might not be Linus's, unlike channels like HUB, GN, J2C, Paul's Hardware, Bitwit, etc.",
      "They're buying time to fix the drivers lol",
      "Release the fucking gpu already",
      "Well that's 95% of what he covers, so I would expect that. Not much of a reason to know about anything else.",
      "The most \"PC\" thing I've heard him talk about is *monitors*.\n\nHe couldn't tell you if a 6600 or a 3080 was the better gpu.  Dude doesn't play computer games or deal in the PC/component/Windows world.  Which is fine, thats just not his wheelhouse.\n\nAfter listening to his internal struggle earlier this year over throwing away his couple-year-old $40k Xeon Mac Pro for a $6k glorified Mac Mini, seeing him pimp an Intel GPU of all things is really quite an ironic combination.",
      "You want an A770, not an A770M.\n\nBut I agree, I'm actually kinda keen to try one out and see how far it overclocks.  That said the 4000 series announcement expected tomorrow will likely scuttle this cards chances before launch sadly.",
      "Forza Horizon 5 is shown briefly in the video, set to 1440P and the High preset... but with dynamic detail enabled. Due to the way that the game changes the internal render resolution and LOD to try and get close to the target framerate, the figures reported are rather meaningless as a performance comparison point.\n\nThe game's perfectly playable (with a few small pauses in the clips shown), but there's no new info here for anyone looking for any sort of meatier \"first impressions\" video.",
      "Nice. thanks for sharing. Maybe it is only me, but I would love a A770M all in Intel blue color. But maybe I am partial to blue shrouds https://imgur.com/UXBs2xZ",
      "Compared to other ratcheting screwdrivers it’s really not overpriced (still expensive though)",
      "Because MKBHD is as big if not bigger and has way more normies watch his channel.",
      "I don't know, ask his 16M subscribers",
      "He’s also got a great golf swing for an amateur in some golf YouTube videos.\n\nNot to mention his ultimate frisbee skills.",
      "One opinion you don't agree with doesn't suddenly make them stop being one of the most respected tech media outlets. I was worried that there might be an actual scandal there for a minute.",
      "they mentioned in one of their podcasts that they were starting to sponsor creators to help expand the brand.",
      "It's Forza Horizon 4 not 5",
      "Ah yeah that makes sense then. I never heard of this guy before today."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "a770"
    ],
    "title": "Hitting the Shelves: Intel® Arc™ A750 and A770 GPUs Release Today!",
    "selftext": "",
    "comments": [
      "OOS @ newegg",
      "Yea this launch has been kind of a mess so far. The A770 and A750 went up earlier than they were supposed to on Newegg and the A770 16gb sold out immediately. The A750 came in stock again on Newegg but now it is also out of stock\n\n**EDIT: THE A770 CAN BE BACKORDERED: https://www.newegg.com/intel-21p01j00ba/p/N82E16814883001**",
      "Aaand .. it's gone",
      "Such a whimper of a launch. They must have made one whole box per country.",
      "True limited edition cards",
      "Complete mess. Release a budget friendly card with all this hype....but don't tell us officially where or when we can buy it until an hour AFTER they sold out of the a770?",
      "Had the A750 in cart and by time I went to check out it went out of stock. Bummer",
      "its my bad, i bought them all",
      "Werent a ton of these things supposedly sitting in warehouses for months?",
      "Man… what a joke of a launch.",
      "https://youtu.be/-DT7bX-B1Mg",
      "Unironically, I think a *lot* of these LE cards are going to end up as shelf art.",
      "If they had all this cards since February.... and they are gone... that meas this is it, there should be no more stock.  \n\n\nThis is the first GPU in history to be designed as a collectible item.",
      "maybe demand is high?\n\nSounds a little like copium",
      "Dumb thing to scalp, frankly. These are the higher end models, and there are already better nVidia and AMD cards in the used market. Prices on those and new old stock of 3000-series cards keep coming down. \n\nNo one is going to pay much above MSRP on an Intel Arc card today 😂",
      "So where's that discount code from the HPG scavenger hunt :/",
      "Scalpers gonna scalp…the shoe market is slowing and PS5’s are in stock.",
      "Fucking backorder won't go through. Just keeps putting me back at the secure checkout page.\n\nEdit: It's aggravating. I work on a PC. I had auto-refresh set every 30 seconds since this morning. Missed the first drop. I got up to use the bathroom ONCE and of course that's then they come back in stock lol. Ugghhhh\n\nEdit 2 hours later: Still won't let me add the backorder.\n\nEdit: 5 minutes after previous edit I finally got the backorder to go through. Had to use the phone app (I was able to spam the confirm order much faster over and over). Worked on the 5th try.\n\nETA on the backorder is 10/19....if of course they get the stock in for it.",
      "They delayed these cards for months in preparation for this launch.   How did they mess this up so bad???",
      "Newegg made them available at 12AM Pacific. Their Chinese overlord’s probably made them do it….but it’s also Newegg so they could have just F’ed up per usual"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Is the ARC A770 a good card in 2023?",
    "selftext": "My RX 570 just bit the dust and I’m looking for a new, ~300 dollar graphics card. The ARC A770 seems like a pretty good pick (especially since it comes with MWII), but all the reviews say it doesn’t perform super well. I know there have been some driver updates that have helped tremendously with performance, so is it a worthwhile card now? \n\nI have an i5-12400F with ReSize Bar enabled so that’s not an issue.",
    "comments": [
      "that title lol you ask like it was released in 2014 or somethin\n\nanyways I'm joking but yes it's a fine card for the right price, though you will have to be aware of comparable cards and the limitations of intel graphics cards, like apis n such",
      "Good card for the price.",
      "A770 is a good GPU that is slowly getting better over time.\n\nHowever either get a 6650xt/6700/xt for better performance and drivers.",
      "It's a good card, I have one in an older system, Intel 9900ks (undervolted) Z370 MB, one that supports resizable bar and it works just fine. I use it mostly for picture and video editing, Topaz Video AI and Handbrake/MakeMKV, and it also doubles up as a Plex server but I have gamed on it. It gets about 75ish% the performance of my AMD 5800X3D/Nvidia 3080 system. '\\*edit. based on 3DMark score and not actual gameplay\\*'\n\nThere's a few quirks though that I hope will be sorted in the future, it idles at 48° and draws 35 watts which compared to Nvidia cards is probably double the temp and power draw, for idle at least. MSI Afterburner doesn't support it yet so I hope that changes in the near future too.\n\nOn a plus side running 3DMark Timespy it never exceeds 71° and while gaming it's usually much lower, it's also a pity you have the 'F' skew of the 12400 processor as the Arc utilises the CPU GPU in some scenarios.\n\nOverall I am very happy with the Arc.",
      "A770 preforms pretty well IMO, obviously far off from high-end Nvidia/AMD but yea; I play in 4K and most games are 60+ fps at mid-high settings, including MW2\n\n\n\nI got it like 1-2 months ago iirc, drivers have (seemingly) improved substantially in that period, went from super buggy for me to rarely encountering a driver issue",
      "Games that run in dx12 or Vulkan run best on ARC. Dxvk is very viable for older games.",
      "Because I don't game as much as I used to and didn't want to spend a bunch of money for a replacement when the 1080ti I had was just fine from a performance perspective.",
      "> MSI Afterburner doesn't support it yet so I hope that changes in the near future too.\n\nMSI Afterburner last I heard may be defunct.",
      "I view the ARC A770 as somewhat of an investment because they’re more than likely going to be collector’s pieces in 5-10 years. Especially the 16GB model.\n\nOther than that, I think it’s a safe purchase if you’re patient. Intel supports their products for a long time, unlike AMD that has dropped support for GPUs after only ~5 years (RIP Fury X, RIP R9 390). Performance and VRAM amount is pretty good at the $350 the 16GB model is currently priced at.\n\nThis is coming from someone than ran AMD GPUs from 2011-2017 though, so I’m more used to dealing with dog shit drivers and other bugs lol. Remember to take care of yourself.",
      "I replaced my 1080ti that I had bought new as a nvidia fe preorder that was dying slowly with an a770 16GB.\n\nIt’s more than enough for 1440 gaming (a bit quicker than a 1080ti) for me and the only game I have had real issues with is darktide where it basically just doesn’t work.\n\nDrivers have come a long way since launch so the real question is if you are ok with it’s level of performance with maybe some bugs here and there.",
      "I'm confused why you would opt for a product with such similar performance after nearly 5 years\n\nIs there some specific productivity application that is better suited to an Intel GPU?",
      "Vulkan runs fantastic on Arc. It's older DirectX versions that struggle",
      "You could do worse for the price.  It's terrible with older titles (DX10 or under), but does okay with DX11 and DX12.\n\nDrivers are still quite unfinished, so the card is very much still a work in progress.  Some stuff will run fine, other things will run like ass, or have lot of graphical glitches or issues.",
      "Personally I think your money could be better spent . Intel is forsure going to crack into the GPU market . It’s not gonna be with this card though .",
      "Intel Finewhiskey?",
      "MSI released a statement that they haven't abandoned Afterburner.",
      "Yeah development is dead on it now.",
      "Yes",
      "If you run AAA titles on Dx12 or Vulkan, it’ll be a really good card (equivalent to 3060 Ti in raster & 3070 in RT performance). \n\nIf you run a lot of older or niche titles that run Dx9, Dx10 or Dx11 you’re not going to have a great experience.\n\nAlso a CPU that supports ReBar is basically a requirement, without that don’t bother.",
      "[The most recent leak shows Battlemage not arriving until 2024](https://videocardz.com/newz/intel-arc-desktop-gpu-roadmap-leaks-alchemist-in-q3-2023-battlemage-in-2024), with a strange Alchemist refresh just before then."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "ACER Arc A770 BiFrost graphics card with 16GB VRAM is now available for just $282 - VideoCardz.com",
    "selftext": "",
    "comments": [
      "the 3060 is the literal worst possible pick for that price range when you have the RX 7600, 6700 XT, and now the A770 16GB.",
      "That is insanely cheap for that much VRAM.",
      "What makes you say so?",
      "vram is cheap, but some companies may give you less in order to lure you into upgrading sooner rather than later",
      "Come on man, no need to sugar coat. Just say its Nvidia.",
      "Looks pretty damn cool, but I don’t really trust Acer products much",
      "I've been using it in my build for a good few months now, and have had zero complaints. My recommendation is to get it if you're looking to buy.",
      "They do well when they have good thermal management (thicker laptops, this GPU, etc)\n\nHistorically its been their thin-and-light laptops with bad thermals that fail",
      "that's not what they said though. They didn't speak about power usage at all, they said that the 3060 has \"more consistent performance\". Based on what would this be, since you determined it correct?",
      "For $280 the A770 is an excellent choice though. An extra 30 bucks for double the VRAM ignoring everything else is a good deal.",
      "I have 2 of their monitors for 4 years now, zero issues. They are a mainstream brand, with hits and misses, I doubt this GPU is a bad buy in any way.",
      "It's miles better than at launch but still nowhere close to AMD/Nvidia level of maturity.",
      "From what I've seen XeSS actually looks better than FSR @1080p and 1440p. It's the same as DLSS @1440p but  very slightly worse at 1080p.",
      "Why? Only Acer products I've bought are their monitors, but they are top tier.",
      "😎👉 **NVIDIA**",
      "If you're interested in the 16gb model. This is the deal to get.\n\nJump on it before the sale is over!",
      "Going after Content Editors and AI Developers?",
      "Only 280, damn that is cheap",
      "I'm inclined to agree for the a770. A750 is a decent choice tho.",
      "And XeSS has become pretty damn good compared to DLSS and FSR"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Installing my Arc A770",
    "selftext": "Just installed my newly arrived A770. Replacing an old GTX 960 and temporary RX570.",
    "comments": [
      "Just letting you know, once you set the rgb patterns to your liking, you can unplug that and it will keep whatever was set to :)",
      "Seriously?? Ty for the advice, dunno if I’ll unplug it or not. Even if I will certainly let Rainbow mode forever.",
      "Friendly remember about activating resizable bar.",
      "This card looks amazing imo!!",
      "You’re leaving performance on the table then, but you do you.",
      "Nice. Make sure to enable rebar!",
      "Maybe he's a behind the scenes tech guy instead of a performer.",
      "Make a reactor from it",
      "Awesome! Enjoy it, glad to see people getting it\n\nIm still trying to find one in stock, I got a 3080Ti but just getting one for fun",
      "Yep, already activated it before buying and aspm L1 too. I made all the steps before it’s arrival but the card at idling keep eating between 37-38w. Need a driver for that.",
      "It is, looks really solid, doesn’t bend while on the PCI, and rgb is nice.",
      "Arc was really interesting me as a sort of \"Painful to use now, but watch it age like fine wine\" sort of gamble, but after seeing how LTT's careful optimism gave way to absolute disdain after having to use it I really don't think it's worth putting up for - *especially* for CSGO. If you were only playing modern DX12 AAA single player games then maybe, but for CSGO and anything else older and multiplayer literally anything else will be worth it more.",
      "I already want one it's just I can't FIND one. \n\nI finally found it though after waiting over a month",
      "No, their overclocking is dumb. Performance bar is so stupid, not a lot of difference between any of the numbers.\n\nPut the performance on 30\n\nPut voltage on .95\n\nPut power on max.\n\nLook at temps, if you see it too hot, bring the 228W back to 210W.\n\nThe voltage does not add that much heat, the power does.",
      "Does anybody have good A770 OC/Undervolt settings via Arc Control?",
      "Don’t even need the rgb cable. I didn’t even pull mine out of the box.",
      "My AIO needs that USB, only one.",
      "If you have money or want a second build why not?\nActually a lot of people bought one, budget gamers (750) or full Intel build gamers (770). When I called to shop who had some in stock, they said they sold the last one 15 minutes before my call… \nWould be glad to see those steam survey or intel sells to see the current marketshare",
      "Max boost is 2725 for me, temp reaches 84c when 100% utilisation though. Doesn't need undervolting, stays in the low/mid 70c at stock.",
      "Already activated it before buying aswell as ASPM L1"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Got a 16gb Intel Arc A770 LE at Microcenter!",
    "selftext": "Nice design, I wonder if it will end up being a collectors item down the road as it doesn’t seem like there’s a huge amount produced. (Regardless of performance discussions)",
    "comments": [
      "You lucky bastard!",
      "Yeah and I like the way the box smells",
      "Want to test it out, plus I liked the design. It won’t be my main gpu",
      "What made you want to take the plunge?\n\n(not trolling)",
      "They have a 4090 so money may not be an issue. I've heard the stock for them was also kinda low so they may have just bought what was there.",
      "What city u in? You bought it this morning (Oct 19)?",
      "It is a 4090 - doing 10,240x1440p with a triple monitor setup for racing sims 😁",
      "Ok.  So A770 really is just for fun!  Nice.",
      "No I got it a few days ago, NY",
      "Can I ask what your main card is and what resolution/speed you run?  Still debating this option in my head but fairly sure it won't push my 1440p UW.",
      "Congratulations on the purchase. One day, I may talk myself into purchasing the 750. Just wish it was a tad cheaper. Enjoy the card!",
      "It’s actually a red video camera, and I did a screen grab from the video with a 24mm canon cine lens",
      "> RED video camera\n\nAh, that explains it haha. Thanks for the response.",
      "That’s me!",
      "Wasn’t the 750 the one to buy?",
      "What camera did you use to take the photos? They look fantastic",
      "Hey do you run the Classical Technology YouTube channel or your username is just a coincidence? Congrats on getting the card! Hopefully you can redeem the free games.",
      "I hope Intel has a top team sorting out the drivers\n\nThis is the card I would buy if I was upgrading",
      "Yep",
      "LTT did a video about running it beside a main graphics card."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Intel CEO confirms first batch of Intel Arc A770 GPUs is getting ready for retail - VideoCardz.com",
    "selftext": "",
    "comments": [
      "More photos of Arc from Intel, seems like the only thing mortals can get :)\n\n[Here's another one](https://i.imgur.com/i5RzYdB.png)",
      "I hope they are $249-$299.",
      "I don't think it makes much sense for it to be priced the same as a 6600 XT/6650 XT when performance will suffer so severely in older games. Tier 3 pricing, right? If it truly was the case then it ought to be $250 or lower. But will that happen? Doubt it. Regardless, I think we all should hope for better than the bare minimum, because it's just too cynical not to.",
      "for 200-250$ it’s ok. If drivers still suck I would only spend 150$ to be an alpha/beta tester.",
      "They are lucky Nvidia will probably announce only the super high end stuff for the time being.",
      "That’s also likely why they haven’t shipped yet. Trying to get the drivers improved as much as possible so they can charge more.",
      "What's the adjective for this whole affair now that officially (unless you can buy one in the next two hours) of them being finally for sale after the 4000 Nvidia announcement today and thus being compared against that line at launch?\n\nIs it *ironic*, *sad*, *unfortunate, emblematic*?",
      "yeah they lost my interest after the whole Q1 disaster",
      "As far as I know, the existing cards in 30xx series already smokes it, but nobody expected anything else. Just getting a product out the door is amazing in itself, let alone getting 3060 like performance on first try.",
      ">at this point who cares?\n\nidk probably anyone thats actually interested in the computer hardware hobby\n\na third player in GPUs isnt a bad thing and it doesnt matter that intel is having a rocky first gen when the other vendors are firing on all cylinders \n\njust remember time is a thing and the future will have a different set of problems and issues unlike we have now. there is a chance nvidia or amd could stumble and next thing you know were in another 5 year gpu slump. this would be a lot less likely with three major vendors",
      "They always announce/release the high end first but it will take away publicity.",
      "What are you even on about.\n\nAlso the reason why people went for macs despite the performance was the software. No one's going to be buying an intel gpu for the software besides av1...which since they took so long to release that nvidia's new gpus are already getting av1 next month. Drivers are literally the worst part about arc gpus. Don't bother me with the fine wine saying either, tech gets superseded faster than the software can mature.\n\nNvidia will still dominate in the workstation/server space too.",
      "As far as maketing stunts go, this one isn't great.",
      "\"getting ready for retail\"...that could mean literally, 1 month, 1 quarter, 1 year. I hate the vagueness that is used. Especially when you listen to a quarterly earnings call \"tracking well\", \"on track\", \"ramping in 2H\", \"sampling well\"....  Just tell us in what month or even quarter(with a year attached to that) consumers should be able to purchase one at a physical or online retailer and be done with the vagueness shenanigan's. I get it that when the physical hardware is in early to mid stages vague is a necessity, but we should be well beyond vague at this point.",
      "Even still, why risk an unknown over the Nvidia/AMD equivalent? Even if they fix the drivers to a workable state it's still less efficient than the Nvidia equivalent on a much better process so you'll waste money yearly just having the PC on.",
      "Depends what you mean by efficient. Performance per watt nvidia wins but power consumption intel wins. \n\nI wouldn’t buy an intel gpu for my gaming pc yet but it would work fine in my other box which I use for daily driver and causal gaming on Linux.  Right now i have an amd 570. \n\nIntel gpus also make sense for content creation workloads based on early benchmarks. \n\nNvidia has a hefty price versus to competition. I upgraded the gaming pc from a 1080ti to a 6900xt last year and no regrets. It runs a lot cooler too.",
      "my 100 USD off from their event is still waiting... yet haven't heard anything from them since Spring. I guess it's competing with regular 6700 costing 400euros. nvidia launch just cemented series 30 prices.",
      "if its 300$ its DOA",
      "In 2-3 years Intel will have 1 card specifically they say is for gaming and then they’ll focus on “Accelerators” down the line-dual purpose cards that will drive your display but possibly also capture input and/or accelerate encoding with next gen codecs. There’s significant hope in the streaming and broadcast markets that Intel may have struck gold with their platform regarding encode/decode/transcode workflows and I guarantee you’ll see them entering this space in a targeted way with multi-linked cards. To do that effectively they’ll have to keep cards at under $400 always. Intel may not become a huge player in the gaming market but they can disrupt other sectors which is a good thing still.",
      "The pictures?\n\nInb4 intel sells GPU NFTs due to poor yields and intransigent driver issues."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Intel Arc A770 16 GB Review and Benchmarks by LTT Labs",
    "selftext": "",
    "comments": [
      "Battlemage is going to be exciting. Much better architecture.",
      "Looks like a pretty good entry level alternative now.",
      "AMD is cooked",
      "imagine giving it the power and cooling of a 4090, equipping it with the fastest memory  and clocking it like a flagship processor...",
      "Soon. Post celestial should be Intel. Falcon shores for data center is first Intel node, Intel gpu",
      "The year where LTT Labs is adding products to their database, and the year where the A770 is still Intel's flagship GPU.",
      "I hope so because damn Intel has been getting its butt kicked lately on the market and they need a win",
      "That interactive (rotatable) Lumafield CT scan feature will never get old. So cool.\n\nAlso, their specs for the memory speed are wrong. The A770 LE 16GB runs at 17.5Gbps (2187MHz), not 16Gbps (2000MHz). The latter is for the A750 and 8GB A770 variants (or a few A770 16GB models from Gunnir).\n\nTechPowerUp never fixed it in their database either, drives me nuts.\n\nWould also be nice if they mentioned the support for 10-bit HEVC 4:2:2 encode/decode, which isn't found on any other current AMD or Nvidia cards. Makes it possible to smoothly edit footage from some Sony and Canon cameras that utilize the format.\n\nInteresting that they've included GravityMark tests already in their synthetics section.\n\nOk, I'll stop adding crap to my comment now. Glad they're putting more tests out, but do wish they were more comprehensive.",
      "Just contact us in any way when you find issues with our database and we‘ll get it fixed",
      "Here ya go: :)\n\nhttps://www.reddit.com/r/IntelArc/comments/1blz05i/comment/kwapd3t/",
      "your looking at it wrong. that wouldnt be a product. that would be a one off tool.",
      "rinse beneficial coherent engine disarm serious tease vanish fine connect\n\n *This post was mass deleted and anonymized with [Redact](https://redact.dev)*",
      "What year is it?",
      "It's a great GPU. It still have problems with some niche titles, but very often you can fix it with dxvk. I could even play Cyberpunk 2077 with some raytracing options on 1440p. Ofc it was with XeSS enabled (Intel's DLSS alternative), but I wouldn't expect any card for the price to be able run raytracing without super sampling",
      "I am always interested in news about Intel GPUs! :3",
      "I have this card in my new rig. I probably won’t be gaming at all, but feels good to have the option.",
      "Arrow lake iGPU has HEVC decode/encode for  422 10 bit, should be good for those with Nvidia or AMd GPUs",
      "Why?",
      "LTT is slowly adding products to the lab page. That's all...today they added the A770.",
      "password hash \"checker\" or something"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Dell now offering $1,949 Alienware Aurora R15 desktops with Intel Arc A770 GPU - VideoCardz.com",
    "selftext": "",
    "comments": [
      "At $1949, that in an absurdly bad price for an A770 PC\n\nEven by prebuilt standards",
      "Who even still buys Alienware in 2023? Dell is out of touch with what people want, just shut down Alienware already",
      "Plus the airflow in that case looks like a nightmare with all that plastic and metal blocking fans. I bet it throttles under load.",
      "Their monitors are pretty good. That’s about it",
      "Gamers Nexus has had some fun deconstructing Alienware cases and dunking on how supremely bad they are all around.",
      "$2,000+ all said and done for a 13th gen x700 chip and an Arc GPU, jesus man.",
      "Dumb people who do 0 research before spending $2000+",
      "Yeah. IMO, a 4070Ti or 4080 (or 7900XT) should be in that thing.",
      "That should be like $1500 tops, but it's an Alienware.",
      "People who buy Alienware currently deserve to be ripped off.",
      "I can build a whole 4080 rig for that much cash",
      "There's some prebuilds that it's actually hard to match the value of with a home build due to economies of scale \n\nBut this ain't one",
      "Yeah I'd argue you'd have a better time with an XPS desktop.\n\nBut I'd argue even harder to pick a different SI to go with.",
      "But how is dell supposed to make any money that way?",
      "Nuts",
      "That’s gonna hurt their rep….unless something has changed Arc drivers usually need more then one day 1 patch for most games coming out not to mention how older games usually have intermittent issues or crashes. It’s not the best experience from a customer perspective.",
      "The Alienware name deserves better than this. The laptops are actually pretty good. They just insist on reusing the same tired chassis design for the desktops.\n\nI'd love to see the level of engineering they put into dressing up an office PC case go into making that Alienware look actually functional.",
      "It’s a crime what they did the the XPS name imo, there’s nothing extreme about them",
      "I owned an xps laptop, put me off on ever buying a dell again.",
      "north automatic possessive handle snow station threatening overconfident hunt silky\n\n *This post was mass deleted and anonymized with [Redact](https://redact.dev)*"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Intel GPU Chief confirms Arc A770 is getting ready for launch - VideoCardz.com",
    "selftext": "",
    "comments": [
      "> , i don't believe the latest rumors saying intel will cancel their gpus, because it would be a waste of money really.\n\nTo be fair, they cancelled their last dGPU effort codenamed Larrabbee despite having poured a ton of money into it.",
      "More talk please, we are all thrilled to hear more talking.",
      "Resizable Bar is nothing new",
      "Not really, a card can be unpopular at $500 and super popular at $150, but if it makes a loss at $150 it's unsustainable. still better to get $150 back for every chip you've made so far rather than throw them away and take even more loss, but if they deem it unsustainable they'll cancel the project.\n\nBut overall that's just this gen, the real question will be if Battlemage (their second gen) is seen by Intel to have closed the gap significantly in efficiency or if it's still so far away they can't see making profit in several more generations. Either it makes small enough gains they can't see profit for the next several generations with similar gains or it makes big enough gains they can see the project churning out profit within another 1-2 generations.",
      "Intel Gpu's for data centers are absolutely not getting cancelled. Those are going full steam ahead with a full future roadmap and support via oneapi. If Intel cancels Arc for consumers/gamers it will be a net negative for that segment. When the inevitable next boom cycle of cryptocurrency comes along these are the same people whining for cheaper Gpus.",
      "Releasing A770 has absolutely no link to if the project is cancelled or not. No one cancels a project with 100s of millions or maybe even billions in stock inventory of a product thereby killing most sales immediately as people won't want a chip they think won't get future support.\n\nIf Intel was to cancel the Arc project it would 100% be after A770 launches, probably 6+ months later if not more like 2 years. They'd just let news of Battlemage drift further and further apart with less and less detail before trying to quietly kill it. That would allow them to shift as much inventory as they can and make as much cash back as they can.",
      "Not really. It turned into their Xeon Phi line and made them a lot of money.",
      "There is little evidence it made them a lot of money, it was largely cancelled due to lack of demand with only 2 actual xeon phi generations with ultimately very little progress (from like 240 cores to 280 cores the following generation despite a node drop). The third gen was cancelled largely due to lack of demand. Super computers are pretty much advertising tools and often have at cost of free gpus/cpus so the companies that provide them get talked about in regards to the most powerful computers in the world. So having supercomputers powered by Phi indicates absolutely nothing in relation to actual demand by customers who would pay for those chips.\n\nBut a project being cancelled doens't mean products can't be sold. Xeon Phi was largely Intel's attempt to salvage something from Larabee but with high R&D costs and low volume sales it's a reach to say it made them a lot of money, in fact we almost certainly know it didn't. If it made them lots of money they wouldn't have cancelled it because no one cancels a product line making them lots of money.",
      "They made products and sold them to companies, primarily for machine learning iirc.",
      "I've got my money on Intel getting things a lot righter for battlemage.",
      "ah another one of those, releasing very soon articles at least this time if from raja. But then again yall in the GPU world dont have a real good opinion about him",
      "If Intel wants traction in this market they need to create the next RX580. Maybe they already have, we’ll see. \n\nThe 580 was the definition of cheap and cheerful. It punched above its price point at launch, and continued to be relevant for a long time first at the mid range, and later the low end.",
      "Sooner than you expect! - 2 months ago",
      "For $249.99?",
      "https://i.imgur.com/MueLuxa.png",
      "I think there are chunk of folks praying that Intel keeps failing for ever. So these rumors keep swirling. If you look at Pat's strategy, computing is no longer just CPU. So they will definitely not give up on GPUs. I think what could get cancelled are anything tangential to computing. If they dont have long term sustainable business model, then they go away. We will get Battlemage/Celestial etc not just for consumer but also DC GPU like Rialto Bridge and XPU(Falcon Shores). Let us see where things are in couple of years.",
      "Whelp I guess the future of arc will depend on how popular the arc cards would be",
      "I do hope there are more choices in gpu consumer market while Intel should take this opportunity to fight with Nvidia and AMD. Let's wait and see.",
      "Just don't give up on the dream. Keep working on the drivers and Intel may have something. Would be good for the market if they can do well in the GPU space. Make it happen, Raja and team!",
      "Not sure how true that is. See optane dcpmm 3rd gen. They announced cancelling the roadmap all things optane, but still releasing 3rd gen optane dcpmm."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Im having some problems with my Arc A770 in sleep mode...",
    "selftext": "",
    "comments": [
      "Report it to intel support, help them and help yourself.",
      "Did you try Ctrl + LShift + Win + B by chance?\n\nEdit: [Also, it's a known issue in the driver report.](https://downloadmirror.intel.com/764788/ReleaseNotes_101.4032_WHQL%20.pdf)",
      "\\>having some problems\n\n\\>Arc A770",
      "Awww it’s having sleep paralysis",
      "I originally had an A770 and loved the thought of supporting it. I can troubleshoot just about anything. Eventually got tired of having to deal with these problems and just random things that would be annoying. Sold it to someone else who wanted to support it and ended up with my first AMD card since the Polaris gen and it’s been great. It just works, every single day lol.",
      "\"fixed\" the same issue, try setting PCI express power management to Deactivated in advanced energy options. It's the same setting that reduces idle power consumption, but for me it introduced this behavior.",
      "This is to be expected with the A770 it's gonna have problems, I would contact Intel and try and submit a bug report this is the only way it's gonna get better!\n\nBut thanks for being one to test the cards. I would but not for current prices.",
      "it's dreaming",
      "Hello Jensen Huang.",
      "Thanks for reminding me. Just updated it.",
      "thats what happens when you use it with its rival processors!!",
      "Its playong atari games",
      "CTRL LSHIFT WINDOWS B should fix it (restarts display drivers)",
      "What's that device that shows up at 0:05(s) of the video, with some lights, one blue, another red... ?",
      "Sleep mode in Windows is an awful feature all around but ARC does have bugs that compound the issue. For instance, my PC will boot cycle a couple times when waking it or turning it on, occasionally and I get a screen saying that the gpu adapter has no vbios, lol. Then it just works. *shrugs*",
      "Your flair is still A770",
      "Flat out wrong. \"With Linux 6.2 there is also now at least the DG2/Alchemist support out-of-the-box, no stability issues were observed, and it's running off an open-source upstream driver stack similar to the AMD Radeon driver stack that is much loved by the Linux gaming/enthusiast community after years of work there.\" https://www.phoronix.com/review/linux62-intel-radeon/5",
      "That's why you don't buy a prototype GPU, good luck.",
      "It’s planning something - Some quote about skynet",
      "Well you got a limited edition. That must mean you get limited graphics.\n\n-\tClosed (by design/won’t fix)\n\n^/s"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Intel officially discontinues Arc A770 16GB Limited Edition - VideoCardz.com",
    "selftext": "",
    "comments": [
      "i guess its people just not drawing connections in the world with this.\n\namd and nvidia only make a certian number of cards a generation and discontinue thier lines too. there just isnt articles about it.\n\nnothing to see here.",
      "No that’s correct. This happens all the time with Nvidia Founder’s edition and AMD Reference cards - they end production to free up space for the next generation cards to begin manufacture. Other models of the A770 such as the Acer 16GB and Asrock 8gb are still being manufactured. Only Intel’s ‘Limited Edition’ reference model has had production end.",
      "I was informed by a multitude of reviewers that \"Limited Edition\" didn't mean that they would end the SKU, just that it's their nomenclature for the card. Was that a lie?",
      "There's an AIB model called the Acer Predator BiFrost and that has 16 GB.",
      "There often are articles when they discontinue chip production for a certain card. Discontinuing production doesnt even mean a whole lot. They could have already produced all the inventory they think they need before battlemage for all we know.",
      "This is only the Intel made \"reference\" card. Like another poster pointed out, Acer has their own A770 with 16GB Vram.",
      "This version of the card has reached the end of production line. There are still AIB A770 16GB cards out there.\n\nI have an A770 LE exactly because I expected this to happen. Wanted to keep a piece of history I suppose. I would hope this means they are paving the way for newer, more powerful cards.",
      "Repost.",
      "Serious question. \nLooking for 16GB card, at least.     \nMy problem was that i was bit afraid that Intel would \"cut\" also GPU division (not considering dedicated igpu).     \nNow i see this information, any one have more insight?        \n8 more GB is like 20$ more cost on card manufacturing.",
      "Yes, missed that ... it was ... ok ill get this Intel, and this is first thing that showed up.",
      "So the headline implies something that is misleading. Wow that's never happened before.",
      "But... that's your only option.",
      "OP did you miss this post from 3 days ago?  This is old news.\n\nhttps://www.reddit.com/r/intel/comments/14fb4ju/intel_discontinues_intel_arc_a770_le_16gb/?utm_source=share&utm_medium=android_app&utm_name=androidcss&utm_term=1&utm_content=share_button",
      "Really wanan buy one . 305$ CAD … but having a hard time deciding if it’s better then my 2060s",
      ">there just isnt articles about it.\n\nYou mean there's no article like [this](https://www.notebookcheck.net/NVIDIA-is-no-longer-selling-the-RTX-3080-Founders-Edition-or-RTX-3090-Founders-Edition-directly.497768.0.html) and [this](https://wccftech.com/amd-to-end-radeon-rx-6900-xt-6800-xt-6800-reference-model-production/).\n\nColour me surprised.",
      "Pretty sure TAP himself said in one of the videos that Limited Edition didn't literally mean limited. They probably stopped production now because they weren't selling enough of them or they are getting ready for the next generation cards.",
      "Yep.",
      "> stopped production\n\nAll of these were produced, boxed, and warehoused in 2022, awaiting the driver team making progress towards a stable retail driver. They've sold through all or most warehoused stock now, and can discontinue the sales support etc. (at least for Intel. Acer's 16gb, etc  may be sitting on tray GPU packages awaiting solder to boards and thus still in production)",
      "> You think whenever AIB makes a card they just go out to open market and buy the GDDR6 module for that card?\n\nYou're making the point you should make, but in the wrong direction.\n\nIf spot price is so low, then surely whatever bulk pricing a major OEM can secure would be *lower*, not *higher*.\n\n> Do you just buy memory chips and it just gets magically superglued to your card directly from fabs?\n\nIt's bought in rolls and fed into machines. The cost to solder a few more chips to a board already on the line is minimal. You think humans are toiling away doing this shit by hand?\n\n> Do you understand that little pesky thing called SUPPLY AND DEMAND? \n\nDo you understand that little pesky thing called *economy of scale*? The more something sells the cheaper it gets *at bulk OEM scales*",
      "Closer to a 3060 isnt it"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "a770"
    ],
    "title": "MW2 running on Arc GPU",
    "selftext": "Hey Intel users,\nI’m running into a minor issue that turn very annoying when playing MW2 with my A770. Idk if some also have this even if you have another GPU from Nvidia/AMD.\n\nEverytime I go into a multiplayer match its laggy to the point of being unplayable during the first 2min (class selection to middle objective in domination for example)  and after that minute or 2 it’s completely fine. It’s like the game was loading a lot at the start and reducing graphics especially shaders to low and textures to med didn’t change the lagginess. I even stopped the better textures downloading, but still laggy. \nIt’s just annoying that you can’t play the first minutes of the match a little competitively. I think that the bigger the map, the longer it’s laggy.\n\nPS: Laggy in the sense of stuttering. Installed the game on a 7200rpm instead of 5400. Tried a match and it’s still stuttering but maybe little less.\n\nSOLVED: bought a Kingston Fury 1TB SSD and it works like a charm, no more stuttering when in game and I can finally play calmly.",
    "comments": [
      "Its going to be a more prominent issue going forward, more and more games are designed for SSD.\n\nYou should invest in an m2 SSD, they're pretty affordable now, you can get a solid 1t for 100 bucks, HDDs are probably going to be relegated to big media drives in the future.",
      "Yea that's the problem the game is stuttery mess in hdd",
      "Do you have the game installed in ssd or hdd? My brother had the same problem with rx 6700 and when he moved the game to ssd the stutters gone!",
      "Sure, but they're not any more expensive than a SATA SSD, and in some cases they're cheaper, and its futureproofing, because Directstorage will come, eventually.\n\nYou can get a solid 1tb NVME for the same price as a 1TB sata thats also solid, so there's no reason not to go NVME",
      "F, I don’t have any ssd for games and didn’t plan to buy one. Never had any prob for games on hdd like this.",
      "I now have a NVMe SSD and it works fine, no more stutter and its smooth af with ultra settings",
      "Nice",
      "DX12 had a unique way of loading shaders which causes stuttering the first time you load an area. Usually you get a few stutters early then it smoothes out. But if there is some kind of driver issue it might not load textures and have prolonged stuttering. That's just my guess without looking at the bench marks.\n  \nAlso, is resizable bar turned on?\n  \nHave you run DDU and fresh installed drivers?",
      "If you’re in the states they’re going for 50-60 bucks for a 1Tb\n\nJust head over to buildapcsales and you’ll find ton of nvme/sata ssd drives for sale",
      "They should make it explicitly clear that there will be issues when the game launches if it detects slow read and write speeds.",
      "honestly nowdays there's not that much value to partitioning anymore so I would just get a solid m2/NVME and put as much as I can on it.",
      "Realistically the amount of rewriting you would have to do on a modern SSD to seriously degrade it would be pretty unreal. For a solid 1tb NVME drive, you would have to write like, over 600-1000TB of data onto it for it to begin to degrade.\n\nThe lifespan of an SSD is probably better than an HDD nowdays, realistically. Its just not something you're going to have to seriously worry about in terms of lifespan for your system, a modern NVME is going to last longer than most other components in your system by a fair amount.",
      "Samsung 970 Evo Plus is 1tb with dram for 100 bucks\n\nIn comparison, the Samsung 870 Evo SATA SSD 1tb is like, 90\n\nFor 10 dollars in difference, you get insanely faster read/writes on the 970 because its an NVME.",
      "When you say laggy, do you mean like rubber banding or choppy and stuttering?",
      "Hdd of 4tb, my ssd is dedicated to windows",
      "Stuttering",
      "Hdd or full spec? The game is on a 5400rpm 64mb cache wd blue of 4tb. Rest is I7-10700f, A770 LE, 2x8 cl16 ballistix 3200mhz ddr4.",
      "Definitely the hdd indeed, more and more recent games do not cope well with an hdd.",
      "Sounds like DX12 stuttering, but like a million times worse than most people experience. You've probably already tried updating drivers, but that's my first suggestion. I would update them across the board, not just GPU. I'd also run a benchmark after updating them to see if your components are performing in the expected ranges. UserBenchMark is a free option that you could try.\n  \nRandom thought, is your GPU in the first pcie slot? If you installed it lower, it might not perform to specs.",
      "First pcie slot as it is the reinforced one. I dunno, why would dx12 stutter just for the first 2 min of a match then disappear? Stuttering would likely occur everytime and in solo if it was dx prob?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "a770"
    ],
    "title": "Intel Arc A750 & A770 Meta Review",
    "selftext": "- compilation of 11 launch reviews with ~2240 gaming benchmarks at all resolutions\n- only benchmarks at real games compiled, not included any 3DMark & Unigine benchmarks\n- geometric mean in all cases\n- standard rasterizer performance without ray-tracing and/or DLSS/FSR/XeSS\n- extra ray-tracing benchmarks after the standard rasterizer benchmarks (at 1080p)\n- stock performance on (usual) reference/FE boards, no overclocking\n- factory overclocked cards _(results marked in italics)_ were normalized to reference clocks/performance, but just for the overall performance average (so the listings show the original result, just the index has been normalized)\n- missing results were interpolated (for a more accurate average) based on the available & former results\n- performance average is (moderate) weighted in favor of reviews with more benchmarks\n- retailer prices and all price/performance calculations based on German retail prices of price search engine \"Geizhals\" on October 9, 2022\n- for the full results plus some more explanations check [3DCenter's launch analysis](https://www.3dcenter.org/artikel/launch-analyse-intel-arc-a750-a770)\n\n&nbsp;\n\n1080p|Tests|6600|6600XT|6650XT|3050|3060|3060Ti|A750|A770LE\n|:--|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n[ComputerBase](https://www.computerbase.de/2022-10/intel-arc-a770-a750-limited-test/)|(10)|-|-|_124%_|81%|114%|143%|100%|107%\n[Eurogamer](https://www.eurogamer.net/digitalfoundry-2022-intel-arc-7-a770-a750-review)|(8)|-|116.4%|-|-|101.6%|131.2%|100%|108.5%\n[KitGuru](https://www.kitguru.net/components/graphic-cards/dominic-moass/intel-arc-a750-limited-edition-review/)|(10)|95.1%|_110.8%_|-|-|_97.6%_|128.0%|100%|108.4%\n[Le Comptoir](https://www.comptoir-hardware.com/articles/cartes-graphiques/46698-preview-intel-arc-a770-le-16-go-a-a750-le.html)|(10)|93.8%|-|_115.5%_|-|101.8%|135.3%|100%|109.2%\n[PCGamer](https://www.pcgamer.com/intel-arc-a770-limited-edition-review-performance-benchmarks/)|(9)|99.8%|119.3%|-|78.4%|106.8%|-|100%|109.9%\n[PCGH](https://www.pcgameshardware.de/Intel-Arc-Grafikkarte-267650/Tests/A770-A750-Test-Benchmarks-Preis-Release-1404382/)|(20)|-|112.7%|118.0%|72.9%|100.3%|-|100%|107.1%\n[PC Watch](https://pc.watch.impress.co.jp/docs/column/hothot/1445247.html)|(10)|-|-|-|-|_104.2%_|-|100%|110.9%\n[PCWorld](https://www.pcworld.com/article/1341464/intel-arc-a770-a750-graphics-card-review.html)|(11)|98.7%|-|-|-|99.3%|-|100%|106.0%\n[TechPowerUp](https://www.techpowerup.com/review/intel-arc-a750/)|(25)|100%|116%|-|76%|104%|132%|100%|106%\n[TechSpot](https://www.techspot.com/review/2542-intel-arc-a770-a750/)|(10)|99.7%|112.1%|119.1%|75.3%|104.7%|130.6%|100%|105.8%\n[Tom's Hardware](https://www.tomshardware.com/reviews/intel-arc-a750-limited-edition-review)|(8)|95.4%|111.5%|113.7%|72.6%|98.8%|128.4%|100%|111.9%\n**average 1080p performance**||**98.4%**|**113.8%**|**118.4%**|**74.6%**|**102.5%**|**131.6%**|**100%**|**107.9%**\n\n&nbsp;\n\n1440p|Tests|6600|6600XT|6650XT|3050|3060|3060Ti|A750|A770LE\n|:--|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\nComputerBase|(10)|-|-|_112%_|74%|107%|137%|100%|109%\nEurogamer|(8)|-|104.6%|-|-|95.8%|126.0%|100%|108.7%\nKitGuru|(10)|86.6%|_102.4%_|-|-|_93.6%_|124.5%|100%|110.9%\nLe Comptoir|(10)|85.0%|-|_104.2%_|-|97.1%|130.6%|100%|110.1%\nPCGamer|(9)|92.3%|111.5%|-|74.8%|103.7%|-|100%|112.6%\nPCGH|(20)|-|104.2%|109.6%|69.5%|97.0%|-|100%|108.8%\nPC Watch|(10)|-|-|-|-|_101.7%_|-|100%|114.4%\nPCWorld|(11)|86.9%|-|-|-|94.2%|-|100%|108.2%\nTechPowerUp|(25)|87%|103%|-|69%|96%|125%|100%|107%\nTechSpot|(10)|86.6%|98.3%|105.2%|68.7%|94.4%|123.8%|100%|106.9%\nTom's Hardware|(8)|85.7%|102.0%|104.1%|69.1%|95.4%|126.7%|100%|112.7%\n**average 1440p Performance**||**88.4%**|**103.3%**|**107.8%**|**69.4%**|**97.0%**|**127.2%**|**100%**|**109.4%**\n\n&nbsp;\n\n2160p|Tests|6600|6600XT|6650XT|3050|3060|3060Ti|A750|A770LE\n|:--|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\nEurogamer|(8)|-|93.4%|-|-|92.9%|124.3%|100%|110.2%\nKitGuru|(10)|75.8%|_89.0%_|-|-|_96.8%_|132.0%|100%|120.5%\nPCGamer|(9)|80.9%|99.0%|-|68.9%|97.2%|-|100%|112.6%\nPCGH|(20)|-|96.5%|102.2%|69.4%|99.8%|-|100%|117.6%\nPC Watch|(11)|-|-|-|-|_104.5%_|-|100%|123.6%\nTechPowerUp|(25)|74%|88%|-|64%|92%|122%|100%|109%\n**average 2160p Performance**||**78.5%**|**93.3%**|**~98%**|**67.0%**|**96.4%**|**127.3%**|**100%**|**114.6%**\n\n&nbsp;\n\nRT@1080p|Tests|6600|6600XT|6650XT|3050|3060|3060Ti|A750|A770LE\n|:--|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\nComputerBase|(4)|-|-|_84%_|74%|115%|148%|100%|111%\nLe Comptoir|(10)|60.1%|-|_73.7%_|-|101.4%|138.9%|100%|107.3%\nPCGH|(10)|-|80.2%|83.8%|73.7%|103.5%|-|100%|119.4%\nTechPowerUp|(8)|67.1%|78.5%|-|67.2%|93.2%|120.7%|100%|107.6%\nTom's Hardware|(5)|62.1%|73.9%|76.1%|65.2%|93.0%|125.0%|100%|114.3%\n**average RT Performance**||**66.5%**|**76.7%**|**80.5%**|**70.3%**|**100.1%**|**131.8%**|**100%**|**112.3%**\n\n&nbsp;\n\n&nbsp;|6600|6600XT|6650XT|3050|3060|3060Ti|A750|A770LE\n|:--|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\nGen & Mem|RDNA2 8GB|RDNA2 8GB|RDNA2 8GB|Ampere 8GB|Ampere 12GB|Ampere 8GB|Alchemist 8GB|Alchemist 16GB\n1080p Perf|98.4%|113.8%|118.4%|74.6%|102.5%|131.6%|100%|107.9%\n1440p Perf|88.4%|103.3%|107.8%|69.4%|97.0%|127.2%|100%|109.4%\n2160p Perf|78.5%|93.3%|~98%|67.0%|96.4%|127.3%|100%|114.6%\nRT@1080p Perf|66.5%|76.7%|80.5%|70.3%|100.1%|131.8%|100%|112.3%\nU.S. MSRP|$329|$379|$399|$249|$329|$399|$289|$349\nGER Retail|290€|380€|380€|300€|380€|470€|~350€|~420€\nPrice/Perf 1080p|119%|105%|109%|87%|94%|98%|100%|90%\nPrice/Perf 1440p|107%|95%|99%|81%|89%|95%|100%|91%\nPrice/Perf 2160p|95%|86%|90%|78%|89%|95%|100%|95%\nPrice/Perf RayTracing|80%|71%|74%|82%|92%|98%|100%|94%\nofficial TDP|132W|160W|180W|130W|170W|200W|225W|225W\nIdle Draw|4W|5W|~5W|9W|13W|10W|40W|46W\nGaming Draw|131W|159W|177W|129W|172W|202W|208W|223W\nEfficiency 1440p|140%|135%|127%|112%|117%|131%|100%|102%\n\n&nbsp;\n\nSource: [3DCenter.org](https://www.3dcenter.org/artikel/launch-analyse-intel-arc-a750-a770)",
    "comments": [
      "Tbh Props to Intel for making a card better than a 3060 as their first gpu",
      "Last gamers Nexus benchmarks the A770 was right behind the 3070 in some cases. So once Intel fixes the driver issues I  really want to see how it shines",
      "Better seems like too much of a blanket statement, especially with the long list of caveats for the A770.",
      "These cards were manufactured Q1 this year (based on the GN teardown video). Drivers have been worked on since then (if not earlier). And the reason these were delayed as much as they were was because of the drivers. I'm gonna go out on a limb here and say that \"a few months of driver work\" have a high chance of amounting to nothing. It could go either way. I am hopeful, but don't count on it.",
      "So all in all, A770 is just beating RTX 3060 and the RTX 3060 Ti smacks them both around.\n\nSounds about right to me. Hopefully they're able to get drivers better, but I don't have any hope for non-DX12 games.",
      "Getting it into the hands of users is the key to making \"game-ready\" driver updates.\n\nSome things will not fix the issues with pre-DX12 games/engines. However, I will at least give Intel props on this - they've been clear they're looking forward with this platform. \n\nDoes that hurt adoption rates in the short term? Yes. But Intel has been pretty clear that these cards aren't for everyone, but that the development of the platform and the drivers is a forward-looking project.",
      "With a few months of driver work it'll FineWine(tm). In some games it's almost 3070 levels and in 1 or 2 compute tests it was hitting 3080 levels.\n\n16GB variant could be an ML monster for the price.",
      "Even if the drivers get sorted, I don't think that solves the old titles issues? I know they are emulating directx9, so I imagine that will take more than driver optimizations to sort out. Next intel cards might be out before that is fixed.",
      "Holy shit, the idle draw. I've missed that part up until now. That's a no thank you from me. That and a bit too much power draw in general.",
      "Yeah [just look at what 2 years did for their DX11 driver!](https://media.discordapp.net/attachments/682674504878522386/999402021474009198/unknown.png?width=1595&height=897)\n\nOh wait.",
      "same, I noticed it recently too. I thought it's just inefficient at gaming and for casual use it would be ok. Can't buy with this idle / multimonitor draw. If you're European then when long term running costs are considered, it's straight up 6700xt/3070 price point competitor.",
      "True. Maybe 50% fps disadvantage for Arc A700 on older games. But still way over 100 fps.",
      "I don't necessarily think it's locking out older gamers. A decent-spec modern PC with a higher-end Arc card should hit 150+ FPS in CS:GO. Keeping in mind most people also don't run a monitor with a refresh rate higher than 144Hz, I don't think this will make the card completely out of reach for budget/mid-range gamers.",
      "Well now they're under market pressure with a real release out and \"many eyes\" reporting bugs.\n\nTake a look at how stable and fast they are under the open source linux drivers, for example. That's the driver where some tests were hitting 3080 levels.",
      "Valve's own Proton compatibility layer operates in similar fashion to whatever Intel is using, and Proton is sometimes even capable of out-performing native support. I'm 100% confident Intel can make improvements, it just takes time.",
      "As much as I want Intel to succeed in the GPU market, and as much as the feature suite is extremely compelling and fully competitive with Nvidia, there’s really no reason to buy either of these cards when the RX 6600, RX 6600 XT and RX 6650 XT are all such amazing $200-300 options.",
      "Right, and I think it was Steve (GN) that said alot of the older games where it falls behind, it's still plenty fps avaliable.",
      "from the same page no, 6750xt jumps from 7w to 39w (6700xt 33w). it's recent chip so recent drivers too. yet still below arc. video playback is 20w for 6750xt, while arc is at 50w...\n\nhttps://www.techpowerup.com/review/intel-arc-a770/38.html https://www.techpowerup.com/review/asus-radeon-rx-6750-xt-strix-oc/35.html",
      "Right, I am i Europe. So I'd much rather run an undervolted 6700xt for efficiency.",
      "Both Ryzen 1000+ and 8th gen Intel support Rebar. Alder Lake represents a fair leap over ol Sky Lake, and each Ryzen generation improved quite significantly."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "I love Acer's Intel Arc A770, but its driver updates are killing me",
    "selftext": "",
    "comments": [
      "It seems like most of these are related to HDMI (which is explained in the article). I use the DisplayPort and the Intel website  drivers and have had none of the issues reported here.",
      "That would make sense, as ARC's HDMI support isn't quite native.",
      "quasi blower cooler for people with restrictive airflow; like SFF users; so at least half the gpu's heat is getting immediately kicked out of the case. Open case coolers in smaller cases can over heat and/or cause the cpu and other parts to run hot too.",
      "How about a DP to HDMI adapter?",
      "It sounds like this isn't Acer-specific",
      "What's the deal with the double fan situation?",
      "more or less the same idea as the founders edition cards from nvidia",
      "It does native HDMI2.0 but 2.1 uses PCON.",
      "I imagine that's fine",
      "What's even the point to get arc anymore when you can get a Radeon that has higher performance for cheaper?",
      "oh, damn, I bought the Acer version and I'm using LG OLED TV, which is HDMI exclusive, double damn.",
      "I am agree, the software package, incl. updates... need some work, and I have the Intel LE. E.g. the best way to get updates for me are the posts here on reddit.\n\nSerious Intel?",
      "This is why I gave up on Acer years ago, much better vendors out there besides them.",
      "That's why I returned my A770, beautiful looking but the drivers are a mess.",
      "Full disclosure, I tried, and Intel branded arc a770, a year ago. It ran great for a day. Then I upped the refresh a little, instantly got the most horrifyingly loud coil wine I’ve ever heard and it never powered on again after that.\n\nHowever, it is common knowledge in PC hardware from the top experts that Intel has made astounding, leaps and bounds in their drivers in the shortest amount of time in history, including team, green and red. Credit or credit is due. They are doing an amazing job.",
      "Then why do you love it?\nJust get a GPU with proper drivers. \nA GPU isn’t something you want to beta test."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "intel arc a770 le everything runs fine out of the box with i3",
    "selftext": "",
    "comments": [
      "Welcome to team blue!\n\nBe sure to visit us at r/IntelArc",
      "What demon possessed you to still be running Windows 8?\n\nEDIT: I should’ve kept scrolling to the CCleaner screenshot before leaving a comment. I stopped at the benchmark lol",
      "Seems like almost a side grade from a 6600xt",
      "Its okay since it blows in wrong direction lol",
      "Ah that makes sense if you have a use case that overwhelms the memory bus.What made you go 770 over say a 6700xt",
      "Looks like the u9s chromax 90mm tower cooler. Definitely far more then the i3 needs but it's pretty small",
      "it is not because 128bit bus was not good enough for armored warfare game which I play a lot",
      "10th as you can see on ccleaner screenshot",
      "I don't know \njust look at it :)\n6700xt was available for a long time and I have upgraded from 5700xt to 3060ti strix oc on my main rig so for this second one, used for traveling I wanted something I can tinker with and learn something new too",
      "this case was used for several builds in my life as it is compact and not just plastic. \n\ngt740\ngtx950\ngtx1050ti\n1060 6Gb\nrx580\ngtx970\n5500xt\n6600xt\na770\n\ncurrently since 5500xt specs:\nZalman T5 case \nmsi b460m mortar wifi  \nall noctua blackout fans 80mm-120mm\ntrident z 3200Mhz cl 16 actually running at 2666Mhz cl 15 1T\ncorsair 650W modular psu \nsamsung 980pro 250Gb\nwin10 64bit pro\n\nupdated bios \nactivated resizable bar \ndownloaded driver\ninstalled gpu\ninstalled driver \nall is running without issues so far even games I'm playing, so hoping for good fps in upcomming games compared to 3070 with only 8Gb vram",
      "Ah that's fair. I always enjoy tinkering with things to a fault sometimes.",
      "zalman t5",
      "I have always wanted to see how space engineers runs on arc cards",
      "it is strange but ryzen 5 3600 with 5500xt was running better than i3 10100 with 6600xt and this a770 is a surprise I'd say. Transition from 1050ti to 6600xt wih DDU was not without issues and now from integrated graphics it is flawless so far, at least for my use case",
      "What case is this? I really like the side panel",
      "Excited to see what the future holds for intel GPUs! With how shtty nvidia and AMD have been with pricing I would love to see Intel give a highend GPU for a cheaper price and since my CPU is already intel I'd love to have an intel GPU too and go full on TEAM BLUE!",
      "it is due to the clearance and max temp I have ever seen was 56C",
      "oh ok",
      "This is my ideal scenario to try out the new tech if the budget allows - keeping a secondary system to try out new hardware. \n\nI have another system that’s capable and currently running integrated graphics, though sadly I have not the budget now to buy another GPU.",
      "1-2C"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Should I buy an Arc a770 ($350) or wait for Battlemage?",
    "selftext": "Hey all!\n\nI'm planning on building my first pc, but have a bit of a conflict. I want to build an all Intel build, but don't know if I should pull the trigger now on an Arc a770 LE for $350 dollars, or wait for Battlemage to come out next year.\n\nI don't need a crazy build as I don't play games all that much, and would like to keep it under a thousand dollars.\n\nThanks for the feedback!",
    "comments": [
      "Honestly, I wouldn't buy ARC for above $300 anymore. You'd be better off spending $329 on a Radeon 6700XT.",
      "why not settle for A750, you get 92% of the performance at 1080p",
      "Looks like they are all up at like $350 right now. We are starting to enter a dead zone where RDNA2 that was on clearance is drying up ahead of RDNA3. Even RX 6600 has backed off the really nice $180 price point.\n\nRX 6700 is still $280. It's a 25% increase in price to get to $350 for something like 10% performance increase. If you're buying today then RX 6700 may be the move.",
      "You want a $350 GPU in a $1000 build?\n\nJust wait for Battlemage if you insist on all-Intel, Alchemist just doesn't have the raw performance.",
      "Yeah I put together a sub $1000 build with the a770. Hadn’t the Arc series improved by a lot?",
      "Even at $350 a 6700XT is a much better idea than ARC a770 IMO\n\nOr they could increase the budget and get a RTX 3070 ($380) or 4060ti ($399) but I wouldn't recommend getting an 8GB card in 2023 unless you're going to stick to 1080p",
      "He said he wants an \"all\" Intel build so Radeon anything is not Intel so he would not be doing what he set out to do. So that doesn't work with his goals.",
      "It'd probably have a better case if it also had a stronger GPU core. Right now the extra VRAM just a niche benefit for a handful of games at certain settings. In the future games will use more VRAM, but they'll also be more demanding computationally. The A770's performance in non-VRAM limited scenarios is around a 6600XT. That's not impressive for $350, especially when the 6700XT exists.",
      "Yeah it's basically A750 or bust for arc. And even then 6600 is an option at that price range.",
      "I'd imagine the a770 will last longer given it has double the vram",
      "Seems like a long time to wait if you don't have a graphics card to game on already. A750 hit $180 recently which seems like a good deal but A770 has held its price while RDNA2 has come down a lot while RDNA3 is rolling out. Comparatively speaking that makes A770 kind of a bad deal at the moment unless you want a creator card with high VRAM at an affordable price.",
      "Then he's waiting for Battlemage lol",
      "The A770 performs like a 6600 XT.",
      "Honestly depends if you want ray tracing or not. I went from a 6700XT to an A770. DX 9/10/11 games are worse a little. DX12 is slightly faster. RT is a lot faster.\n\nNote that my 6700XT ran at 2800MHz core / 2050 VRAM+if and the A770LE runs at 2750MHz @300W so not settings on either.",
      "Now it’s more like a 7600 or 4060, so maybe like 8% faster than the 6600XT",
      "I don't wanna be rude or anything but you're completely uninformed on Intel Arc 😅\n\nA750 is on par with 3060, even performs better in new titles. A770 is on par with 3060 ti, same story. In terms of compute power both cards are a lot stronger than their counterparts. XeSS is pretty good. In terms of price/performance, A750 is on top of GPU value list. They release new drivers almost weekly and their driver support is unheard of in 30 years of discrete graphics cards industry. \n\nNow those are facts, let's skip to the possibilities. Both A750 and A770 are still not at optimum performance. What does this mean? Intel created a hybrid architecture and even they themselves don't know how to unlock the full potential of it, yet. They're however on the right path, I'm expecting some kind of eureka moment from drivers team, the hardware is simply a beast on Arc.",
      "I personally intend to wait for Battlemage, will use the RTX 2070 I have until then.  \n\n\nAnyone have a reliable source for the latest Battlemage news that they recommend btw?",
      "I heard Battlemage may have software and corresponding hardware to run psth tracing better than amd or nvidia; but do your own research on that one.\n\nAlso im certain you can do better than 350 ive seen them go for 300 a while ago without even looking",
      "I had an Arc A770 for like half a year I think (swapped back to nvidia now); I would not reslly recommend it for your first pc\n\nThere were many issues that were somewhat difficult to diagnose/solve and I am pretty experienced with PC building; those issues have generally lessened with driver updates, but they were not all gone when I swapped GPUs",
      "unless you are collecting them, other wise stay away arc for now. \nI have a a770, it is a great toy, not good if you use it on main pc."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "ARC A770 v RTX 3060",
    "selftext": "At this current timeline, which GPU should I go? \nI am liking the fact that arc is 16gb ensuring future safety. But also so the fact that it requires driver update regularly. \nWhere else RTX drivers are more advanced and going on for a long time. But is 12gb. \nI have seen both are at similar levels, which one should I go for?",
    "comments": [
      "In the long and short term, the RTX 3060 12 GB card will out compete the ARC Alchemist A770 16 GB card for stability and overall quality of gaming, because the first generation discrete Intel video cards have a design flaw that will randomly crash the system and there is no indication when the event will happen.\n\nThe Intel card will have a significant advantage if you compress and decompress videos which is a better quality product that saves business money, and the hardware has at least 16 GB of memory that does better in artificial intelligence programs.\n\nIt depends on your use case as to whether your top priority of use will determine if you can be happy with the hardware you select. I would likely use the RTX 3060 12 GB for gaming while using another computer using the ARC A770 to stream and create videos for the game session. This approach makes the best assets of each video card highlight the advantages of both. The video cards do not exactly occupy the same purpose or use case segment of the market.",
      "100% 3060",
      "RX 7600 8GB vs RTX 3060 12GB - Test in 10 Games [https://www.youtube.com/watch?v=JsvSRSmoi4M](https://www.youtube.com/watch?v=JsvSRSmoi4M)\n\nAccording to steam, the top 9 most popular GPUs currently are all GPUs offering 8GB models. So the time when 8GB is not enough for 1080p gaming is not yet for the next 4.5 years.\n\nI recommend these four RX 7600 models for 1080p gaming.\n\nASUS Dual Radeon RX 7600 OC Edition 8GB GDDR6 V2  \nPowercolor Fighter AMD Radeon RX 7600 8GB GDDR6  \nXFX Speedster QICK 308 AMD Radeon RX 7600 Black Edition  \nPowercolor Hellhound AMD Radeon RX 7600 8GB GDDR6",
      "Finding ARC to be lucrative, but not trust worthy. I’m planing for gaming mostly",
      "If you play solo games and games that can accomplish goals in less than 15 minutes without upsetting people you play with if your game stutters or crash, then you are okay with the first generation ARC cards for gaming. You can use the ARC cards for large games, but the games need to be primarily one person games and linear plot stories, so, that anything wrong with your particular gaming session does not interrupt other gamers experience.\n\nYou will not have a problem using an ARC A770 for games such as Borderlands single player mode, but you are gambling that the game won't crash if you are in a boss mode fight. (Usually this won't happen, because if any stalls happen it's in a normal situation run and it won't feel bad at all.)"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "My Intel Arc A770 build !",
    "selftext": "",
    "comments": [
      "Hey guys ! \n\nJust wanted to showcase my new build, featuring a A770 \n\nThe complete build : \n\n* NZXT H510 Flow\n* Ryzen 7 5700x\n* Intel arc A770\n* 2x 16 Go DDR4 Corsair VENGEANCE RGB PRO SL 3600MHz\n* NZXT Kraken Z53\n* 1To Crucial P3 NVMe M.2 SSD\n* Cooler Master MWE 850w Gold V2\n* Be Quiet MC1 M.2 SSD Cooler Heatsink",
      "I think this is the first time I've seen anyone pairing an Arc with a Ryzen CPU.",
      "It's not proprietary. It's part of the PCIe spec.\n\nhttps://www.intel.com/content/www/us/en/support/articles/000090831/graphics.html",
      "AMD has rebar after zen 2",
      "This is so incredibly wrong.",
      "That looks clean AF. Love it! Hope you enjoy the Arc!",
      "It’s cool how you routed your AIO tubes behind your 24 pin. Seen thousands of PCs on here, YouTube, etc and never once seen that done. 👍🏻",
      "Nice, my new build also has one.\n\nCase: Lian Li Q58\n\nPSU: Corsair SF750\n\nMobo: MSI Z690I Unify\n\nCPU: Intel 13700KF\n\nCooler: MSI S280 AIO\n\nGPU: Intel Arc A770\n\nRam: Corsair 6600MHz DDR5\n\nSSD: Corsair 2TB",
      "As long as the tubes are above the pump, it's ok, no problem there.\nJaystwocents and Gamernexus explained that very well on their channel on youtube.",
      "Not propriety at all.",
      "Hopefully the gpu doesnt get too dusty. Heard they are a pain to take apart. Really sick build though",
      "Great! How's the GPU treating you so far?",
      "What GIF is on your cooler? I need it if you don't mind",
      "Hi, what mobo are you using? You forgot to include it in the list.",
      "Hey I'm interested in this build. How much did it cost and where did you buy the parts?",
      "yea man \\\\m/ here is mine: https://preview.redd.it/dfmxwrla8w8a1.jpg?width=1536&format=pjpg&auto=webp&s=229b1a0110c1b832997947e02c84699cc13ec31c",
      "Why is it every time this is mentioned, people completely miss the takeaway point of the GN video?",
      "I've been going through some benchmarks and to my surprise arc works better with ryzen than with intel cpu",
      "this is 22 days late but thats the NZXT N7 B550",
      "Very nice build, but here is a tip: the AIO liquid cooling tubes need to be on the bottom of the cooler if mounted on the side of the case. It will improve the acoustics and longevity of your cooler. Here is explanation from AIO manufacturer: [Corsair](https://help.corsair.com/hc/en-us/articles/360049358271-How-should-I-mount-the-radiator-of-my-AIO-cooler-)"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "ASRock launches Arc A770/A750 Challenger SE graphics cards",
    "selftext": "",
    "comments": [
      "article says they will be available in japan on the 12th. Other places \"mid july\" so yeah, wouldn't expect them to be available anywhere right now.",
      "doesn't look like it. The source for it posted it on the 5th, and asrock will be shipping the cards starting this month.",
      "yeah, these are new it seems. It makes sense to think it was old though since a770 and a750 are old hat at this point. The article starts out with \"with no battlemage in sight...\". It doesn't sound good to me to have the next generation nowhere to be found and for partners to be launching new versions of the existing stuff this late in the stage.",
      "Can’t be found anywhere, except all the articles talking about the performance gains. They probably bought left over chips for a discount and released some budget friendly cards.",
      "Oh, i probably confused it with some other ASRock arc cards",
      "Isn't this old news?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "GUNNIR launches Arc A770/750 Photon \"Elden Ring\" special edition GPU - VideoCardz.com",
    "selftext": "",
    "comments": [
      "Underpowered, sure. But overpriced is wrong. They have very competitive pricing and for the money the performance is actually pretty good.",
      "man this looks so cool",
      "Competitive is a strong word. Over on newegg there's fewer A750 models for sale than there are 6600s, and the most of them are more expensive.",
      "On the eve of Alchemist's replacement it's barely moving the needle in terms of price to performance while still being less reliable than AMD. An a750 for $200, a 6650XT for $220.\n\nCompetitive is a strong ass word. All I can say is I hope Battlemage is a stronger showing.",
      "The A750 is closer to the 6600XT than the 6600.",
      "GUNNIR cards look cool, if only they weren't underpowered and overpriced intel alchemist cards I would have liked to buy one."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "[ARC] Is ASRock a Good Brand for Intel Arc GPUs?",
    "selftext": "Hello! i've recently been looking for an upgrade for my 1060 3GB, posted multiple posts in different subreddits, looked at RTX 3000 series, 4000 series, RX 6000 series, 7000 series, but then i found an A770 which had a pretty nice price.\n\nThough the main question is - is ASRock a good brand for Intel Arc? since the A770 i found is an ASROCK Intel Arc A770 Phantom Gaming OC Edition, and i'm not very familiar with ASROCK or Intel Arc for that matter.\n\nWhat are your opinions regarding ASRock? is it fine? Thanks in advance!\n\nNote : I have a 12th Gen Core i3 if that matters at all, probably does since Arc ReBar is a beast",
    "comments": [
      "I've had ASRock motherboards in the past and they've been fine. I'd consider them a mid-tier board partner. No frills, budget focused, but decent quality. I haven't heard too many complaints about their GPUs.\n\nThat said, just be aware that the Phantom Gaming A770 only has 8GB of VRAM, while the reference card and the Acer BiFrost have 16GB. I'm not sure how much that will realistically impact performance. Especially if you're playing at 1080p or lower. But a lot of recent games have been getting bad press for high VRAM requirements. The big draw to the A770 is that it is an affordable 16GB/1440p card. So it's an odd thing to cheap out on. Especially considering how overkill the cooler is that ASRock is using. And without the deal Newegg is running right now, it's not that much cheaper than the 16GB models.",
      "I have two of the A380s from them, for about 3 months now with no issues. ASRock in general is a budget brand but seems in the past few years have upped their quality.",
      "It's funny because ASRock was the cheapy-brand that spun off of ASUS.",
      "ASRock is a good brand. I've used their motherboards exclusively for the last 9 years and never had a single failure. Even had a cat piss on one and it survived.",
      "With EVGA leaving multiple markets, ASRock and ASRock Rack have become my new favorite brands.\n\nA380\n\nDeskmeet\n\n5405m low power boards\n\nI've been very happy with all of it",
      "It's OK.",
      "\"OK\" is enough for me, especially since the FE version of that card isn't even available here, thanks!",
      "Which benchmarks are you basing this on? It’s been incredibly hard to find good review data for Arc since its performance is so volatile, but I haven’t found any reviews suggesting that the A770 is anywhere close to the RX 6700 over any large sample of games.",
      "Cat: Also mine.",
      "If AV1 isn't a must, the 6650XT makes the most sense out of all of those (compared to the A770, it offers a bit more performance at 1080p, similar but slightly less at 1440p, but better drivers), unless you want the 2GB extra vram the 6700 offers, although performance increase arguably isn't enough to justify that 27% higher price.\n\nA750 is *kind of* an option, but arguably not with the 6650XT so close in price, unless you want AV1.",
      "No issues on my ASRock A750 besides terrible idle power consumption @ ~35-40w with 2 monitors plugged in with the \"fix\", but that's an Arc thing.\n\nAlthough the cooler on my A750 was larger than I was expecting coming from an RX 570 (243mm for my RX 570 vs 271mm on my A750), I didn't even think of checking the size, but my case was large enough for it.\n\nI'd only suggest Arc if you're fine being a Beta tester, that's what it's going to feel like, March drivers were mostly fine (Ignoring performance related issues, the only thing that was noticeable was some weird flickering on youtube videos when pausing and hovering over them), after that I've been having issues, current beta drivers (31.0.101.4514) half the time I can't use quicksync with OBS (Sometimes I can get it back by restarting drivers a few times) and it's completely gone in HandBrake, previous driver I was having random flickering, that was getting pretty annoying.\n\nWith all that said, I'd not suggest the 8GB A770, if you're in the US it's 28.6% more expensive ($270 vs $210) for, iirc, [~6% more performance](https://youtu.be/xUUMUGvTffs?t=578), and you aren't even getting more vram, if you're gonna get an A770 it should be the 16GB version since it's just not enough of an improvement over the A750 for the price.\n\nIf you're gonna spend $270, you could get the 6700 non-xt, it's faster (At least according to older drivers, but there hasn't been a huge change in performance overall, usually just a handful of games at a time), better drivers, and 2GB more vram, the only downside is that it doesn't have AV1.\n\nAlternatively, the 6650XT is also an option, same 8GB of vram, but faster (At least for 1080p, [it's *barely* slower for 1440p](https://youtu.be/xUUMUGvTffs?t=598)) and better drivers, however it's cheaper, although it lacks AV1, just like the 6700 non-xt.\n\nIf you want AV1, the 7600 is an option, it's a few percent faster than the 6650XT, although it does cost ~12% more comparing the cheapest of each, which makes it pretty bad value for around this price range.",
      "I have an ASRock z690 Mobo, ASRock phantom gaming monitor and ASRock 6750xt. They all work great!",
      "My old computer that I built in 2014 with an ASRock Z97 OC Formula motherboard was still going strong when I built my new computer like 2-3 weeks ago. It was pretty expensive cuz it's made for overclocking, which I don't even do, but I figured the capacitors and everything were higher quality since it's made for overclocking.",
      "There was a time I would have avoided them like the plague, but they've been stellar in my books for a while now. I'm really impressed with their progression in quality, particularly while some other notable brands *cough* ASUS *cough* seem to be back sliding so hard.",
      "Those deskmeets and deskminis are a hoot to play with.",
      "I'm from slovakia, the A750 is about 250 euros, the 6650XT is 260 euros, the 7600 is 300 euros, the 770 8GB is 310 euros, and the 6700 is 330 euros ( and the 770 16GB is like 400 euros )",
      "I wasn't there when it happened but from what I heard the cat pissed on the top of the vented case and onto the board (and video card but that was thankfully only on the backplate). Let the board sit for a while and used some contact cleaner and it worked as if nothing happened.",
      "There's a few redditors that design custom 3D printed chassis for Deskmeet and Deskminis. I'm sure they'd be more than happy to design one with active cooling. \n\n[I use this for my Deskmini X300.](https://www.reddit.com/r/ASRock/comments/yy3i33/3d_printed_alternative_chassis_for_asrock/) The guy uploaded the STLs online for free.",
      "Is 6700XT significantly more expensive? I second that 6650xt looks best of all for the price here so far. Is 6600XT much cheaper tho? :-)",
      "6700XT is about 360-370 but I cannot go above 330, that's the highest I can stretch my original budget of 200€\n\n6600XT isn't available, the 4 choices are:\n\nASUS Rog Strix RX 6650XT OC Edition V2 - 262€\n\nSapphire RX 7600 Pulse OC Edition - 290-299€\n\nXFX Speedster SWFT309 RX 6700 Core - 332€\n\nASRock Intel Arc A770 Phantom Gaming 8GB OC Edition - 316€"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Should You Buy an Intel Arc A770?",
    "selftext": "Includes extensive benchmarks at 1080p and 1440p with the latest drivers.",
    "comments": [
      "What does that mean? This is a discrete gpu.",
      "Given the price, Intel for sure unless I really need day one drivers or CUDA for work.",
      "At this point unless you absolutely need a cheap card and are willing to test the waters it's not worth getting an Arc card really. Battlemage is supposed to launch in a few months, it is targeting 4070S~4070Ti performance and should come with decent amount of memory and affordable price. It's a no brainer if you want to go Intel. A750 is a great budget card, but it launched too late to be worth it, pretty much every halfway decent Battlemage GPU is gonna smoke it.",
      "That’s a reasonable assessment however I am happy to confirm that the drivers work well now, so you are no longer testing the waters. The A770 is a good card, just too little, too late for this round.",
      "They are getting way better about day one support for new popular games.",
      "Rule 5: AyyMD-style content & memes are not allowed. Please visit /r/AyyMD, or it's Intel counterpart - /r/Intelmao - for memes. This includes comments like \"mUh gAeMiNg kInG\"",
      "I was mostly thinking about the A750, apparently there are some minor issues in a few games that occur only when using an A750, the same games run flawlessly on the A770 and the other Arc cards. Maybe it's because this card came out later and Intel didn't have enough time to fix its drivers. \n\n\nEither way the sensible thing right now is to wait for Battlemage or at least get an A770, in my country the price difference between the A750 and A770 is relatively small and it is definitely worth the extra money.",
      "Which rumors are you talking about? I hope it's not from MLID or RGT because these guys are clueless clowns.",
      "odd\n\neverything I saw online was showing Battlemage would be a Q3/Q4 launch",
      "There’s no reason battlemage will release in 2024, all the rumors are showing us another late arrival that will be eaten by  NVIDIA Blackwell",
      "From the rumor mill in general all intel is doing is elaborating on their \"labor of love\" which means don't get your hopes up. Im not saying its been cancelled or even delayed but if you read between the lines Nvidia has shown they are prepared to release Blackwell (2024) and intel has said they are still working on it (battlemage) whatever that means.\n\n&#x200B;\n\nEDIT: FYI intel has still not shown us their Frame Gen technology since it was announced, I would expect to hear more about this first before we see battlemage."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "a770"
    ],
    "title": "Intel Arc A380, A750 and A770 8GB GPUs price slashed, A380 now listed for $120 - VideoCardz.com",
    "selftext": "",
    "comments": [
      "Man, I realy wish to have your prices in Europe",
      "Never buy a promise. The A380 *might* be better than that by now, but there’s no 2023 review data for it, so we really don’t know. At any rate, it has a long-ass way to go before it hits GTX 1650 Super performance, which is really the minimum you should get for this price in 2023.",
      "The A380 still isn’t even remotely cheap enough, considering that it loses to a GTX 1650. That class of product should really be, like, $70 by now.",
      "Wish there was a arc a380 low profile",
      "Cheapest gtx 1650 on newegg is $170 and amazon for $160. A380 at $120 is not bad at all, especially with more RAM, AI cores, and a way better encoder with AV1 support.",
      "It’s nonsensical to blame increasing tech prices on inflation when tech markets have found ways to exponentially improve price/performance regardless of inflation all throughout their history. The A380 is at best *identical* value to a GTX 1650 Super from four and a half years ago. That’s abysmal.",
      "So far this is true, but nobody knows what tommorow will bring.",
      "Uk prices have been comparable with the cheapest European markets (ie Germany & Netherlands) for years  and they still are. Nowt to do with Brexit. The relative strength of the dollar due to their interest rate policies are why prices are higher in europe, including the UK.",
      "meanwhile in brexit britain A770 16gb acer bifrost is gbp 427 = 484 euro = 518 usd .....  or could just get an asus strix oc RX6750 instead.",
      "Brilliant!",
      "will only buy it under $50",
      "Let’s just remember that the GTX 1650 Super launched for $160 *four years ago*, and outperformed the A380 by ~35%. That’s the only benchmark worth comparing new budget cards to, because everything else that’s available in that segment right now is trash.\n\nI’m in full agreement that the A380 might well be the relatively best option in the hellhole that is the post-2020 sub-$200 market, but when compared to the market we should have, it’s a detestable waste of sand. Sub-$200 price/performance hasn’t improved in *four and a half years*.",
      "And if they dont? It's also perfectly possible performance could go down with more stable drivers.",
      "yes indeed. even worse when you compare it to the rx 480 which launced for around €200 7 years ago, and still should be around 25% faster on average.  \nthe rx 400 and rx 500 series gpu's also where insanely good at raw performance/compute tasks compared to other gpus. since while in gaming the difference is only around 25% back then a single rx 480 could easily beat and sometimes even double the performance of a gtx 1080 in cad software and similar compute heavy things that wheren't optimized for a speciffic set of hardware and instead relied on raw performance.  \n\n\nso actually the last 7 years there hasn't really been much advancement in gpu's in some cases the ai or raytracing can be usefull however. but ofcource we have to see how well it works on low end cards, since if it works bad then the raw performance of the old 480 might still manage to beat it in such things.",
      "I mean, Alchemist seems similarly compute-heavy, but point taken.",
      "perhaps it is indeed, I didn't yet see as many benchmarks from it outside of gaming and don't own one right now.  \nif that fully is the case, there might actually be a lot of improvement in price per performance next gen, or alchemist gpu's mught be capable of much more performance(probably won't really see that for most people, but some might experience it).  \nit makes sense since typically raytracing cores can be used quite much like cuda on nvidia, so they are typically capable of quite some raw performance.  \n\n\nso sad about performance per price not going up, but intell arc indeed seems like quite much a good trend in the gpu market, since they push the prices less insanely high. perhaps next gen or such might be a lot cheaper per performance since after all this was the first gen, and so it likely had by far more reasearch and producion cost into it, due to much more reasearc being needed for a completely new line of products, also early on optimizations for reducing cost are also limited. so I by far am more angry at amd and nvidia now.\n\nopenly I hope that Intell actually uses this, since amd and nvidia increased prices so insanely much that even their first gen was a quite good or the best competor on the market despite the pricing being as high as  7 year old gpu which performed the same. this could reduce the amount of money loss on the first gen or possibly even generate a lot of proffit and name loyality so also driver support, which might make a next generation much better in price for performance. I hope.",
      "Yeah, I think Arc has a really high ceiling in terms of raw compute performance, but I doubt it’ll ever get particularly close to that ceiling in games. It feels like a Polaris-type architecture, with more features.",
      "Your arguments make no sense, even not taking features into account. And zero goalposts were moved except by yourself.",
      "You manifestly could get something equal to or better than an A380 in… hm, probably early 2020 for $100. Also, GTX 1650 Super, for $160, 35% faster than the A380, available everywhere. We still haven’t beat that, you know, and cheap cards are supposed to be *better* value than expensive ones.",
      "And if the drivers improve?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "ARC A770 performing worse than an RX570 on the same system",
    "selftext": "First, I acknowledge my system is very old and the processor is weak. I bought the A770 LE some months ago just because I really wanted it (it was being discontinued) and because I already planned a system upgrade by the end of the year.\n\nMy gaming rig comprise of a Xeon E5 1620v2 (slightly overclocked at 4.4 GHz via base clock), 16 GB of DDR3 ram running in quad channel at 1866 MHz and, now, an ARC A770 LE.\n\nBefore the Arc I had an RX570 4GB.\n\nWhat I noticed is that the performance of the ARC gpu is much worse than the one of the RX570, specifically in Overwatch: while with the old RX570 I was able to lock on 144 fps without issues, with the A770 I see fps as low as 80 in some scenarios (usually when there are multiple characters on screen).\n\nAlso the usage of the CPU stays at around 50-60%, while for the GPU is 40-50%.\n\nI really cannot think of an explanation for it: why would a much more powerful GPU perform worse on the same exact system and settings? Could it be a DX11 driver optimization issue? (in Cyberpunk, that is DX12, the ARC is performing as expected).\n\nI was expecting to have at least the same performance while waiting to build the new system...\n\n**PS: thanks to the ReBarUefi project, I have Resizable BAR active.**",
    "comments": [
      "Intel themselves said that running ARC on systems that don't support resizable bar is not recommended and will not give optimal performance.",
      "Try DXVK wrappers for games that dont work.\n\n[https://www.youtube.com/watch?v=wktbj1dBPFY](https://www.youtube.com/watch?v=wktbj1dBPFY)\n\nAnd look at this video maybe it will help you if DXVK wont work.\n\n[https://www.youtube.com/watch?v=hUmwhLMdLk4&list=WL&index=5](https://www.youtube.com/watch?v=hUmwhLMdLk4&list=WL&index=5)  \n\n\nAnd i hope you uninstall old AMD drivers using DDU in safemode.",
      "Yeah, I'm not that brand-loyal either but sometime I'm a bit of a \"collector\": I bought the ARC A770 Legendary Edition mostly because it was an interesting piece of tech and it was getting discontinued. xD  \nObviously it had the kind of performance and functionalities I was looking for but maybe I could've gotten a better deal overall by going for another brand (see performance issues I'm having).\n\nStill happy to have it tho.\n\nAs for Unreal, I've tried making a game many times in the past but I always stop at the point where I start to need some 3d assets and other content to continue: I'm no 3d artist sadly.\n\nNowadays I just have some small projects I do for fun, the last one I started was to try and make an Hack'n Slash combat system on the style of Platinum Games (Metal Gear Rising, Bayonetta, Nier Automata, etc.)",
      "I have resizable BAR enabled tho..\n\nEvery PCIE 3.0 system has the hardware to support ReBAR/SAM, the only thing missing is the UEFI driver for it in the UEFI bios, luckly there's a project that allows users of old platforms like me to add said driver to the bios and have ReBAR working.",
      "Thanks! I didn't know you could use DXVK on windows too, I always thought it was mainly thought to run games on linux. I will give it a try, the only fear I have is if Overwatch detects the modified dlls as cheats...\n\nYes, I uninstalled the AMD drivers with DDU before putting the ARC in.",
      "I bought ARC A770 16G LE because my GTX 1080ti was dying, cant run games over 70% of core usage on it.\n\nI was really interested in first gen intel GPU, new oportunity to learn new things and try new things, i am computer enthusiast.\n\nI found and report few conflicts with drivers and software and now intel use them as advice on support :D.\n\nAt start i was not happy with the GPU, DXVK tweaking for ARC was in diapers, after comunity begin to trying it, it start to be much better.\n\nI learner so much things using ARC GPU and now i love it because it work on 70% of games that i tryed and it is fun to do tweaking with it, and the comunity is really interested and resourcefull.\n\nAnd of course it will be a piece of history and after i replace it with something more powerfull it will be my colectible, it is first gen, it looks nice and it is limited edition.\n\nI was hoping you were making games :D,  i am really interested on small project from independet developers or small studios it can be pixel art games or something more complicated, if it looks good and looks fun i am buying it :D",
      "Hm didn't know that, that's pretty cool. Honestly I've got no idea, hope you fix it.",
      "Borrow a PC from your friends for a weekend and try the GPU there.\n\nIf there's a problem with the GPU (for example it's overheating due to not sufficient die contact) then return it or RMA.\n\nIf it works then look at upgrading your PC.\n\nGo for an X3D CPU from AMD, or wait for the new desktop CPU from Intel next year because they're going to switch motherboard sockets again.",
      "I run ARC card before my new ryzen system on I7-9700k, i had Asus MB and they release BIOS that supports ReBar, it wasnt that great, it shows that is isnt supportet by Intel but it work at least.",
      "Do you know how to use them properly because not every game works, i know how to use basic DXVK but dont know how to implement async DXVK, it need setting file to work properly.",
      "Thanks man, hopefully using DXVK will improve the situation. In the end I just have to survive until I build a new system.",
      "I will only try basic DXVK since async-DXVK seem to have cause some bans in the past due to how it interacts with the game's rendering.\n\nI will probably make a throwaway account anyway just to be safe.",
      "What are you building in future ? :) just curious.",
      "Not have defined it yet really.\n\nIf I go for AMD I will probably choose the 5800x3d to save on RAM: I don't really see DDR5 giving an advantage that justifies the price difference right now.\n\nWith Intel I was looking at the i5 14600k.\n\nIt is true though that those are both pretty much dead platforms. If I went for a 7800x3d from AMD I would have a future proof system at the cost of paying more now.\n\nSo, yeah, I'm still weighting my options.\n\nFor sure I want at least 32GB of RAM cause 16GB are quite tight when I work with Unreal Engine. xD",
      "Looks like you did your research and know what you can afford and what you need.  \n\n\nI am not the tipe of person who will judge for brand choises, i build my systems what is best for the best price in that time for me.  \n\n\nWhat are you doing in Unreal Engine ? if you are making game then i am really interested :D",
      "How much faster is it relative to the 1080ti?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "True Limits of Intel ARC A770 16GB. Extreme Overclocking (XOC)",
    "selftext": "Introduction\n\nI recently acquired an Intel ARC A770 16 GB and I wanted to XOC it as it something that not many have done. I will be talking about my experience and results XOCing an Intel ARC A770. Any mods, software used, and software tricks are NOT endorsed by Intel. Use at your own risk if you want to reproduce anything described here.\n\n&#x200B;\n\nPreparation/Mods\n\nThe first step was of course prepping /modding the card. The first standby mod is to shunt mod the resistors from the power connectors to increase the power limit of the card. However, Intel used an interesting resistor. The resistor has a full metal shroud and is not possible to stack another resistor on it as it would short. I did not want to remove the resistor and add a different at the time, so I just left it as it. Instead, to unlock the power limit, I used the Acer Predator Bifrost trick and increased the limit to 400W. In testing, the card never needed to draw more power than that (this is due to the voltage limitation, more on this later).\n\nAnother mod is soldering cables to the I2C buses to control the voltage through an external controller such as the Elmor EVC. There two I2C interfacing on the A770 and they are on two separate buses. As a result, it required attaching two different sets of wires. The external voltage controller recognizes that there are MP2979 controllers, however when I tried to modify the values they did not appear to do anything. I assume that Intel has some sort of lock on it. If anyone has more information on it, write it in the comments.\n\nSince, nobody makes a LN2 pot or a plate adaptor for the ARC series graphics card; I had to manufacture my own using a 3D printer. Despite being plastic, the plate worked wonderfully and survived the experience to be used again in the future.\n\nFront of PCB: [https://i.imgur.com/O0F4y5T.jpg](https://i.imgur.com/O0F4y5T.jpg)\n\nBack of PCB: [https://i.imgur.com/XnBgLC1.jpg](https://i.imgur.com/XnBgLC1.jpg)\n\nLN2 Pot Mount: [https://i.imgur.com/c40ylPY.jpg](https://i.imgur.com/c40ylPY.jpg)\n\n&#x200B;\n\nARC Temperature Bug???\n\nWhen the A770 dropped below 0˚C, HWiNFO showed that the GPU core temperature reached a whopping 255˚C. This temperature would keep decreasing, despite the error, as the card kept getting cooler. So, if the Pot was at -38˚C, HWiNFO showed the GPU core 221˚C. It seems like there is some kind of overflow error and the firmware on the card does not know what to do if the card goes below 0˚C. I am not sure if this was Intel’s intension or a bug, I am leaning towards a bug though. If someone knows more write it in the comments. If it does end up being a bug, hopefully they can fix it in the future.\n\nTemperature Bug Below 0˚C: [https://i.imgur.com/ErQGYZU.jpg](https://i.imgur.com/ErQGYZU.jpg)\n\nTemperature of Card at -38.4˚C on Pot: [https://i.imgur.com/hykBwXb.jpg](https://i.imgur.com/hykBwXb.jpg) [https://i.imgur.com/0SfmHVM.jpg](https://i.imgur.com/0SfmHVM.jpg)\n\n&#x200B;\n\nFrequency VS. Temperature VS. Voltage\n\nAs many popular overclockers have stated in the past, the ARC series drops frequency as the voltage increases. However, there is another part that no one has seemed to mention yet. The A770 will also drop frequency based on the temperature. During the XOC session, if the card dropped below 8˚C, the frequency would drop 50MHz and this trend would continue as the temperature decreases. When I set the desired frequency to 2900MHz and cooled the pot to -70˚C, the frequency dropped to 2200MHz. Another interesting relationship is that if the voltage was dropped and the temperature kept the same, the frequency would increase again. But the card would fail to run any benchmark due to low voltage for the specified frequency.\n\n&#x200B;\n\nSetting a Record\n\nWith the power limit of the card unlocked and using ARC OC TOOL (NOT AN INTEL APPROVED SOFTWARE), the frequency was fixed to 2880MHz and the voltage was fixed to 0.941V, which is approximately 1.010V actual. Any more voltage and the card would down clock the frequency. Also, due to the Frequency VS. Temperature behavior, the temperature had to be controlled so that the GPU core would not go below 8˚C to maximize the frequency. It took a few tries to make it through with these settings; however, it eventually did make it through a Port Royal run. The final Port Royal score was 8,277 with a 2,845MHz, which is a world record for this benchmark and card combination. Unfortunately, this is all I could achieve without the ability to add more voltage, cool it more, and no ability to overclock the memory on the card.\n\nRecord Link: [https://www.3dmark.com/pr/2334224](https://www.3dmark.com/pr/2334224)\n\n&#x200B;\n\nThoughts and Conclusions\n\nThis was a fun and interesting experiment XOCing an Intel ARC A770. I wanted to write about it for a couple reasons:\n\n1. To see if the “temperature bug” is a bug or by design.\n2. To give some exposure to XOCing an Intel graphics card and hopefully the next series of cards they will be more flexible on what can be controlled and modified.\n\nIn conclusion, I hope that Intel allows more flexibility in XOCing their graphics card in future like they currently do with their processors. Thank you all for reading!\n\n&#x200B;\n\nSession Pictures:\n\n[https://i.imgur.com/a6jbnxI.jpg](https://i.imgur.com/a6jbnxI.jpg)\n\n[https://i.imgur.com/Uhqg0NO.jpg](https://i.imgur.com/Uhqg0NO.jpg)  \n \n\nThe system used for testing:\n\nCPU: Intel i5-12400\n\nRAM: DDR5 32GB 6000MHz\n\nMobo: Asus ROG Strix Z690-F\n\nGPU: Intel Arc A770 16GB\n\nGPU Driver: 31.0.101.4314\n\nGPU Pot: KINGPIN Cooling TEK-9 ICON EXTREME V5\n\nOS: Windows 10 22H2",
    "comments": [
      "That is a really cool idea. Congrats on achieving a WR! That 255° C temperature bug is hilarious though, lol. Does something like that happen when XOC'ing Nvidia or AMD GPUs?\n\nIn any way, I hope Battlemage turns out amazing - Intel could dominate the budget segment with cheap CPU+GPU combos.",
      "Try posting this to r/overclocking too, I think they'll be quite interested in these results",
      "I think the numbers looping is because it uses 8bits of memory for showing the temperature so when going negative it loops round to the maximum value. 8bits of memory has 256 possible states shown as 0-255.",
      "Nice, more people trying new thing like this should always be encouraged. Well documented, and keep up the good work!",
      "Nvidia and AMD GPU temperature readings are different in XOCing. Most Nvidia cards stop reading the temperature once the core hits -40°C and will just hold it there even though they are actually colder. For AMD, at least on the 6900xt, in a De8auer video the temperature read 65521°C, which is funny. At some point in the future, I do plan on XOCing a 7900xtx.",
      "It seems your core isnt very good. I could score 2800 at max vf points in all the heavy benches on stock cooling just low ambients. (~50C loar temps) and judging by other peoples results its not that good of a core. \n\n\n\nBought the card for the similar reason - to play around with it, so finding out that it only provided 2 evenings of OC shenanigans was quite upsetting. Voltage lock is the biggest shame.",
      "That ARC Temperature bug is such an Intel thing (in a funny but positive way in my head) - they used an 8-bit positive integer for temperature.. lol \n\nOP - so awesome you did this experiment!  Thank you for sharing",
      "Fantastic writeup and looks like a really fun bench session!\n\nI really wish these cards had more adjustability, but they're so friggin' locked down that it sucks a lot of the fun out of them.\n\nVoltage, power limits, clock scaling, fixed memory clocks...\n\nThere's a little bit of frequency adjustment for the core and a small amount of power curve/PL headroom, but they're *very* conservative and you're still essentially at the whim of the VBIOS. The fact that you threw LN2 at the thing and still only beat the second place score (done at ambient) by *66 points* paints a picture at just how stubborn it is.\n\nThe multiple I2C part is pretty funny, I'd never really looked into the PCBs too much but now I kind of want to. Between things like that, the wonky OPROM, bolted on HDMI PCON and the functionally separated RGB controller basically treating the card like an ARGB fan, it's kind of amazing that it functions at all - but I'm glad it does!\n\nSide note: Does the Predator OC panel trick work with any drivers other than 4314? I tried to get it to trigger the PL glitch unsuccessfully when I tried a while back, but I didn't spend too much time tinkering with it and am pretty sure I was on an earlier driver version at the time. If it worked without issue for you maybe I'll give it another go to test some power scaling stuff.\n\nETA: This is my top PR score for the moment, done under the stock 228W PL (A770 LE) and unfortunately held back ever so slightly by the 5700X, which was interesting. Normally PR doesn't give a hoot about CPU, but it still seems to make a tangible difference with the A770.\n\nhttps://www.3dmark.com/pr/2239288",
      "I will!",
      "Thank you for reading!",
      "Hahaha yeah.",
      "Did it run Crysis though?",
      "Yeah that would make a lot of sense.",
      "Thank you!",
      "Yeah, it was 2 evenings for me of OC shenanigans as well. The core did not seem too impressive and with a better core I could see someone beating my score. Voltage lock is a letdown, but also the fact I could not cool it more to make up for it.",
      "Yeah the card is really locked down and, in the end, it really comes down to the bin on the card as you can't do any tricks to get beyond it.\n\nOn 4314 I ran into some issues using the Predator trick. But then I read somewhere that you should not set the limit beyond 400W, or it will glitch out. So, I set mine at 398W and I had no issues. Have not tried it since 4314 however. \n\nHmmmmm that is interesting to know. I normally do PR since a CPU does not matter as much like you said and that fact that I currently do not have a higher end CPU on my bench.",
      "Oh so one could say that Intel has the best temperature reading mechanism for XOCing. After all, you can just calculate 356-(temp reading) and that number in negative is your actual temperature 🤣."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "a770"
    ],
    "title": "Intel Arc Graphics A750 + A770 Linux Gaming Performance Review",
    "selftext": "",
    "comments": [
      "They seem solid. We just don’t get insane frame rates on ancient games.",
      "Pulling for Intel, currently have an all AMD build but I plan on upgrading once I get a 4K monitor that meets all my personal requirements.. hopefully by then (a few years) Intel will have a high end card out and their drivers are fully sorted out, I’d love an all Intel build.",
      "I'm really rooting for Intel, and it feels good to see some competition, as the 3060 is mainly crushed and being sold for way more. However, AMD has the high ground on ~$300 with the 6600 and 6600 xt's, that's some crazy value for the money that can hardly be beaten.",
      "That performance in dirt rally is weirdly high",
      "I don't think anyone buying a budget card should care about raytracing.",
      "Unless you actually like Raytracing. That's one place the ARC series seems to justify its cost vs AMD",
      "Wednesday the 12th from the looks of it",
      "tbh,i think the cards could go even higher than 6600xt.",
      "Buying parts with the promise of future updates is almost always a bad idea \n\nHowever, in the case of ARC where the biggest issue seems to be drivers I would imagine these cards will age well. But we’ll see. \n\nMaybe they’ll copy AMD’s old FineWine technology lol",
      "launch day is october 12th",
      "I’d like to know how dx9 titles run as on windows it uses an translation layer IIRC so how would something like proton compare?",
      "Raytracing is not a gimmick - it's not an Nvidia creation, it's a lighting technique that's been around for some time (e.g. CGI film creation) but only relatively recently been possible to render in real time. \n\nWhether it's good value or not is entirely different, and certainly less relevant to lower priced GPUs, but it is something that is here to stay and will be improved upon by all 3 companies over time.",
      "I was watching the LTT Livestream, and it did give me hope, but it did also highlight issues with frame consistency.",
      "Then it wouldn't be constructive criticism. The article seems to answer some questions I had, therefore it was valuable to me. I have issues with Phoronix's website though:\n\n1. site stopped loading for me on page 4. Honestly, it could be Internet or my phone. But I suspect the website.\n2. I in the past I tried to give Phoronix some money, but their website makes it a hassle to do so. You have to do a bunch of steps, that I just aint going to do. I like things like Patreon sites. I recall I set up a forums account but ran into issues.  And loss interest in figuring it out.\n\n![gif](giphy|sDcfxFDozb3bO)",
      "Also the AV1 codex encoding support seems to be a selling point for a handful of people who want to stream.  I believe the RTX 3060 decodes but not encodes. The RTX 40xx also encodes. I imagine AMD's next card will also.\n\nOf course, i didn't read the whole article to see if Phoronix tested this with Mesa (page wouldn't load). It would suck if you buy if for that and it doesn't work.",
      "Considering AMDs OpenGL compatibility has been a mess through the entire time I have owned my rx580 I don’t have particularly high hopes for older APIs",
      ">Unless you actually like Raytracing\n\nRay tracing is only relevant at the high-end unless you enjoy playing games at 20fps. \n\nMore realistic, AV1 support is a nice thing to have.",
      "on linus tech tips they ran some games on a770 recently and TF2 and Half Life 2 were same performance as RTX 3060.",
      ">With the current market, it doesn’t make sense for buyers of mid range cards to care about raytracing.\n\nEspecially on AMD'S cards, where it might as well be broken.",
      "Raytracing is a gimmick from Nvidia to sell things for higher price. With the current market, it doesn’t make sense for buyers of mid range cards to care about raytracing."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "[GN] Intel Arc A770 16GB Limited Edition GPU Review & Benchmarks",
    "selftext": "",
    "comments": [
      "They *did*, that's the problem. These card should have been out in *January*, but were held back to fix and rework the drivers.",
      "I'd take MLID with an enormous grain of salt. He seems biased against Intel in some way in regards specifically to Arc, and I speculate he's trying to manipulate stock prices to some end.\n\nIt's late to market, but they legitimately launched a 3060 competitor, at a low price.\n\nFor their first real attempt at a GPU, that's a win.\n\nOnce the drivers are cleaned up, they just have to scale the compute up, and it's smooth sailing.\n\nAlso remember that DIY AIB is *not* their target market for Arc. Their target market for Arc is replacing nvidia in laptops so they can sell cpu/gpu bundles to OEM's. DIY and desktop is such a small % of the market to Intel.",
      "They should have delayed this, but it's at least a start for a new branch of cards in the market.",
      "MLID has ruined any and all Arc discussion whatsoever.",
      "It makes literally 0 sense for Intel to ditch this project not even 1 release in. One of the major points of Arc is to make sure Intel doesn't get edged out in the laptop market further by AMD bundling a CPU+GPU. Them giving up on Arc is literally them losing laptop marketshare.",
      "As a developer working specifically on graphics, it's interesting and I intend to grab an A750 or A770 Limited if I can find the latter. Considering some DX12/VK titles punch near 70-class, I'm inclined to believe that's the \"true\" performance of these cards and they're being held back by drivers. Definitely want to throw some Vulkan stuff at it and try to tune things to see how they behave. I'd also like to see some of the DX11- titles on DXVK. I actually used DXVK on my 6900XT to get better performance in God of War and Baldur's Gate 3. Curious what it does for Arc.\n\nThat being said, it is unfortunately true that these are not ready for the average consumer and most would be better served by an RX 6600 or RTX 3060. I do hope Arc isn't dead after these like some say. If Intel comes out swinging with Battlemage, things could get interesting.",
      "Check out OneAPI if you haven’t",
      "> I wonder how bad it was a year ago.\n\nMinesweeper sub 60fps",
      "The launch was for March 30; the invasion started February 24. If you want to believe 5 weeks would have saved them, that's a choice.",
      "I like GN since he uses 4K benchmarks. A real GPU bottleneck test.",
      "Hard to belive that such early alpha level was released... even for benchmarking review - truly embarrassing...\n\nI can understand all level of dev.; testing of HW, SW, drivers tuning etc., but more should be done before release, while still huge homework to do. \nAnyway, I am keeping fingers crossed, it will be interesting to see Intel as competitor for Nvidia and AMD. Good luck!",
      "They delayed it for almost a year. This is the result. I wonder how bad it was a year ago.",
      "Also didn't help a significant amount of Intel's driver team was based in Russia and the Ukraine invasion nuked any sort of Q2 launch anyways....",
      "I'm okay with the performance. I knew these weren't going to be the best. I just need them out already.  Let them get out there and see what problems we can improve on. I like how they look so going to get one.",
      "There's no way Intel abandons GPUs, if only because of their necessity to remain as a viable competitor in the data centre space.\n\nAnd likewise, there's no reason for them not to make consumer-grade GPUs in that case, because they desperately need a way to bring in revenue to fund AXG and to do that only through data centre/HPC requires Nvidia-levels of marketshare, which they obviously don't have.",
      "They will continue to improve it.",
      "These is not for General Audience for sure. I think Raja should setup a War Room with Driver team and keep working on all open issues. Basically those who buy this are doing testing for Intel across different setups :-)\n\nI am looking forward to raptor lake reviews.",
      "check first gen amd and nvidia gpus and you will realize what intel indeed achieved. In near feature I expect intel to be real nvidia and amd competitor.",
      "Not sure about windows, but the GPU isn't that great when it comes to Vk either. https://www.phoronix.com/review/intel-a750-a770-arc-linux/4",
      "There's a lot of nuance when it comes to Vulkan. Tons of ways to accomplish the same goals. I'm curious to see how it performs with some custom code. Just out of curiosity really."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Intel Arc A770 16GB and Arc A750 8GB available now on Newegg",
    "selftext": "Looks like the listings are now active, and the GPUs can be added to your cart and checked out. Have fun, everyone!\n\nA770 16GB is $349.99, with the A750 8GB at $289.99.\n\nEdit: It's now showing out of stock for the A770, but I'll leave this post in case they bounce back in and out throughout the day.",
    "comments": [
      "Grabbed one!",
      "Already sold out for me.  I hate this so much.",
      "I think 750 would be the card to get if you wanna support intel. However, if you are on a tight budget and want the most from your pc, 6700xt is 360 now, and 6650 is 270.",
      "They made millions of units.\n\nTBD if they just sold through them all or if it's a trickle.",
      "https://www.anandtech.com/show/17263/intel-arc-update-alchemist-laptops-q1-desktops-q2-4mil-gpus-total-for-2022\n\n\"Intel is expecting to ship over 4 million units/GPUs for 2022\"\n\nThat was published in feb, the same month TSMC production ramped for these. Meaning it's probably an accurate accounting sourced from Intel.",
      "([Availability showing on my end](https://imgur.com/a/db6j5mH))\n\nLink to Newegg landing page, if you didn't grab it from the other posts:\n\n[https://www.newegg.com/promotions/intel/22-1809/index.html](https://www.newegg.com/promotions/intel/22-1809/index.html)\n\n[Link to A770 16GB](https://www.newegg.com/intel-21p01j00ba/p/N82E16814883001?Item=N82E16814883001&Tpk=14-883-001)\n\n[Link to A750 8GB](https://www.newegg.com/intel-arc-a750-21p02j00ba/p/N82E16814883002?Item=N82E16814883002&Tpk=14-883-002)",
      "No Canadian Newegg joy for me :(",
      "I'm seeing the same thing. It's possible that they're going to go in and out of stock, as I have a hard time believing it would have completely sold through this fast. Unless the stock consisted of whatever Ryan Shrout could fit in his car's trunk to drop off at Newegg HQ this morning. There are also the ASRock A770 and A750 cards which have not yet appeared in listings, although the LE cards look much cleaner.\n\nETA: ASRock cards are listed, but not live yet.\n\nA770 **8**GB: $329.99\n\n[https://www.newegg.com/asrock-arc-a770-a770-pgd-8go/p/N82E16814930077](https://www.newegg.com/asrock-arc-a770-a770-pgd-8go/p/N82E16814930077)\n\nA750 8GB: $289.99\n\n[https://www.newegg.com/asrock-arc-a750-a750-cld-8go/p/N82E16814930078](https://www.newegg.com/asrock-arc-a750-a750-cld-8go/p/N82E16814930078)",
      "It makes no sense to me either. I mean, there can't be this much demand, especially when Nvidia/AMD cards are all in stock. Something doesn't seem right, unless they launched with 10 cards in inventory.",
      "I can see the A750 and that hasn’t gone out of stock at all.  I was able to add A770 to my cart but went out of stock again.",
      "Had one in my shopping cart but it was gone before I pulled my credit card out of my wallet.",
      "I was able to place a back order too but the 19th has come and passed with no change to my order’s status. Any luck for you?",
      "To be fair that was including both the laptop arc cards and their add in cards for prebuilts(like NUCs). I really don't think there is really as much volume left over for DIY as we would hope.",
      "How is this out of stock??? I've been checking since 8:15 est and did not see the a770 listed as available even one time....\n\n Intel didn't even officially announce where they were selling these units unitl 9am est this morning.....\n\nWhat a joke of a launch",
      "I managed to place an order for an A770 when it was listed as a back order. Newegg took my order.  My order says \"release date: 10/19/2022.\" Maybe that's when Newegg thinks they'll get the next restock.",
      "Are there any retailers up here who have this up yet?",
      "I agree with you. I woke up this morning a little late (Intel never said what time it was launching) I think it was around 11AM and there was nothing. No way these cards are that popular. Intel probably made 100 total for the whole US.",
      "My suspicion is either they have yield issues, which would explain why there seemed to be many more A750s available, or they’re purposely slowing supply so they have more time to improve the drivers.\n\nThey won’t be slammed with as many bad reviews and tech support problems if they release the supply slowly.",
      "Yeah, there's some BS going on. How is it sold out, can't be that popular when there are tons of AMD/Nvidia cards in stock.",
      "Canada Computers has it listed, but OOS for now: https://www.canadacomputers.com/index.php?cPath=43\\_557\\_5769"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Unfortunate experience with Arc A770",
    "selftext": "Now, to preface this, I have to say I still really love the software and the Arc cards in general. I would like to still use an Arc A770, provided I can get one that actually works this time.\n\nFor the first time in my life, I've got a fried graphics card on my hands.\n\nGot my A770 a week or two ago after being a longtime Nvidia user, and came to love the card's value and software. However, when I installed it, I had two problems; either games would crash outright, or the computer would go to a black screen and just restart in the middle of gameplay. This was extremely annoying. I thought that maybe it could be a power supply issue, but my Seasonic Vertex GX-1000 is more than enough for the card.\n\nI checked, made sure Rebar was enabled, Above 4G encoding was enabled. I thought that maybe my old Ryzen 3900x was not compatible, so I installed a Ryzen 5500 in my board. Still had black screen shutdowns and game crashes.\n\nI thought, maybe Windows 10 was not compatible. I bought an SN850X and installed Windows 11 on it (didn't want to get rid of all of my old data on the old NVMe). Still had black screen shutdowns and crashes.\n\nI said, alright, I wanted a motherboard upgrade for a while anyway. I bought a new MSI Pro Z790-A Wifi DDR4 motherboard with a 13600K processor, because maybe my old board was bad, and I figured an Intel processor would pair better with the card anyway. \n\nReinstalled Windows 11, made sure Rebar and Above 4G were enabled, and I still had black screens and game crashes. I also noticed something interesting about 20 minutes ago.\n\nDuring my gameplay of the RE4 demo when the card was functioning, the framerate was rapidly decreasing and I couldn't figure out why. I tried reinstalling drivers, but it didn't help. I checked the card temp with the overlay. It was 90 degrees! How is this possible, I thought. I have a Phanteks Enthoo Pro 2 full of Noctua A12 fans, there is no way it's coming close to overheating.\n\nI looked in the case and saw the RGB on the card no longer functioned, and the fans weren't spinning at all. I smelled the card and it smelled of burnt plastic. It still functions, but has no cooling or RGB. Pulled the card out and it was burning hot to the touch.\n\nHave to submit the RMA, which I'm sure won't be a big headache. I just wish I was able to play RE4 Remake on launch day, definitely not happening now. Oh well.\n\nLike I said, still love the card and the software and I will gladly use another, provided the next one is functional.",
    "comments": [
      "I RMA'd my A770se more about 2 weeks ago because the aRGB stopped working. I didn't have any of your other problems though in the 2 months of ownership. Still haven't heard back if it'll be fixed or replaced.. bought from eBuyer here in the UK. I did get an email 10 days ago saying they had received it, I guess I'll just have to wait.\n\nGood luck with yours.",
      "Why was this downvoted? lmao",
      "Because ranting about drivers and reviewers when it's clearly a hardware issue is off-topic. OP just did an unfortunately bad job at diagnosing a broken GPU. For the record, I didn't downvote it myself.",
      "Because reddit",
      "I just had exactly the same problem. When I got the card I installed the rgb cable and turned off all colors. Then I removed the cable and uninstalled the rgb software. 6 weeks later the fans stopped and the card was in the 60s with no activity. Just for kicks I reinstalled the rgb cable. When I tried to install the rgb software it told me it could not find an ARC card. ARC control showed the card. It seems that some controller for the fans and rgb went bad. I also submitted an RMA request to intel",
      "Wow that's pretty unlucky. Sounds like you've spent a fair bit in recent times. \nHopefully they can rma and get you a new card. \nWhile i didnt but an arc card, id strongly consider it in the future. I hope they can improve and become dominant in the market",
      "tease subtract intelligent reminiscent jellyfish tender innate cover birds wide\n\n *This post was mass deleted and anonymized with [Redact](https://redact.dev)*",
      "Why did you pair it with a 4th gen Intel lol. It doesn't support ReBAR, so even if it worked, you'd get extremely low FPS",
      "I had suspected that my motherboard was at fault because it previously had many blue screens trying to enable D.O.C.P. for my RAM, and I wanted to upgrade to something newer and larger anyway because the board was an ITX board in a massive case, and it was showing its age. I was going to upgrade it anyway at some point so I figured now is the perfect time.\n\nI couldn't rule out how the board performed with Above 4G and Rebar enabled on my other card either because those settings were completely foreign to me until the ARC program told me to enable them, and I had already sold my buddy the old card.",
      "The problem is likely with your motherboard. Check for the most recent bios to see if it has an update for Intel GPU. I doubt it though.",
      "Something that i have learned (not sure if its similar) older intel motherboards (i had the same gens paired with a gtx 980) seem to prefer DVI. My newer board (same card) will prefer HDMI. maybe as your card doesnt have DVI it wont give a signal at all",
      "It was not intentional, my existing GPU was showing issues that could be thermal/age-related so I wanted to see if the card would output in my existing environment.",
      "Thank you for your experience with this, I too have an Arc and will be fitting it into a new system as I wish to upgrade everything as my hardware is ageing.\n\nMy issue is that I tried to use the DP to connect to a newly purchased monitor as part of a 2-monitor setup and the monitor does not receive a signal. I put the A750 into a LGA1151, Z97 motherboard with a 4th gen Intel, and when the monitor reported that it didn't receive a signal, I did basic troubleshooting. I tested the cable on an independent environment (which was good) and the card ports (which also worked) so either that is the DP port of the 2016 monitor (a Prolite XUB2492HSN) OR it cannot operate in a dual monitor setup using HDMI and DP (which I haven't tested yet as I wanted to get the DP port on the card to function)\n\nLike you, the green and red options do not appeal, and I suppose that we have to live with issues using bleeding edge stuff.\n\nThanks for sharing, I would be interested to know how your situation improves.",
      "thanks for the suggestion - will do. \n\nHowever, I might just wait until I get a new MB / CPU.",
      "Very interesting possibility as my current card uses and operates 2 monitors on mini-HDMI and DVI.\n\nIs it possible that to use DP it must be activated on the MB - and if so, I cannot seem to find the category that would be in. Any ideas?",
      "Try maybe using the iGPU (could have to be activated in BIOS). I have tried changing the default and haven't had success.",
      "Your experience is pretty typical. When professional reviews with a million + hours of testing and troubleshooting under their belt struggle with drivers, what did you expect? Everybody from Gamers Nexus, Hardware Unboxed,and PCWorld all have issues with their drivers. All I can say is the best play is to pick up a rx6700xt for cheap and run with that. Alternatively, set your expectations lower and expect some of your games to not work and try to stick to ones that do until they work out the driver issues.\n\nOP, it's obvious that your problem is 2 part. Software problems and physical problems, Dead card. After you RMA, just replace it with something else and sell your RMA card afterwards or use it in your backup build. What's the point of getting a card that fails to run games some of the time due to driver problems. The whole point of a discrete you is to run games.",
      "And has had that before also or up the power slider first!",
      "Bump the voltage a bit probably undervolting stock!"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "How do I buy the now intel arc a770?",
    "selftext": "",
    "comments": [
      "Centralcomputers.com has them now",
      "Hi there! Thanks for shouting out to us! We have Intel ARC A750s and A770s available now for sale! Feel free to check it out on our website! \n\n&#x200B;\n\nhttps://www.centralcomputer.com/catalogsearch/result/index/?cat=55&q=Intel+ARC",
      "Currently, our only MW2 Game promo is with the purchase of an Intel Core 11th and 12th Gen i7 and i9 CPUs. However, we are currently in talks about securing future promotions with Intel. We will definitely update our website and social media channels when this happens.",
      "No prob! Love going to your stores!",
      "Newegg have A750 in stock right now.",
      "Does purchasing from CentralComputers include the MW2 game promo?",
      "Thanks, I’ll check it out",
      "Hey there! Thanks for letting us know about the price difference, typically we do price competitively against our competitors but we will certainly share this with our products team and go from there. As for not being an Authorized Dealer, we can reassure you that we are an authorized Intel reseller and have been for a very long time.",
      "Hey there, I have gone ahead and notified our products team about the pricing difference between us and our competitors. As for now having the other bundles, we are in talks with Intel right now to see if we can be part of future promotions from them.",
      "I’m not complaining personally. If I wasn’t able to get an a770 on launch day I would have no issue paying the extra to get it locally from you guys!",
      "Appreciate the support! FYI, we don't even make $50 on these cards. Margin is low. Well under 10%.",
      "With money, usually.",
      "https://game.intel.com/story/intel-arc-graphics-release/",
      ">Centralcomputers.com\n\nBut the price is $50 higher than newegg and not bundled with MW2 + Adobe",
      "I wanna get the best in the series because I have more then enough for it, thanks anyways",
      "Okay where can I buy",
      "Central has always had a small price hike… but they are my local store… so they get a shoutout",
      "I have both models, it's literally single digit gains between them, a770 not worth imo.",
      "Computer shop.",
      "Thanks again for shouting us out! We do typically price competitively with our competitors, however, I will go ahead and share the price difference with our products team and see where it goes from there. Thanks again!"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Purchased an Arc graphics card, pleased with the results!",
    "selftext": "Backstory is that I wated to build a Minecraft PC for my nephew from spare parts, so I had to free up a GPU in the process. I decided to buy an Arc A770 to replace the 1070 in the Media PC, so I could put the 1070 in said nephew's PC.\n\nI've been plesantly surprised by the Arc's capabilities. I had to install the latest Beta drivers to even get it to install (Win 11 on the Media PC), but apart from that I'm seeing pretty good results in most games I have tried. Sure, there is a lot of driver work and improvements to do but so far I haven't seen any major issues, at least from the standpoint of a Media PC connected to a 4K TV.\n\nI think it's good there is a 3rd player in the market and I really hope that Arc becomes a bigger thing as time goes by. Some random photos below.\n\nArc in the Media PC: [https://i.imgur.com/XZP0ZsR.jpg](https://i.imgur.com/XZP0ZsR.jpg)\n\nArc being all lightey: [https://i.imgur.com/o7ZcCji.jpg](https://i.imgur.com/o7ZcCji.jpg)\n\nArc running Stray: [https://i.imgur.com/kL1tCVZ.jpg](https://i.imgur.com/kL1tCVZ.jpg)\n\nIf I had a message to Intel I'd say this is a great first attempt into the GPU market, it represents good price to performance, keep at it. It's extremely hard to make GPUs, and I think perseverence will pay off in the longer term.",
    "comments": [
      "Glad to hear you’re enjoying your new toy! Thank you for sharing 👍 \n\nI’m excited for intel GPU future also.",
      "well, it's not perfect, there graphics glitches in some games, and Resident Evil HD Remake and Resident Evil Zero dont run for me, but other than that it's not that bad as some say, specially with DirectX 12 and Vulkan games.",
      "It’s actually performing great in most games that I play. The only game I had issues with is battlefield 2042, but I wasn’t too fond of that game anyways. So it wasn’t a major loss.",
      "It has fine driver support for most games, and poor driver performance in a few games.",
      "Battlefield has problems with Battlefield 2042",
      "I have 0 issues at 4k resolution on mw2 with Intel arc. Runs smooth as butter. What's funny is some reddit clown bashing something other people are reporting works fine because of a YouTube benchmark. Reddits full of know it all's!",
      "That is one beautiful GPU",
      "I love my A750 and it does perform better than my 1080ti. But the 1080ti is the gpu that never dies. It was/is a really great gpu. If you’re happy with what you got right now just keep the 1080ti. Otherwise if you’re looking for a worthy upgrade I would consider the next Gen cards from either Nvidia or AMD.",
      "It falls right in place in the performance to price ratio. I’m happy with my 1440p full ultra with XeSS set to quality. Really smooth.",
      "Arc A750 16 GB doesn't exist. All Arc A750 cards are 8 GBs. Depending on where you live the Arc is way more worth it. Not everyone lives in the UK. In Canada, the RTX 3060 on sale is $500 CAD + 13% sales tax. In terms of Price to Performance the Arc is way more worth it.",
      "Given how well theyve performed this early on, i think intel is going to be a major competitor. There was never any doubt that theyd cause a disruption, but i think itll be a bug one, which is good.",
      "How does it compare to the 1080ti? I’m still limping along with this 6 yr old gpu",
      "391.97 UK including tax & shipping",
      "Same here. I'd like to know because a $350 upgrade isn't bad.",
      "They did say it was for their media PC, so might not even be used for games.",
      "Big one for sure. Once they get the software and drivers optimized to the hardware it’ll be huge. I’m super happy with my purchase.",
      "Not massive enough upgrade compared to a 1080 Ti to be worth it yet, though with time, drivers should get better to the point it would be worth it.",
      "Classic EA!",
      "LOL what a joke comment.",
      "Find me a single reason why it’s better for the price than a 200$ Rx 6600."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Why is the Arc A770 so much slower than the Radeon RX 6750 XT",
    "selftext": "\nOn techpowerup’s website it says the A770 has similar theoretical performance to the Radeon RX 6800 XT but in relative performance they say it's worse than the Radeon RX 6600 XT. With all the extra cores and other hardware the A770 has I don't understand why it preforms so bad. What am I missing? Is it because the A770 doesn't have any L3 cache or does it and techpowerup doesn't list it? Why is the 6750 XT so much better for gaming? Is the software the only thing holding the A770 back and how?",
    "comments": [
      "Driver and hardware design.\n\n&#x200B;\n\nIts hardware in testing seems to be a monster at parallel compute, scaling well with resolution, but does poorly with latency focused applications.\n\n&#x200B;\n\nDrivers don't help either.",
      "I'm not sure what theoretical performance they're referring to, but even in ARC's best case scenarios it's a 3060ti in performance. It's rumored Intel was aiming for 3070 performance, but failed. It's performance is usually just a bit better than the 3060 and 6600XT.\n\nThey probably could extract a higher level of performance from the GPU, but it would require lifting the power limits to absurd levels. \n\nIt's worth noting that in Ray Tracing, ARC performs better than Radeon in general.",
      "It does not generally perform like a 3060Ti. Those are edge cases.",
      "2x A770 encode faster than a 4090. Nvidia just 'unlocked' better encoding on their latest drivers. just sayin.",
      "Basically, compute tasks and gaming tasks are very different things. The A770 is a very large die, almost matching the 3070 Ti, but there’s some sort of fundamental flaw in the architecture and/or drivers that doesn’t let it use all those shaders consistently for gaming.\n\nAlchemist feels like Polaris all over again - its compute performance is great, but that doesn’t mean its gaming performance will ever be.",
      "Driver and hardware design",
      "I mean aiming for 3070 and hitting 3060ti is like giving up 5% at most so they did pretty well for a first shot.",
      "Fine wine™️",
      "If you don’t mind a technical read, then Chips and Cheese’s [analysis of the A770](https://chipsandcheese.com/2022/10/20/microbenchmarking-intels-arc-a770/) sheds some light as to why it’s not as good as it should be.\n\nBrief summary: struggles compared to the latest AMD/Nvidia GPUs when shader occupancy is low; can’t fully utilise internal bandwidth as well as the competition; worse instruction latencies, too. Drivers play a part, of course, but they can’t make up for fundamental architecture design choices.",
      "Driver.",
      "Wrong subreddit to ask this since you're gonna get the r/Intel apologists, but the reality is Intel has a brand new architecture and brand new drivers without any development. \n\nEven IF their hardware was theoretically capable of high end performance, it would take years for the drivers to mature and catch up.\n\nEven in 2023, AMD is FAR behind Nvidia in driver maturity.",
      "haha, we can all dream",
      "I've heard people say that their A770 performs just as good as their 3070 or 6700xt. It's really depends on the game. \n\nLTT did a full 30 day review (by Linus and Luke) where, after the latest driver updates, they didn't have anything bad to say about the performance. They still swapped back to their Nvidia top tier cards, but A770 wasn't supposed to be a top tier card.\n\nI think in the next few releases, Intel will have, or be extremely close to having a top tier card. I really hope they get it figured out.\n\nI will say that if I didn't already have a 3070. I would have picked up a A770 16GB card. They are almost half the price of my 3070 when I bought it.",
      "When you look at tests it might not perform as well as more expensive cards, but save the money, you don’t really notice the difference.",
      "It's closer to a RX6660XT or an RTX3060TI\n\nBut really depends on the game as sometimes it matches a RX6750XT or quicker than a RTX3070 \n\nAnd most importantly the drivers and driver updates, for example it seems to be really slow in games like CSGO, until the drivers were updated and now it's much much quicker\n\nIntel will give it some fine wine",
      "Yeah the drivers are pretty bad for dx9, so they pretty much used a translator that uses Vulcan instead of dx9. Kinda odd, but it seemed to work.",
      "The a770 won’t be slow for long. Intel just needs to figure out driver stuff and then it’ll be sweet",
      "A770 looks towards the future DX9 to 11 games has weaksauce performance, they did improve it a bit a few months back but buying this you really wanna focus on DX12 performance and consider DX9 and 11 games as secondary.",
      "Unfortunately, due to money woes Intel won’t be continuing their fledgling discrete graphics journey it seems. My guess would be the tech learned will possibly get into better APUs. Even this is a big who knows. If it were me, I’d try to keep at least that part going because AMD wipes the floor with them in this market. Look at Steam deck, PS5, Xbox (both series). Not only have they been the hare from the old story, they let their tortoise get so steady it’s cost them in all markets. Before i get accused of being some AMD fanboy, which honestly I am as well as being an Intel fanboy, my most current system i built around the 13700k. My latest AMD built system is a 3900x i think. The two tech companies i dislike the most are Apple (snobby hardware, no repair, specialized everything that can’t be replaced outside their eco-system which is tons of steps back from the rest of the industry) and nVidia. TBC, I use nVidia cards mostly and enjoy them, but they are total ROI whores now. It’s kinda like they really have the same thing anyone else can but they only sell to high end clients snd have proprietary “Like a Virgin” tech, get my drift? Totally off their rockers.",
      "To add, we really don’t want any of these companies to fail. Intel, especially, can fabricate in the USA which is a big deal, although their nodes have seemed to be problematic for a time now. We could end up in a situation where only Intel sells to us or supports our efforts in the USA and that is more important than anything else if/when."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Intel expected to lift Arc A770 GPU review embargo in early October - VideoCardz.com",
    "selftext": "",
    "comments": [
      "review Oct. 5 but with a release dater of \"very soon™ \" ??? Surprised they can say that with a straight face when they said 1Q22 then Spring then summer and now \"very soon\".  LOL",
      "I mean, it’ll be an interesting product, but I won’t be able to seriously recommend these to people.",
      "Schrodinger's GPU",
      "See you in 2023?",
      "Expect these to be ass. But that's not the point guys. When Nvidia and AMD cards came out..they were trash. With in time, I expect them to get better. The thing is Intel has to stick with it\nThat's going to be the test. Everyone is going to point and laugh at them but within the 3rd or 4th generation I expect a real 3rd competitor.",
      "This doesn't really make any sense. Intel said they'd release their skus starting from the bottom with the a380 then working up to the a770. What of the a580 and a750, will they just release review embargos on those at the same time too then?",
      "> Everyone is going to point and laugh at them but within the 3rd or 4th generation I expect a real 3rd competitor.\n\nThe question is, has Intel the perseverance to sustain such a long haul and financial burden?\n\nOr are they going to cut losses and call it a day? Remember that Gelsinger expressively stated, he not only WANTS but NEEDS to exit business or at least will knife lossy divisions to cut losses. Intel hasn't the money power anymore.",
      "I'm definitely interested in them. I use Linux and currently have an all AMD setup with a Vega 56 that's starting to show its age. I need to see how it attacks up in perf/$ to RX 7000 first of course. I really want to see some one do some in depth testing with DXVK to see if it circumvents their driver issue with older graphics apis",
      "At what prices?",
      "While I'd love for Intel to be competitive in the GPU market, it's obvious that won't be the case this time around unless they are willing to sell these cards at Costco (or even below them). I have a really hard time imagining someone choosing (as in not sticking with the option for your prebuilt) to go with ARC with its inferior drivers in older games and with less features than even AMD, especially with Nvidia and AMD's next gen right around the corner. Hopefully next gen doesn't suffer from the problems ARC has had with its \"Q1\" launch.",
      "Imagine if the a770 gets destroyed by AMD/Nvidia xx50 class next gen card, lol...",
      "Picked up a FE 3070 for $300 and an RTX 3070 MSI for $350  off Marketplace. The used market has dropped substantially and I feel as though Intel completely missed their window",
      "They are going to be very niche for anyone that needs high double precision but can’t afford enterprise cards.",
      "Presumably. I'm guessing the 770 was singled out because it's direct from Intel, not an AIB.",
      "Gelsinger will quietly strangle Arc graphics division to death long before we reach 3rd gen. \n\nThe whole graphics release has been an unmitigated disaster for Intel.",
      "If anyone has the money to burn on a 4000 series, they were never looking at Arc in the first place. But with the 3000 series dropping lower and lower, that kills Intel's window for the 200-500 market. And nobody's sure what rabbit AMD is going to pull out of their hat.",
      "It will, when they release them 1-2 years from now, since they need to move old stock",
      "The x50 class from nvidia/amd next gen will be yet another rx580 with lower power draw or a 2060 at 2060 launch MSRP.",
      "Intel has the money the problem is Intel is run by the damn shareholders.  Business lose money all the time but Intel is always a shareholder first which sucks",
      "The simple answer is “it won’t”, top ARC is basically an RX 6650 XT competitor at this point."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Will intel arc a770 le go on sale below 300$ again?",
    "selftext": "I’ve been looking for different Gpu’s for my build and I wanted to buy the a770 le because of the looks, I remember it being on sale on amazon (even after it was discontinued) brand new below 300, but now it’s not there anymore. It either has to be this gpu or Ill have to go with another brand, unfortunately other versions of this gpu look goofy to me.",
    "comments": [
      "I'm looking at two on newegg right now that are below $300.",
      "Thats too bad. For what its worth I agree the limited edition was the best looking design.",
      "Unfortunately it is discontinued. You’ll need to find used most likely at this point.",
      "What a bummer",
      "Its not discontinued, just the first party ones. You can still find a few board partners. Acer and sparkle come to mind. But I think there's another 1 or 2.",
      "Yeah but not le (limited edition manufactured by intel)",
      "Those two look very immature for me unfortunately",
      "Those two look very immature for me unfortunately",
      "I get not liking the look of the rgb cards. But you don't like the sparkle titan or the asrock challenger?",
      "They don’t even come close to the original intel design from a  mature and simplistic style perspective. To each their own :) I’ll be going with a different gpu unfortunately",
      "They don’t even come close to the original intel design from a  mature and simplistic style perspective. To each their own :) I’ll be going with a different gpu unfortunately",
      "They don’t even come close to the original intel design from a  mature and simplistic style perspective. To each their own :) I’ll be going with a different gpu unfortunately"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Arc A770 16GB ($349) replaces Quadro RTX 5000 16GB (~$2299) in CFD",
    "selftext": "The Intel Arc A770 LE 16GB shows solid OpenCL performance in [FluidX3D](https://github.com/ProjectPhysX/FluidX3D), a bandwidh-bound  computational fluid dynamics software, going head to head with the Quadro RTX 5000 16GB and beating the RX 6900 XT 16GB. Of all devices tested so far, the A770 LE is #2 in price/performance after the GTX 1660 Super 6GB.\n\nhttps://preview.redd.it/4udbdlpgwpz91.png?width=1776&format=png&auto=webp&s=217fb3f2b4229a2b9d0789be72c4f8d6b95e94e3",
    "comments": [
      "What is the source of the results tabulated and your claim regarding price-to-performance? Are these your own values?\n\nFor example, in this chart, a GTX 1080Ti and RTX 2060 Super have higher MLup/s and are cheaper.",
      "A lot less impressive when a 2060s is better than both. Is the quadro much more expensive than the arc? Yes, but that's clearly not its intended purpose since a much weaker GeForce card beats it.",
      "A GPU is a GPU, a vector processor with fast memory attached; they don't have an \"intended purpose\". Quadros have never been faster than their GeForce counterparts, but their (typically) larger VRAM capacity allowed for price gouging. With much cheaper gaming cards now also getting 16-24GB, all but the highest-end 32-48GB Quadros are obsolete.",
      "I've done most of the benchmarks myself, and some are submitted by users, such as the A770 results by [RawMango](https://twitter.com/mango_raw/status/1591451934241980417). This is a purely VRAM bandwidth-limited algorithm (like most compute applications), and on top it's non-cacheable, so FP32 performance and cache are irrelevant for this particular application. Only VRAM capacity and bandwidth matter. For price/performance I assume MSRP prices, see [here](https://twitter.com/ProjectPhysX/status/1591458548437090304) for the full plot.\n\nMLUPs/s means Million Lattice Point UPdates per second, or how many grid points are computed per microsecond.",
      "Studio. There is no more Game Ready drivers for Quadro.",
      "Look closely, the Ti is only \\~5% faster, due to its slightly faster VRAM (1008 GB/s vs 936 GB/s). The top 3 bars are A100.",
      "No idea why people still restrict themselves to a proprietary ecosystem with CUDA/HIP/Metal, just to run into porting trouble on the next (super)computer with hardware from the other vendor.\n\nThis wouldnt be a single % faster if I used CUDA, because it's already at the bandwidth limit. OpenCL is exactly as fast and efficient as CUDA, and on top it is compatible with literally every GPU from every vendor since \\~2009.",
      "512.78\n\nI wouldn't expect any difference between different driver versions though. 80-82% efficiency is already as close to the roofline curve as it gets with that mix of coalesced/misaligned memory access.",
      "Most of them go head to head with the GDDR cards, depending on bandwidth and efficiency of architecture / memory controllers.\n\n\\- GCN: Vega 64 performs rather poor at 50-60% of 484GB/s, Radeon VII is faster at 40-70% of 1024GB/s\n\n\\- CDNA2: MI200 is only \\~50% of 1638 GB/s\n\n\\- Pascal: P100 are at \\~68% of 549/732 GB/s\n\n\\- Volta: V100 is exceptionally efficient at \\~88% of 900 GB/s, Quadro GV100 is only \\~60% of 870 GB/s\n\n\\- Ampere: the A100s dominate everything at \\~60-80% of 1.5/2.0 TB/s",
      "I'm comparing the A770 to the expensive RTX 5000 because both have 16GB. Memory capacity limits the maximum grid resolution you can simulate, so both cards have the same capabilities in CFD. With a 2060 Super or 3060 Ti (both 8GB) you can only do half the grid resolution.\n\nThe only workloads where mid-range Quadro is faster is when the software artificialy cripples itself it detects \"GeForce\", such as nicely demonstrated by [Siemens NX and CATIA](https://youtu.be/z14bPZcwHck).\n\nThe AMD cards just have poor VRAM bandwidth to begin with, and can't handle full load on the memory controllers as a legacy of GCN. The infinity cache can compensate the slow VRAM very well in games, but in non-cacheable compute workloads it's basically useless.",
      "Whoops",
      "This commercial is so incredibly cringe, nice find on that one. Also amazing work writing your own CFD tool. Pretty impressive",
      "True, but I still think the comparison and the way you worded the title is kinda misleading. The arc is faster than a much more expensive card but just because said card is extremely overpriced. It would have been more fair to compare it to a GeForce GPU, like the 2060s for example which is much closer in price.\n\n\nAlso I said \"intended purpose\" just because I assumed such an expensive card would have a significant advantage over gaming cards at least in a few scenarios, otherwise who the hell is going to buy them at that price. But honestly it wouldn't be surprised of Nvidia scamming people, and I still don't know any workload where quadros consistently beat their gaming counterparts (ignoring vram bound situations).\n\n\nP.s. just a curiosity, any idea of why new amd cards (6000 series) perform so badly? I guess it's the fact they have only gddr6 right?",
      "How do HBM2 cards do in these tests? Vega 56's etc?",
      "Quadro on which drivers?",
      "Let me add that OpenCL isn't used as much as other alternatives, specially in the industry, where Quadro cards reign supreme due to nvidia's CUDA technology.",
      "It's not that you restrict yourself, but the software you use does.",
      "How is the 3090 ti so far ahead of the 3090?",
      "Presence of A100 distorts the chart and makes the difference between various consumer GPUs less clear. CPUs, mobile GPUs, GPUs older than 5 years should have been in another graph",
      "Yes, but studio or game?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Acer Arc a770 for $319",
    "selftext": "Over at B&H. I already own a 4080 but a recent gpu with 16gigs of vram at that price is insane. I also heard the drivers for arc cards are a lot better now so I would definitely grab one if I needed it",
    "comments": [
      "Sure, why not especially if you are adventurous person.",
      "I have both versions of the A770 for multimedia production. March/April was a bit rough, but it's all good now. Especially for media work. Gaming, I can't comment, but they do update the drivers every few weeks now according to what people report.",
      "I'll admit I went team Blue and got the A770. After awhile the drivers beta where terrific but then random things would not work, suddenly I cant use the camera like I used to before, now REsizableBAR would not be recognized as enabled even though CPU-Z and my BIOS reflected it did. I decided to flip to the 4070FE and I have not regretted it.",
      "As far as being difficult to work with in terms of gaming, I'd say not very. I got the A770 LE because of the price, built my first CPU with it, and it's been running great. \n\nI've only had two problems with it, and both were easily solved. Fallout 4 had cruddy performance, which was solved by installing a DXVK mod. XCOM 2 wouldn't start on Driver 4514, which I solved by going back to the stable branch of released drivers. \n\nAny game running on DX12, the A770 will be able to run just fine. Older games (pre-DX11) are sort of hit-or-miss, but if we're being honest, those games are hit or miss regardless if your OS is Windows 10/11.",
      "It was on sale of even less at newegg, I think right now it is (I'm not US)",
      "Did you try using a regular driver? Beta drivers are a bit iffy sometimes but switching to a stable one solves the problem",
      "That's not a bad deal.... too bad it wasn't yesterday. There was a powercolor fighter 6700xt for $299 on Amazon, and it came with Starfield",
      "Is it that difficult to work with it? I'm seriously thinking about getting it for like a backup card or just to try it out. The price isn't a problem for me but I just want to know if I should get it or not. It's a beautiful card and I've read the drivers are improving with each release so it's tempting not gonna lie",
      "What's the direct competing card to this from the other two?",
      "If you already have a 4080, what do you need the A770 for? The 4080 is much more capable",
      "Well it would be used for gaming but the bro is right. I don't really need it my 4080 runs circles around it but I'm just hella curious about these arc cards. They look fantastic and the specs are good for 1080/1440p and the drivers are improving so yeah a 16gb card at this price is a no brainer for anyone looking for a gpu to complete their build. It's a cheaper alternative to what's already out there from both AMD and Nvidia",
      "6700 and 3060",
      "Do you need a second GPU? \n\nIf the answer to that is yes, then it is a really great but as it can perform a lot of work in the background.\n\nIf you just want to try it out, try making some videos about your experience and being an active member of the arc beta program by reporting issues.\n\nOtherwise, it's kinda pointless to buy it just cause when you are rarely ever going to use it anyway.",
      "I decided not to get it I already have an awesome gpu and I don't want to be selfish and take away another card unnecessarily from someone needing one. My 4080 should last me for a long time so I'm content with it, for now"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "What is the AMD and Nvidia equivalent (when looking at performance) of Intel Arc A770?",
    "selftext": "",
    "comments": [
      "Depends on the application.\n\nIn gaming, it sits between RTX 3060 and RTX 3060 Ti, or between RX 6600 XT and RX 7600.\n\nIn memory-bound OpenCL compute, the A770 punches well above it's weight, and is about equivalent to an RTX 4070 or RX 7800 XT.",
      "In (practically achieved) VRAM bandwidth they are similar, and that is what matters for compute workloads.",
      "Intel's initial projection of the ARC Alchemist A780 or A770 GPU was to equal or surpass the Nvidia RTX 3070 or AMD's RX 6700XT.\n\nThey never made their video cards come close to beating the RTX 3060ti nor the RX 6600XT.\n\nThere are two aspects of why the Intel video cards did not and/or could never achieve their stated goals. The GPU chips they manufactured has a design latency flaw, and the second problem is that the company did not have enough experience and development of the video drivers used by the graphics.\n\nThe A770 will never surpass the performance of the RTX 3060ti 8 GB nor even the RX 6700 10 GB because of design flaws built into the hardware.\n\nIntel never issued the A780, but they released the A770 and A750 where both cards should be able to beat the RTX 2060 Super and RX 6600 8 GB while gaming with the latest games using RE-Bar (Smart Access memory).\n\nIntel's video cards use emulation software to utilize older programs, because the design and video drivers of the GPU was never created to use older software prior to 2020, which means programs that need to use protocols for DirectX10 and prior are slower than they should be. The performance is acceptable now due to improved video drivers, but some of the modifications will not be liked by some. They increased the FPS rate by increasing the stutter and 1% low frequency rates on many games, so, you sacrifice smoothness for increased overall speed.\n\nIf for any reason you cannot use Smart Access memory for any type of use with the Intel video cards, your performance will be slower by more than 10%.\n\nYour performance experience will be based on what programs you use, and if the video drivers have an optimum adjustment to perform well using that application. On a price basis, an Intel A770 purchased below 240 dollars provides decent value, because you are buying something that potentially performs above an Nvidia GTX 1660 with a goal to do as well as an RTX 3060 12 GB card. As for comparison with AMD, the card games above an RX 6600 but it has a hard time when it can process workloads to perform above an RX 6700XT. There are some programs that the Intel A770 has difficulty using such as any software that uses DirectX 9 and prior. For those programs, your performance will likely be at the GTX 1050ti level or lower (such as the GTX 960).",
      "http://www.3dcenter.org/artikel/fullhd-ultrahd-performance-ueberblick-2012-bis-2023 says between 6600XT and 6650XT for AMD, between 3060 12GB and 4060 for Nvidia at 1080p. At 4K, the 3060Ti GDDR6X and 6700XT should beat it.",
      "Don’t listen to the haters, it’s between 3060 and 4060. Also slightly above the 6600XT. Probably one of the best GPU’s at the price range if you plan on playing newer games, as Intel drivers are less reliable with older ones. \n\nI don’t know why you are asking this, but if it’s for perspective on the card you should buy it’s this. \n\nAMD still reigns supreme in the low end GPU scene, though Intel is a very good alternative if you  primarily play games newer than 2019. It has issues (though mostly resolved) with older games, and is still much cheaper. It’s a constantly evolving NEW platform, so there may or may not be issues but it does seem like they are fixing anything they can. If you’re making a budget build and you are really trying to see where you want to make cuts, I think Intel arc is a great alternative. \n\nI don’t think Nvidia GPU’s are worth it in the budget range unless you buy used. It would take a bit of justification as to why you would need a budget gpu in a system that is also probably lower quality. If you are building a completely new rig and need it to be budget, Nvidia is out of the question a bad pick.",
      "Is 7800xt an equivalent of 4070?",
      "I come from the future and I can confidently say my A770 outperforms my 3060 Ti\n\nThey’ve immensely improved their drivers",
      "You have a zotac Nvidia then.",
      "Same here."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Is ARC A770 Compatible with B450M",
    "selftext": "I'm thinking to buy an ARC GPU but not sure if my motherboard works just fine with it. Is there driver or performance problem with B450M?   \nI'm thinking of buying 6650XT (They are the same price in my country) but 16GB VRAM makes me think of ARC A770. By the way I will buy Ryzen 5500 or 5600 in future. Want to make sure everything just works fine.",
    "comments": [
      "Get the A770, it will only get a better with driver updates. and having 16gb of vram it should last too.",
      "IMHO it depends on what you play and what resolution you play at.  The 770 is best at 1440P, and could potentially last longer due to higher specs and RAM.  The 6650XT does OK at 1440p, but best at 1080p.  YMMV, but I'd go for the ARC.",
      "The board needs to support resizable BAR (REBAR). Do some research on your board to see if it does.  It will work if it does not, but you do not get all the performance of the card.  If it does, you're good.  Otherwise the 6650XT is a baller card for the money.\n\nEDIT:  additional details!",
      "The guy who is selling the card told me to check. He said that it won't work with some motherboards.",
      "I just checked mine and it seems I can enable REBAR. Do you think A770 is worth it? As I have seen on youtube it seems like I will face random fps drops and low performance in older games. Should I just go with 8GB AMD? I'm expecting to use my new GPU next 5 years.",
      "The a770 is borderline PS5 level performance. If you're gonna hold on to your GPU for a while, I think everyone agrees that 8gb is not enough. ARC 16GB is the better option",
      "Wow I wasn't expecting to see such result with RT on. Thank you for your comment. I think I will go with A770 - Ryzen 5600 combo.",
      "Thank you for comment. I checked with GPU-Z and it says supported. I think I will buy A770 but still have some questions like why the card works so hot in youtube videos.",
      "I've had more issues with amd cards than my a770. Rx 570 only used 4 of 8gb vram, 5600xt green screened and black screened, R9 290 straight up died on me. HD7870 was the best amd card I ever had, got a 1060 6gb after that and every time I've used an amd card since then it has been no bueno.\n\nThere's some weird little issues with the arc, but I've been using mine since just before Christmas. It's actually ridiculous that 3 amd cards have caused me more grief than the first gen arc, it was not an opinion I expected to have after 6 months.\n\nIt's as good if not better than my 3070 laptop gpu (don't judge I didn't buy a gpu mid pandemic, I'm the winner here lol) meaning games like control or red dead 2 the arc tends to do better because of the dx12 and high vram, but I played Half Life 2 and it was much smoother than it was 6 months ago, and I made a game with unity in dx11 and that went fine. \n\nBiggest peeve with arc is that it has issues turning on the monitor or will throw a momentary error regarding the hdmi output, but when the monitor says 'there was a problem with your hdmi device' and it works just fine a split second later, you're more confused than anything.",
      "You will probably need a bios update if you are getting a 5600 anyway, so yes, you should be fine.",
      "The fan curves was adjusted (for the better) in the last driver update.",
      "why shouldn't it be compatible?",
      "The card works fine without ReBAR support. I installed an A770 in my girlfriend's PC using her as a guinea pig of sorts to test the Intel GPUs. Her build is AMD and doesn't support ReBAR. She has a 2560x1440 144hz monitor.\n\nWhile you would get a performance uplift with ReBAR, the card does deliver around ~130FPS in games, such as Deep Rock Galactic, with ReBAR off.\n\nThe drivers started off quite rocky but they have definitely improved over time thus making the Arc series a great budget friendly option.\n\nTLDR; ReBAR is a recommendation, not a requirement.",
      "Intel arc a380 user here and i have a b450 msi tomahawk mobo with a ryzen 3 3300x with rebar enabled and the arc380 handles like a champ. I purchased the card for $101 at microcenter with 2 year warranty included and i have yet to see a game stutter or have any issues in 1080p with that card. Forza Horizon 5 in 1080p high settings with ray tracing and this card plays well over 70 fps without intel xess. With intel xess i get maybe 5-7 frames more and really makes the game visually stunning on my oled c1.",
      "' the card does deliver around ~130FPS in games, such as Deep Rock Galactic, with ReBAR off'\n\nPlease always mention resolution",
      "> As I have seen on youtube it seems like I will face random fps drops and low performance in older games.\n\nIf you play a lot of older games, it might be better to go with Radeon or Nvidia. You'll have to use google to see if the games you play are impacted, unfortunately, there's not really a compiled list of \"games that have problems with Intel ARC\"",
      "Sorry about that, it's 2560x1440.\n\nI'll update my comment."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Acer Arc A770 Predator BiFrost GPU with 16GB VRAM drops to just 259 USD - VideoCardz.com",
    "selftext": "",
    "comments": [
      "Still £400 in UK and almost €400 in major EU markets 🥴\n\nVery good strategy to make the 4060Ti and 7700XT look like good value. They're doing the AMD move. Only lower price in US and maybe in a couple months it'll be lowered in other markets",
      "Yeah EU and UK pricing is not very attractive when it comes to Arc cards. The A580 probably won’t be available for another month or so in majority of EU just like what happened when arc initially released. Still not see any listings anywhere.",
      "Really where it should be for an inconsistent rival of a last gen x60/x70 tier.",
      "wtf",
      "For anyone who doesn't know, with last weeks updates it now run starfield moderately well at 1440",
      "Yep, the title says USD not EUR or GBP. Hope that helps."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "a770"
    ],
    "title": "Intel Arc Graphics A580 / A750 / A770 Linux Performance For Early 2024",
    "selftext": "",
    "comments": [
      "This subreddit is in manual approval mode, which means that **all submissions are automatically removed and must first be approved before they are visible**. Your post will only be approved if it concerns news or reviews related to Intel Corporation and its products or is a high quality discussion thread. Posts regarding purchase advice, cooling problems, technical support, etc... will not be approved. **If you are looking for purchasing advice please visit /r/buildapc. If you are looking for technical support please visit /r/techsupport or see the pinned /r/Intel megathread where Intel representatives and other users can assist you.**\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/intel) if you have any questions or concerns.*"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "a770"
    ],
    "title": "Intel confirms upcoming Arc desktop SKUs: A770, A750, A580, A380 and A310",
    "selftext": "",
    "comments": [
      "Did they also leak in which decade they'll be released? /s",
      "No way. It's 3070 level hardware with unpolished drivers. Expect 3060Ti real-world performance.",
      "Good luck guys.",
      "Intel should’ve been very clear about the release timeline. I think they still can be transparent about it.",
      "IIRC the high end is supposed to be up around 3080 tier",
      "This century. Stay tuned.",
      "They already went transparent with it. \n\nThey told that early summer desktop GPUs will be launched as OEM only in china. \n\nThen in late summer launch OEM only worldwide. \n\nThen \"later\" launch as standalone GPUs.\n(So one could expect October/November for this)\n\nOverall it seems like they really really want to have most of the driver issues sorted out before releasing standalone GPUs.",
      "Do we have any idea how they will compare to existing gpus yet?",
      "There are low end GPU out there, it is just the value is so bad that people ignore it. With possible bad driver due to first launch, it will be irrelevant",
      "Existing in may 2022 or April 2024 when they are actually available? 🤣",
      "You expect the first iteration of GPU to meet the halo products of established GPU manufacturers such as AMD/NVIDIA?",
      "the lack of vram makes me sad, though i'm not sure what i should've expected from the bus widths. wish 8-12gb was standard because 4gb is proving not to be enough and 6gb is next",
      "And I honestly agree with their way of going right now. Why do people care so much that it only gets released in China. Makes no difference if they release it worldwide now, or later.",
      "Those were likely early laptop performance leaks at reduced clocks.",
      "Yes. It has enough transistors at like 22 billion and already has RT performance better than NVidia or AMD. It's tier 4 vs Tier 3, and Tier 2 for AMD.\n\nIf you include the games where it crashes due to drivers to drag down averages it'll be like 3060ti levels.",
      "It's because Raja Koduri promised stuff. \n\nHe promised the GPUs will be ready early, in Q1 2022. \nTurned out to be a lie.\n\nThen he promised GPUs in the hand of gamers for cheap - again a lie because they prioritize OEM.\n\n\nPeople say Intel GPU will be irrelevant if they launch after Nvidia launches RTX 4000 and AMD Rx 7000. \nI don't think so because all the Nvidia and AMD products from new gen are gonna be 600$+(real price, not msrp).\nIntel's best GPU is supposed to be under 500$.",
      ">He promised the GPUs will be ready early, in Q1 2022\n\nHe actually said that? When?",
      "I don't know for sure if it was him but there were messages from Intel that arc GPU lineup will roll out in Q1 2022."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Should I get an Intel Arc A770? I heard mixed reviews about them, but apparently it’s on par with a RTX 3080, and I see it at Microcenter for $350.",
    "selftext": "",
    "comments": [
      "Hahahahahahah it’s not even close to being on par with a 3080, this website you are looking at is a joke",
      "What? No. No to all of this.",
      "No - all glued together, cant do maintenance.\n\nhttps://www.youtube.com/watch?v=N371iMe\\_nfA&ab\\_channel=GamersNexus",
      "Honestly bro, not even...",
      "I found it hard to believe as well, probably gonna get a 6700xt",
      "Absolutely not. Intel Arc sucks. Stick to the tried and true. Get Nvidia.",
      "It depends. In some scenarios it's on par with a 3070TI, in AV1 encoding it beats the 4090, and in other scenarios like some DX11 games it gets sub 30 fps at any resolution.",
      "Yeah don't be. It's weird. I got an A770 for free, and I use it sporadically and just give up after drivers inevitably cause some fatal issue.\n\nI used it for a while, then it stopped displaying.\n\nThen I tried again and logging into windows would hard crash the system\n\nThen it BSODed over and over.\n\nThen the driver installer crashed the system so installing drivers was hard.\n\nThen the system just crashed.\n\nThen it was unstable under load... at stock levels. Undervolting helped delay crashing, but never fixed it.\n\nAfter that I've just given up for now. Maybe I'll try again, but it's ridiculous. I want to use it over my 3070TI for playing AAA games at 4K since it's got 16gb of VRAM, but it's so randomly unstable it just doesn't work for me.\n\nOh and despite all the random issues it outperformed the 3070ti in shadow of the tomb raider at 4K. And when I got it to transcode some stuff, it did it twice as fast.",
      "On par with RTX 3060...",
      "wow... reminds me of an apple product. hard, HARD nope",
      "No way!",
      "If your going to spend 1129 on a 3080 just spend the extra 80 and get a 4080",
      "Yeah use case is a huge factor, but overall I'm not set on the Arc just yet...",
      "Only if intel started making gpu few years ago… would gone into crypto market earlier.",
      "Lol nowhere near the performance of a 3080.",
      "There will be a new crypto boom on some new stupid token. There always is.",
      "Not really, more like 1060.",
      "I really hope intel take their 2nd generation seriously. They could claim a part of the market just for budget cards",
      "They trade blows especially on anything with ray tracing very similarly to a 3070 or 3060. It isn't a bad card.   If you want ray tracing, it is better than that than 6000 amds.   It is a beefy card that might get better with better drivers.",
      "or AMD"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "How to enable VRR on my Intel Arc A770 16GB via HDMI 2.1? My Q80A is in game mode and all, but VRR is always disabled and the Control Center shows no info on how to enable VRR.",
    "selftext": "Anyone having a similar problem? I don't know what to do anymore, I tried all the options I could, and it has nothing to do with my Samsung Q80A.",
    "comments": [
      "Is this even supposed to be a feature?\n\nIntel has stated that the HDMI 2.1 output ist not native, but is via DP-to-HDMI conversion with a dedicated adapter, just like all the external adapters. And so far none of them has supported translating Adaptive Sync to HDMI VRR.\n\nAlso Intel has largely equated \"VRR\" with DP Adaptive Sync, so unless Intel stated somewhere explicitly that they support HDMI VRR, I think they might still only support DP Adaptive Sync. And unless some DP-to-HDMI 2.1 adapters come out, that support HDMI VRR you will probably not be able to do that.",
      "Maybe it's a bug, report to intel.",
      "Download intel graphics command center and enable it there.",
      "I'm having the same issue with DP and HDMI.",
      "Having the same issue, using a Dell S2721DGH monitor, can't get VRR to work on HDMI or DP with a A770.  No adaptive sync options at all in the ARC control center and installed Intel Command Center has enabled adaptive sync but it made no difference. Freesync and Gsync works fine with Radeon and GeForce on the monitor.",
      "That's odd. After I installed my a750 my vrr was on. I used the OSD on my LG C1 to confirm VRR was active. I am using the hdmi port and a 2.1 cable that I used with my previous gpu. Maybe this can help, not sure if the option is greyed out for you. [https://twitter.com/CapFrameX/status/1587377690977394691?s=20&t=EaKEf9Gs3wLJdgfJ1s6YNw](https://twitter.com/CapFrameX/status/1587377690977394691?s=20&t=EaKEf9Gs3wLJdgfJ1s6YNw) . I did go in and enable asp, but in the command center it was under preferences global settings for me. [https://gyazo.com/collections/1d34c8ccb9c8d421f5c5a1aa617a4e54](https://gyazo.com/collections/1d34c8ccb9c8d421f5c5a1aa617a4e54) screenshots of it and where in the app you can see if it applied. Hopefully this helps!",
      "I have the QN90B and I'm able to use VRR over \"HDMI 2.1\" aka DP 2.0 to HDMI 2.1 PCON there, although it isn't working flawlessly. I tried out some games and I discovered, that most aren't using VRR at all, which seems pretty weird. The QN90B auto detects 4096x2160@120Hz with HDR and VRR. Games I tested and which seem to work with Arc HDMI VRR:\n\n\\- Metro Exodus Enhanced Edition: VRR not working- Assassin's Creed Valhalla: VRR not working- Riders Republic: VRR not working- The Witcher 3 Next Gen Update: VRR working- Dishonored 2: VRR working- Deathloop: VRR not working\n\nI thought, that it might be the launcher with overlays or something like that (because W3 and D2 are the only games I play on GOG), but I own D2 on Steam too and it's working fine there.\n\nAs my QN90B supports HDMI Forum VRR and Freesync Premium Pro I've decided to connect my primary PC with a 3080 over HDMI and enabled G-Sync (compatible, so it's using the Freesync Feature) and tested AC Valhalla which wasn't working before and now it did without any problems.\n\nW3 and D2 are working as expected so I'd assume that there's something going in on the drivers side, which prevents the card from sending frame-info to the monitor (?). The two games should proof, that VRR over the HDMI 2.1 PCON is indeed possible and that Intel should look into that.\n\nI'm already in contact with their customer service on that case and they've said that they're going to test it internally, after saying, that 4096x2160@60Hz is max for HDMI on the Arc and that I'd need a DP to DP connection in order to use VRR, but I insisted that it seems to be perfectly fine on W3 and D2 and I've sent them some pictures and system reports too.\n\nI hope they can figure something out on that one (well... NVIDIA already did, so the ball is in Intels court now).",
      "Check this out, SHOULD support all vrr techs, but unsure on hdmi\n\nhttps://www.extremetech.com/gaming/338376-intel-arc-tech-talk-focuses-on-vrr-hdr-and-hdmi-2-1",
      "This is weird because my LG CX was displaying it was using VRR upon first boot of the A770 until I until it off in the graphics control panel.",
      "According to the HDMI Forum, HDMI 2.0 has been replaced by HDMI 2.1. Technically all new products are HDMI 2.1. But as this number is basically just the version of a PDF, it says next to nothing about the supported features.\n\nOfficially, manufacturers should have to list all of the optional features of HDMI 2.1, such as VRR, max resolution or for example the maximum bandwidth (48GBit/s FRL would be the maximum).\n\n>You cannot use version numbers by themselves to define your product or component capabilities or the functionality of the HDMI interface.\n\nhttps://www.hdmi.org/spec/hdmi2\\_1\n\nAnd yes, many manufacturers use \"HDMI 2.1\" to indicate DSC, 48G FRL and VRR support, but there have also been others that have not. Customers expect this, but it is not guaranteed by anyone and just a common assumption, just as with USB versions like USB 3.2.\n\nJust as an example, Intel's PR representatives have stated that the HDMI 2.1 output is achieved via dedicated DP adapter and Intels UHBR10 Displayports only support a max. bandwidth of 40 GBit/s. So it is already impossible that the Arc GPUs would support the full 48 GBit/s worth of bandwidth plus DSC, because they already need to use at least part the available compression to fit this through DP.\n\nThe saddest part is, that for Intel's iGPUs the detailed specs are public and list all those details, but for the Arc A GPUs, the only official specs on Intel's website only state the max supported resolution via HDMI as 4K (available natively) , even though Intel itself sells variants of that GPU with better HDMI-ports. So unless there are already other existing DP-HDMI converters that support VRR (to my knowledge, those do not yet exist) I would not assume that Intel has such a thing, if they never marketed the existence of such a new functionality anywhere...",
      "Is it working for you on every game? \nI just found out that it also works with The Division 2.",
      "Situation is still the same, but they are aware of it afaik.",
      "Support rep I've contacted back in January said that they are investigatig this situation based upon my findings. That's all.",
      "nVidia -dunno about AMD- supports VRR via HDMI 2.1, which is to be expected tbh. As for adapters, I could buy one but I fear it might not work and also I rather prefer if they support the feature on other ports, not only displayport.  \n\n\nOther than that, the A770 16GB is one heck of a GPU, specs wise, it's really working surprisingly well with my games. Older games might need some tweaking in some cases, but still... A great gpu.",
      "I got the most recent drivers, the beta ones, and the Command Center shows an option for Adaptive Tessellation, but nothing about Adaptive Sync anywhere.",
      "hope they add this feature, because nVidia and AMD allow VRR to work through HDMI 2.1. I just use Smooth Sync for now, but while it works great, it isn't the same as VRR.",
      "thanks for the help. But alas, it didn't work for me. I event went to install the beta version of the app, but to no avail, although a new option appeared regarding games compatibility.",
      "great info! Good to know that you got in touch with them, because knowing that VRR works perfectly fine with other GPUs shows that maybe the drivers are to blame and Intel might have to work on it.\n\nVRR works superb on my 32\" 165Hz monitor, but on my TV, that's a totally different story.",
      "So I know what's causing problems with VRR.\n\nFirst of all for OP: Are you connecting your Arc via HDMI 2.1 to Input HDMI-Port 4 on your TV? It's important to use HDMI-Port 4, as the other ones don't support 4K@120Hz. Even if you plan to use 2K@120Hz on every other port I could imagine that there isn't enough bandwidth for VRR and that's because:\n\nIntel Arc is using full 10-bit color depth instead of 8-bit dithering. Most people trying to get VRR running are using a monitor with DP 1.2 and a refresh rate over 120Hz, which isn't enough bandwith for VRR due to the color depth that Arc uses. In that case the users would need to reduce the refresh rate manually to 120Hz (maybe 144Hz works, can't confirm, as I don't have DP 1.2 on my monitor). Disabling HDR could also help to reduce the bandwith.\n\nFurthermore:\n\nThe games that I tested have different full screen implementations. I noticed that older titles (like before 2010) all work with VRR but newer ones are hit and miss.\n\nSo I deactivated the full screen optimizations for those particular game. This can be done by right clicking on the .exe of the game and select sth like \"disable full screen optimizations\" in the compatibitlity tab (could be named differently, because my Windows is set to German).\n\nThis fixed my VRR issues on all the games that weren't working properly before.",
      "does it still work for you via HDMI 2.1? 'Cos I don't see the Adaptive Sync option anywhere in the Command Center."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Refurbished Acer Predator BiFrost Intel Arc A770 16 GB - $220.79 [eBay]",
    "selftext": "",
    "comments": [
      "I wonder why they have so much refurbished stock. 67 already sold and more than 10 for sale. Is there a common part going faulty in this model?",
      "Good question",
      "Or were they mining? High stress on VRMs 24/7 etc",
      "Returns more likely. Disappointed customers etc. It is not for everyone."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Arc a770 or Arc a750?",
    "selftext": "I'm looking at building my first pc and I want to go with an intel arc graphics card. I don't need crazy performance, but I want something that can run the latest games and will last for a decent amount of time. \n\nWith that being said, I'm debating between the Arc a750 and Arc a770. I can buy the a750 for $220 on Amazon and the a770 for $460 on Amazon (also have a bid on ebay open box with a maximum bid of $350). What is your recommandation?\n\nThank you!",
    "comments": [
      "The A750, it’s not even a question. You’re getting 92% of the performance for half the price.",
      "Just get a750, there is no point of 770 . a750 is 90% of a770",
      "A770 should be around 350$ and I'd go for it, I already have an A750 and I feel like I'm gonna need the extra VRAM in a not so distant future.",
      "The problem with that is the RX6700XT exists for the price of the A770. The A750 is easier to justify the cost of.",
      "Fair enough, my A750 is unbeatable in price/performance.",
      "If you want longevity and insist on an Arc card then id suggest the 770 over the 750",
      "I have the 750 and like it a lot but the only game I play is a modded GTA V with video quality maxed out. I still get 40fps",
      "I went for the 750 with my Ryzen 7 5700X as it had the two items I was looking for. 1) An 8-core processor, and 2) good 1080p graphics.\n\nWhat is now being raised as an interesting question is how much 8MB VRam is useful in today's GPU - but for low spec gaming - I can't see how that will affect me for the next 2 years",
      "Out of the two, I'd reccomend splurging on the A770. But either way, be prepared for possible issues with different apps, including games. I was an early adaptor if the A770, bought two different models, and had so much trouble I sent them both back. But from what I understand, Intel has come a long ways since then and they're looking like a pretty good option with the understanding that you may run into some snags along the way. Good luck and have fun Mate!",
      "A750 is way better since it's almost all the performance for a much cheaper price. But 16GB of VRAM on the A770 I think is mandatory for a new card in 2023 unless you're planning on playing in 720p.",
      "When you say low spec gaming, do you mean 1080p?",
      "Yes, when compared to 4/5/8K @ 150fps average.\n\nOne thing that I didn't include in the op was my need to be as efficient as possible with the power.\n\nHaving an 8-core at 65W met my requirements for the CPU, however, researching power draw on graphics I found that the higher resolutions require more energy. \n\nI then had to make a choice based on voltages against price - and price won out with the 750."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Intel ARC A770 + Ryzen 7 7700X | 25 Games Tested at 1080P & 1440P",
    "selftext": "",
    "comments": [
      "Not really how it works because it depends on the design, if the design isn't efficient, even with more physical units, you'll have some of them just standing there with no use, same happened with FURY and VEGA cards, that's why they weren't that good for gaming",
      "The A770 is nowhere near the 6800, it is actually WAYYY behind, catching only in terms of ray tracing",
      "having better potential or better computational power never had much to do with gaming, The same can be seen when comparing VEGA to RDNA2. Now, Spider-Man remastered is heavily CPU Bottlenecked in most scenarios at 1080P, so the 1440P and 4K are usually the real GPU tests",
      "This gpu is not still utilized fully by drivers.",
      "There is no consistency in testing. Showing one scene in 1080p and a different one in 1440p tells me nothing about the scaling. You need to isolate for the variables, not introduce new ones. This is why I stopped watching most of these YouTube CPU/GPU comparison videos. If at all they aren't completely faked, the methodology is flawed.",
      "Agreed, but this is not and never was the point of the video. These are gameplay benchmarks, for actual benchmarks with super accurate data I have my GPU comparisons, which will come soon with the A770 as well, including Rasterization, Ray Tracing, FSR and even XeSS tests",
      "Xess is much better than FSR 1.0, and FSR 2.0/2.1/2.2 are aexcellent in most games, they're nothing compared to previous FSR 1.0 and DLSS 1.0",
      "Vega did really well in Doom. Actually scaled with its shaders.",
      "Indeed it is because ARC GPUs love Vulkan, the thing is the DXVK still has considerable stutters with these GPUs",
      "Looking forward to it.  Enjoyed this initial run through.  I am waiting on my ARC GPU i am very curious how it will perform and on XeSS.  Of all these upscaling technologies I have only tried FSR 1.0 once on the Riftbreaker demo as a curiosity on an APU 4650g so this should be quiet the difference.",
      "Why buy Arc GPU riddled with SW issues and inefficient while you can buy a faster radeon GPU for the same price?\n\nSeems so illogical to me.",
      "Good to know",
      "Yah, that's explained in the beginning of the video",
      "Mind linking that Techspot article? I can't find the latest driver result",
      "Not if you use the async version of DXVK",
      "It’s not even the latest ones https://www.techspot.com/review/2542-intel-arc-a770-a750/",
      "It isn’t, when you see specs the A770 has a better hardware than 3070 and even RX 6800 (not die size and transistors but rops and cores for example). Aswell, A770 have a bigger Die and more transistors than the 3070, so fully optimized Arc should be above them or at same level. New drivers apparently already bring the A770 above the 6800 by Techspots",
      "For Spider-man remastered*. Also if you check each hardware, the a770 has better potential.",
      "The more hardware you have the more you can exploit with drivers"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Arc A770 16GB Question",
    "selftext": "I am ready to pull the trigger on the A770 16GB card at Newegg for $349 to replace my GTX 1070. I currently have an 8700K and Z370 Taichi that I just download the BIOS with Resizeable bar support. In a couple of months I will be upgrading to a 13700K and MSI Tomahawk DDR 5board I wanted to. My long winded question is should I get a GPU sag support bar for this card?",
    "comments": [
      "It's not that big of a card. You don't need a support for it.",
      "Don’t listen to people who say it’s too much CPU and such. Buy whatever CPU you want, but if it’s a high end one like 13700k, then it’s only for the better. You will get better 0.1% lows and it’s a guarantee of a less bottleneck for a better GPU you will buy in the future.",
      "Yeah. I do mostly productivity work with multiple VMs. I only do occasional stress relief moderate gaming. I just want to replace a 6 year old 1070. The 13700 is going to happen anyway.",
      "By the sounds of it they don’t seem to be using the gpu to do any actual work except for providing video output.\n\nThere’s not really any “stability” issues with arc.. there’s some minor things like it not turning your monitor on when it wakes from sleep properly but that’s probably the only bug I’ve experienced in quite a while outside of a game.",
      "Idk what people are on about… there’s literally no such thing as “too much cpu for your gpu”. \n\nIf you want a budget performance level gpu, your current setup should work fine with the a770 and the new setup when that comes should be better even if the gpu is going to be the limiting factor. I’m currently using an a770 with a 9900k in a z370 asus board without issue.\n\nAn a770 isn’t that big and heavy, but I wouldn’t say you need a support or not without seeing it in the case and how well it’s supported by the case from the bracket.",
      "Any reason to not get something like a RX 6700xt for better drivers and better rasterization performance?",
      "Isnt arc getting good reviews for blender type work?",
      "There is no sag with this card.",
      "Nope",
      "Thanks",
      "Are you sure you want to use an Arc card in your work machine? It’s fine if you want to use it in a side build as something to play around with, but I don’t think the drivers are in a state where you should be doing actual work on it.",
      "The 13700k is a great CPU for that workload. The more cores, the better.",
      "It’s quite a bit slower than an RTX3060 in Blender\n\nhttps://www.pugetsystems.com/labs/articles/intel-arc-a750-a770-content-creation-review-2376/#Blender",
      "bust out some extra $$$ for the 13900KS. get one A770 and purchase another later for multi-gpu situation. No on else except intel, will be doing multi-gpu going forward for DX12.\n\nso far, I can't find one title, this gpu can't handle over 60fps in 4k (mostly \\~120-240fps in 4k full detail.)",
      "I'm just going to leave this here :\n\n[https://www.techpowerup.com/review/intel-arc-a770/32.html](https://www.techpowerup.com/review/intel-arc-a770/32.html)\n\nUsed 3070 and 3070ti's going for cheap if you look around a bit.",
      "The 13700K seems like too much CPU for the A770, I’d get an i5-13400, i5-13500 or Ryzen 5 7600, and even that’s excessive - probably a Ryzen 5 5600 or i5-12400F would do, of which the former is the better buy.\n\nI’m also not sure how the A770 has aged, but the RX 6650 XT might still be an overall better buy.",
      "13700K is way overkill for a GPU that has basically \"only\" GTX 1080ti performance, which will be considered low end this generation.\n\nYour 8700K is adequate for arc a770."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Intel Arc A770 good for 1440p 60fps in modern games?",
    "selftext": "Current GPU is a 980TI which struggles with even GTA 5 at 4k.\n\nI am looking for a stop gap upgrade until the 5090 comes out in 2024/25. My goal is to get at least a stable 60fps in Starfield and run stable RTX mods on minecraft (for the kids) and keep the budget around $500. Also I am looking for the best card with the most VRAM for the money within this budget as well which is why I saw the A770 with 16Gb Vram.\n\nIf it can get 4k60fps on games like Hitman, GTA 5, Flight Simulator. That would be a bonus.\n\nMy other specs   \nAMD 5900x\n\n32Gb ddr4\n\n&#x200B;\n\nIf you guys have any other recommendations within that budget please let me know. \n\n&#x200B;",
    "comments": [
      "In most games I play at 3440x1440 I can get 60-80fps with ARC A770",
      "Intel Arc are very price competitive and have full support for all new titles released on DX12 and beyond. The important improvements are for DX9 which they've done this past March 2023. No more native DX9 support. Instead they got a DX12 to DX9 translation going on. It looks to be solid. League of Legends and CS:GO titles perform much better now.\n\nhttps://youtu.be/ecfV17wrjfs&t=12m00s the whole video is worth watching but this chart summaries the important bit. Price to performance competitive. So it makes a really good stop gap for you and a GREAT hand me down to your kids and their gaming future.\n\nI would buy one base on the price alone. price competitve ray tracing gaming!",
      "I’d wait until Starfield benchmarks are out before deciding.  They didn’t do a great job in system requirements so there is not a good way to even aim for the target right now (if you really care).\n\nIt may be relatively inexpensive, only work on one vendor, or not even be possible with a 4090.",
      "You can try find a old 3080 for around $400ish if you are lucky. That would be minimum for 1440p 60f for new games. They are just not very optimized.",
      "Not even close in price. That card costs triple what an A750 costs but it's not 3x better.",
      "Problem: Starfield is AMD sponsored",
      "Well, at least FSR is platform agnostic. Hardware XeSS is actually noticeably better",
      "If you're looking to do Minecraft RTX, it'd be best to look at an Nvidia card, like an RTX 4070 or better.\n\nhttps://www.tomshardware.com/news/minecraft-rtx-gpus-benchmarked",
      "If your games supports XeSS then yes. Source: have a750 and it struggles without upscaling.",
      "Star Wars Jedi Survivor has no problem playing at 1440p 60.",
      "XeSS is also available on competitors’ hardware, it just gives less of a performance boost.",
      "4070."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "a770"
    ],
    "title": "Intel Arc A750/A770 will be available at 6 Micro Center locations",
    "selftext": "List:\n\n-\tDallas, TX\n-\tHouston, TX\n-\tChicago, IL\n-\tWestbury, NY\n-\tDenver, CO\n-\tOverland Park, KS\n\nSource: https://game.intel.com/story/intel-arc-graphics-release/\n\nEDIT: Looks like the A750 only, I don't see any A770s even listed at these locations.\n\nEach location seemingly only received seven A750s and no A770s. So the entire Micro Center chain received 42 Arc 7 GPUs lol",
    "comments": [
      "What went so drastically wrong that it seems only a few hundred GPUs were made available? Intel certainly is no stranger to product launches, I wonder if there was pressure just to launch something at this point.",
      "Yep that seems to be the case. The fact that they only had stock at a handful of Micro Center locations is a pretty big red flag. Meanwhile my single Micro Center location had over 120 4090s this morning.",
      "About par for Intel with how the Arc GPU is going. So many people want them to succeed after being fleeced by Nvidia and AMD over the last couple years, but that just can't deliver on the demand that is there waiting... I almost went to my local Micro Center in Fairfax, VA but glad I didn't waste my time",
      "All these cards were made back in Q1, only 4 million arc cards were made overall and that covers the entire stack from a380 to a770 (not sure if that is just discrete cards or whether that includes laptop gpus as well).\n\nIt's pretty much a case of get these cards from somewhere now because they ain't coming back (great for gpu collectors though)",
      "Wow, that's a lot of 4090s...who has the money to blow on a 4090?! I was hopeful with the 770 to finally get a GPU with good specs at a reasonable price. Gonna have to hope my 1070 holds out a little longer.",
      "No 770 at overland park.. i hope there are some",
      "Nice, did you receive a software bundle email from Micro Center?",
      "In the UK, only seen listings so far for just one major online retailer and the big pc one (overclockers) , haven't heard anything from them. There is zero volume but hopefully it's the sort of thing where it'll be available on off for a few weeks on Newegg for you guys",
      "It seems I did. I looked back at the receipt and I got COD MWII for Free.",
      "I hope they have more than 100000 gpu in stock.",
      "I stopped by the NJ Microcenter in Paterson. They said that they just did not receive them yet and to check back tomorrow. I also called their support line and it seems like they don't really have any answers.\n\n\nEdit: Spoke with a store manager and they confirmed that Intel barely sent any stock. They suggested to keep an eye out on the site and to check back in on delivery days (wed,fri). They also said they did not receive any promotional material, what’s going on Intel!?!?",
      "What a 🚀 launch Intel!",
      "I actually just got one. They just received some at the Dallas Store. The funny thing is that no one here seems to want one.",
      ">certainly is no stranger to product launches, I wonder if there was pressure just to launch something at this point.\n\ni heard they didn't want to sell directly to people because there is no margin. The plan is to try to bundle with systems, and cpus to make money.\n\n\\>> The source is moreslawisdead and some one line articles. Couple. Nobody is on the record but is the likely explanation. \n\n&#x200B;\n\nMy take:\n\nSo i'm cool with companies making money and such, but they annoying thing is this isn't' how ti was marketed, and i'm kind of annoyed about it tbh. wasting our time with a marketing gimmick if true. \n\nMy guess: it's all about money though they have cards. they are trying to look like a release, part marketing stunt, and part use it as a loss leader as part of the stunt to sell more computers etc. Right because once you bundle the idea is it can help sell some chips.  I can see the point.   \n\n\nSo weird to have a release of product like this that you don't intel to really sell, so you don't believe it in but your customers do. Never seen anything like this i can think of.",
      "That's a lot of foot warmers.",
      "that's what i feared, i would happily buy one if I could get a 770",
      "Just to put this all into perspective, Nvidia + AMD shipped together something around ~50 million desktop units for 2021. This is likely 4 mil all in, including mobile.",
      "So, if they produced these back in Q1 is production done or what? Surely, we'd see more form AIBS etc later on. I have seen rumors about how the entire gpu division is done for. I am having a hard time reading the market and seeing if they are in this for the long haul or not. I ask bc what's the point of fixing this if they aren't?",
      "No, just walked in and got the card no bundles.",
      "When gamers nexus did his teardown, confirmed that that die was produced in Q1 and the rumours have been of a warehouse full of alchemist dies waiting for the software to catch up.\n\nAs far as I'm aware in terms of alchemist, that initial Q1 run was it, especially considering 6nm is in much hotter demand than it was back at the start of the year.\n\nThe gpu division isn't done for but I think it's going to be a very very long time before we see the arc division on parity with someone like amd where obviously they have the cpu side of the business, while also being able to make half a dozen different gpu dies for their products stack. Current rumours are Intel focusing on data centre and laptop (which makes a lot of sense) and then just keeping some sort of presence in the discrete market, so maybe just one gpu die for the battlemage launch to keep costs low. This is a awful market to be launching a new gpu into when supply is far outpacing demand.\n\nSo long story short, Intel is in the long haul for the graphics business but no one including Intel has a clue about them being involved in the discrete gpu space long term"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Intel Making Moves: Acer Arc A770 BiFrost GPU Review, Thermals, & Tear-Down",
    "selftext": "",
    "comments": [
      "Hey! We got a board partner",
      "well nice to see acer in the race 😎👍🏼",
      "Lol Acer really cant help but put aeroblades on every product. Not gonna complain though, it looks cool, but the rattle and noise are genuine concerns from their laptop range.",
      "With the latest Performance test (actual driver) i will say. For the Price performance it is really more interesting as nvidia or AMD in the mid range.  If i had not buyed last yeard direct on release date my AMD RX 6750 XT Referenz model this GPU will be more a 1st chose as any other brand atm."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Arc A770 display goes nuts during driver install?",
    "selftext": "Hi guys, doing a build for a friend, NZXT Z790 board / i7 13700k and his A770 just won't work. It works perfectly fine up until driver install, than during the install the screen just goes to this mess: https://postimg.cc/zyDfxBjS\nI tried everything, latest bios, chipset drivers and Intel ME firmware. All Intel recommended bios settings for Arc are enabled. Also tried 2 different driver versions. The DP cable works fine on my own PC, so I know it's fine. Any idea what to do? Could the card just have come broken?\n\n**EDIT: The display works fine with HDMI only, but same problem on every DP port. Hopefully it's just Arc not vibing with my DP cable, will test with another cable.**\n\n**EDIT: It was the cable! This card is very sensitive to cables obviously, this same cable has no issues with my RTX 2060. But all good, he is keeping the card.**",
    "comments": [
      "I had one for 4 days, it has worn down my sanity almost to a depression, i ended up returning it for the sake of my sanity. Intel needs to get their drivers worked on as their main goal, too ignored atm.",
      "If the display is fine until you install the driver, then card is not broken, try ddu in safe mode and then let windows install drivers on its own, restart and try again, if this doesn't work, get in touch with intel maybe?",
      "Welcome to Arc. The drivers were doing pretty well until a few weeks ago, when it seems that they broke something. Performance wasn't great and there were bugs, but at least they worked. \n\nI've got a laptop that was kind of obnoxious the whole time, but it's barely worked the last couple of weeks.\n\nSo with the new beta driver all of my problems have been fixed. Maybe give that a shot?",
      "I tried all that unfortunately. I can get into windows with on board graphics no problem, and uninstalling the drivers will make it work again, but then as soon as I try to re-install Arc drivers, same thing. You don't think there could be something wrong with the card? One thing that's odd, is that I also tried HDMI output as well on it and I can actually see the desktop in that case, but it flashes like a grainy overlay every second.",
      "What doesn't seem weird? It's stuck on screen like this forever. I don't mean it just happens briefly during the install (I wish).",
      "Yea, I think my next step is to try it in my own system and see how it goes. His PSU is brand new and pretty high end (Lian Li SP750) so I am thinking a power issue is unlikely.",
      "Ah pic didn't load when I originally posted",
      "Are you guys both on Windows 11? Was just thinking an easy way to test it across different systems and OSes quickly would be to use some of those windows live usb like Gandolf or Hirens https://alternativeto.net/software/gandalf-s-windows-10pe/\n\nThen you wouldn't have to touch your filesystem and you can test anything from Windows 8 to 11.\n\nKeep us updated. I am rooting for Intel here. Thanks for attempting being early adopter! I don't have the money or balls to but we need more competition in the GPU market. They have the money to get those drivers worked out. Hopefully they make an AMD rx 5/6××× style comeback with compatible, solid drivers.",
      "It actually turns out it works fine with HDMI on my monitor. None of the Display Ports work at all, but as I was looking up this issue I found out that Arc is very sensitive with regards to DP cables and other people have had similar problem. My friend hasn't come over yet to get his PC, but when he does I told him to bring his own DP cable, mine is from around 2016 so hopefully his newer ones works.\n\nBut I stress tested the card with some benchmarks and it works perfectly fine otherwise. He doesn't game, it's a work station only build (graphic design and some rendering) so the 16gb vram and price point of Arc is actually really good value for his case use. I'll update my original post when I found out if it's the cable, just so people don't get the wrong idea. I too would like Intel to succeed with their gaming GPU's as well.",
      "Update? Did you guys return it or get it working. I lose sleep over stuff like this, doesn't matter if it affects me or not...",
      "Hey dude! Yes! Display ports worked just fine when he brought it home. It was just the card not liking my cable (which for the record work perfectly fine with my RTX 2060). Glad he could keep the Arc. Here is a photo of the final build if you are curious: [https://imgur.com/a/BHOWAlZ](https://imgur.com/a/BHOWAlZ)",
      "Nice! Glad everything worked out. Clean build man!",
      "This doesn't seem that weird during display driver installs the default windows driver is loaded in between.",
      "Isn't it possible that parts of the card aren't utilized until the proprietary drivers are installed? I am asking, not challenging your idea. I remember like a gtx 260 or something like that I was messing with would not act up until I ran certain physx games or something. It was ages ago so don't remember specifics.\n\nMaybe try like PopOS with steam/proton if you Linux like that. \n\nAlso, maybe try a different PCIE slot, make sure PSU is powerful enough and re-seat the card and power to the card.\n\nObviously an entire different system would be the ideal way to rule out a bad card."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "How's the price of Arc gonna be in EU?",
    "selftext": "The Arc A770 should be around 399$/449$, but how does it translate in euro? \nI'm from Italy and I'm really looking into buying an Arc GPU, but I have a very big question mark on prices... 450€? I might buy it, but 500€ or more, not sure, I currently can find 6750XTs for 450€ :/",
    "comments": [
      "Well $=€ +tax from Italy ~20%? easy math I think you can figure it out yourself.",
      "The A770 needs to be a hell of a lot cheaper than that to be worthwhile, there are RX 6700 XTs going for $360-380 right now.\n\nIntel is basically 18 months late with this one, I wouldn’t be surprised if Arc never gets off the ground.",
      "399$? Maybe from a partner but Intel is gonna make their board cheap. Definitely cheaper than a 3060 ti. Most of us are hoping for below 3060 pricing.",
      "With those prices nobody will buy them in the present state.",
      "Intel is not AMD. They can afford to lose hundreds of millions of dollars. They underdelivered on alchemist, but Intel is not going anywhere. All the tech tubers and leak bullshit about Arc and future dGPUs getting axed was debunked.",
      "Yeah well intel has a big choice ahead of them. Give people a great deal in exchange for basically being a tester. Or give people an equivalent value to the competition and become irrelevant in the GPU sector.\n\nIf intel goes with the first one they can scrape all the budget oriented gamers and PC's market share away from AMD and Nvidia (Although Nvidia doesn't seem to even care about that segment anymore)",
      ">The Arc A770 should be around 399$/449$\n\nThis is still very much in the air. Intel hasn't released any pricing info on the A770 apart from assurances that they will be priced aggressively.",
      "A380 is already in Germany in few stores so you can check what's the EUR end user price vs USD to get the conversion rate for the USD MSRP of the 770. It's still unsure what decision Intel will make as they may for example back down with prices even more.",
      "In italia minimo 1000€, prezzo amico eh",
      "We are talking about regular pricing in Europe, not the prices some US retailers empty their warehouses. The cheapest 6700xt in Europe is around 550€. Edit: no, found one for 484€.",
      "Well, the A380 is actually the perfect server card. A1 & h.265 transcode that provides very good performance at the price range. \n\nShit gaming card at that price, but amazing for a media server.",
      "Understood, tnx for the heads-up, I really hope they'll back down further, 3060ti performance for like 350€ would be really good, also almost inline with a 6600XT",
      "A380 is in German stores? I haven't seen any yet!",
      "They can't sell at parity, they have to sell cheaper as there still are a lot of fixes and tuning to do.",
      "Mindfactory for 189 EUR as per recent news.",
      "Hmm, for janky 1060 performance i just saw. In my mind I thought it was 2060.thanks for looking it up. I was at Netbook billiger, but I'm steered back towards the Netherlands by Google constantly ;)",
      "Nope, it is unlisted now.",
      "I hope the prices will stay far below 450€, but we all know that these companies are not that into charities :D",
      "I hope so, AMD didn't do that with the 5000 series tho 😂",
      "Don't buy an Arc GPU ffs.\n\nIntel missed the boat. These GPUs are still janky, and they don't really offer a compelling value proposition compared to the existing AMD and Nvidia offerings, and AMD and Nvidia are in the process of releasing their next gen GPUs that will deliver even better price/performance options. Couple that with a used market that is seeing prices get crushed by a flood of used mining cards and unsold 30 series new stock.\n\nArc is going to be dead soon, the writing is on the wall. They didn't get Alchemist out soon enough, and it's likely to be the same issue **IF** they decide to do an actual release of Battlemage. I suspect Battlemage won't be released as a mainstream discrete graphics card, I think Intel will quietly smother the whole Arc project and you'll be left with orphaned hardware that receives next to no support from either Intel or game developers."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Arc a770 | PC not booting | VGA debug LED lights up on motherboard",
    "selftext": "I recently bought a new GPU arc a770 and it doesn't work. My PC turns on but wont boot. The screen shows no signal and I can see the **debug LED on my motherboard lighting up on \"VGA\"**.\n\n  \nPutting my HDMI cable on the motherboard didn't change anything (with the new GPU installed)  \nMy previous GPU will boot perfectly fine (GTX 1050)  \nI bought a new PSU because i had a terrible cheap one and I thought that was the problem but it didnt change anything. (the PSU listed is my new and current one)  \n\n\n**Specs:**   \n\\-Intel Core i5 9600K 3.70GHz  \n\\-MSI H310M PRO-VDH PLUS (MS-7C09)  \n\\-2x 8GB Patriot Viper Steel 3200MHz  \n\\-AsRock Arc a770 8GB GDDR6  \n\\-Aerocool Aero Bronze 750M 750W\n\nthanks in advance.",
    "comments": [
      "Try clearing the cmos to see if that will help",
      "Did you update the bios?",
      "I had similar issues with my A750 when I first installed it, my computer was acting like it wasn't even there, it would turn on and boot but no video (I thought it might've been a dead GPU), eventually after many restarts it started working.\n\nSince you have an iGPU, I would remove the dGPU, change the settings in the motherboard to enable the iGPU rather than it being set to automatic, DDU drivers in safe mode, put the dGPU back in, then install intel's drivers, I believe part of the reason I was having issues was that it had no drivers installed and I didn't have an iGPU to fall back on.",
      "I did clear it multiple times by removing the battery but nothing happened",
      "Yes its latest version",
      "Thank you for your suggestions, I tried them both DDU and Integrated GPU, unfortunately nothing changed.   \nDid your issue just resolve itself just by restarting?",
      "Yeah, many restarts later the display eventually worked with the GPU.",
      "Wish i was that lucky ;-;"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Witcher 3 ARC A770 LE Quick Performance Analysis - 120 fps without RT, but RT On is a stuttery mess!!",
    "selftext": "",
    "comments": [
      "> Source ? \n\nMe\n\n>Is this with latest drivers \n\nYes",
      "Isn't the game currently a mess on everything pc atm though?",
      "Honestly impressive then for the price, especially given that it hasn't \\*really\\* been optimized.",
      "Whats the resolution tho ?",
      "Source ? \nIs this with latest drivers https://www.intel.com/content/www/us/en/download/729157/intel-arc-graphics-windows-dch-driver-beta.html that released yesterday with new optimization for The Witcher 3: Wild Hunt Next-Gen Update.",
      "Doesn't support XESS, pretty big for Arc.",
      "2560x1440",
      "It is real performance, but it just fails to mention what the actual rendering resolution is on the chart.\n\n1440p FSR2 Quality is scale factor 1.5, in other words, 960p. Well, 960p with improved temporal AA. 4K FSR2 Quality is actually 1440p.",
      "Need an arc A970 for this",
      "Resolution?",
      "Have you tried running DX11? What is the result there for non rt performance",
      "Witcher 3 RT is destroying even $1000 newly released GPU:\n\nhttps://www.youtube.com/watch?v=HbFNQbXDfmE",
      "It kind of is, in DX12. Stutters galore. DX11 runs about 10% worse than previous patch, but it does have some visual upgrades so it's to be expected.\n\nRT features are also quite heavy, so losing half of your framerate seems more like a given.",
      "w8 - that does not exist... ?",
      "Nah I wish",
      "This RT update adds nothing to the game, probably sponsored by Nvidia . Like all the game works anti-performance effects.",
      "People really need to stop making these comparisons when using DLSS or FSR. It’s not real performance. 1440p ultra with FSR at only 120 FPS for a game released in 2015 and without any of the new ray tracing isn’t anything to brag about. The performance without FSR must be horrible. I don’t take any FSR, DLSS, or XeSS benchmarks seriously."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "With the Intel arc a770 or RTX3060ti I am torn.",
    "selftext": "I want to build a pc and I'm wondering between Intel arc a770 or RTX3060ti.\n\nI mainly want to play games (Apex Valorant simulation game).\n\n&#x200B;\n\nI also want to do some video encoding.\n\n&#x200B;\n\nWhich one would you recommend?\n\n&#x200B;\n\nI'm thinking of going with the i7 12700F CPU.",
    "comments": [
      "Unless you think Intel will improve the video driver software for the ARC Alchemist A770 video card by more than 40% in less than a year (No, they won't), the Intel video card is significantly less efficient for most programs compared to the RTX 3060ti. It has a hard time barely competing with an RTX 2060, RX 6600, or RTX 3060, and the initial testing of the card's performance is slightly above that of an RX 580 or just below a GTX 1660.\n\nThe A770 has hardware and software design problems that probably will never let it compete well enough to come close to an RTX 3060ti. The primary use you can use it for is encoding and decoding video, rendering files, and using tasks that take advantage of the 16 GB of DDR6 video memory. If you play games that are below 2k resolution and less than 120 FPS, then you will not notice many of the defects of the A770 card, but if you use those less demanding programs then why buy either an A770 or an RTX 3060ti, since, you can buy less expensive and equally competent video cards from AMD and Nvidia.\n\nNeither is the best value from either Intel or Nvidia, and currently AMD's video cards are the low cost leaders in every single market segment.\n\nWithout knowing anything about price, model, size, etc. from either the A770 or RTX 3060ti, I would always prefer the RTX 3060ti. The RTX 3060ti works in the same gaming range as an RTX 2080 Super.",
      "Get your self a amd rx 6600xtvand you will be happy",
      "Go with 3060ti Zotac AMP Trinity OC Edition (5 Year warranty) or with the ASUS variant (3 year warranty). This is best value for money = performance card in the market you could get right now, so get it.",
      "Thank you very much.\r  \nThe Palit RTX3060ti had a superior price (at a store near me), but considering stability, etc., would the ZOTAC be better?",
      "Check the difference between all the Zotac variations on the internet for 3060ti. Get the best one, you won't regret that card for atleast the coming 5 years. Hell I'm chucking my 1060 from last 6 years or so. It's good for 1080p gaming.",
      "It is certainly better to have a 5-year peace of mind, since it is an expensive item.\r  \nThank you very much!",
      "Intel GPUs recently received excellent driver updates, but it's still hard to recommend them. A 3060ti is a superb 1080p or 1440p card, and will easily handle e-sports games, especially with the 12700 that you are pairing it with. \n\nUnless you are getting the A770 for half the price of the 3060ti, just get the latter.",
      "I still think Intel GPUs are no good....\r  \nLet's hope for the next generation.\r  \n\r  \nThank you for your detailed information.",
      "\"Unless you think Intel will improve the video driver software for the ARC Alchemist A770 video card by more than 40% in less than a year (No, they won't), \" Oh how that has changed",
      "Thank you for telling us about it.\r  \n　(I'm sorry you had to learn that.)\n\n\r  \nIn my area, rx6600xt is about $40 more than RTX3060, so I will choose RTX3060.\r  \n\r  \n\r  \nBut the rx6600xt also consumes less power, so I'm honestly not sure lol.",
      "He said video encoding. AMD sucks at that. Great gaming cards but they aren't good at anything else really. \n\nAlso, at his price range, the 6700 XT seems more acceptable.",
      "Thank you for the details.\r  \n\r  \nI see that there are still many inadequacies...\r  \nI would like to purchase another Intel GPU when it becomes cheaper in the future.",
      "If you want to take the chance go ahead. The better perspective is to see if there are reviews and people on YouTube and online that have said that their Intel video cards are now equal to what was promoted. \nI stand by my opinion. But go ahead, and buy and take the risk. I notice that no one that has purchased the cards are saying good things yet.\n\nHere's the situation. Why even buy Intel GPU hardware when there are tons of video cards dropping in price every day now.",
      "Lol what he meant to say with his comment is that you're already wrong because they've shown massive improvements in the 2 months since you wrote your false comment....",
      "No. My statement holds even today. And I notice no one that has used an Intel GPU has said anything, and even your opinion isn't putting new data. Did anyone on YouTube test the new drivers to see if the promoted claims that the best cards A750, A770, or the special Edition A770 come close to the RTX 3060ti? Or even the RTX 3060 12 GB? No, not yet. I'm still waiting. You are still falling for Intel's shill propaganda which they have been spewing for the last two years.\n\nThe major improvements they upgraded to was the ability to play more DX9 games at better performance where they couldn't play many of those games at all. The average performance remains somewhere between an RX 6500XT and an RX 6650XT in terms of AMD gaming and when comparing to Nvidia the performance level is slightly above the GTX 1650 Super and the RTX 2060 12 GB card. That jump is not 20%. And I haven't even mentioned the things you needs to do to find out what games you can play and at what settings, because their Intel ARC gaming interface is divided between the Microsoft operating system video settings and Intel's own video software. (This is due to gaming API problems and the DirectX versions you are using for the games you want to play.)\n\nWhy don't you just watch the Gamer's Nexus video about the current status of Intel ARC video cards from https://www.youtube.com/watch?v=b-6sHUNBxVg\n\nMy opinions are NOT false. They never were and they still stand. And the 'massive' improvements are basically allowing games that use older video applications that use DirectX versions before 11 to be playable. That doesn't mean most games that are playable now have improved 20%, not even 10%. You won't like what they did to get the so-called improvements in some games.\n\nI gave you some backup information and even a video reference. That's not a 'lazy' opinion.",
      "here ya go! A video that isn't over two months old... [https://www.youtube.com/watch?v=teXtQ\\_hRXYM](https://www.youtube.com/watch?v=teXtQ_hRXYM) \\--  After recent updates, it's beating out BOTH of the cards you mentioned at pretty much all of the tested games in 1440P and 4K gaming... And this is just the beginning, with every driver update it will become better and better. With RT enabled.... the Arc 770LE absolutely demolishes the cards you mentioned, get your head out of your ass, please",
      "This just keeps ageing so well."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "a770"
    ],
    "title": "Current state of VR support on Intel Arc (A750, likely also applies for the A770 and A380)",
    "selftext": "I hadn't posted about this here yet, but I am a collector of VR Hardware, and when the A750 and A770 first launched in Germany, I was very eager to try out VR performance on as many headsets as I could.\n\nI personally bought the A750, mostly as I intend on mostly using the card for AV1 encoding in my Server, but might as well try VR on it as well I thought. Here's my findings so far:\n\nThe Intel Arc drivers currently **don't seem to support Direct Display Mode devices at all**. This is a requirement for VR headsets that use DisplayPort or HDMI to receive the image they're supposed to display. That means that I wasn't able to test any of these headsets, and I've tried a lot of them: Oculus/Meta Rift CV1, Valve Index, HP Reverb G2, Vive Pro 2 and the Varjo Aero (tho in the case of the Varjo, there was also a software lock-out, as they don't want their customers running into issues with unsupported GPUs, which also includes ones from AMD).\n\nOfficial Link support with the Quest (both with a cable and wireless) also wasn't working. Quest Link refused to find it's connection (the PC dialog never found the headset's USB connection and I never got the Quest Link prompt in the headset itself). and Air Link straight up threw me an error that the Video Encoder was unsupported. I have no idea what kind of magic other people have pulled off to get Air Link or Quest Link working on Intel Arc, I sure wasn't able to.\n\nI was however able to run VR on my A750 through 2 other means. For one Virtual Desktop basically runs on anything that has a DirectX 11 capable video output and a CPU, so that worked with no issues at all on Arc too, and I was also able to use my Vive Pro 1 using the Vive Wireless adapter.\n\nWith Virtual Desktop performance was quite stellar and I didn't really experience any stutters or similar. With the Vive Wireless Adapter however, I immediately started noticing hitching and stuttering every so often. The more CPU heavy of a game I tried, the worse the stuttering got, hinting that large parts of the stuttering may have to do with GPU drivers reliance the CPU.\n\nFor reference, I'm running a 5900X with 32GB of DDR4 3200MHz CL16 memory.\n\nIf anyone is interested in the performance numbers using the OpenVR Benchmark Tool, I'd be more then happy to provide them later as well. I've already run the benchmark, but the results I have not saved on my main data drive...",
    "comments": [
      "Yeah, this is true. Linus from Linus Tech Tips also faced this issue. Intel has responded saying they are in the alpha stage. They should be able to release beta drivers for VR support shortly.",
      ">The Intel Arc drivers currently don't seem to support Direct Display Mode devices at all.\n\nI am curious if the Direct Mode does work on Linux, since [the implementation is shared with AMD.](https://monado.freedesktop.org/direct-mode.html#intelamd)",
      "No unfortunately not, and I was not aware of it and bought a NUC with arc card to play specifically VR....",
      "I’m running the same specs as you plus an A380 (cause why not see what it can do with VR). Oculus/Meta Rift S didn’t throw me any errors, just said display port was disconnected even tho it was connected.\n\nOn the Intel discord, I’ve brought this up and have been reassured multiple times that VR isn’t software blocked nor hardware blocked. So I’ve been assuming that Oculus/Meta VR and Steam VR hasn’t whitelisted them yet for the direct connection headset. I’m thinking they’re waiting for the rest of the 40 series and 7000 series to release before doing a bulk VR whitelisting of said cards. *People with 40 series have been reporting the same problem so I’ve heard*\n\nFrom what I understand, the wireless alternatives ignore the gpu hardware check and goes by what the gpus can run as you said",
      "I know you probably won't respond but, since then have you tried again? And if you did has oculus link still worked?",
      "I'm curious, at a collector of vr hw, you likely have some great opinions.  I'm looking to buy an older lower cost but still serviceable vr computer to drive an oculus 2.  Only need to do something like Alyx at \"ok\" levels.  It's hard to discern where that sweet spot of age vs functionality vs price is.  If you've thoughts on those lines, love to hear them!",
      "still waiting :(",
      "As soon as a Mesa driver version with Arc support ships with PopOS, I will try that out. I'm just not familiar enough with Linux yes to feel comfortable modifying the system itself through the terminal ![gif](emote|free_emotes_pack|sweat)",
      "40 Series does work with all the headsets. Nvidia changed something with the Framebuffer handling in Ada Lovelace that causes heavy stutters in VR at higher resolutions if not accounted for.\n\nThis whole topic was already discussed in great detail on the official Varjo discord, and Varjo has already released a patch to fix 5hose stutters on the 40-Series GPUs.\n\nIntel Arc isn't blacklisted or anything. Direct display mode devices shouldn't show up to the Windows desktop, but they do on Arc, and usually at the wrong resolution too. The Rift CV1 showed up at the right resolution, but I'm guessing, since it wasn't found by the Oculus software, that there's some flag missing, like HDMIs 3D side-by-side display stuff. The Index shows up as a 640x480 display, and looking into the SteamVR web console it actually says that no display with the right resolution was found, but it does list all displays connected to the Arc GPU. And in the case of the Reverb G2, the display didn't show up on the desktop (it's a built-in Windows driver, go figure), and WMR did actually launch as if everything was working right, but the displays in the headset stayed black. My guess is that yet again, the driver wasn't able to initiate the vorrect display mode (resolution, refresh rate, direct display, 2 DP lanes per screen, etc.) on the G2...\n\nThe tl;Dr is, it doesn't seem to have anything to do with the software, at least for the display connections, it seems to be a driver related issue with the hardware not showing up correctly...",
      "I haven't, tho my intention was to at some point do a 30 day Arc trial, using nothing but my Arc card for 30 days and when I do I can try Quest/Air Link again :D",
      "Honestly, that I can't really say where the sweet spot for price to performance is for VR capable PC Hardware. What I can say is that you'd want a GPU with at the very minimum 6GB of VRAM (which at least all of the Intel A7 cards do fulfill), at minimum 16GB of System Memory and preferably 6 CPU cores with HT or more. I wouldn't to older then an Intel 8th Gen CPU (or the generation after if you're going Team Red) and in terms of GPU not older then Nvidia 10-Series or RX6000.\nIf you have a Quest 2 then that leaves Intel Arc also as a GPU option open for you, as long as you're using Virtual Desktop (as like I've said before Headsets that require a Display connection to the GPU currently don't work on Arc and the Oculus software refuses to work completely on Arc)",
      "They have not fixed it yet!!!?",
      "You could try something more up-to-date like Fedora or endavourOS.\n\nI'd be very interested to see whether this works.",
      "Okay, so there has been some change as the driver updates have been releasing. My experience was with either the launch driver of the update after launch driver which just said *headset is disconnected.* I’ll try launching VR on my Rift S and see if I match what you’ve been experiencing",
      "It still says that, but at least with the CV1 a 2160x1200 display was showing up on the desktop.",
      "Damn. Sorry man"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "I7 4790k with arc A770 8gb",
    "selftext": "Hey guys ! \nI am looking for an upgrade path for my first ever pc build. In 2015 i built a pc with an i7-4790k 16gb of ram and a gtx 970.\nNow it s slowly turning 8 years old and while I don t really play a lot of intensive games some hiccups show up. Like Spiderman Miles Morales is running at about 50 fps and so on. Not huge deals but as PC s are a hobby of mine I started looking around. \nI slightly overclocked the cpu to 4.2ghz and in afterburner it says that the cpu is only at 60-70 ish usage while the gpu is at 90-95 ish. Looking around i found the new intel Arc A770 gpu with 8gb of vram at a reasonable price same price i paid for my 970 back in the day. \nSo I guess the question is \nWhat do you think guys is this a gpu that could work well in my pc and keep when I eventually upgrade my cpu, mobo and so on ?",
    "comments": [
      "Arc cards need a 10th gen or better cpu to function correctly. There is unofficial ways to get them to work but YMMV.\n\nIf your motherboard doesn't support resizable bar, then the ARC's performance will be absolutely horrendous.\n\nI'm extremely happy with the A770 but you really really need resizable bar for it to do well.",
      "Do that and buy a 6700xt, not a A770, that should be quite good tbh",
      "Don't bother, get a 6700xt, 6650xt or 6600xt, you will be bottlenecked by the 4790k, but it's a far better way to go if you plan to upgrade the rest of the PC soon.",
      "At no cost? There is not much to think about if it won't cost you anything. Even if it did, it's a considerable upgrade.\n\nI'm still baffled as to how are you considering not upgrading for free... What are you going to be waiting for? Do you expect someone to offer you a 13900K in exchange for your 4790k? It's a great CPU, I loved mine, but come on...\n\nThe only drawback is that it doesn't have an iGPU, but you have a graphics card, so that shouldn't be a problem.",
      "Absolutely do this free swap, you are being gifted basically. Just make sure those parts are working before you do it of course so no one screws you over.",
      "Do you think the processor mobo combo is good enough for one ?",
      "If you are upgrading to the i7 9700f then it will be absolutely fine and that's still a quick eight core processor and should be an ample match for that ARC770",
      "Another Question \nI have the chance to swap my z97 mobo and i7-4790k processor 16gbs of ddr3 for a z390 mobo with an i7-9700F 32 gbs of ddr4 at no cost just a straight swap. Is it worth it ? Should i just wait as these are pc components released in 2019 and are not a huge deal better than what I already have ?",
      "> If your motherboard doesn't support resizable bar, then the ARC's performance will be absolutely horrendous.\n\nIs there benchmarks for that?",
      "Please someone answer this.  8th gen on z390 w rebar enabled checking in here",
      "Ey thanks man \nI guess i just needed someone to point out the obvious. I overthink something like this cause i don t want to mess up a working system",
      "https://www.intel.ca/content/www/ca/en/support/articles/000091128/graphics.html",
      "I would take the free upgrade platform and add the A770 but get the 16gb vram version and the drivers for Intel are improving massively",
      "There is a way to mod a bios for older chipsets to Unlock ReBAR, but I did not do this for my 5775c yet and can't say anything about it, except for internet claims that the mod works as intended. Still much better to upgrade to literally any modern platform.",
      "Doesn’t it work with 9th gen if rebar is enabled?",
      "So you think i m good to buy a gtx 2070 used and rock my old setup for a while longer ?",
      "I know the official specs but I’ve seen people run it on older hardware as Intel pretty much supports rebar on older gen cpus as long as the MB does it"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Stable diffusion benchmark. ARC 770 close to 4070.",
    "selftext": " [Stable Diffusion Optimized for Intel Silicon Boosts Arc A770 Performance by 54% | Tom's Hardware (tomshardware.com)](https://www.tomshardware.com/news/stable-diffusion-for-intel-optimizations) ",
    "comments": [
      "*looks at benchmarks*\n\nI see the A750 just above a 4060, and the a770 behind the RTX 4060ti.",
      "*It's basically a 4090!*\n\nStill a great improvement, though. Nice to see the progress.",
      "i’ll be buying one if benchmarks can substantiate the info.  Wonder what the power usage will look like..",
      "Depends on the power draw if it's a buy or not, price not too bad for 16gb vram in the current market. The performance isn't great in SD, considering older nvidia cards can be faster for similar price.\n\nI'm running into a vram limitation with the 2080ti, though the performance isn't bad with an undervolt draw of 160w. Im definitely looking for 16gb or more vram, but also power efficiency and performance.",
      "Similar story with [FluidX3D](https://github.com/ProjectPhysX/FluidX3D) CFD: the A770 even slightly [outperforms](https://github.com/ProjectPhysX/FluidX3D#single-gpucpu-benchmarks) the 4070 here. This is due to the higher VRAM bandwidth.",
      "Is this on Linux or Windows? Do you need the newest Mesa/Linux kernel? Possibly versions not out yet?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Is the Arc A770 stable?",
    "selftext": "I built my PC in the middle of the GPU crisis, so I have an i5-12600K paired with a Quadro T600 (a cut-down, slower 1650). \nIt is holding up decently, but it is starting to struggle in Premiere, After Effects, Resolve and Blender. \nI'm in between the 3060 Ti and the Arc A770 16GB. The former is more poweful and possibly more reliable, but the Arc has AV1 encoding and twice as much VRAM.\nI'd like to get the Arc, but how are the drivers? Is it stable enough not to cause issues in the programs I use?",
    "comments": [
      "Yeah sounds like ark is completely viable now dudes from gamers nexus and pc gamer recently talked about this.",
      "Can someone answer? I also want to know",
      "My a770 has been great except for some old directx9 games. No noticeable driver issues here.",
      "Had no problem yet with my A770,played various games ranging from dx9 to dx12 to vulcan",
      "Post on /r/IntelArc",
      "Interestingly, Intel recently launched the workstation versions of Arc GPUs such as the Arc Pro A60 12 GB. I just don't see where to purchase them.   \n\nhttps://www.intel.com/content/www/us/en/products/docs/discrete-gpus/arc/workstations/a-series/overview.html",
      "Doesn't crash, programs run as they should, no graphics glitches...",
      "In resolve, if I enable use Intel neural engine \"optimization\", crashes whenever using superscale (and text may glitch when you overlay them, idk if it's fixed now). Planar motion tracking may cause hard lock of the system. Overall ok, but your mileage may vary. I kinda have to use my much weaker Nvidia system to bail arc out sometimes.",
      "I can't say that there are no issues, but the drivers are the best they've ever been. When they fix the sleep mode bug, then I think I'll finally be able to say that Arc is viable for the average user, if still fairly inconsistent.",
      "Define stable, but for you nvidia is best choice ATM."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Is there Intel arc a770 VR support?",
    "selftext": "Hello everyone I’m currently looking into getting the a770 for a build but I want to know if there is any VR support or improvements since it has launched, I have the quest 2, any help or info would be very appreciated thanks.",
    "comments": [
      "No support.",
      "Looks like this guy said it works with virtual desktop only right now (no airlink or oculus link), but said it ran a few games well.\n\nhttps://www.reddit.com/r/oculus/comments/104fs86/oculus_pc_vr_w_intel_770_750_gpu_yes_you_can_play/",
      "How sad🥲",
      "This is still a 'priorities' stage, and system stability and overall performance is more of a priority than VR support. Arc isn't really powerful enough for an enjoyable VR experience anyway.",
      "Stay Tuned™",
      ">Arc isn't really powerful enough for an enjoyable VR experience anyway.\n\nPretty sure a A770 is faster than my 980ti, which was fast enough for most VR content.",
      "Yeah that's pretty surprising",
      "Just like the 7000 series you aren’t missing out much",
      "I would like to ask any recommended VR device to recommend?",
      "No official VR support and not with SteamVR headsets when tested in late December/\n\n[https://babeltechreviews.com/intels-arc-cards-do-not-work-with-native-steamvr-headsets/](https://babeltechreviews.com/intels-arc-cards-do-not-work-with-native-steamvr-headsets/)\n\nHowever, Arc A770 worked with Reverb G2/WMR in January and was benchmarked.\n\n[https://babeltechreviews.com/first-look-at-arc-vr-performance/](https://babeltechreviews.com/first-look-at-arc-vr-performance/)",
      "The ARC 770 is more powerful as my RTX2060super, so it's more than capable for VR (I'm using HTC Vive Pro/wireless)."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "a770"
    ],
    "title": "Intel Arc Alchemist desktop series including A770, A750, A580 and A380 SKUs reportedly delayed till late Q2/early Q3",
    "selftext": "",
    "comments": [
      "Honestly, it doesn't matter.\n\nThey missed their window by not launching during the GPU apocalypse uncontested, so now they have to go against Nvidia and AMD offerings with mature drivers in a health(ier) market.\n\nMight as well take the L at this point and just figure out your drivers.",
      "What they've officially stated is launch in 'summer', compared to mobile 5/7 'early summer'. Anything past 'early summer' puts it in calendar Q3.",
      "They're going to have to seriously discount those cards to sell next to Nvidia and AMD. Can't see many people taking a chance on the first gen of cards especially in the enthusiast market when the top Arc card is only at 3070 level of performance. \n\nAnd then by August and September all of the talk will be on RTX 40 and RDNA3 which will blow these cards out of the water.",
      "This will get steam rolled by Lovelace and rdna3. Should’ve released this in June 2021",
      "Not if it's dirt cheap like polaris. \nBoth lovelace and rdna3 will be starting at minimum 400$ for the lowest model.",
      "Their gonna take an L. The market is getting better and by the time it comes out, amd and nvidia will have their gpus at a few % above msrp. Therefore making these card useless. They won't be faster, they might be a bit less expensive, and their launching gpus close to next gen.",
      "To be honest, even if they launched in June 2021, they'd have probably all been gobbled up by miners.",
      "most of them are probably going straight to OEM.\n\nOEMs will use it as a way to tell AMD/Nvidia to fuck off with their horrid \"incentives\" like priority allocation. Most people buying computers don't know what the hell goes in them so the main problem is drivers. if the drivers continue to be miserable then intel will get nowhere even giving these things away for free.",
      "At least they would’ve sold (Not defending miners). Now they have zero chance of becoming dominant in any market",
      "More competition, more fun.",
      "Well considering the frame stuttering in the arc a350m for mobile that was recently tested, my money is on bad drivers and at this point Intel should just work on the drivers. They missed a critical chance to disrupt the market. Now new gpu prices have fallen down hard. Not yet at msrp but much better. The second hand market will be flooded with rdna2 and 3000series from Nvidia and not mention the rtx4000 series and rdna 3 gpus coming soon. This will be really rough  on Intel.",
      "When will Intel learn that blatantly lying to investors will get them nowhere?",
      "Efficiency wise it's better than GTX 1600, at least the already released laptop 350m which at around 40W has same performance as GTX 1650m at 50W. \nThat's not too bad. Probably on the same level as RTX 3000 as efficiency didn't improve much compared to GTX 1600 and 2000.",
      "Well, thats basically all she wrote for ARC...\n\nI know for a fact Intel won't price them relative to their performance vs other cards.\n\nI hope I am incorrect, as aggressive pricing is the only thing that will save them. That creates an even bigger problem for them though, if they start out low priced, they will remain there for many years. Thats just the way the market works.",
      "They're making them at TSMC so it didn't matter. if these were being made at Intel fabs, well they'd be worse if they were being pushed out on a broken 10nm a year ago or 14nm due to power efficiency.\n\nBeing made at TSMC means when TSMC were starved for capacity these would have been even worse.\n\nUsing TSMC 6nm indications would be that performance is closer to AMD/Nvidia parts that use half the die size. \n\nThe only way these matter in the first gen or two is if Intel does it's Atom/phone/tablet tactic and just firebombs pricing to force their way into relevancy but if the architecture doesn't catch up then eventually when they try to charge real prices they'll be pushed out.",
      "Mmmhmm.",
      "More competition more fun works if wafer supply is ample and performance is competitive. If performance sucks and they eat up supply of incredibly limited wafers then it's terrible for everyone, Intel included. They'll be eating shit selling these at below what they cost to produce and AMD/Nvidia lose out supply to make twice as many gpus from the same wafers and the end user loses out.",
      "Thats just clock gating. Easily fixable.\n\nLook at the GPU speed every stutter, you will see it drop to 1150mhz.",
      "TSMC 6nm is a retooled 7nm, so the density is not that dissimilar.",
      "How is it fixed if u dnt mind explaining?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Intel Arc A770 on an old machine",
    "selftext": "Hello, my PC is rather old and I was wondering if it would benefit me to upgrade to an Intel Arc Card or keep using a gtx 1070ti?\n\nMy processor is an i7 6700k with 32gb of ram and the Motherboard is Asus Z170-A. Windows 10 tells me I cannot upgrade to windows 11 so I'm not sure if getting an Arc card would benefit me more, or just get an AMD graphics card?",
    "comments": [
      "Intel recommends 10th gen or newer for REBAR. Arc really needs resizable BAR.\n\nYou’d probably get more out of a CPU/MoBo upgrade than A770 on an “ancient” 6700K.",
      "If you can’t get REBAR don’t even consider an Arc GPU at this point. Intel officially supports that back to 10th gen CPU but most motherboard manufacturers added it to 300 series motherboards(8th and 9th gen CPUs)",
      "For old CPU like that, it's best to go for AMD GPU, they scale best on slower CPUs compared to Nvidia. And Intel is worst as it needs Rebar .",
      "Absolutely not. If you're not running a pretty modern rig, Intel is definitely not an option.\n\nGet AMD card instead, those work well with weaker CPU's. Probably some discounted 6000-series card is the best bet.",
      "Outside of a few niche cards, the gen 3 to gen 4 difference is like 3%. Hardly enough to get worried about.",
      "Some older boards have hidden settings for Re-bar. If you're confident enough to go down that rabbit hole, you could do that.",
      "Arc is cool and I want to support competition but no, without resizeable bar on 10th gen Intel or Ryzen 3000 the performance would not be worth it and could even go backwards.\n\nDepending on availability in your area a rx 6700 from AMD could work",
      "Well your options are limited. Arc GPUs need REBAR support, and AMD/Nvidia need PCIE 4.0. \n\nYou can of course still hook them up to a PC with PCIE 3.0, but you'll be losing a good chunk of performance. A whole platform upgrade is the best option at this point."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "16GB Arc A770 in Stock at Newegg. Just ordered. Question anyone try Steam Link or Parsec etc... streaming? Does the encoder do well vs AMD, Nvidia?",
    "selftext": "So wanted to post that Newegg had it in stock.  But also figured i would ask about Steam game streaming whether parsec, steam link whatever other ways there are.  And if it worked well?  I know Nvidia is supposedly better at this than AMD because of the encoder but curious how Intel Arc compared.\n\nThanks!\n\n\nhttps://www.newegg.com/intel-21p01j00ba/p/N82E16814883001?Description=arc%20a770&cm_re=arc_a770-_-14-883-001-_-Product",
    "comments": [
      "They've said time and time again that even if the alchemist GPUs flop, they will continue with their plan for future GPU architectures. They've invested billions; they're in it for the long haul.",
      "AMD has the worst encoder out of everyone, and nvidia has the best (considering the highest end option for both AMD INTEL and Nvdia)\n\nThis is from the best to worse\n\n1. Nvenc \n2. Quicksync with a dedicated gpu\n3. Quicksync with a iGPU\n4. Amd encoder",
      "I have a A770 but don't use it for any streaming, however I do hope you're not using the card in a primary system. You're going to spend a lot of time trying to get things to work, and if you do, will have to settle with mediocre performance. I don't mind the tinkering as I have it in a secondary system I play around with but no way would I use it in my main system for gaming- at least not yet.",
      "Hello, thanks for the warning but i saw the reviews and I am aware of the issues.  It will go in my main system but I will work around the issues and i have a laptop for work.  So its mostly a curiosity and to see how it changes over time and to support a 3rd player i guess too.   \n\nI have been playing a lot of 90s adventure games and Vampire Survivors lately so i should be alright ha!  Otherwise i game on Xbox and XCloud mostly.",
      "Just to add to this. This may change with the new AV1 encoder that amd is saying for their new gpus next month. \n\nSo it could change but its unknown at the moment.",
      "And hopefully they aren't in it the way Google claimed to be investing in Stadia for the long haul.",
      "I'll be curious to see if these continue to sell well. I'm sure Intel is paying close attention to Arc sales in determining the future of their GPU sector.",
      "Both the RTX 4090 and 4080 are much more efficient than their 30-series counterparts. For the same or even less power usage, you are getting 50-100% more performance. From purely a power efficiency standpoint, it'd be strange to consider the 30-series impressive but suggest the 40-series isn't.",
      "Thank you for your sacrifice. I want good things from Intel and people like you putting the money into the risks are what will propel that forward.",
      "I've got my eye on a Battlemage series card for sure! I want to see how these do, but NVIDIA has become just so power-hungry (take that however you like). 30-series was impressive, I dont' know why but 40 series kinda isn't. I don't run actual space heaters because of the electrical cost, I don't want to have the same feeling about my PC.",
      "That, and better software to use it. We'll see how it goes.",
      "We need people like this, putting themselves forward to help Intel out with driver issues, hardware issues, and who knows what else. I think that they can genuinely be a great contender if they stick with their low cost GPUs, especially with NVIDIA pulling their shenanigans",
      "No billion dollar multinational needs charity from its customers whatsoever. We don't need end users suffering so Intel can half arse their drivers.",
      "That's pretty typical for Google though"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "111°c On Intel Arc A770?",
    "selftext": "I recently stress tested my New Rig using 3DMark's Time Spy and got a score of 13,450, but my gpu temps were 111°c. Is that abnormal or is that just typical Benchmark/Stresstest temps? Im fairly new to this stuff and Im still learning.",
    "comments": [
      "Score is ok, I highly doubt you temps though. Under load temps for this card are generally around 70-75°",
      "Check the reviews for this card, you will see that 111c is very high.",
      "What's your CPU temp like? Other case components? Is your case properly ventilated? If so, you need to contact Intel tech support because you may have faulty thermal paste in your GPU. I would advise against taking this card apart. If you were to watch gamers Nexus's tear down, you will see how much of a nightmare it must be to put back together. The bifrost probably isn't too bad, but the Intel LE looks pretty scary.",
      "Ik that 111 is high but fir a stresstest idk if that would be typical?",
      "What software are you using to check the temperature? The gold standard is hwinfo64, so I would check with that.",
      "Time Spy (especially the non-Extreme version) is not that heavy of a stress test. If you're hitting 111c in that, then you will have high temps in regular games as well, unless you play really old games.",
      "I don't know what I should do about this. I don't want to take the card apart for fear of breaking something. I know I don't have the best airflow but ever since I got the card I've been looking at temps and it sits at around 70-75. Im using basic fans that came with the Deepcool LS720 360mm in the front and The case fans that come with my current case the Corsair 4000d RGB. I do get a lot of air pushing out the back of the case. I'll update you when I get home.",
      "Was it a visual glitch?",
      "Return it. That’s super not normal",
      "Check your GPU fan speed readings with HWinfo, and check if the actual fans are spinning and working properly, I mean by actually looking at GPU when it is under load.",
      "Damn... my 2021 laptop scores that high.",
      "Same here, gaming is 70-75° tops for me.",
      "Im going to run it again.. maybe the fans werent doing its thing or it was giving me a false read 🫡",
      "IVE RESOLVED THE PROBLEM"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Arc a770 or the RTX 3070",
    "selftext": "Been doing some research on the intel arc a770 with recent updates with the card 4090 update, this card from what I've seen can ultra most games I looked at warzone 2.0, borderlands 3, guardians of the galaxy, and metro exodus, and red dead redemption 2, all these games maxed out, 70 to 120fps varies with these games lol but still thats good maxed out, now since intel is still figuring out their card is it worth to buy now and be a beta tester for the card or just buy a 3070 and be happy lol, I found a 3070 on fb for 600$ talked down to 500$ Canadian money, and the intel arc memory express for 479$ plus a bonus comes with modern warfare 2 for battle net. \n\nWhich one of these cards would u recommend the most ?\n\nSpecs \nRyzen 5 5600 \nGA-A320M s2h \n2x8 corsair rgb ram at 3600\n500 plus 80 evga\n1650 OEM model",
    "comments": [
      "3070 for $500",
      "I would go for the 3070 yet. One problem is I can’t really find good comparisons on YT. In one video it is on par with the 3070 on 1440p in the next it is around 30% slower in the same games. But if you want to give the Arc A770 a chance go for it if you play on 1440p (it should be the best resolution for it), imo it has a great improvement potential with his drivers.",
      "it will boot, but shutdown under load. depending on your cpu at how much load it will",
      "Definitely go for the 3070, ARC GPU's are really new and dont have great driver support yet. And if you can find a 3070 for a decent price I would definitely go for it.",
      "GPU'S should be half their price and we would replace them much more often. They have ramped out of logic in 4 years.",
      "Definitely go for 3070. \nIntels first gen gpus just aren’t worth it for most of us who only upgrade gpus every 2 years or so, next gen of intel could be very promising thought",
      "3070.\nmuch better driver support.\n\n\nnVidia launch driver. 550MB (now 800MB)\n\nIntel. 1200MB.. and they still get their act together.",
      "Used 3070 or new a770 for the same price ?",
      "Intel arc a770 is 349.00 founders edition!?",
      "I found this \n\n\nKey Differences\nIn short — GeForce RTX 3070 outperforms the cheaper Arc A770 on the selected game parameters. However, the worse performing Arc A770 is a better bang for your buck. The better performing GeForce RTX 3070 is 771 days older than the cheaper Arc A770.\n\nAdvantages of Intel Arc A770\nUp to 51% cheaper than GeForce RTX 3070 - CA$477.99 vs CA$972.51\nUp to 51% better value when playing Call of Duty: Warzone than GeForce RTX 3070 - CA$3.32 vs CA$6.71 per FPS\nUp to 100% more VRAM memory than NVIDIA GeForce RTX 3070 - 16 vs 8 GB\nAdvantages of NVIDIA GeForce RTX 3070\nPerforms up to 1% better in Call of Duty: Warzone than Arc A770 - 145 vs 144 FPS\nConsumes up to 2% less energy than Intel Arc A770 - 220 vs 225 Watts",
      "If I were to make a video what would you want me to test?",
      "Would 500 plus 80 be enough for the 3070 as is ? Like will it boot in other words",
      "500 used 3070 and a new arc a770 for like 459$ new"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "What can cause Intel Arc A770 unstable, mostly low gpu usage in some games?",
    "selftext": "In some games my A770's usage is between 20 and 100%, mostly around 40%, I have huge lag spikes and fps drops. The cpu usage is low in every game. I have deleted all of the old nvidia drivers with DDU and did a clean arc driver install. Some games just working fine, even older games, with 99% gpu usage.\n\nMy config: i7-8700k, Z370 Taichi, 16GB 3200Mhz CL16 ram, 2TB nvme ssd. Will the cpu be the problem?\n\nRebar is turned on.\n\n&#x200B;\n\nedit: temperatures are under 65 celsius. I have tried from 1080p to 2160p, every settings, but nothing has changed.",
    "comments": [
      "Some games just run very badly on arc, it's because of the drivers that are far from fully optimized",
      "DX12 and vulkan games should all be alright, dx11 most of them should be ok, dx9 is mixed bag",
      "Yes, it’s in the description.",
      "You require resizable bar on in bios pick dx12 games best!",
      "Could bump power or voltage in oc settings see if it stabilizes",
      "GPU and CPU usage is not a good indicator of bad performance.",
      "I also have an 8700K and Asrock Z370 taichi 32g cl32 ddr4 and 3 NVME M2 drives.Win 11. Have my A770 installed about 2 weeks. I only play medium to low level games but it's been pretty much flawless. The only problem I had was when I screwed around with bios settings trying to reduce the 40W idle power consumption. I crashed windows with a tdr video error. Once I put the bios settings back I have had no problems. Running with the Feb Intel drivers.",
      "Do you have resizeable BAR enabled? If not, performance will suck bigtime!",
      "Is your ReBar really on? When I first got my a770, I had my ReBar turned on in the bios, but according to Intel software, it wasn't. Turns out, I needed to switch from MBR to GPT for ReBar to work properly. \n\nIf that isn't it, could be hardware incompatibility. While it works, it isn't working to the best of its ability.",
      "If you have rebar on, then it's probably either a cpu bottleneck (unlikely most of the time with an 8700k) or buggy drivers with the game you're trying to run (far more likely). For example, I only get around 50% gpu utilization in F1 2020 and F1 2022 with my A370m.",
      "I don't know how, but 2 games now are running much much better. Dota 2 is playable now, but still has some small fps drops, but Valheim runs perfectly (4K, almost max settings with stable 60 fps (without vsync 70+ fps).\n\nI have to download some more games, I played on PC 2 years ago and I don't have games on it.\n\nBut I think cpu is okay.",
      "I wanted to update this thread, but I don’t have time to test the pc. I’m just sleeping, because I got covid. As soon as I will be better, I will update this post.",
      "Yes, I know. Valheim was unplayeble few minutes ago. Between 1 and 80 fps I had everything."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Is the intel arc a770 good/decent at gaming?",
    "selftext": "I’m aware it’s a cheaper gpu but from what I’ve been researching on my own it’s comparable to a 4060/ti while being 100/150 dollars less.\n\n I mainly play games like Fortnite, CoD and other 3rd person shooters/fps and I’d like to play in 1440p at 100hz+. (I’d assume it can do that as my laptop with a 1660ti hits 120fps in Fortnite with my settings in creative and 60fps+ in games with my settings at 1440p) \n\nI’d pair it with the intel i7-13700k so would that be a good pair?\n\nThank you to anyone who helps 🙏",
    "comments": [
      "The main issue with the arc cards is the drivers. Since they’re new and not really in high use, not many games focus on optimizing for them. Intel is frequently updating drivers to improve performance which saw a recent uplift of up to 200% increase in performance in certain games. If you’re getting it brand new for like $250, it’s definitely a good deal, just make sure you have a lengthy return window and don’t void the warranty. In theory it’s a great investment but only time will tell. I recently put an a750 in my old computer to upgrade a 1660 and it’s been solid aside from starfield. Game barely ran but I’ve heard that’s been fixed since. I believe there’s a similar Intel variant of smart access memory so having your cpu be able to access gpu memory if necessary can help for shooters at 1440p. I will say, if you can scale back the cpu to a 13600kf or even 12th gen and get a 4070 instead, you’ll have a much better experience at 1440p as it’s more gpu bound than 1080p."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Arc A770 16GB - OpenCL performance",
    "selftext": "I am running **clpeak** on Arc A770, and I am getting half of advertised half-float performance: \n\n    $ clpeak\n    Platform: Intel(R) OpenCL HD Graphics\n      Device: Intel(R) Graphics [0x56a0]\n        Driver version  : 22.49.25018.23 (Linux x64)\n        Compute units   : 512\n        Clock frequency : 2400 MHz\n    \n        Global memory bandwidth (GBPS)\n          float   : 363.95\n          float2  : 383.93\n          float4  : 385.50\n          float8  : 396.99\n          float16 : 400.12\n    \n        Single-precision compute (GFLOPS)\n          float   : 12338.24\n          float2  : 10562.94\n          float4  : 9856.40\n          float8  : 9476.16\n          float16 : 9204.72\n    \n        Half-precision compute (GFLOPS)\n          half   : 18397.46\n          half2  : 18378.09\n          half4  : 18413.70\n          half8  : 18233.57\n          half16 : 18371.29\n\nAdvertised performance ([https://www.techpowerup.com/gpu-specs/arc-a770.c3914](https://www.techpowerup.com/gpu-specs/arc-a770.c3914)):  \n\n\n    ...\n    \n    FP16 (half)\n        39.32 TFLOPS (2:1) \n    \n    FP32 (float)\n        19.66 TFLOPS \n    \n\nThe GPU is rendering idle desktop during the test but that should have minimal impact.\n\nWhy factor x2 difference? Is the website accurate?\n\n(Debian Testing/kernel 6.3/CPU: i9-12900k)",
    "comments": [
      "I think this is somewhat of a problem with clpeak. I get very similar results with my A750:\n```\nPlatform: Intel(R) OpenCL HD Graphics\n  Device: Intel(R) Graphics [0x56a1]\n    Driver version  : 22.49.25018.23 (Linux x64)\n    Compute units   : 448\n    Clock frequency : 2400 MHz\n\n    Global memory bandwidth (GBPS)\n      float   : 396.12\n      float2  : 396.38\n      float4  : 402.52\n      float8  : 408.13\n      float16 : 410.27\n\n    Single-precision compute (GFLOPS)\n      float   : 11371.21\n      float2  : 9733.42\n      float4  : 9092.19\n      float8  : 8757.09\n      float16 : 8488.46\n\n    Half-precision compute (GFLOPS)\n      half   : 17088.24\n      half2  : 17036.59\n      half4  : 17067.08\n      half8  : 16973.48\n      half16 : 16903.71\n\n    No double precision support! Skipped\n```\n\nBut my own OpenCL benchmark program reports the correct values:\n```\n|----------------.------------------------------------------------------------|\n| Device ID      | 0                                                          |\n| Device Name    | Intel(R) Graphics [0x56a1]                                 |\n| Device Vendor  | Intel(R) Corporation                                       |\n| Device Driver  | 22.49.25018.23                                             |\n| OpenCL Version | OpenCL C 1.2                                               |\n| Compute Units  | 448 at 2400 MHz (3584 cores, 17.203 TFLOPs/s)              |\n| Memory, Cache  | 7721 MB, 16384 KB global / 64 KB local                     |\n| Buffer Limits  | 3860 MB global, 3953458 KB constant                        |\n|----------------'------------------------------------------------------------|\n| Info: OpenCL C code successfully compiled.                                  |\n| FP64  compute                                          not supported        |\n| FP32  compute                                        18.533 TFLOPs/s ( 1x ) |\n| FP16  compute                                        71.712 TFLOPs/s ( 4x ) |\n| INT64 compute                                         0.558  TIOPs/s (1/32) |\n| INT32 compute                                         3.632  TIOPs/s (1/4 ) |\n| INT16 compute                                        12.969  TIOPs/s (2/3 ) |\n| INT8  compute                                        13.201  TIOPs/s (2/3 ) |\n| Memory Bandwidth ( coalesced read      )                        265.82 GB/s |\n| Memory Bandwidth ( coalesced      write)                        257.27 GB/s |\n| Memory Bandwidth (misaligned read      )                        265.45 GB/s |\n| Memory Bandwidth (misaligned      write)                        267.82 GB/s |\n| PCIe   Bandwidth (send                 )                          1.33 GB/s |\n| PCIe   Bandwidth (   receive           )                          1.28 GB/s |\n| PCIe   Bandwidth (        bidirectional)            (Gen1 x16)    1.33 GB/s |\n|-----------------------------------------------------------------------------|\n```\n\nFor reference, here is it with early drivers on Windows 10. The single-precision benchmark in clpeak was broken, but my own benchmark program indicated ~16.5 TFLOPs/s.\n```\nPlatform: Intel(R) OpenCL HD Graphics\n  Device: Intel(R) Arc(TM) A750 Graphics\n    Driver version  : 31.0.101.3802 (Win64)\n    Compute units   : 448\n    Clock frequency : 2400 MHz\n\n    Global memory bandwidth (GBPS)\n      float   : 400.11\n      float2  : 397.44\n      float4  : 404.31\n      float8  : 410.60\n      float16 : 414.91\n\n    Single-precision compute (GFLOPS)\nclCreateBuffer (-61)\n      Tests skipped\n\n    Half-precision compute (GFLOPS)\n      half   : 17103.30\n      half2  : 17056.69\n      half4  : 17079.48\n      half8  : 17038.33\n      half16 : 16890.57\n\n    No double precision support! Skipped\n```\n\n```\n|----------------.------------------------------------------------------------|\n| Device ID      | 1                                                          |\n| Device Name    | Intel(R) Arc(TM) A750 Graphics                             |\n| Device Vendor  | Intel(R) Corporation                                       |\n| Device Driver  | 31.0.101.3802                                              |\n| OpenCL Version | OpenCL C 1.2                                               |\n| Compute Units  | 448 at 2400 MHz (3584 cores, 17.203 TFLOPs/s)              |\n| Memory, Cache  | 6476 MB, 16384 KB global / 64 KB local                     |\n| Buffer Limits  | 3238 MB global, 3316120 KB constant                        |\n|----------------'------------------------------------------------------------|\n| Info: OpenCL C code successfully compiled.                                  |\n| FP64  compute                                          not supported        |\n| FP32  compute                                        10.993 TFLOPs/s (2/3 ) |\n| FP16  compute                                        16.525 TFLOPs/s ( 1x ) |\n| INT64 compute                                         1.156  TIOPs/s (1/16) |\n| INT32 compute                                         3.839  TIOPs/s (1/4 ) |\n| INT16 compute                                        26.797  TIOPs/s ( 2x ) |\n| INT8  compute                                        10.262  TIOPs/s (2/3 ) |\n| Memory Bandwidth ( coalesced read      )                        251.20 GB/s |\n| Memory Bandwidth ( coalesced      write)                        408.41 GB/s |\n| Memory Bandwidth (misaligned read      )                        406.35 GB/s |\n| Memory Bandwidth (misaligned      write)                        441.38 GB/s |\n| PCIe   Bandwidth (send                 )                          6.84 GB/s |\n| PCIe   Bandwidth (   receive           )                          7.13 GB/s |\n| PCIe   Bandwidth (        bidirectional)            (Gen3 x16)    7.95 GB/s |\n|-----------------------------------------------------------------------------|\n```",
      "Thanks, interesting. Is your benchmark app publicly available, or can you suggest any other bench app?",
      "Not yet. Will upload it on my GitHub eventually.\n\nEDIT: It's opensourced now on my GitHub: https://github.com/ProjectPhysX/OpenCL-Benchmark",
      "The A770 is supposed to have 2:1 throughput for FP16 and you're seeing 4:1, I'd have expected to see roughly 36 TFLOPS at FP16, and you're getting a massive 71.712 TFLOPS. What's going on there?",
      "I've opensourced my [OpenCL-Benchmark](https://github.com/ProjectPhysX/OpenCL-Benchmark) utility now. Have fun!",
      "It's 2:1 FP16:FP32 for general compute, but 8:1 for XMX matrix operations. The benchmark does two fused-multiply-add operations on a \\`hlaf2\\` vector in an unrolled loop 512 times. It's possible that the compiler converts this to matrix operations for a 4:1 ratio.",
      "I gave it a try and I do not get as good results as you:\n\n&#x200B;\n\nI am curious how you are getting 71 TFlops of FP16, on Arc A750, while I am still on 18TFlops. \n\nI'll have a look at the code later. Anyway, thanks for sharing it, much appreciated!",
      "This seems to differ significantly between Windows/Linux Arc drivers. Not sure why.",
      ">I am running clpeak on Arc A770, and I am getting half of advertised half-float performance:\n\nYou're very unlikely to ever get the peak throughput, and especially not in a random benchmark not specifically made for your device."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Arc A770 worked fine until...",
    "selftext": "I installed an Intel Arc A770 in my rig running on an i7-9700k CPU set on a Gigabyte z-370 mobo with 32 GB of RAM. So, all was fine until I upgraded to Win 11 Pro. I ventured into Steam to play Cyberpunk and found my game hit play. It took a while to load up, but when it did, it stopped. A message screen came up informing me Cyberpunk has flatlined.  As I said before, I am running this GPU on a 9th gen CPU; it is recommended a 10th or 11th-gen.  Maybe I pushed my luck too far.  Okay, has anyone had gaming issues at all with this intel GPU?",
    "comments": [
      "Before my recent upgrade, I was playing with a 9600k. I can't see a 9700k being any issue. Given that the Intel GPUs still have some growing to do when it comes to drivers, the GPU would be my first guess as being the issue.",
      "If this was a windows upgrade and not a fresh install, I would approach as follows:\n\n&#x200B;\n\n1. DDU the intel drivers and reinstall from a fresh download\n2. if 1 didnt work, consider a clean install of win11",
      "Just clean install Win 11.",
      "Definitely the drivers. I did a fresh install of Win11 and both windows and Intel update assistant didn’t see the Intel card… just the iGFX. I scrubbed the BIOS but nope just had to download the actual gfx driver from Intel (not just the control center) now everything works. \n\nGood luck.",
      "All the drivers are up to date.  Intel Arc Control says the current driver version. 31.0.101.4644",
      "The upgrade was through Windows. I can reinstall with a media creation tool. Thanks for your input."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Intel Arc gpus prices in Europe",
    "selftext": "Does anyone know why Arc gpus prices are so high in Europe compared to North America?\n\n&#x200B;\n\nIsn't Intel Arc A770 supposed to be 350$?\n\n&#x200B;\n\nhttps://preview.redd.it/z0rh87ejbvfa1.png?width=920&format=png&auto=webp&s=f9a09cdf3029e58a58ffc2e4541919a68b792b50",
    "comments": [
      "Likely import duties, and remember the pricing you see in North America is always without sales tax, and in Europe, from my experience, it always includes it.",
      "Prices in the EU always have VAT included, which for hardware is 16-27% depending on the country. An MSRP of $350 translates to 370-410€ + import tariffs.\n\nIn Spain, Pc componentes is selling them quite expensive. They should be roughly 390€. Either the store or some national/local distributor are getting very high margins.",
      "Shouldnt you convert all the prices to a single currency unit to really compare? Nordic prices are they NOK, DKK, or SEK, regardless they actually look the cheapest of the lot",
      "German prices already dropped to 370 for the 770LE",
      "Electronics are almost always more expensive in Europe than NA. I haven't seen a situation where this hasn't been the case for me personally over the past 5 or 6 years.",
      "Because Europe is conglomerate  of thieves.",
      "the A750 is 274€ in Germany, which converted to USD is even less than $250 once you remove the VAT.",
      "Didn't know about that. \n\nThanks for the info!",
      "Still 370-410€ is still too expensive in my opinion.  \nFor example, in PC Componentes the Intel Arc a770 is 462€ when you have RTX 3060 that is going around for 376-450€, that being said going for a a770 is a no go for most people when you have a more \"stable\" GPU for cheaper price.",
      "The screenshot was taken from this [article](https://www.eurogamer.net/digitalfoundry-where-to-buy-intel-arc-gpus-american-and-european-prices).",
      "Good to hear, can you link some german pc shops that sell the card for that pricing so I can keep and eye out?\n\nThanks!",
      "I just hope that the prices of electronics in Europe start to drop, guess I have to wait and see.",
      "Oh, no doubt. I was just doing the math for the direct translation of the MSRP to the EU.\n\n$350 in the US is not equivalent to 350€ in the EU",
      "Its 379 but close enough I guess https://www.notebooksbilliger.de/intel+arc+a770+16gb+grafikkarte+782171"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Acer Predator BiFrost Arc A770 vs. ASRock Phantom Gaming Arc A770?",
    "selftext": "So let me start off by saying, I'm running on a [Radeon HD 6850](https://www.newegg.com/asus-radeon-hd-6850-eah6850-dc-2dis-1gd5-v2/p/N82E16814121419) (pre-dates Blu-Ray) on a Ryzen 7 3700X system.  I've been wanting to upgrade since at least 2019 but as everyone knows a bunch of things hit the fan between chip shortages and crypto mining.\n\nI originally wanted to buy either an RTX 4070, RTX 3080, RX 7800 XT, or RX 6800 XT.  Black Friday came and went and I didn't pull the trigger because the pricing still seems obscene to me.\n\nI've noticed the Arc 770 GPUs that Intel is putting out and I've heard the pros and cons.  I've accepted the fact that there are going to be shortcomings with the Arc GPU, but **ANYTHING** I buy will be a dramatic improvement over what I'm crawling on at the moment.\n\nIs there anyone out there who insists I should wait till the economy craps the bed possibly next Fall and get either nVIDIA or AMD, or do I settle with an Intel to get by with?\n\nIf I go with the Arc 770, I'm looking at either the [Acer Predator BiFrost](https://www.newegg.com/acer-predator-bifrost-intel-arc-a770-oc/p/N82E16814553001) or the [AsRock Phantom Gaming](https://www.newegg.com/asrock-arc-a770-a770-pg-16go/p/N82E16814930102).  My only experience with Acer in the past was buying a value-offering laptop with cheaper build quaility.  I've not bought an AsRock, but I'm aware it's Asus pedigree.  The Acer is $30 cheaper than the AsRock.  What I'm not clear on is which of the two OEMs offer a better cooling system and which has better support for their product?\n\n&#x200B;",
    "comments": [
      "I ended up getting a 6600 for $185 off of NewEgg on Cyber Monday.  I'm pretty satisfied with its performance, but then again ANYTHING would have been better.\n\nI was a little concerned with whether or not Intel would play the long game with their Arc GPUs, which is why I ended where I'm at.\n\nI'll jump up sometime in 3 or 4 years barring any future pandemic or chip shortage.",
      "I had this same battle in my head and actually just purchased the Bifrost A770 a couple days ago and it’s been amazing so far. The build quality and RGB is crazy it’s very well built (better build quality than my 4070 and 6800XT) and thermals between the two are about the same. I was skeptical on the Acer branding too but the Predator stuff tends to have a premium feel and it definitely exceeded my expectations. If you have a microcenter near you I snagged mine for $269 last Saturday."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Arc A770 vs RTX 3060 vs RX 6700XT - The FULL GPU COMPARISON",
    "selftext": "",
    "comments": [
      "We are not here to laugh. While nvidia currently is greedy we hope that by supporting both financially and mentally intel and AMD to do better we can get the gpu market Back to normal",
      "I thought dxvk could fix everything but the scores in ff14 leave a lot to be desired. Also being able to do so well in metro exodus yet worse in cp77 kinda weird outcome, since metro uses RTGI which is a heavier RT effect and 770 crunches through it fine.\n\nMy one hope is someone benchmarks this with DLSS2XeSS, but then again its really bizzare how FSR2 runs better than XeSS quality in spiderman.",
      "Not that unsual as the stock CP2077 is insanely bad...if the stock perf gets improved, the RT one will have a boost as well",
      "This!",
      "Not sure why we're even laughing.  Team blue is patting themselves on the back for paying red prices for blue performance in this video.  Green is still making money on having the best top-end that enthusiasts overpay for.  The same youtubers selling you on pooping on nvidia are simultaneously advertising for them.",
      "It’s going to be some time before the gpu market can be called normal",
      "Yea realized my mistake a month ago,",
      "I didn't even notice the post was 3 months old. lol",
      "Let’s take moment to laugh at team green"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Does Intel ARC have something like Radeon's VSR or GeForce's DSR?",
    "selftext": "basically the question is, **can you force/run a higher resolution using GPU scaling with ARC just like it can be done with RADEON/GeForce?** \n\nI run my 1080p144 monitor at 2160p with 150% scaling in windows desktop and game at native 1080p for games that don't feature FSR 2.0+ and at 2160p with FSR2.0 Balanced (or Performance) for the games that do feature the tech, a friend of mine is interested in an ARC A770 16GB for his creatives tasks (mostly 3D creative apps & video, casually gaming just from time to time) and wants to do the same thing, **this feature is basically THE BIGGEST reason he has been thinking of dropping from A770 16GB to RTX 3060 12GB** since he isn't rich and cannot afford to ALSO replace his current IPS 1080p monitor for a 1440p or 4K display but wants/needs the higher content real state, looking around online we haven't found an answer about this so we are asking here, he really wants that 16GB of GDDR6 and the video encoders featured with the ARC GPU but if he cannot run his 1080p monitor as 4K with ARC he will go GeForce.\n\nThanks in advance.",
    "comments": [
      "Why the old link? It's XeSS and it's not the same as DSR or VSR because... it doesn't work on video.",
      "I'd really encourage your friend to simply upgrade their monitor first. You can get a decent 4k panel for much less than either card, something like this: [Acer VG281K bmiipx 28.0\" 3840 x 2160 60 Hz](https://pcpartpicker.com/product/dP4Ycf/acer-vg281k-bmiipx-280-3840x2160-60-hz-monitor-umpv1aa001), and it will make a massive difference. Downscaling for content creation just seems like a bad idea.",
      "https://www.techradar.com/news/intel-teases-reveal-of-arcs-ai-powered-dlss-super-sampling-alternative-this-week",
      "he doesn't have the budget for that, buying another screen is not an option at this time since that will simply take the money away from the GPU purchase and delay him quite a lot (took him a couple months to save the $400 he currently has), guess he'll have to suffice with the 3060 12GB.",
      "It's actually a [work in progress](https://www.techpowerup.com/305558/intel-outs-video-super-resolution-for-chromium-browsers-works-with-igpus-11th-gen-onward), but it's some months that they are not giving new updates about it",
      "he bought a prebuilt with integrated graphics a while ago and is in the process of upgrading to a dGPU, since we could not find any info regarding ARC offering anything similar to GeForce's DSR he decided to get the 3060.",
      "https://www.reddit.com/r/IntelArc/comments/12tc6pg/super_resolution_in_mpc_video_renderer_supported/\n\nI tried it and I couldn't tell if it's running or not. Nvidia VSR was pretty obvious.",
      "I have done some test, and there are some small differences. It works only with sub 720p streams and videos, and it's not always effective.  Considering that it must work with igpus, I suspect that they set it with a very light preset.  \nI hope that they will add some setting to the driver soon.",
      "Is their current gpu broken or something?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Issue with Arc A770 LE fan speed",
    "selftext": "I've been having some trouble with A770's cooling, specifically the fan curve. When idling, the card runs at 600Mhz, 50C, and around 600 rpm fan speed. When running games with an uncapped framerate, the card would slowly climb to 90C while the fan speed remains at about 1000 rpm. And when it hits the thermal threshold, it would only lower the gpu clock.\n\nI've only seen the fan speed go up to 1700 rpm once, and it was AFTER I turned off the game to let the card cool down. When I launched the game again to see I could replicate it, the fans never went past 1200 rpm.\n\nIs this a driver issue or hardware issue? Has anyone encountered the same problem? My case is small, which may affect cooling, but having the fans not speed up at all is another problem entirely.  I can't even test to see if the fans are faulty because Arc Control doesn't let me control the fan speed.\n\nHere are the specs:\n\nCPU: Ryzen 5 5600X\n\nRAM: Corsair Vengeance LPX 16G 3600 (x2)\n\nMotherboard: MSI b550m Mortar MAX WIFI\n\nSSD: Samsung 970 EVO Plus 1T\n\nSystem: Windows 11 2H22\n\nA770 Driver: 31.0.101.3430 DCH\n\nArc Control Panel: 1.0.4759.0\n\n&#x200B;\n\nUPDATE: My new card doesn't seem to exhibit the same problem, so I guess this issue has been resolved! I know, it could be the latest driver that fixed it, but I don't feel like testing out old drivers at the moment since everything is working fine now and I just want this to be over.",
    "comments": [
      "Sounds like you’re running with the initial drivers. Try installing the Beta drivers. This should fix your issues. \n\nUninstall the driver (Run [DUU](https://www.guru3d.com/files-details/display-driver-uninstaller-download.html)) and install the Beta drivers - [link to drivers](https://www.intel.com/content/www/us/en/products/sku/229151/intel-arc-a770-graphics-16gb/downloads.html).",
      "Thank you for the suggestion. I installed the beta driver, but the problem persists. The card climbed to 90C and throttling kicked in, with fan speed still at 1000 rpm. After running like that for 1 minute, I closed the game and saw fan speed immediately increased to 1200-1258 rpm, which persisted even minutes after the card had cooled back down to 50C.",
      "By all accounts, even overclocked it shouldn't be running that hot. I would get a replacement if I were you.",
      "I don’t have many games to test as I just built the pc. I tried Overwatch 2 and Deep Rock Galactic, covering both dx11 and dx12. And yes, it’s the same situation in both games.\n\nEdit: Oh right I also tested Doom Eternal. The same happened there, with the only difference being I didn’t even need to leave the menu for the card to heat up. Doom’s menu is just more demanding, I guess.",
      "I do have Afterburner, but I installed it after I found out about the fans, I think. I wanted to use it to adjust fan speed, but it doesn’t support the card and gives no reading on anything.",
      "Yeah, I’ll give another try some time. Thanks.",
      "I would say that i has had the same issue. But Today i have solve it!\n\nLast week i shipped my A770 back to the retailer cause it was overheating to 95°C and even on Desktop it was sometimes freezing the system.\n\nToday my new A770 arrived. First thing i did was to run 3DMark with the DX12 Speed Way Benchmark. It directly hits the 90°C. After seeing that, i had the idea to use \"Fan Control\" to force more speed, cause i never heard the fans i was sure they must have more speed to give.\n\nWell, Fan Control don't work with the A770.\n\nAfter that fail, i asked google and i found this post here on Reddit.\n\nAfter reading all the comments, I thought maybe its my Bios Set Up that is making the Arc overheating, but at the same time i remember that my Asus Strix B660-I Gaming Wifi got the latest bios Update with the Description:\n\n......\"1. Improve system performance\r  \n2. **Support Intel Arc graphics**\r  \n3.Many ME updates and optimizations for....\n\nThan i thought, maybe this resolution i used from beginning on: [Intels High Power Consumption on Idle Issue Resolution](https://www.intel.com/content/www/us/en/support/articles/000092564/graphics.html) is still active in bios and its causing some issues against the bios update that supports Intels Arc. \n\nSo i opened my bios, and put everything that belongs to the PCI-E to \"Auto\"\n\nAfter the restart i used 3DMark again. The Arc just hits only the 80°C and the fan goes finally up until over 2000RPM.",
      "I think I ran into a similar issue here. Just got an ARC A770, fresh installed 4032 drivers, and realized that the moment I touched the performance options in ARC, the fans would refuse to spin up past \\~1200 RPM or so. Rebooting fixed the problem for me, but I guess overclocking is just not feasible for now.",
      "As of today, it has not been fixed. Only the LE version of the graphics card will be affected by the performance tuning of the ARC control center. The current temporary solution is to restart the system to restore the default settings of ARC and do not set any options in the performance tuning.",
      "Yeah, I'm considering it, but I'll have to wait until there's new cards in stock. For now I'll just have to run it underclocked.",
      "Glad to hear you were able to solve the issue. I’m a day or two away from getting my replacement card. When I do, I’ll check if your solution applies, if mine still overheats that is.",
      "Same here. If i lower the temp target it will lower clocks and wattage instead of ramping up the fans. I wish we had a fan curve to adjust.",
      "Just curious. Does this also happen when you play a different game?",
      "Interesting. Do you have MsI afterburner installed? Or any other type of GPU control software other than the intel program?",
      "Yeah I was gonna say to uninstall it as it might be interfering somehow.\n\nOne last go run DUU have it shut down. Remove the GPU and install it back in (make sure it’s seated all the way). Install the power cables (make sure they’re properly seated). Load into bios and ensure rebar is on. Load into windows and install Beta drivers. Once installed restart immediately.\n\nIf this doesn’t work it could potentially be a hardware issue.",
      "Sorry forgot to mention. Make sure you run DUU for every GPU you would’ve had installed. Make sure you clear if ALL old drivers before installing the new intel beta drivers."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "NF I2C Slave Device - unkown device from Intel Arc A770",
    "selftext": "There was an archived post on this a few months back. The advice was to download and install the Asrock Polychrome Sync software.  Just installed an Asrock A770 Phantom Gaming 16GB. Same problem. Installing the Polychrome Sync doesn't remove the yellow exclamation mark. Just in case anyone had the same problem, it can be solved by manually updating the driver using the one found in one of the folders created by Polychrome Sync (in my system it was in \"C:\\\\Program Files (x86)\\\\ASRock Utility\\\\ASRRGBLED\\\\ASRISP\\\\AsrNfDrv\\\\\"), and it will install the \"Asrock SPB Device\"",
    "comments": [
      "Thank you, this should be higher!"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Intel Arc A770 issues",
    "selftext": "I decided to purchase the Intel Arc A770 and it arrived yesterday. I tore my PC apart and installed it last night. I did a fresh Windows 11 install and everything was going smoothly until I went to install the Arc drivers. During the installation, the monitor loses signal and doesn’t come back on. I rebooted the machine and now it loses signal once it starts loading Windows. I manage to get into safe mode and rollback the drivers to default. I tried both the current beta and stable releases as of 5/9/23 while getting the same result, no signal from the HDMI port after installing drivers.",
    "comments": [
      "1. Go over to /r/intelarc \\- great support for the Arc over there\n2. Download DDU and an older driver (some report rolling back to older versions like 4255)\n3. Boot into safe mode\n4. Run DDU and reboot into safe mode\n5. Install 4255 drivers in safe mode\n6. Reboot into normal mode\n\nI think that'll probably fix your problem.",
      "Pretty sure your monitor doesn't play well with arc, an older monitor by any chance? I had issues where ARC forces a 1080p resolution on a monitor much smaller (output out of bounds) and only solution was use another GPU/iGPU.",
      "Nope, I gave up and returned the card. I ordered an 4070 instead.",
      "PC specs.",
      "Did you fix the issue?",
      "The monitor I’m using was purchased in 2015 so it is quite old. LG 23MP47HQ",
      "CPU: Ryzen 9 3900X\n\nMotherboard: MSI MPG X570 Gaming Plus\n\nRam: Corsair 32GB kit (cmw32gx4m2e3200c16)\n\nSSD M.2: Samsung 980 Pro 2TB",
      "Previous GPU: MSI Gaming X GTX 1060 6GB",
      "I have an older monitor and I was planning to buy this card too. I wonder if i'll face the same issue...",
      "I have an older monitor and a brand new one, it didn’t work on either one. I find it may be an coincidence for whoever."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "the intel arc a770 16gb le is getting way better!!",
    "selftext": "wow!!!\n\nhttps://preview.redd.it/izzb21grsdea1.png?width=1510&format=png&auto=webp&s=92bc9d4067e284a5dea6fef680ce9286df5fa749",
    "comments": [
      "wow!!!\n\n&#x200B;\n\nexcept benchmarks are not entirely reflective of gaming performance",
      "it doesnt touch the 7900 xtx but for 350 dollars i dont expect it to.",
      "its in the rtx 3070 territory i think a little better",
      "How does this compare to a 7900XTX? I’m genuinely curious, this is still majorly impressive.0",
      "What about the 3060? I know it was competitive with it in newer games and fell behind on older ones. does that still match?",
      "Damn those drivers really are aging like fine wine then hot damn.\n\nEDIT: I thought this couldn't possibly be that big of an increase but it is. The average 3070 score is 13507, aka 3 points less lol",
      "a few more driver updates and i might go ahead of the 3070",
      "How is it with older games? I'm thinking less of stuff so old that it doesn't matter (Ex. Half life, Counter strike, etc.) and more like.. heavy skyrim/fallout 4 maybe bf1/bfv which uses and older frostbite engine even though its DX11?",
      "it works great now, driver update fixed that issue.",
      "hot damn man! A770 looking nice\n\nRip the graphics division",
      "I really want this to work good so I can swap it into my unraid server.",
      "Best value GPU there is in my opinion"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "intel arc a770 16gb vs amd 6700 xt in stable diffusion on windows 10?",
    "selftext": "I have an offer to swap my 6700 xt for a770. Saw on youtube that the a770 generates at 10 iterations per second on windows. Is this true? P.S. I've run SD on different versions of linux, but get black images as a result of generation (possibly a hardware issue). On windows my speed is about 3 iterations.",
    "comments": [
      "I would go for intel day and night. Better card. Better technology. Better support.\n\nHere's an article about it.\nhttps://www.tomshardware.com/news/stable-diffusion-for-intel-optimizations"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "a770"
    ],
    "title": "Will Intel Arc Gpu work on Amd Ryzen 7600",
    "selftext": "I am creating this post want to get feedback, bought an Intel Arc A750 along with Ryzen 5 7600 since the X version was not available am preparing to build a pc, and not yet ready for 4k. Also have the A770 limited edition as plan B, & very willing to try their Intel gpu.",
    "comments": [
      "Yes it'll work. What makes you think it won't?\n\nAlso the A770 is barely faster, only get it if you absolutely need the 16GB VRAM.",
      "First time building it myself, and asking those who know what they are talking about appreciate the response.",
      "As long as your CPU supports Resizeable BAR, you should be fine with Intel GPUs. Which the 7600 does. You may as well use the A770 since it has more VRAM since games even at 1080p these days seem to be gobbling up as much VRAM as you can throw at it.\n\nI actually daily drive a 3700X and A770 and the experience has been decent so far. You're actually way better prepared for future games though than me since PS5/Series X games tend to make CPUs cry no matter the resolution.",
      "VRAM is video RAM, which is how much graphics data your GPU can store. Bigger is better, because it lets you play the game at higher resolutions and texture quality. Doesn't mean that smaller is necessarily worse though. The performance will be about the same at lower resolutions when the compute power of the smaller VRAM GPU equals that of the higher VRAM GPU.\n\nArc 770 with it's 16GB of RAM is good for 1440p gaming and even 4K in some cases. Arc 750 is only good for 1080p gaming.",
      "yes, but if he wants to the silicon to go a little further, the 770 is better for the 16gb along - also, plenty of things will increasing use above 8gb (RTX for example) as time goes on.\n\nWe need to know his price difference."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "arc a770",
      "a770"
    ],
    "title": "Drivers not working on arc a770 16gb",
    "selftext": "When ever I try to install the latest drivers for my GPU it works until the end and it says Exit code 1 and that happens every time I try. I've checked the Intel support page on the Intel website and that doesn't help any help is appreciated.",
    "comments": [
      "Try DDUing and reinstalling.\n\nJust the other day I had an issue with installing 101.4900 (I think it looked like it finished installing and then tried restarting, but waiting something like 10 minutes nothing happened), so had to restart my computer then DDU and reinstall again.\n\nI also went to look for that error and it also says to [DDU the driver](https://www.intel.com/content/www/us/en/support/articles/000094237/graphics.html), although the installing they mention isn't just opening/pressing install/etc.",
      "Did you read how others fixed this problem in [/r/IntelArc](https://www.reddit.com/r/IntelArc) ?"
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "a770"
    ],
    "title": "With the new A770 and the Arc software, can you cap FPS in all games automatically?",
    "selftext": "NVIDIA has a way to cap FPS across all games by the click of a button in the Control Panel.\n\nDoes the Intel Arc software also have this capability?",
    "comments": [
      "Recommend a gpu with 3x the price not have sense.  \nWaiting first for the review.",
      "“I would recommend a flagship model of a GPU that is over 3x the price of the a770 and that being the liquid cooled model, would work great for your 60fps monitor”\n\nwhat…? lol",
      "Is this a troll post? why would you recommend a last gen $1,699.99 GPU?",
      "Haha, I'm going to upgrade for sure but the lesson learned from that horrible mess was that the same issue could happen even with a 144hz monitor because if let's say CSGO runs at 277hz, but my monitor is 144hz, I still get the same problem.",
      "3070 is a tier above the A770. They're trying to compete with the 3060.",
      "144 is a huge upgrade over 60 and worth it. You can use Fast sync (nvidia) or Enhanced Sync (AMD) to compensate for frames over 144. Not sure if ARC will have that same technology- I'd assume it would. Guess we all see in about 30 minutes.",
      "Never heard of vsync?",
      "Hope so. Problem is I'm going to buy a new PC soon and I'm wondering if I should go for the ARC 770 or Nvidia 3070/80.\n\nThat could be a difference maker for me because I've got a 60fps monitor and the only way I managed to fix the lag in my games was to use that fps cap from Nvidia. It was unplayable otherwise.",
      "it\\s turned on.. still lag in games",
      "You should try fixing that issue first... That's not normal. Capping your fps is just as applying a bandaid on your issue. \n\nAlso, considering an arc 770 vs an rtx 3070+ is a bad idea and not even up to discussion, no one should be buying an arc 770 currently, performance is all over the place. Go for the established brand and card.\n\nAdditionally, 60hz in 2022 is really outdated and going for such a good card is a waste of money unless you're on 4k.",
      "Did you see the reviews for the ARC770? it's not close to the 3070/3080 in any way. Plus it has a lot of issues and it's not a finished product.\n\nWatch the [GN review](https://www.youtube.com/watch?v=nEvdrbxTtVo) if you haven't.",
      "The problem is that my work requires a riduculous amount of monitors connected, and in high resolution.\n\nI've done a lot of research on this and it seems like it's crucial to get a card that has at a minimum 16gb of VRAM. \n\nThat's why the 770 is interesting to me, so I don't have to spend too much on the 24gb 3090 card or the 4080 16gb version..\n\nIt's really annoying but that's the predicament I'm in. I can wait until December so hopefully there will be enough improvements in driver support that it seems stable enough. If not I hope to God GPU prices have crashed by then.",
      "Seems pretty bad yea. \n\nI'm just going the market crashes next 2 months so I've got a chance at a good deal on the 3090 maybe",
      "Are you working on an investment bank or something? :D\n\nIn any case that really is a very specific case so disregard what I said before. I read games so I thought this was only for gaming.",
      "I just hope the Arc 770 performs well on software that isn't gaming, or if the optimization problems will be the same in that area as well. If the Arc can't even do render software correctly, there's no way I'm getting it.\n\nThanks for the heads up anyways.",
      "Honestly don't buy a new PC now . Wait for 4000er release . Huge price drops will come . \n\nBut Arc has its appeal . First of all you have something unique and help a healthy competition . But i would check prices first and if the drivers are stable . I know i won't get an arc because I have a slightly weaker gpu , the upgrade wouldn't be worth it and i love retro gaming and it's clear that this is a major issue still. If you don't care about older games then go for it .",
      "You're going to but a $300-$500 video card and can't get a better monitor? You are always better to cap framerate inside the game first before you do it at the driver level. I'd get the ARC and use the rest of your money on a 144hz monitor."
    ]
  },
  {
    "brand": "intel",
    "generation": "alchemist",
    "tier": "770",
    "matched_keywords": [
      "a770"
    ],
    "title": "What is the current state of ARC GPU's and multiple monitors?",
    "selftext": "I have a weird setup where my desktop feeds multiple monitors across the house using Fiber HDMI/DP. Currently I utilize the 12700k iGPU + a 6600x, but I wonder if I would have better system stability if I replaced the 6600x with an a770 so I could have one driver for both. I know that there were some issues with ARC and multiple monitors upon release and just wasn't sure if that was still the case.\n\nI use this PC for different multimedia tasks like music and photo editing as well as entertainment, I don't use it for gaming.",
    "comments": [
      "No one is going to have exactly the same display combo, just buy it and return it if doesn't work the way you want.\n\nI returned my A380 when it doesn't do HDR tonemapping in Plex, but AMD GPU does even it wasn't in the official supported GPUs.",
      "Keep your AMD GPU.",
      "You shouldn't be having stability issues with both drivers enabled. I've had the opposite setup - Ryzen 7700x + RDNA2 iGPU + ARC a770 and didn't have any issues. \n\nI've also used Intel i9-13900K + Xe iGPU + Arc A770 and it works well with the latest drivers  - although it was a little rocky until recently because of some odd conflicts that occured in previous drivers.",
      "Never had any issues since Nov '22 driving 2 Dell 27\" 4k montiors over DP with an A770.",
      "I have no issues using multiple monitors on my a770. You have a weird setup so I'm gonna go out on a limb and suggest you won't get a solid answer without trying it for yourself. I honestly never really had issues related to dual monitors and I bought mine on launch day.\n\n&#x200B;\n\nThe only issue I have is it still does not wake up my monitor/s coming out of sleep.",
      "I'm only wanting to find out if ARC has issues with multiple monitors and if a single GPU driver is more stable than 2.",
      "Gotcha, no advantage to having single vs multiple gpu drivers?",
      "No one knows, because they don't have big enough install base.",
      "Gotcha",
      "I’m pretty sure it’s fixed now"
    ]
  }
]